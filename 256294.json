{"path":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    state=State.CREATED;\n    this.regenerator = regenerator;\n    name = (String)args.get(\"name\");\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    autowarm = new AutoWarmCountRef((String)args.get(\"autowarmCount\"));\n    description = \"LRU Cache(maxSize=\" + limit + \", initialSize=\" + initialSize;\n    if (autowarm.isAutoWarmingOn()) {\n      description += \", autowarmCount=\" + autowarm + \", regenerator=\" + regenerator;\n    }\n    description += ')';\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    state=State.CREATED;\n    this.regenerator = regenerator;\n    name = (String)args.get(\"name\");\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    autowarm = new AutoWarmCountRef((String)args.get(\"autowarmCount\"));\n    description = \"LRU Cache(maxSize=\" + limit + \", initialSize=\" + initialSize;\n    if (autowarm.isAutoWarmingOn()) {\n      description += \", autowarmCount=\" + autowarm + \", regenerator=\" + regenerator;\n    }\n    description += ')';\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    state=State.CREATED;\n    this.regenerator = regenerator;\n    name = (String)args.get(\"name\");\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    autowarm = new AutoWarmCountRef((String)args.get(\"autowarmCount\"));\n    description = \"LRU Cache(maxSize=\" + limit + \", initialSize=\" + initialSize;\n    if (autowarm.isAutoWarmingOn()) {\n      description += \", autowarmCount=\" + autowarm + \", regenerator=\" + regenerator;\n    }\n    description += ')';\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    state=State.CREATED;\n    this.regenerator = regenerator;\n    name = (String)args.get(\"name\");\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    autowarm = new AutoWarmCountRef((String)args.get(\"autowarmCount\"));\n    description = \"LRU Cache(maxSize=\" + limit + \", initialSize=\" + initialSize;\n    if (autowarm.isAutoWarmingOn()) {\n      description += \", autowarmCount=\" + autowarm + \", regenerator=\" + regenerator;\n    }\n    description += ')';\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    state=State.CREATED;\n    this.regenerator = regenerator;\n    name = (String)args.get(\"name\");\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    autowarm = new AutoWarmCountRef((String)args.get(\"autowarmCount\"));\n    description = \"LRU Cache(maxSize=\" + limit + \", initialSize=\" + initialSize;\n    if (autowarm.isAutoWarmingOn()) {\n      description += \", autowarmCount=\" + autowarm + \", regenerator=\" + regenerator;\n    }\n    description += ')';\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    state=State.CREATED;\n    this.regenerator = regenerator;\n    name = (String)args.get(\"name\");\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    autowarm = new AutoWarmCountRef((String)args.get(\"autowarmCount\"));\n    description = \"LRU Cache(maxSize=\" + limit + \", initialSize=\" + initialSize;\n    if (autowarm.isAutoWarmingOn()) {\n      description += \", autowarmCount=\" + autowarm + \", regenerator=\" + regenerator;\n    }\n    description += ')';\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"63131741120598595ba46620adaf3fad049ca291","date":1335567423,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    state=State.CREATED;\n    this.regenerator = regenerator;\n    name = (String)args.get(\"name\");\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    autowarm = new AutoWarmCountRef((String)args.get(\"autowarmCount\"));\n    description = \"LRU Cache(maxSize=\" + limit + \", initialSize=\" + initialSize;\n    if (autowarm.isAutoWarmingOn()) {\n      description += \", autowarmCount=\" + autowarm + \", regenerator=\" + regenerator;\n    }\n    description += ')';\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7530de27b87b961b51f01bd1299b7004d46e8823","date":1355236261,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9d496ec5dd42f7b76312f7ba5ac6666f1ed0730d","date":1428726211,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    str = (String) args.get(\"maxRamMB\");\n    final long maxRamBytes = this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit || ramBytesUsed > maxRamBytes) {\n            if (maxRamBytes != Long.MAX_VALUE && ramBytesUsed > maxRamBytes) {\n              long bytesToDecrement = 0;\n\n              Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n              do {\n                Map.Entry<K, V> entry = iterator.next();\n                if (entry.getKey() != null) {\n                  if (entry.getKey() instanceof Accountable) {\n                    bytesToDecrement += ((Accountable) entry.getKey()).ramBytesUsed();\n                  } else  {\n                    bytesToDecrement += DEFAULT_RAM_BYTES_USED;\n                  }\n                }\n                if (entry.getValue() != null) {\n                  bytesToDecrement += ((Accountable) entry.getValue()).ramBytesUsed();\n                }\n                bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n                ramBytesUsed -= bytesToDecrement;\n                iterator.remove();\n                evictions++;\n                evictionsRamUsage++;\n                stats.evictions.incrementAndGet();\n                stats.evictionsRamUsage.incrementAndGet();\n              } while (iterator.hasNext() && ramBytesUsed > maxRamBytes);\n              // must return false according to javadocs of removeEldestEntry if we're modifying\n              // the map ourselves\n              return false;\n            } else  {\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              evictions++;\n              stats.evictions.incrementAndGet();\n              return true;\n            }\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit) {\n            // increment evictions regardless of state.\n            // this doesn't need to be synchronized because it will\n            // only be called in the context of a higher level synchronized block.\n            evictions++;\n            stats.evictions.incrementAndGet();\n            return true;\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bc0fbfa191179ae7a0081ee1cf7da0464bcd8078","date":1469530061,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    str = (String) args.get(\"maxRamMB\");\n    final long maxRamBytes = this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit || ramBytesUsed > maxRamBytes) {\n            if (maxRamBytes != Long.MAX_VALUE && ramBytesUsed > maxRamBytes) {\n              long bytesToDecrement = 0;\n\n              Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n              do {\n                Map.Entry<K, V> entry = iterator.next();\n                if (entry.getKey() != null) {\n                  if (entry.getKey() instanceof Accountable) {\n                    bytesToDecrement += ((Accountable) entry.getKey()).ramBytesUsed();\n                  } else  {\n                    bytesToDecrement += DEFAULT_RAM_BYTES_USED;\n                  }\n                }\n                if (entry.getValue() != null) {\n                  bytesToDecrement += ((Accountable) entry.getValue()).ramBytesUsed();\n                }\n                bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n                ramBytesUsed -= bytesToDecrement;\n                iterator.remove();\n                evictions++;\n                evictionsRamUsage++;\n                stats.evictions.increment();\n                stats.evictionsRamUsage.increment();\n              } while (iterator.hasNext() && ramBytesUsed > maxRamBytes);\n              // must return false according to javadocs of removeEldestEntry if we're modifying\n              // the map ourselves\n              return false;\n            } else  {\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              evictions++;\n              stats.evictions.increment();\n              return true;\n            }\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    str = (String) args.get(\"maxRamMB\");\n    final long maxRamBytes = this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit || ramBytesUsed > maxRamBytes) {\n            if (maxRamBytes != Long.MAX_VALUE && ramBytesUsed > maxRamBytes) {\n              long bytesToDecrement = 0;\n\n              Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n              do {\n                Map.Entry<K, V> entry = iterator.next();\n                if (entry.getKey() != null) {\n                  if (entry.getKey() instanceof Accountable) {\n                    bytesToDecrement += ((Accountable) entry.getKey()).ramBytesUsed();\n                  } else  {\n                    bytesToDecrement += DEFAULT_RAM_BYTES_USED;\n                  }\n                }\n                if (entry.getValue() != null) {\n                  bytesToDecrement += ((Accountable) entry.getValue()).ramBytesUsed();\n                }\n                bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n                ramBytesUsed -= bytesToDecrement;\n                iterator.remove();\n                evictions++;\n                evictionsRamUsage++;\n                stats.evictions.incrementAndGet();\n                stats.evictionsRamUsage.incrementAndGet();\n              } while (iterator.hasNext() && ramBytesUsed > maxRamBytes);\n              // must return false according to javadocs of removeEldestEntry if we're modifying\n              // the map ourselves\n              return false;\n            } else  {\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              evictions++;\n              stats.evictions.incrementAndGet();\n              return true;\n            }\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b013574eedcdbac35dc7e35b0ee616ffc38895d","date":1470897818,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    str = (String) args.get(\"maxRamMB\");\n    final long maxRamBytes = this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit || ramBytesUsed > maxRamBytes) {\n            if (maxRamBytes != Long.MAX_VALUE && ramBytesUsed > maxRamBytes) {\n              long bytesToDecrement = 0;\n\n              Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n              do {\n                Map.Entry<K, V> entry = iterator.next();\n                if (entry.getKey() != null) {\n                  if (entry.getKey() instanceof Accountable) {\n                    bytesToDecrement += ((Accountable) entry.getKey()).ramBytesUsed();\n                  } else  {\n                    bytesToDecrement += DEFAULT_RAM_BYTES_USED;\n                  }\n                }\n                if (entry.getValue() != null) {\n                  bytesToDecrement += ((Accountable) entry.getValue()).ramBytesUsed();\n                }\n                bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n                ramBytesUsed -= bytesToDecrement;\n                iterator.remove();\n                evictions++;\n                evictionsRamUsage++;\n                stats.evictions.increment();\n                stats.evictionsRamUsage.increment();\n              } while (iterator.hasNext() && ramBytesUsed > maxRamBytes);\n              // must return false according to javadocs of removeEldestEntry if we're modifying\n              // the map ourselves\n              return false;\n            } else  {\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              evictions++;\n              stats.evictions.increment();\n              return true;\n            }\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    str = (String) args.get(\"maxRamMB\");\n    final long maxRamBytes = this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit || ramBytesUsed > maxRamBytes) {\n            if (maxRamBytes != Long.MAX_VALUE && ramBytesUsed > maxRamBytes) {\n              long bytesToDecrement = 0;\n\n              Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n              do {\n                Map.Entry<K, V> entry = iterator.next();\n                if (entry.getKey() != null) {\n                  if (entry.getKey() instanceof Accountable) {\n                    bytesToDecrement += ((Accountable) entry.getKey()).ramBytesUsed();\n                  } else  {\n                    bytesToDecrement += DEFAULT_RAM_BYTES_USED;\n                  }\n                }\n                if (entry.getValue() != null) {\n                  bytesToDecrement += ((Accountable) entry.getValue()).ramBytesUsed();\n                }\n                bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n                ramBytesUsed -= bytesToDecrement;\n                iterator.remove();\n                evictions++;\n                evictionsRamUsage++;\n                stats.evictions.incrementAndGet();\n                stats.evictionsRamUsage.incrementAndGet();\n              } while (iterator.hasNext() && ramBytesUsed > maxRamBytes);\n              // must return false according to javadocs of removeEldestEntry if we're modifying\n              // the map ourselves\n              return false;\n            } else  {\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              evictions++;\n              stats.evictions.incrementAndGet();\n              return true;\n            }\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    str = (String) args.get(\"maxRamMB\");\n    final long maxRamBytes = this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit || ramBytesUsed > maxRamBytes) {\n            if (maxRamBytes != Long.MAX_VALUE && ramBytesUsed > maxRamBytes) {\n              long bytesToDecrement = 0;\n\n              Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n              do {\n                Map.Entry<K, V> entry = iterator.next();\n                if (entry.getKey() != null) {\n                  if (entry.getKey() instanceof Accountable) {\n                    bytesToDecrement += ((Accountable) entry.getKey()).ramBytesUsed();\n                  } else  {\n                    bytesToDecrement += DEFAULT_RAM_BYTES_USED;\n                  }\n                }\n                if (entry.getValue() != null) {\n                  bytesToDecrement += ((Accountable) entry.getValue()).ramBytesUsed();\n                }\n                bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n                ramBytesUsed -= bytesToDecrement;\n                iterator.remove();\n                evictions++;\n                evictionsRamUsage++;\n                stats.evictions.increment();\n                stats.evictionsRamUsage.increment();\n              } while (iterator.hasNext() && ramBytesUsed > maxRamBytes);\n              // must return false according to javadocs of removeEldestEntry if we're modifying\n              // the map ourselves\n              return false;\n            } else  {\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              evictions++;\n              stats.evictions.increment();\n              return true;\n            }\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    str = (String) args.get(\"maxRamMB\");\n    final long maxRamBytes = this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit || ramBytesUsed > maxRamBytes) {\n            if (maxRamBytes != Long.MAX_VALUE && ramBytesUsed > maxRamBytes) {\n              long bytesToDecrement = 0;\n\n              Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n              do {\n                Map.Entry<K, V> entry = iterator.next();\n                if (entry.getKey() != null) {\n                  if (entry.getKey() instanceof Accountable) {\n                    bytesToDecrement += ((Accountable) entry.getKey()).ramBytesUsed();\n                  } else  {\n                    bytesToDecrement += DEFAULT_RAM_BYTES_USED;\n                  }\n                }\n                if (entry.getValue() != null) {\n                  bytesToDecrement += ((Accountable) entry.getValue()).ramBytesUsed();\n                }\n                bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n                ramBytesUsed -= bytesToDecrement;\n                iterator.remove();\n                evictions++;\n                evictionsRamUsage++;\n                stats.evictions.incrementAndGet();\n                stats.evictionsRamUsage.incrementAndGet();\n              } while (iterator.hasNext() && ramBytesUsed > maxRamBytes);\n              // must return false according to javadocs of removeEldestEntry if we're modifying\n              // the map ourselves\n              return false;\n            } else  {\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              evictions++;\n              stats.evictions.incrementAndGet();\n              return true;\n            }\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a71ca10e7131e1f01868c80d228f26a855e79dd0","date":1562166223,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    str = (String) args.get(\"maxRamMB\");\n    final long maxRamBytes = this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit || ramBytesUsed > maxRamBytes) {\n            if (maxRamBytes != Long.MAX_VALUE && ramBytesUsed > maxRamBytes) {\n              long bytesToDecrement = 0;\n\n              Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n              do {\n                Map.Entry<K, V> entry = iterator.next();\n                if (entry.getKey() != null) {\n                  bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n                }\n                if (entry.getValue() != null) {\n                  bytesToDecrement += ((Accountable) entry.getValue()).ramBytesUsed();\n                }\n                bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n                ramBytesUsed -= bytesToDecrement;\n                iterator.remove();\n                evictions++;\n                evictionsRamUsage++;\n                stats.evictions.increment();\n                stats.evictionsRamUsage.increment();\n              } while (iterator.hasNext() && ramBytesUsed > maxRamBytes);\n              // must return false according to javadocs of removeEldestEntry if we're modifying\n              // the map ourselves\n              return false;\n            } else  {\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              evictions++;\n              stats.evictions.increment();\n              return true;\n            }\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    str = (String) args.get(\"maxRamMB\");\n    final long maxRamBytes = this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit || ramBytesUsed > maxRamBytes) {\n            if (maxRamBytes != Long.MAX_VALUE && ramBytesUsed > maxRamBytes) {\n              long bytesToDecrement = 0;\n\n              Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n              do {\n                Map.Entry<K, V> entry = iterator.next();\n                if (entry.getKey() != null) {\n                  if (entry.getKey() instanceof Accountable) {\n                    bytesToDecrement += ((Accountable) entry.getKey()).ramBytesUsed();\n                  } else  {\n                    bytesToDecrement += DEFAULT_RAM_BYTES_USED;\n                  }\n                }\n                if (entry.getValue() != null) {\n                  bytesToDecrement += ((Accountable) entry.getValue()).ramBytesUsed();\n                }\n                bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n                ramBytesUsed -= bytesToDecrement;\n                iterator.remove();\n                evictions++;\n                evictionsRamUsage++;\n                stats.evictions.increment();\n                stats.evictionsRamUsage.increment();\n              } while (iterator.hasNext() && ramBytesUsed > maxRamBytes);\n              // must return false according to javadocs of removeEldestEntry if we're modifying\n              // the map ourselves\n              return false;\n            } else  {\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              evictions++;\n              stats.evictions.increment();\n              return true;\n            }\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ce13e934d6cfdcc82d51e85de460cf9790e97566","date":1563877281,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(SIZE_PARAM);\n    this.sizeLimit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), sizeLimit);\n    str = (String) args.get(MAX_RAM_MB_PARAM);\n    this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription();\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (ramBytesUsed > getMaxRamBytes()) {\n            Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n            do {\n              Map.Entry<K, V> entry = iterator.next();\n              long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n              ramBytesUsed -= bytesToDecrement;\n              iterator.remove();\n              evictions++;\n              evictionsRamUsage++;\n              stats.evictions.increment();\n              stats.evictionsRamUsage.increment();\n            } while (iterator.hasNext() && ramBytesUsed > getMaxRamBytes());\n            // must return false according to javadocs of removeEldestEntry if we're modifying\n            // the map ourselves\n            return false;\n          } else if (size() > getSizeLimit()) {\n            Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n            do {\n              Map.Entry<K, V> entry = iterator.next();\n              long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n              ramBytesUsed -= bytesToDecrement;\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              iterator.remove();\n              evictions++;\n              stats.evictions.increment();\n            } while (iterator.hasNext() && size() > getSizeLimit());\n            // must return false according to javadocs of removeEldestEntry if we're modifying\n            // the map ourselves\n            return false;\n          }\n          // neither size nor RAM exceeded - ok to keep the entry\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(\"size\");\n    final int limit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    final int initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), limit);\n    str = (String) args.get(\"maxRamMB\");\n    final long maxRamBytes = this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription(limit, initialSize);\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (size() > limit || ramBytesUsed > maxRamBytes) {\n            if (maxRamBytes != Long.MAX_VALUE && ramBytesUsed > maxRamBytes) {\n              long bytesToDecrement = 0;\n\n              Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n              do {\n                Map.Entry<K, V> entry = iterator.next();\n                if (entry.getKey() != null) {\n                  bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n                }\n                if (entry.getValue() != null) {\n                  bytesToDecrement += ((Accountable) entry.getValue()).ramBytesUsed();\n                }\n                bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n                ramBytesUsed -= bytesToDecrement;\n                iterator.remove();\n                evictions++;\n                evictionsRamUsage++;\n                stats.evictions.increment();\n                stats.evictionsRamUsage.increment();\n              } while (iterator.hasNext() && ramBytesUsed > maxRamBytes);\n              // must return false according to javadocs of removeEldestEntry if we're modifying\n              // the map ourselves\n              return false;\n            } else  {\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              evictions++;\n              stats.evictions.increment();\n              return true;\n            }\n          }\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e33a2e75ecee8b06fba2bd570c0fb9273962bc7","date":1565791119,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(SIZE_PARAM);\n    this.maxSize = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), maxSize);\n    str = (String) args.get(MAX_RAM_MB_PARAM);\n    this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription();\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (ramBytesUsed > getMaxRamBytes()) {\n            Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n            do {\n              Map.Entry<K, V> entry = iterator.next();\n              long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n              ramBytesUsed -= bytesToDecrement;\n              iterator.remove();\n              evictions++;\n              evictionsRamUsage++;\n              stats.evictions.increment();\n              stats.evictionsRamUsage.increment();\n            } while (iterator.hasNext() && ramBytesUsed > getMaxRamBytes());\n            // must return false according to javadocs of removeEldestEntry if we're modifying\n            // the map ourselves\n            return false;\n          } else if (size() > getMaxSize()) {\n            Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n            do {\n              Map.Entry<K, V> entry = iterator.next();\n              long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n              ramBytesUsed -= bytesToDecrement;\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              iterator.remove();\n              evictions++;\n              stats.evictions.increment();\n            } while (iterator.hasNext() && size() > getMaxSize());\n            // must return false according to javadocs of removeEldestEntry if we're modifying\n            // the map ourselves\n            return false;\n          }\n          // neither size nor RAM exceeded - ok to keep the entry\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(SIZE_PARAM);\n    this.sizeLimit = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), sizeLimit);\n    str = (String) args.get(MAX_RAM_MB_PARAM);\n    this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription();\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (ramBytesUsed > getMaxRamBytes()) {\n            Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n            do {\n              Map.Entry<K, V> entry = iterator.next();\n              long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n              ramBytesUsed -= bytesToDecrement;\n              iterator.remove();\n              evictions++;\n              evictionsRamUsage++;\n              stats.evictions.increment();\n              stats.evictionsRamUsage.increment();\n            } while (iterator.hasNext() && ramBytesUsed > getMaxRamBytes());\n            // must return false according to javadocs of removeEldestEntry if we're modifying\n            // the map ourselves\n            return false;\n          } else if (size() > getSizeLimit()) {\n            Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n            do {\n              Map.Entry<K, V> entry = iterator.next();\n              long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n              ramBytesUsed -= bytesToDecrement;\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              iterator.remove();\n              evictions++;\n              stats.evictions.increment();\n            } while (iterator.hasNext() && size() > getSizeLimit());\n            // must return false according to javadocs of removeEldestEntry if we're modifying\n            // the map ourselves\n            return false;\n          }\n          // neither size nor RAM exceeded - ok to keep the entry\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fbd58791ecf2b92d8917c2f4aab0e50965ec6a83","date":1568645407,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","pathOld":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(SIZE_PARAM);\n    this.maxSize = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), maxSize);\n    str = (String) args.get(MAX_RAM_MB_PARAM);\n    this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    str = (String) args.get(MAX_IDLE_TIME_PARAM);\n    if (str == null) {\n      maxIdleTimeNs = Long.MAX_VALUE;\n    } else {\n      int maxIdleTime = Integer.parseInt(str);\n      if (maxIdleTime > 0) {\n        maxIdleTimeNs = TimeUnit.NANOSECONDS.convert(Integer.parseInt(str), TimeUnit.SECONDS);\n      } else {\n        maxIdleTimeNs = Long.MAX_VALUE;\n      }\n    }\n    description = generateDescription();\n\n    map = new LinkedHashMap<K, CacheValue<V>>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          // remove items older than maxIdleTimeNs\n          if (maxIdleTimeNs != Long.MAX_VALUE) {\n            long idleCutoff = timeSource.getEpochTimeNs() - maxIdleTimeNs;\n            if (oldestEntry < idleCutoff) {\n              long currentOldestEntry = Long.MAX_VALUE;\n              Iterator<Map.Entry<K, CacheValue<V>>> iterator = entrySet().iterator();\n              while (iterator.hasNext()) {\n                Map.Entry<K, CacheValue<V>> entry = iterator.next();\n                if (entry.getValue().createTime < idleCutoff) {\n                  long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n                  bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n                  bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n                  ramBytesUsed -= bytesToDecrement;\n                  iterator.remove();\n                  evictions++;\n                  evictionsIdleTime++;\n                  stats.evictionsIdleTime.increment();\n                  stats.evictions.increment();\n                } else {\n                  if (syntheticEntries) {\n                    // no guarantee on the actual create time - make a full sweep\n                    if (currentOldestEntry > entry.getValue().createTime) {\n                      currentOldestEntry = entry.getValue().createTime;\n                    }\n                  } else {\n                    // iterator is sorted by insertion order (and time)\n                    // so we can quickly terminate the sweep\n                    currentOldestEntry = entry.getValue().createTime;\n                    break;\n                  }\n                }\n              }\n              if (currentOldestEntry != Long.MAX_VALUE) {\n                oldestEntry = currentOldestEntry;\n              }\n            }\n          }\n          if (ramBytesUsed > getMaxRamBytes()) {\n            Iterator<Map.Entry<K, CacheValue<V>>> iterator = entrySet().iterator();\n            do {\n              Map.Entry<K, CacheValue<V>> entry = iterator.next();\n              long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n              ramBytesUsed -= bytesToDecrement;\n              iterator.remove();\n              evictions++;\n              evictionsRamUsage++;\n              stats.evictions.increment();\n              stats.evictionsRamUsage.increment();\n            } while (iterator.hasNext() && ramBytesUsed > getMaxRamBytes());\n          } else if (size() > getMaxSize()) {\n            Iterator<Map.Entry<K, CacheValue<V>>> iterator = entrySet().iterator();\n            do {\n              Map.Entry<K, CacheValue<V>> entry = iterator.next();\n              long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n              ramBytesUsed -= bytesToDecrement;\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              iterator.remove();\n              evictions++;\n              stats.evictions.increment();\n            } while (iterator.hasNext() && size() > getMaxSize());\n          }\n          // must return false according to javadocs of removeEldestEntry if we're modifying\n          // the map ourselves\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","sourceOld":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(SIZE_PARAM);\n    this.maxSize = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), maxSize);\n    str = (String) args.get(MAX_RAM_MB_PARAM);\n    this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    description = generateDescription();\n\n    map = new LinkedHashMap<K,V>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          if (ramBytesUsed > getMaxRamBytes()) {\n            Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n            do {\n              Map.Entry<K, V> entry = iterator.next();\n              long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n              ramBytesUsed -= bytesToDecrement;\n              iterator.remove();\n              evictions++;\n              evictionsRamUsage++;\n              stats.evictions.increment();\n              stats.evictionsRamUsage.increment();\n            } while (iterator.hasNext() && ramBytesUsed > getMaxRamBytes());\n            // must return false according to javadocs of removeEldestEntry if we're modifying\n            // the map ourselves\n            return false;\n          } else if (size() > getMaxSize()) {\n            Iterator<Map.Entry<K, V>> iterator = entrySet().iterator();\n            do {\n              Map.Entry<K, V> entry = iterator.next();\n              long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n              ramBytesUsed -= bytesToDecrement;\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              iterator.remove();\n              evictions++;\n              stats.evictions.increment();\n            } while (iterator.hasNext() && size() > getMaxSize());\n            // must return false according to javadocs of removeEldestEntry if we're modifying\n            // the map ourselves\n            return false;\n          }\n          // neither size nor RAM exceeded - ok to keep the entry\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d7d3943904804560937e6239effeebda0f920e4","date":1573762904,"type":4,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/search/LRUCache#init(Map,Object,CacheRegenerator).mjava","sourceNew":null,"sourceOld":"  @Override\n  public Object init(Map args, Object persistence, CacheRegenerator regenerator) {\n    super.init(args, regenerator);\n    String str = (String)args.get(SIZE_PARAM);\n    this.maxSize = str==null ? 1024 : Integer.parseInt(str);\n    str = (String)args.get(\"initialSize\");\n    initialSize = Math.min(str==null ? 1024 : Integer.parseInt(str), maxSize);\n    str = (String) args.get(MAX_RAM_MB_PARAM);\n    this.maxRamBytes = str == null ? Long.MAX_VALUE : (long) (Double.parseDouble(str) * 1024L * 1024L);\n    str = (String) args.get(MAX_IDLE_TIME_PARAM);\n    if (str == null) {\n      maxIdleTimeNs = Long.MAX_VALUE;\n    } else {\n      int maxIdleTime = Integer.parseInt(str);\n      if (maxIdleTime > 0) {\n        maxIdleTimeNs = TimeUnit.NANOSECONDS.convert(Integer.parseInt(str), TimeUnit.SECONDS);\n      } else {\n        maxIdleTimeNs = Long.MAX_VALUE;\n      }\n    }\n    description = generateDescription();\n\n    map = new LinkedHashMap<K, CacheValue<V>>(initialSize, 0.75f, true) {\n        @Override\n        protected boolean removeEldestEntry(Map.Entry eldest) {\n          // remove items older than maxIdleTimeNs\n          if (maxIdleTimeNs != Long.MAX_VALUE) {\n            long idleCutoff = timeSource.getEpochTimeNs() - maxIdleTimeNs;\n            if (oldestEntry < idleCutoff) {\n              long currentOldestEntry = Long.MAX_VALUE;\n              Iterator<Map.Entry<K, CacheValue<V>>> iterator = entrySet().iterator();\n              while (iterator.hasNext()) {\n                Map.Entry<K, CacheValue<V>> entry = iterator.next();\n                if (entry.getValue().createTime < idleCutoff) {\n                  long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n                  bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n                  bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n                  ramBytesUsed -= bytesToDecrement;\n                  iterator.remove();\n                  evictions++;\n                  evictionsIdleTime++;\n                  stats.evictionsIdleTime.increment();\n                  stats.evictions.increment();\n                } else {\n                  if (syntheticEntries) {\n                    // no guarantee on the actual create time - make a full sweep\n                    if (currentOldestEntry > entry.getValue().createTime) {\n                      currentOldestEntry = entry.getValue().createTime;\n                    }\n                  } else {\n                    // iterator is sorted by insertion order (and time)\n                    // so we can quickly terminate the sweep\n                    currentOldestEntry = entry.getValue().createTime;\n                    break;\n                  }\n                }\n              }\n              if (currentOldestEntry != Long.MAX_VALUE) {\n                oldestEntry = currentOldestEntry;\n              }\n            }\n          }\n          if (ramBytesUsed > getMaxRamBytes()) {\n            Iterator<Map.Entry<K, CacheValue<V>>> iterator = entrySet().iterator();\n            do {\n              Map.Entry<K, CacheValue<V>> entry = iterator.next();\n              long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n              ramBytesUsed -= bytesToDecrement;\n              iterator.remove();\n              evictions++;\n              evictionsRamUsage++;\n              stats.evictions.increment();\n              stats.evictionsRamUsage.increment();\n            } while (iterator.hasNext() && ramBytesUsed > getMaxRamBytes());\n          } else if (size() > getMaxSize()) {\n            Iterator<Map.Entry<K, CacheValue<V>>> iterator = entrySet().iterator();\n            do {\n              Map.Entry<K, CacheValue<V>> entry = iterator.next();\n              long bytesToDecrement = RamUsageEstimator.sizeOfObject(entry.getKey(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += RamUsageEstimator.sizeOfObject(entry.getValue(), QUERY_DEFAULT_RAM_BYTES_USED);\n              bytesToDecrement += LINKED_HASHTABLE_RAM_BYTES_PER_ENTRY;\n              ramBytesUsed -= bytesToDecrement;\n              // increment evictions regardless of state.\n              // this doesn't need to be synchronized because it will\n              // only be called in the context of a higher level synchronized block.\n              iterator.remove();\n              evictions++;\n              stats.evictions.increment();\n            } while (iterator.hasNext() && size() > getMaxSize());\n          }\n          // must return false according to javadocs of removeEldestEntry if we're modifying\n          // the map ourselves\n          return false;\n        }\n      };\n\n    if (persistence==null) {\n      // must be the first time a cache of this type is being created\n      persistence = new CumulativeStats();\n    }\n\n    stats = (CumulativeStats)persistence;\n\n    return persistence;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["63131741120598595ba46620adaf3fad049ca291","7530de27b87b961b51f01bd1299b7004d46e8823"],"fbd58791ecf2b92d8917c2f4aab0e50965ec6a83":["0e33a2e75ecee8b06fba2bd570c0fb9273962bc7"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"4d7d3943904804560937e6239effeebda0f920e4":["fbd58791ecf2b92d8917c2f4aab0e50965ec6a83"],"a71ca10e7131e1f01868c80d228f26a855e79dd0":["bc0fbfa191179ae7a0081ee1cf7da0464bcd8078"],"bc0fbfa191179ae7a0081ee1cf7da0464bcd8078":["9d496ec5dd42f7b76312f7ba5ac6666f1ed0730d"],"63131741120598595ba46620adaf3fad049ca291":["c26f00b574427b55127e869b935845554afde1fa"],"ce13e934d6cfdcc82d51e85de460cf9790e97566":["a71ca10e7131e1f01868c80d228f26a855e79dd0"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["9d496ec5dd42f7b76312f7ba5ac6666f1ed0730d","bc0fbfa191179ae7a0081ee1cf7da0464bcd8078"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9d496ec5dd42f7b76312f7ba5ac6666f1ed0730d":["7530de27b87b961b51f01bd1299b7004d46e8823"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7530de27b87b961b51f01bd1299b7004d46e8823":["63131741120598595ba46620adaf3fad049ca291"],"0e33a2e75ecee8b06fba2bd570c0fb9273962bc7":["ce13e934d6cfdcc82d51e85de460cf9790e97566"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":["9d496ec5dd42f7b76312f7ba5ac6666f1ed0730d","bc0fbfa191179ae7a0081ee1cf7da0464bcd8078"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4d7d3943904804560937e6239effeebda0f920e4"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"fbd58791ecf2b92d8917c2f4aab0e50965ec6a83":["4d7d3943904804560937e6239effeebda0f920e4"],"c26f00b574427b55127e869b935845554afde1fa":["63131741120598595ba46620adaf3fad049ca291"],"4d7d3943904804560937e6239effeebda0f920e4":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a71ca10e7131e1f01868c80d228f26a855e79dd0":["ce13e934d6cfdcc82d51e85de460cf9790e97566"],"bc0fbfa191179ae7a0081ee1cf7da0464bcd8078":["a71ca10e7131e1f01868c80d228f26a855e79dd0","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"63131741120598595ba46620adaf3fad049ca291":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","7530de27b87b961b51f01bd1299b7004d46e8823"],"ce13e934d6cfdcc82d51e85de460cf9790e97566":["0e33a2e75ecee8b06fba2bd570c0fb9273962bc7"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"9d496ec5dd42f7b76312f7ba5ac6666f1ed0730d":["bc0fbfa191179ae7a0081ee1cf7da0464bcd8078","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"7530de27b87b961b51f01bd1299b7004d46e8823":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","9d496ec5dd42f7b76312f7ba5ac6666f1ed0730d"],"0e33a2e75ecee8b06fba2bd570c0fb9273962bc7":["fbd58791ecf2b92d8917c2f4aab0e50965ec6a83"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","a258fbb26824fd104ed795e5d9033d2d040049ee","3b013574eedcdbac35dc7e35b0ee616ffc38895d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}