{"path":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","commits":[{"id":"955c32f886db6f6356c9fcdea6b1f1cb4effda24","date":1270581567,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"/dev/null","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n\n    Directory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(new Field(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(new Field(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = w.getReader();\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = w.getReader();\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d572389229127c297dd1fa5ce4758e1cec41e799","date":1273610938,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n\n    Directory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(new Field(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(new Field(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = w.getReader();\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = w.getReader();\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n\n    Directory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(new Field(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(new Field(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = w.getReader();\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = w.getReader();\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Random random = newRandom();\n    Directory dir = newDirectory(random);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(new Field(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(new Field(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = w.getReader();\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = w.getReader();\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n\n    Directory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(new Field(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(new Field(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = w.getReader();\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = w.getReader();\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(new Field(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(new Field(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = w.getReader();\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = w.getReader();\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Random random = newRandom();\n    Directory dir = newDirectory(random);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(new Field(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(new Field(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = w.getReader();\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = w.getReader();\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["7740a3e0858e88aaf6b09efe52e35c04a0d717f7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = w.getReader();\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = w.getReader();\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(new Field(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(new Field(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = w.getReader();\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = w.getReader();\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"296e4ed69ccbda3c7b5fdb86c7acaa43c9074e01","date":1286712181,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = w.getReader();\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = w.getReader();\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7740a3e0858e88aaf6b09efe52e35c04a0d717f7","date":1286877329,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4ecea1664e8617d82eca3b8055a3c37cb4da8511","date":1287578668,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"747a37a5b5aed2d615e3b2545a8fc8ac5c60a1d2","date":1288191652,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ca0ffea399542e8aac8ed7608f34f8ec4cb8904d","date":1288424244,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a493e6d0c3ad86bd55c0a1360d110142e948f2bd","date":1289406991,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(provider));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(provider));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e8cc373c801e54cec75daf9f52792cb4b17f536","date":1291116159,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(provider));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).setCodecProvider(provider));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = 173;\n\n    Directory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir,\n                                    new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setCodecProvider(new MyCodecs()));\n\n    w.setMergeFactor(3);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(new Field(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    doc.add(new Field(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    \n    Field idField = new Field(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = w.getReader();\n    IndexReader[] subs = r.getSequentialSubReaders();\n    assertTrue(subs.length > 1);\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      //System.out.println(\"test i=\" + i);\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = w.getReader();\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b1add9ddc0005b07550d4350720aac22dc9886b3","date":1295549635,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e79a6d080bdd5b2a8f56342cf571b5476de04180","date":1295638686,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eb378f8bdee16a26810e086303a4a86b4930ea12","date":1296410797,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7d16aff6229cca84309d03d047cd718946bd4b43","date":1296516600,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"790e1fde4caa765b3faaad3fbcd25c6973450336","date":1296689245,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762","date":1297938719,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f1bdbf92da222965b46c0a942c3857ba56e5c638","date":1298297608,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new MyCodecs();\n    Codec pulsing = new PulsingReverseTermsCodec();\n    provider.register(pulsing);\n    \n    \n    final int NUM_DOCS = 173;\n    Directory dir = newDirectory();\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), pulsing.name);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), pulsing.name);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    // test each segment\n    for(int i=0;i<subs.length;i++) {\n      testTermsOrder(subs[i]);\n    }\n    // test each multi-reader\n    testTermsOrder(r);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = new IndexSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    testTermsOrder(r);\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, true, true)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0762b640e0d0d12b6edb96db68986e13145c3484","date":1307575932,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = 173;\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", Field.Store.NO, Field.Index.ANALYZED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", Field.Store.NO, Field.Index.ANALYZED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", Field.Store.NO, Field.Index.NOT_ANALYZED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    CodecProvider provider = new CoreCodecProvider();\n    provider.register(new RAMOnlyCodec());\n    provider.setDefaultFieldCodec(\"RamOnly\");\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setCodecProvider(provider).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    provider.setFieldCodec(field2.name(), \"Pulsing\");\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n    provider.setFieldCodec(idField.name(), \"Pulsing\");\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.forceMerge(1);\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.optimize();\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0e7c2454a6a8237bfd0e953f5b940838408c9055","date":1323649300,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.forceMerge(1);\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.forceMerge(1);\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.forceMerge(1);\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n    s.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.forceMerge(1);\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n    s.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ce667c6d3400b22523701c549c0d35e26da8b46","date":1324405053,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n    w.forceMerge(1);\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"64d12ff8f1998324df34446e9530414ad6e7e9c9","date":1327848598,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    IndexReader[] subs = r.getSequentialSubReaders();\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","pathOld":"lucene/src/test/org/apache/lucene/TestExternalCodecs#testPerFieldCodec().mjava","sourceNew":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  // tests storing \"id\" and \"field2\" fields as pulsing codec,\n  // whose term sort is backwards unicode code point, and\n  // storing \"field1\" as a custom entirely-in-RAM codec\n  public void testPerFieldCodec() throws Exception {\n    \n    final int NUM_DOCS = atLeast(173);\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_DOCS=\" + NUM_DOCS);\n    }\n\n    MockDirectoryWrapper dir = newDirectory();\n    dir.setCheckIndexOnClose(false); // we use a custom codec provider\n    IndexWriter w = new IndexWriter(\n        dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n        setCodec(new CustomPerFieldCodec()).\n            setMergePolicy(newLogMergePolicy(3))\n    );\n    Document doc = new Document();\n    // uses default codec:\n    doc.add(newField(\"field1\", \"this field uses the standard codec as the test\", TextField.TYPE_UNSTORED));\n    // uses pulsing codec:\n    Field field2 = newField(\"field2\", \"this field uses the pulsing codec as the test\", TextField.TYPE_UNSTORED);\n    doc.add(field2);\n    \n    Field idField = newField(\"id\", \"\", StringField.TYPE_UNSTORED);\n\n    doc.add(idField);\n    for(int i=0;i<NUM_DOCS;i++) {\n      idField.setValue(\"\"+i);\n      w.addDocument(doc);\n      if ((i+1)%10 == 0) {\n        w.commit();\n      }\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now delete id=77\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"77\"));\n\n    IndexReader r = IndexReader.open(w, true);\n    \n    assertEquals(NUM_DOCS-1, r.numDocs());\n    IndexSearcher s = newSearcher(r);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-1, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    r.close();\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now delete 2nd doc\");\n    }\n    w.deleteDocuments(new Term(\"id\", \"44\"));\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now force merge\");\n    }\n    w.forceMerge(1);\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now open reader\");\n    }\n    r = IndexReader.open(w, true);\n    assertEquals(NUM_DOCS-2, r.maxDoc());\n    assertEquals(NUM_DOCS-2, r.numDocs());\n    s = newSearcher(r);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field1\", \"standard\")), 1).totalHits);\n    assertEquals(NUM_DOCS-2, s.search(new TermQuery(new Term(\"field2\", \"pulsing\")), 1).totalHits);\n    assertEquals(1, s.search(new TermQuery(new Term(\"id\", \"76\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"77\")), 1).totalHits);\n    assertEquals(0, s.search(new TermQuery(new Term(\"id\", \"44\")), 1).totalHits);\n\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now close NRT reader\");\n    }\n    r.close();\n\n    w.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"747a37a5b5aed2d615e3b2545a8fc8ac5c60a1d2":["7740a3e0858e88aaf6b09efe52e35c04a0d717f7"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["29ef99d61cda9641b6250bf9567329a6e65f901d","4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762"],"0762b640e0d0d12b6edb96db68986e13145c3484":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd","0e7c2454a6a8237bfd0e953f5b940838408c9055"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["0762b640e0d0d12b6edb96db68986e13145c3484"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["747a37a5b5aed2d615e3b2545a8fc8ac5c60a1d2"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["06584e6e98d592b34e1329b384182f368d2025e8"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"b1add9ddc0005b07550d4350720aac22dc9886b3":["4e8cc373c801e54cec75daf9f52792cb4b17f536"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["d572389229127c297dd1fa5ce4758e1cec41e799","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"7d16aff6229cca84309d03d047cd718946bd4b43":["eb378f8bdee16a26810e086303a4a86b4930ea12"],"7740a3e0858e88aaf6b09efe52e35c04a0d717f7":["296e4ed69ccbda3c7b5fdb86c7acaa43c9074e01"],"296e4ed69ccbda3c7b5fdb86c7acaa43c9074e01":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["3bb13258feba31ab676502787ab2e1779f129b7a","790e1fde4caa765b3faaad3fbcd25c6973450336"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9ce667c6d3400b22523701c549c0d35e26da8b46":["0e7c2454a6a8237bfd0e953f5b940838408c9055"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"ca0ffea399542e8aac8ed7608f34f8ec4cb8904d":["4ecea1664e8617d82eca3b8055a3c37cb4da8511","747a37a5b5aed2d615e3b2545a8fc8ac5c60a1d2"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["e79a6d080bdd5b2a8f56342cf571b5476de04180","4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a3776dccca01c11e7046323cfad46a3b4a471233","0762b640e0d0d12b6edb96db68986e13145c3484"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["d572389229127c297dd1fa5ce4758e1cec41e799"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"85a883878c0af761245ab048babc63d099f835f3":["ca0ffea399542e8aac8ed7608f34f8ec4cb8904d","a493e6d0c3ad86bd55c0a1360d110142e948f2bd"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["f1bdbf92da222965b46c0a942c3857ba56e5c638","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"d572389229127c297dd1fa5ce4758e1cec41e799":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"962d04139994fce5193143ef35615499a9a96d78":["bde51b089eb7f86171eb3406e38a274743f9b7ac","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"eb378f8bdee16a26810e086303a4a86b4930ea12":["b1add9ddc0005b07550d4350720aac22dc9886b3"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["0762b640e0d0d12b6edb96db68986e13145c3484","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","b1add9ddc0005b07550d4350720aac22dc9886b3"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["9ce667c6d3400b22523701c549c0d35e26da8b46","64d12ff8f1998324df34446e9530414ad6e7e9c9"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["7d16aff6229cca84309d03d047cd718946bd4b43"],"7b91922b55d15444d554721b352861d028eb8278":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"64d12ff8f1998324df34446e9530414ad6e7e9c9":["9ce667c6d3400b22523701c549c0d35e26da8b46"],"a3776dccca01c11e7046323cfad46a3b4a471233":["4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["135621f3a0670a9394eb563224a3b76cc4dddc0f","0762b640e0d0d12b6edb96db68986e13145c3484"],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["296e4ed69ccbda3c7b5fdb86c7acaa43c9074e01","7740a3e0858e88aaf6b09efe52e35c04a0d717f7"],"3bb13258feba31ab676502787ab2e1779f129b7a":["85a883878c0af761245ab048babc63d099f835f3","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd"]},"commit2Childs":{"747a37a5b5aed2d615e3b2545a8fc8ac5c60a1d2":["a493e6d0c3ad86bd55c0a1360d110142e948f2bd","ca0ffea399542e8aac8ed7608f34f8ec4cb8904d"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"0762b640e0d0d12b6edb96db68986e13145c3484":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","d083e83f225b11e5fdd900e83d26ddb385b6955c","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":[],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["1509f151d7692d84fae414b2b799ac06ba60fcb4","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"a493e6d0c3ad86bd55c0a1360d110142e948f2bd":["85a883878c0af761245ab048babc63d099f835f3","4e8cc373c801e54cec75daf9f52792cb4b17f536"],"955c32f886db6f6356c9fcdea6b1f1cb4effda24":["d572389229127c297dd1fa5ce4758e1cec41e799"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","0e7c2454a6a8237bfd0e953f5b940838408c9055"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["296e4ed69ccbda3c7b5fdb86c7acaa43c9074e01"],"b1add9ddc0005b07550d4350720aac22dc9886b3":["eb378f8bdee16a26810e086303a4a86b4930ea12","e79a6d080bdd5b2a8f56342cf571b5476de04180"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["e79a6d080bdd5b2a8f56342cf571b5476de04180"],"7d16aff6229cca84309d03d047cd718946bd4b43":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"7740a3e0858e88aaf6b09efe52e35c04a0d717f7":["747a37a5b5aed2d615e3b2545a8fc8ac5c60a1d2","4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"296e4ed69ccbda3c7b5fdb86c7acaa43c9074e01":["7740a3e0858e88aaf6b09efe52e35c04a0d717f7","4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["0762b640e0d0d12b6edb96db68986e13145c3484","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","9ce667c6d3400b22523701c549c0d35e26da8b46"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["f1bdbf92da222965b46c0a942c3857ba56e5c638"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["955c32f886db6f6356c9fcdea6b1f1cb4effda24"],"9ce667c6d3400b22523701c549c0d35e26da8b46":["5cab9a86bd67202d20b6adc463008c8e982b070a","64d12ff8f1998324df34446e9530414ad6e7e9c9"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["7b91922b55d15444d554721b352861d028eb8278"],"ca0ffea399542e8aac8ed7608f34f8ec4cb8904d":["85a883878c0af761245ab048babc63d099f835f3"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["962d04139994fce5193143ef35615499a9a96d78"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762":["f1bdbf92da222965b46c0a942c3857ba56e5c638","f2c5f0cb44df114db4228c8f77861714b5cabaea","bde51b089eb7f86171eb3406e38a274743f9b7ac","a3776dccca01c11e7046323cfad46a3b4a471233"],"06584e6e98d592b34e1329b384182f368d2025e8":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"85a883878c0af761245ab048babc63d099f835f3":["3bb13258feba31ab676502787ab2e1779f129b7a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"d572389229127c297dd1fa5ce4758e1cec41e799":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"962d04139994fce5193143ef35615499a9a96d78":[],"eb378f8bdee16a26810e086303a4a86b4930ea12":["7d16aff6229cca84309d03d047cd718946bd4b43"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"790e1fde4caa765b3faaad3fbcd25c6973450336":["29ef99d61cda9641b6250bf9567329a6e65f901d","4fd64b6aa64934b0e35d9ec3e6c5a5c60ffc1762"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"64d12ff8f1998324df34446e9530414ad6e7e9c9":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["ca0ffea399542e8aac8ed7608f34f8ec4cb8904d"],"3bb13258feba31ab676502787ab2e1779f129b7a":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"4e8cc373c801e54cec75daf9f52792cb4b17f536":["b1add9ddc0005b07550d4350720aac22dc9886b3","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","3bb13258feba31ab676502787ab2e1779f129b7a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","962d04139994fce5193143ef35615499a9a96d78","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}