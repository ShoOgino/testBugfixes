{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newField(\"content\", contents, TextField.TYPE_UNSTORED));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random, reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    false);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newField(\"content\", contents, TextField.TYPE_UNSTORED));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random, reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    false);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newField(\"content\", contents, TextField.TYPE_UNSTORED));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    false);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newField(\"content\", contents, TextField.TYPE_UNSTORED));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random, reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    false);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newField(\"content\", contents, TextField.TYPE_UNSTORED));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    false);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newField(\"content\", contents, TextField.TYPE_UNSTORED));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = IndexReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    false);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    false);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newField(\"content\", contents, TextField.TYPE_UNSTORED));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newField(\"content\", \"aa bb cc dd\", TextField.TYPE_UNSTORED));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    false);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":["05fe562aa248790944d43cdd478f512572835ba0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    false);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    false);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    false);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = _TestUtil.docs(random(), reader,\n                                    t.field(),\n                                    new BytesRef(t.text()),\n                                    MultiFields.getLiveDocs(reader),\n                                    null,\n                                    0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.shutdown();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0a16b40feb4e6e0d55c1716733bde48296bedd20","date":1400540388,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.shutdown();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(2, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.shutdown();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45a621dd071a902e1fd30367200d7bbbea037706","date":1400686915,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.shutdown();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.shutdown();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(2, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.shutdown();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.shutdown();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.shutdown();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    PostingsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    DocsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    PostingsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        null,\n        0);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      if (liveDocs == null || liveDocs.get(tdocs.docID())) {\n        count++;\n      }\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    PostingsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        MultiFields.getLiveDocs(reader),\n        null,\n        0);\n\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      count++;\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05fe562aa248790944d43cdd478f512572835ba0","date":1455901667,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document brokenDoc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    brokenDoc.add(newTextField(\"content\", contents, Field.Store.NO));\n    expectThrows(Exception.class, () -> {\n      writer.addDocument(brokenDoc);\n    });\n\n    // Make sure we can add another normal document\n    Document doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    PostingsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        null,\n        0);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      if (liveDocs == null || liveDocs.get(tdocs.docID())) {\n        count++;\n      }\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(newTextField(\"content\", contents, Field.Store.NO));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    PostingsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        null,\n        0);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      if (liveDocs == null || liveDocs.get(tdocs.docID())) {\n        count++;\n      }\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["8560794cda5bcd510c60e38ed553e9c5a6204983","04f07771a2a7dd3a395700665ed839c3dae2def2"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a90cc8c90aa53ddf51fbd15019989ac269514a3","date":1531845066,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n    conf.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document brokenDoc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    brokenDoc.add(newTextField(\"content\", contents, Field.Store.NO));\n    expectThrows(Exception.class, () -> {\n      writer.addDocument(brokenDoc);\n    });\n\n    // Make sure we can add another normal document\n    Document doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    PostingsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        null,\n        0);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      if (liveDocs == null || liveDocs.get(tdocs.docID())) {\n        count++;\n      }\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document brokenDoc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    brokenDoc.add(newTextField(\"content\", contents, Field.Store.NO));\n    expectThrows(Exception.class, () -> {\n      writer.addDocument(brokenDoc);\n    });\n\n    // Make sure we can add another normal document\n    Document doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    PostingsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        null,\n        0);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      if (liveDocs == null || liveDocs.get(tdocs.docID())) {\n        count++;\n      }\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n    conf.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document brokenDoc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    brokenDoc.add(newTextField(\"content\", contents, Field.Store.NO));\n    expectThrows(Exception.class, () -> {\n      writer.addDocument(brokenDoc);\n    });\n\n    // Make sure we can add another normal document\n    Document doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    PostingsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        null,\n        0);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      if (liveDocs == null || liveDocs.get(tdocs.docID())) {\n        count++;\n      }\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document brokenDoc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    brokenDoc.add(newTextField(\"content\", contents, Field.Store.NO));\n    expectThrows(Exception.class, () -> {\n      writer.addDocument(brokenDoc);\n    });\n\n    // Make sure we can add another normal document\n    Document doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    PostingsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        null,\n        0);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      if (liveDocs == null || liveDocs.get(tdocs.docID())) {\n        count++;\n      }\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testExceptionFromTokenStream().mjava","sourceNew":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n    conf.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document brokenDoc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    brokenDoc.add(newTextField(\"content\", contents, Field.Store.NO));\n    expectThrows(Exception.class, () -> {\n      writer.addDocument(brokenDoc);\n    });\n\n    // Make sure we can add another normal document\n    Document doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    PostingsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        null,\n        0);\n\n    final Bits liveDocs = MultiBits.getLiveDocs(reader);\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      if (liveDocs == null || liveDocs.get(tdocs.docID())) {\n        count++;\n      }\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-1072\n  public void testExceptionFromTokenStream() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig(new Analyzer() {\n\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.SIMPLE, true);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          private int count = 0;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.incrementToken();\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            this.count = 0;\n          }\n        });\n      }\n\n    });\n    conf.setMaxBufferedDocs(Math.max(3, conf.getMaxBufferedDocs()));\n    conf.setMergePolicy(NoMergePolicy.INSTANCE);\n\n    IndexWriter writer = new IndexWriter(dir, conf);\n\n    Document brokenDoc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    brokenDoc.add(newTextField(\"content\", contents, Field.Store.NO));\n    expectThrows(Exception.class, () -> {\n      writer.addDocument(brokenDoc);\n    });\n\n    // Make sure we can add another normal document\n    Document doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(newTextField(\"content\", \"aa bb cc dd\", Field.Store.NO));\n    writer.addDocument(doc);\n\n    writer.close();\n    IndexReader reader = DirectoryReader.open(dir);\n    final Term t = new Term(\"content\", \"aa\");\n    assertEquals(3, reader.docFreq(t));\n\n    // Make sure the doc that hit the exception was marked\n    // as deleted:\n    PostingsEnum tdocs = TestUtil.docs(random(), reader,\n        t.field(),\n        new BytesRef(t.text()),\n        null,\n        0);\n\n    final Bits liveDocs = MultiFields.getLiveDocs(reader);\n    int count = 0;\n    while(tdocs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n      if (liveDocs == null || liveDocs.get(tdocs.docID())) {\n        count++;\n      }\n    }\n    assertEquals(2, count);\n\n    assertEquals(reader.docFreq(new Term(\"content\", \"gg\")), 0);\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a16b40feb4e6e0d55c1716733bde48296bedd20":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"45a621dd071a902e1fd30367200d7bbbea037706":["0a16b40feb4e6e0d55c1716733bde48296bedd20"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["02331260bb246364779cb6f04919ca47900d01bb"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"05fe562aa248790944d43cdd478f512572835ba0":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"4a90cc8c90aa53ddf51fbd15019989ac269514a3":["05fe562aa248790944d43cdd478f512572835ba0"],"51f5280f31484820499077f41fcdfe92d527d9dc":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["51f5280f31484820499077f41fcdfe92d527d9dc"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["05fe562aa248790944d43cdd478f512572835ba0","4a90cc8c90aa53ddf51fbd15019989ac269514a3"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["04f07771a2a7dd3a395700665ed839c3dae2def2","02331260bb246364779cb6f04919ca47900d01bb"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["04f07771a2a7dd3a395700665ed839c3dae2def2","02331260bb246364779cb6f04919ca47900d01bb"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["4a90cc8c90aa53ddf51fbd15019989ac269514a3"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"02331260bb246364779cb6f04919ca47900d01bb":["04f07771a2a7dd3a395700665ed839c3dae2def2"]},"commit2Childs":{"0a16b40feb4e6e0d55c1716733bde48296bedd20":["45a621dd071a902e1fd30367200d7bbbea037706"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"45a621dd071a902e1fd30367200d7bbbea037706":[],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["6613659748fe4411a7dcf85266e55db1f95f7315"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"05fe562aa248790944d43cdd478f512572835ba0":["4a90cc8c90aa53ddf51fbd15019989ac269514a3","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5"],"4a90cc8c90aa53ddf51fbd15019989ac269514a3":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","04e775de416dd2d8067b10db1c8af975a1d5017e"],"51f5280f31484820499077f41fcdfe92d527d9dc":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","02331260bb246364779cb6f04919ca47900d01bb"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["05fe562aa248790944d43cdd478f512572835ba0"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["0a16b40feb4e6e0d55c1716733bde48296bedd20","54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"02331260bb246364779cb6f04919ca47900d01bb":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["45a621dd071a902e1fd30367200d7bbbea037706","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}