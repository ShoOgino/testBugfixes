{"path":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","commits":[{"id":"3e45d45bc3730ddd1341f4eb6025f33b8482e6e2","date":1346834651,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"46885bf5f669268ac6235cce5c62fdd68b4e490c","date":1397242003,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.length()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"72feb75398b9bcb44dddbdbf9fd81f6ab6646d0f","date":1397994222,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"75472665e304d781686f64f703ae64d8d7feafd6","date":1397995495,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1e6717cee2dbaffe636b5145c27276eec7b4599d","date":1397995793,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            ros.flush();\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2bb2842e561df4e8e9ad89010605fc86ac265465","date":1414768208,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n\n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      PostingsEnum postingsEnum = null;\n      PostingsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.postings(null, docsAndPositionsEnum, PostingsEnum.FLAG_ALL);\n        } else {\n          postingsEnum = termsEnum.postings(null, postingsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final PostingsEnum postingsEnum2;\n        if (hasPos) {\n          postingsEnum2 = docsAndPositionsEnum;\n        } else {\n          postingsEnum2 = postingsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n\n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n      \n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      DocsEnum docsEnum = null;\n      DocsAndPositionsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);\n        } else {\n          docsEnum = termsEnum.docs(null, docsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final DocsEnum docsEnum2;\n        if (hasPos) {\n          docsEnum2 = docsAndPositionsEnum;\n        } else {\n          docsEnum2 = docsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n        \n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = docsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n\n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      PostingsEnum postingsEnum = null;\n      PostingsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.postings(null, docsAndPositionsEnum, PostingsEnum.ALL);\n        } else {\n          postingsEnum = termsEnum.postings(null, postingsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final PostingsEnum postingsEnum2;\n        if (hasPos) {\n          postingsEnum2 = docsAndPositionsEnum;\n        } else {\n          postingsEnum2 = postingsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n\n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n\n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      PostingsEnum postingsEnum = null;\n      PostingsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.postings(null, docsAndPositionsEnum, PostingsEnum.FLAG_ALL);\n        } else {\n          postingsEnum = termsEnum.postings(null, postingsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final PostingsEnum postingsEnum2;\n        if (hasPos) {\n          postingsEnum2 = docsAndPositionsEnum;\n        } else {\n          postingsEnum2 = postingsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n\n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n\n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      PostingsEnum postingsEnum = null;\n      PostingsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator();\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.postings(null, docsAndPositionsEnum, PostingsEnum.ALL);\n        } else {\n          postingsEnum = termsEnum.postings(null, postingsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final PostingsEnum postingsEnum2;\n        if (hasPos) {\n          postingsEnum2 = docsAndPositionsEnum;\n        } else {\n          postingsEnum2 = postingsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n\n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n\n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      PostingsEnum postingsEnum = null;\n      PostingsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator(null);\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.postings(null, docsAndPositionsEnum, PostingsEnum.ALL);\n        } else {\n          postingsEnum = termsEnum.postings(null, postingsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final PostingsEnum postingsEnum2;\n        if (hasPos) {\n          postingsEnum2 = docsAndPositionsEnum;\n        } else {\n          postingsEnum2 = postingsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n\n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n\n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      PostingsEnum postingsEnum = null;\n      PostingsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator();\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.postings(docsAndPositionsEnum, PostingsEnum.ALL);\n        } else {\n          postingsEnum = termsEnum.postings(postingsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final PostingsEnum postingsEnum2;\n        if (hasPos) {\n          postingsEnum2 = docsAndPositionsEnum;\n        } else {\n          postingsEnum2 = postingsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n\n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n\n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      PostingsEnum postingsEnum = null;\n      PostingsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator();\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.postings(null, docsAndPositionsEnum, PostingsEnum.ALL);\n        } else {\n          postingsEnum = termsEnum.postings(null, postingsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final PostingsEnum postingsEnum2;\n        if (hasPos) {\n          postingsEnum2 = docsAndPositionsEnum;\n        } else {\n          postingsEnum2 = postingsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n\n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"409da428f28953cf35fddd5c9ff5c7e4f5439863","date":1547556145,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.DirectField#DirectField(SegmentReadState,String,Terms,int,int).mjava","sourceNew":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n\n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      PostingsEnum postingsEnum = null;\n      PostingsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator();\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final ByteBuffersDataOutput ros = ByteBuffersDataOutput.newResettableInstance();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.postings(docsAndPositionsEnum, PostingsEnum.ALL);\n        } else {\n          postingsEnum = termsEnum.postings(postingsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final PostingsEnum postingsEnum2;\n        if (hasPos) {\n          postingsEnum2 = docsAndPositionsEnum;\n        } else {\n          postingsEnum2 = postingsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads = hasPayloads ? ros.toArrayCopy() : null;\n          final int[] postings = scratch.get();\n\n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","sourceOld":"    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {\n      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n\n      sumTotalTermFreq = termsIn.getSumTotalTermFreq();\n      sumDocFreq = termsIn.getSumDocFreq();\n      docCount = termsIn.getDocCount();\n\n      final int numTerms = (int) termsIn.size();\n      if (numTerms == -1) {\n        throw new IllegalArgumentException(\"codec does not provide Terms.size()\");\n      }\n      terms = new TermAndSkip[numTerms];\n      termOffsets = new int[1+numTerms];\n\n      byte[] termBytes = new byte[1024];\n\n      this.minSkipCount = minSkipCount;\n\n      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS) > 0;\n      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;\n      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;\n      hasPayloads = fieldInfo.hasPayloads();\n\n      BytesRef term;\n      PostingsEnum postingsEnum = null;\n      PostingsEnum docsAndPositionsEnum = null;\n      final TermsEnum termsEnum = termsIn.iterator();\n      int termOffset = 0;\n\n      final IntArrayWriter scratch = new IntArrayWriter();\n\n      // Used for payloads, if any:\n      final RAMOutputStream ros = new RAMOutputStream();\n\n      // if (DEBUG) {\n      //   System.out.println(\"\\nLOAD terms seg=\" + state.segmentInfo.name + \" field=\" + field + \" hasOffsets=\" + hasOffsets + \" hasFreq=\" + hasFreq + \" hasPos=\" + hasPos + \" hasPayloads=\" + hasPayloads);\n      // }\n\n      while ((term = termsEnum.next()) != null) {\n        final int docFreq = termsEnum.docFreq();\n        final long totalTermFreq = termsEnum.totalTermFreq();\n\n        // if (DEBUG) {\n        //   System.out.println(\"  term=\" + term.utf8ToString());\n        // }\n\n        termOffsets[count] = termOffset;\n\n        if (termBytes.length < (termOffset + term.length)) {\n          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);\n        }\n        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);\n        termOffset += term.length;\n        termOffsets[count+1] = termOffset;\n\n        if (hasPos) {\n          docsAndPositionsEnum = termsEnum.postings(docsAndPositionsEnum, PostingsEnum.ALL);\n        } else {\n          postingsEnum = termsEnum.postings(postingsEnum);\n        }\n\n        final TermAndSkip ent;\n\n        final PostingsEnum postingsEnum2;\n        if (hasPos) {\n          postingsEnum2 = docsAndPositionsEnum;\n        } else {\n          postingsEnum2 = postingsEnum;\n        }\n\n        int docID;\n\n        if (docFreq <= lowFreqCutoff) {\n\n          ros.reset();\n\n          // Pack postings for low-freq terms into a single int[]:\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            scratch.add(docID);\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              scratch.add(freq);\n              if (hasPos) {\n                for(int pos=0;pos<freq;pos++) {\n                  scratch.add(docsAndPositionsEnum.nextPosition());\n                  if (hasOffsets) {\n                    scratch.add(docsAndPositionsEnum.startOffset());\n                    scratch.add(docsAndPositionsEnum.endOffset());\n                  }\n                  if (hasPayloads) {\n                    final BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      scratch.add(payload.length);\n                      ros.writeBytes(payload.bytes, payload.offset, payload.length);\n                    } else {\n                      scratch.add(0);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          final byte[] payloads;\n          if (hasPayloads) {\n            payloads = new byte[(int) ros.getFilePointer()];\n            ros.writeTo(payloads, 0);\n          } else {\n            payloads = null;\n          }\n\n          final int[] postings = scratch.get();\n\n          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);\n        } else {\n          final int[] docs = new int[docFreq];\n          final int[] freqs;\n          final int[][] positions;\n          final byte[][][] payloads;\n          if (hasFreq) {\n            freqs = new int[docFreq];\n            if (hasPos) {\n              positions = new int[docFreq][];\n              if (hasPayloads) {\n                payloads = new byte[docFreq][][];\n              } else {\n                payloads = null;\n              }\n            } else {\n              positions = null;\n              payloads = null;\n            }\n          } else {\n            freqs = null;\n            positions = null;\n            payloads = null;\n          }\n\n          // Use separate int[] for the postings for high-freq\n          // terms:\n          int upto = 0;\n          while ((docID = postingsEnum2.nextDoc()) != PostingsEnum.NO_MORE_DOCS) {\n            docs[upto] = docID;\n            if (hasFreq) {\n              final int freq = postingsEnum2.freq();\n              freqs[upto] = freq;\n              if (hasPos) {\n                final int mult;\n                if (hasOffsets) {\n                  mult = 3;\n                } else {\n                  mult = 1;\n                }\n                if (hasPayloads) {\n                  payloads[upto] = new byte[freq][];\n                }\n                positions[upto] = new int[mult*freq];\n                int posUpto = 0;\n                for(int pos=0;pos<freq;pos++) {\n                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();\n                  if (hasPayloads) {\n                    BytesRef payload = docsAndPositionsEnum.getPayload();\n                    if (payload != null) {\n                      byte[] payloadBytes = new byte[payload.length];\n                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);\n                      payloads[upto][pos] = payloadBytes;\n                    }\n                  }\n                  posUpto++;\n                  if (hasOffsets) {\n                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();\n                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();\n                  }\n                }\n              }\n            }\n\n            upto++;\n          }\n          assert upto == docFreq;\n          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);\n        }\n\n        terms[count] = ent;\n        setSkips(count, termBytes);\n        count++;\n      }\n\n      // End sentinel:\n      termOffsets[count] = termOffset;\n\n      finishSkips();\n\n      //System.out.println(skipCount + \" skips: \" + field);\n\n      this.termBytes = new byte[termOffset];\n      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);\n\n      // Pack skips:\n      this.skips = new int[skipCount];\n      this.skipOffsets = new int[1+numTerms];\n\n      int skipOffset = 0;\n      for(int i=0;i<numTerms;i++) {\n        final int[] termSkips = terms[i].skips;\n        skipOffsets[i] = skipOffset;\n        if (termSkips != null) {\n          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);\n          skipOffset += termSkips.length;\n          terms[i].skips = null;\n        }\n      }\n      this.skipOffsets[numTerms] = skipOffset;\n      assert skipOffset == skipCount;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"3e45d45bc3730ddd1341f4eb6025f33b8482e6e2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["1e6717cee2dbaffe636b5145c27276eec7b4599d"],"1e6717cee2dbaffe636b5145c27276eec7b4599d":["75472665e304d781686f64f703ae64d8d7feafd6"],"75472665e304d781686f64f703ae64d8d7feafd6":["72feb75398b9bcb44dddbdbf9fd81f6ab6646d0f"],"51f5280f31484820499077f41fcdfe92d527d9dc":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"409da428f28953cf35fddd5c9ff5c7e4f5439863":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"72feb75398b9bcb44dddbdbf9fd81f6ab6646d0f":["46885bf5f669268ac6235cce5c62fdd68b4e490c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["409da428f28953cf35fddd5c9ff5c7e4f5439863"],"46885bf5f669268ac6235cce5c62fdd68b4e490c":["3e45d45bc3730ddd1341f4eb6025f33b8482e6e2"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"3e45d45bc3730ddd1341f4eb6025f33b8482e6e2":["46885bf5f669268ac6235cce5c62fdd68b4e490c"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["51f5280f31484820499077f41fcdfe92d527d9dc"],"1e6717cee2dbaffe636b5145c27276eec7b4599d":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"75472665e304d781686f64f703ae64d8d7feafd6":["1e6717cee2dbaffe636b5145c27276eec7b4599d"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"409da428f28953cf35fddd5c9ff5c7e4f5439863":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["409da428f28953cf35fddd5c9ff5c7e4f5439863"],"72feb75398b9bcb44dddbdbf9fd81f6ab6646d0f":["75472665e304d781686f64f703ae64d8d7feafd6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3e45d45bc3730ddd1341f4eb6025f33b8482e6e2"],"46885bf5f669268ac6235cce5c62fdd68b4e490c":["72feb75398b9bcb44dddbdbf9fd81f6ab6646d0f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}