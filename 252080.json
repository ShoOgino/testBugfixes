{"path":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,boolean,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","commits":[{"id":"44fcbde6fb2ac44ee3b45e013e54a42911e689ff","date":1292065621,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,boolean,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"/dev/null","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, boolean closeDocStore, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocsInRAM == 0 && numDocsInStore == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n\n      assert waitQueue.waitingBytes == 0;\n\n      assert docStoreSegment != null || numDocsInRAM == 0: \"dss=\" + docStoreSegment + \" numDocsInRAM=\" + numDocsInRAM;\n\n      assert numDocsInStore >= numDocsInRAM: \"numDocsInStore=\" + numDocsInStore + \" numDocsInRAM=\" + numDocsInRAM;\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 docStoreSegment, numDocsInRAM, numDocsInStore, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocsInRAM, directory, false, -1, null, false, hasProx(), flushState.segmentCodecs);\n\n      if (!closeDocStore || docStoreOffset != 0) {\n        newSegment.setDocStoreSegment(docStoreSegment);\n        newSegment.setDocStoreOffset(docStoreOffset);\n      }\n\n      if (closeDocStore) {\n        closeDocStore(flushState, writer, deleter, newSegment, mergePolicy, segmentInfos);\n      }\n\n      if (numDocsInRAM > 0) {\n\n        assert nextDocID == numDocsInRAM;\n        assert waitQueue.numWaiting == 0;\n        assert waitQueue.waitingBytes == 0;\n\n        if (infoStream != null) {\n          message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocsInRAM);\n        }\n    \n        final Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n        for(int i=0;i<threadStates.length;i++) {\n          threads.add(threadStates[i].consumer);\n        }\n\n        final long startNumBytesUsed = bytesUsed();\n        consumer.flush(threads, flushState);\n\n        if (infoStream != null) {\n          message(\"flushedFiles=\" + flushState.flushedFiles);\n          message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n        }\n\n        if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n\n          final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n          if (infoStream != null) {\n            message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n          }\n\n          CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n          for(String fileName : flushState.flushedFiles) {\n            cfsWriter.addFile(fileName);\n          }\n          cfsWriter.close();\n          deleter.deleteNewFiles(flushState.flushedFiles);\n\n          newSegment.setUseCompoundFile(true);\n        }\n\n        if (infoStream != null) {\n          message(\"flush: segment=\" + newSegment);\n          final long newSegmentSize = newSegment.sizeInBytes();\n          String message = \"  ramUsed=\" + nf.format(startNumBytesUsed/1024./1024.) + \" MB\" +\n            \" newFlushedSize=\" + nf.format(newSegmentSize/1024/1024) + \" MB\" +\n            \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) +\n            \" new/old=\" + nf.format(100.0*newSegmentSize/startNumBytesUsed) + \"%\";\n          message(message);\n        }\n\n      } else {\n        if (infoStream != null) {\n          message(\"skip flushing segment: no docs\");\n        }\n        newSegment = null;\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    docStoreOffset = numDocsInStore;\n\n    return newSegment;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e1cbd7e289dc1243c7a59e1a83d078163a147fe","date":1292268032,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,boolean,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,boolean,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, boolean closeDocStore, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocsInRAM == 0 && numDocsInStore == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n\n      assert waitQueue.waitingBytes == 0;\n\n      assert docStoreSegment != null || numDocsInRAM == 0: \"dss=\" + docStoreSegment + \" numDocsInRAM=\" + numDocsInRAM;\n\n      assert numDocsInStore >= numDocsInRAM: \"numDocsInStore=\" + numDocsInStore + \" numDocsInRAM=\" + numDocsInRAM;\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 docStoreSegment, numDocsInRAM, numDocsInStore, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocsInRAM, directory, false, -1, null, false, hasProx(), flushState.segmentCodecs, false);\n\n      if (!closeDocStore || docStoreOffset != 0) {\n        newSegment.setDocStoreSegment(docStoreSegment);\n        newSegment.setDocStoreOffset(docStoreOffset);\n      }\n      \n      boolean hasVectors = false;\n\n      if (closeDocStore) {\n        closeDocStore(flushState, writer, deleter, newSegment, mergePolicy, segmentInfos);\n      }\n\n      hasVectors |= flushState.hasVectors;\n\n      if (numDocsInRAM > 0) {\n\n        assert nextDocID == numDocsInRAM;\n        assert waitQueue.numWaiting == 0;\n        assert waitQueue.waitingBytes == 0;\n\n        if (infoStream != null) {\n          message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocsInRAM);\n        }\n    \n        final Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n        for(int i=0;i<threadStates.length;i++) {\n          threads.add(threadStates[i].consumer);\n        }\n\n        final long startNumBytesUsed = bytesUsed();\n        consumer.flush(threads, flushState);\n\n        hasVectors |= flushState.hasVectors;\n\n        if (hasVectors) {\n          if (infoStream != null) {\n            message(\"new segment has vectors\");\n          }\n          newSegment.setHasVectors(true);\n        } else {\n          if (infoStream != null) {\n            message(\"new segment has no vectors\");\n          }\n        }\n\n        if (infoStream != null) {\n          message(\"flushedFiles=\" + flushState.flushedFiles);\n          message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n        }\n\n        if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n\n          final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n          if (infoStream != null) {\n            message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n          }\n\n          CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n          for(String fileName : flushState.flushedFiles) {\n            cfsWriter.addFile(fileName);\n          }\n          cfsWriter.close();\n          deleter.deleteNewFiles(flushState.flushedFiles);\n\n          newSegment.setUseCompoundFile(true);\n        }\n\n        if (infoStream != null) {\n          message(\"flush: segment=\" + newSegment);\n          final long newSegmentSize = newSegment.sizeInBytes();\n          String message = \"  ramUsed=\" + nf.format(startNumBytesUsed/1024./1024.) + \" MB\" +\n            \" newFlushedSize=\" + nf.format(newSegmentSize/1024/1024) + \" MB\" +\n            \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) +\n            \" new/old=\" + nf.format(100.0*newSegmentSize/startNumBytesUsed) + \"%\";\n          message(message);\n        }\n\n      } else {\n        if (infoStream != null) {\n          message(\"skip flushing segment: no docs\");\n        }\n        newSegment = null;\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    docStoreOffset = numDocsInStore;\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, boolean closeDocStore, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocsInRAM == 0 && numDocsInStore == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n\n      assert waitQueue.waitingBytes == 0;\n\n      assert docStoreSegment != null || numDocsInRAM == 0: \"dss=\" + docStoreSegment + \" numDocsInRAM=\" + numDocsInRAM;\n\n      assert numDocsInStore >= numDocsInRAM: \"numDocsInStore=\" + numDocsInStore + \" numDocsInRAM=\" + numDocsInRAM;\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 docStoreSegment, numDocsInRAM, numDocsInStore, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocsInRAM, directory, false, -1, null, false, hasProx(), flushState.segmentCodecs);\n\n      if (!closeDocStore || docStoreOffset != 0) {\n        newSegment.setDocStoreSegment(docStoreSegment);\n        newSegment.setDocStoreOffset(docStoreOffset);\n      }\n\n      if (closeDocStore) {\n        closeDocStore(flushState, writer, deleter, newSegment, mergePolicy, segmentInfos);\n      }\n\n      if (numDocsInRAM > 0) {\n\n        assert nextDocID == numDocsInRAM;\n        assert waitQueue.numWaiting == 0;\n        assert waitQueue.waitingBytes == 0;\n\n        if (infoStream != null) {\n          message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocsInRAM);\n        }\n    \n        final Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n        for(int i=0;i<threadStates.length;i++) {\n          threads.add(threadStates[i].consumer);\n        }\n\n        final long startNumBytesUsed = bytesUsed();\n        consumer.flush(threads, flushState);\n\n        if (infoStream != null) {\n          message(\"flushedFiles=\" + flushState.flushedFiles);\n          message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n        }\n\n        if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n\n          final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n          if (infoStream != null) {\n            message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n          }\n\n          CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n          for(String fileName : flushState.flushedFiles) {\n            cfsWriter.addFile(fileName);\n          }\n          cfsWriter.close();\n          deleter.deleteNewFiles(flushState.flushedFiles);\n\n          newSegment.setUseCompoundFile(true);\n        }\n\n        if (infoStream != null) {\n          message(\"flush: segment=\" + newSegment);\n          final long newSegmentSize = newSegment.sizeInBytes();\n          String message = \"  ramUsed=\" + nf.format(startNumBytesUsed/1024./1024.) + \" MB\" +\n            \" newFlushedSize=\" + nf.format(newSegmentSize/1024/1024) + \" MB\" +\n            \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) +\n            \" new/old=\" + nf.format(100.0*newSegmentSize/startNumBytesUsed) + \"%\";\n          message(message);\n        }\n\n      } else {\n        if (infoStream != null) {\n          message(\"skip flushing segment: no docs\");\n        }\n        newSegment = null;\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    docStoreOffset = numDocsInStore;\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30ca900054c38836c7c167379e300af4dabb34c3","date":1292602599,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,boolean,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,boolean,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, boolean closeDocStore, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocsInRAM == 0 && numDocsInStore == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n\n      assert waitQueue.waitingBytes == 0;\n\n      assert docStoreSegment != null || numDocsInRAM == 0: \"dss=\" + docStoreSegment + \" numDocsInRAM=\" + numDocsInRAM;\n\n      assert numDocsInStore >= numDocsInRAM: \"numDocsInStore=\" + numDocsInStore + \" numDocsInRAM=\" + numDocsInRAM;\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 docStoreSegment, numDocsInRAM, numDocsInStore, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocsInRAM, directory, false, -1, null, false, hasProx(), flushState.segmentCodecs, false);\n\n      if (!closeDocStore || docStoreOffset != 0) {\n        newSegment.setDocStoreSegment(docStoreSegment);\n        newSegment.setDocStoreOffset(docStoreOffset);\n      }\n      \n      if (closeDocStore) {\n        closeDocStore(flushState, writer, deleter, newSegment, mergePolicy, segmentInfos);\n      }\n\n      boolean hasVectors = flushState.hasVectors;\n\n      if (numDocsInRAM > 0) {\n\n        assert nextDocID == numDocsInRAM;\n        assert waitQueue.numWaiting == 0;\n        assert waitQueue.waitingBytes == 0;\n\n        if (infoStream != null) {\n          message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocsInRAM);\n        }\n    \n        final Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n        for(int i=0;i<threadStates.length;i++) {\n          threads.add(threadStates[i].consumer);\n        }\n\n        final long startNumBytesUsed = bytesUsed();\n        consumer.flush(threads, flushState);\n\n        hasVectors |= flushState.hasVectors;\n\n        if (hasVectors) {\n          if (infoStream != null) {\n            message(\"new segment has vectors\");\n          }\n          newSegment.setHasVectors(true);\n        } else {\n          if (infoStream != null) {\n            message(\"new segment has no vectors\");\n          }\n        }\n\n        if (infoStream != null) {\n          message(\"flushedFiles=\" + flushState.flushedFiles);\n          message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n        }\n\n        if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n\n          final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n          if (infoStream != null) {\n            message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n          }\n\n          CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n          for(String fileName : flushState.flushedFiles) {\n            cfsWriter.addFile(fileName);\n          }\n          cfsWriter.close();\n          deleter.deleteNewFiles(flushState.flushedFiles);\n\n          newSegment.setUseCompoundFile(true);\n        }\n\n        if (infoStream != null) {\n          message(\"flush: segment=\" + newSegment);\n          final long newSegmentSize = newSegment.sizeInBytes();\n          String message = \"  ramUsed=\" + nf.format(startNumBytesUsed/1024./1024.) + \" MB\" +\n            \" newFlushedSize=\" + nf.format(newSegmentSize/1024/1024) + \" MB\" +\n            \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) +\n            \" new/old=\" + nf.format(100.0*newSegmentSize/startNumBytesUsed) + \"%\";\n          message(message);\n        }\n\n      } else {\n        if (infoStream != null) {\n          message(\"skip flushing segment: no docs\");\n        }\n        newSegment = null;\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    docStoreOffset = numDocsInStore;\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, boolean closeDocStore, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocsInRAM == 0 && numDocsInStore == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n\n      assert waitQueue.waitingBytes == 0;\n\n      assert docStoreSegment != null || numDocsInRAM == 0: \"dss=\" + docStoreSegment + \" numDocsInRAM=\" + numDocsInRAM;\n\n      assert numDocsInStore >= numDocsInRAM: \"numDocsInStore=\" + numDocsInStore + \" numDocsInRAM=\" + numDocsInRAM;\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 docStoreSegment, numDocsInRAM, numDocsInStore, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocsInRAM, directory, false, -1, null, false, hasProx(), flushState.segmentCodecs, false);\n\n      if (!closeDocStore || docStoreOffset != 0) {\n        newSegment.setDocStoreSegment(docStoreSegment);\n        newSegment.setDocStoreOffset(docStoreOffset);\n      }\n      \n      boolean hasVectors = false;\n\n      if (closeDocStore) {\n        closeDocStore(flushState, writer, deleter, newSegment, mergePolicy, segmentInfos);\n      }\n\n      hasVectors |= flushState.hasVectors;\n\n      if (numDocsInRAM > 0) {\n\n        assert nextDocID == numDocsInRAM;\n        assert waitQueue.numWaiting == 0;\n        assert waitQueue.waitingBytes == 0;\n\n        if (infoStream != null) {\n          message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocsInRAM);\n        }\n    \n        final Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n        for(int i=0;i<threadStates.length;i++) {\n          threads.add(threadStates[i].consumer);\n        }\n\n        final long startNumBytesUsed = bytesUsed();\n        consumer.flush(threads, flushState);\n\n        hasVectors |= flushState.hasVectors;\n\n        if (hasVectors) {\n          if (infoStream != null) {\n            message(\"new segment has vectors\");\n          }\n          newSegment.setHasVectors(true);\n        } else {\n          if (infoStream != null) {\n            message(\"new segment has no vectors\");\n          }\n        }\n\n        if (infoStream != null) {\n          message(\"flushedFiles=\" + flushState.flushedFiles);\n          message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n        }\n\n        if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n\n          final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n          if (infoStream != null) {\n            message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n          }\n\n          CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n          for(String fileName : flushState.flushedFiles) {\n            cfsWriter.addFile(fileName);\n          }\n          cfsWriter.close();\n          deleter.deleteNewFiles(flushState.flushedFiles);\n\n          newSegment.setUseCompoundFile(true);\n        }\n\n        if (infoStream != null) {\n          message(\"flush: segment=\" + newSegment);\n          final long newSegmentSize = newSegment.sizeInBytes();\n          String message = \"  ramUsed=\" + nf.format(startNumBytesUsed/1024./1024.) + \" MB\" +\n            \" newFlushedSize=\" + nf.format(newSegmentSize/1024/1024) + \" MB\" +\n            \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) +\n            \" new/old=\" + nf.format(100.0*newSegmentSize/startNumBytesUsed) + \"%\";\n          message(message);\n        }\n\n      } else {\n        if (infoStream != null) {\n          message(\"skip flushing segment: no docs\");\n        }\n        newSegment = null;\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    docStoreOffset = numDocsInStore;\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,boolean,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":null,"sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, boolean closeDocStore, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocsInRAM == 0 && numDocsInStore == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n\n      assert waitQueue.waitingBytes == 0;\n\n      assert docStoreSegment != null || numDocsInRAM == 0: \"dss=\" + docStoreSegment + \" numDocsInRAM=\" + numDocsInRAM;\n\n      assert numDocsInStore >= numDocsInRAM: \"numDocsInStore=\" + numDocsInStore + \" numDocsInRAM=\" + numDocsInRAM;\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 docStoreSegment, numDocsInRAM, numDocsInStore, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocsInRAM, directory, false, -1, null, false, hasProx(), flushState.segmentCodecs, false);\n\n      if (!closeDocStore || docStoreOffset != 0) {\n        newSegment.setDocStoreSegment(docStoreSegment);\n        newSegment.setDocStoreOffset(docStoreOffset);\n      }\n      \n      if (closeDocStore) {\n        closeDocStore(flushState, writer, deleter, newSegment, mergePolicy, segmentInfos);\n      }\n\n      boolean hasVectors = flushState.hasVectors;\n\n      if (numDocsInRAM > 0) {\n\n        assert nextDocID == numDocsInRAM;\n        assert waitQueue.numWaiting == 0;\n        assert waitQueue.waitingBytes == 0;\n\n        if (infoStream != null) {\n          message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocsInRAM);\n        }\n    \n        final Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n        for(int i=0;i<threadStates.length;i++) {\n          threads.add(threadStates[i].consumer);\n        }\n\n        final long startNumBytesUsed = bytesUsed();\n        consumer.flush(threads, flushState);\n\n        hasVectors |= flushState.hasVectors;\n\n        if (hasVectors) {\n          if (infoStream != null) {\n            message(\"new segment has vectors\");\n          }\n          newSegment.setHasVectors(true);\n        } else {\n          if (infoStream != null) {\n            message(\"new segment has no vectors\");\n          }\n        }\n\n        if (infoStream != null) {\n          message(\"flushedFiles=\" + flushState.flushedFiles);\n          message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n        }\n\n        if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n\n          final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n          if (infoStream != null) {\n            message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n          }\n\n          CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n          for(String fileName : flushState.flushedFiles) {\n            cfsWriter.addFile(fileName);\n          }\n          cfsWriter.close();\n          deleter.deleteNewFiles(flushState.flushedFiles);\n\n          newSegment.setUseCompoundFile(true);\n        }\n\n        if (infoStream != null) {\n          message(\"flush: segment=\" + newSegment);\n          final long newSegmentSize = newSegment.sizeInBytes();\n          String message = \"  ramUsed=\" + nf.format(startNumBytesUsed/1024./1024.) + \" MB\" +\n            \" newFlushedSize=\" + nf.format(newSegmentSize/1024/1024) + \" MB\" +\n            \" docs/MB=\" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) +\n            \" new/old=\" + nf.format(100.0*newSegmentSize/startNumBytesUsed) + \"%\";\n          message(message);\n        }\n\n      } else {\n        if (infoStream != null) {\n          message(\"skip flushing segment: no docs\");\n        }\n        newSegment = null;\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    docStoreOffset = numDocsInStore;\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"30ca900054c38836c7c167379e300af4dabb34c3":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["30ca900054c38836c7c167379e300af4dabb34c3"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"]},"commit2Childs":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["7e1cbd7e289dc1243c7a59e1a83d078163a147fe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"30ca900054c38836c7c167379e300af4dabb34c3":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"7e1cbd7e289dc1243c7a59e1a83d078163a147fe":["30ca900054c38836c7c167379e300af4dabb34c3"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}