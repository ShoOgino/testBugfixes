{"path":"lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f080986da691a3bba7b757f43ab72cdc82b57ce","date":1273069619,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","pathOld":"lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/query/QueryAutoStopWordAnalyzer#addStopWords(IndexReader,String,int).mjava","sourceNew":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","sourceOld":"  /**\n   * Automatically adds stop words for the given field with terms exceeding the maxPercentDocs\n   *\n   * @param reader     The {@link IndexReader} which will be consulted to identify potential stop words that\n   *                   exceed the required document frequency\n   * @param fieldName  The field for which stopwords will be added\n   * @param maxDocFreq The maximum number of index documents which\n   *                   can contain a term, after which the term is considered to be a stop word.\n   * @return The number of stop words identified.\n   * @throws IOException\n   */\n  public int addStopWords(IndexReader reader, String fieldName, int maxDocFreq) throws IOException {\n    HashSet<String> stopWords = new HashSet<String>();\n    String internedFieldName = StringHelper.intern(fieldName);\n    TermEnum te = reader.terms(new Term(fieldName));\n    Term term = te.term();\n    while (term != null) {\n      if (term.field() != internedFieldName) {\n        break;\n      }\n      if (te.docFreq() > maxDocFreq) {\n        stopWords.add(term.text());\n      }\n      if (!te.next()) {\n        break;\n      }\n      term = te.term();\n    }\n    stopWordsPerField.put(fieldName, stopWords);\n    \n    /* if the stopwords for a field are changed,\n     * then saved streams for that field are erased.\n     */\n    Map<String,SavedStreams> streamMap = (Map<String,SavedStreams>) getPreviousTokenStream();\n    if (streamMap != null)\n      streamMap.remove(fieldName);\n    \n    return stopWords.size();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}