{"path":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","commits":[{"id":"d68e5c46e6a5ebdf4dafec4a123344092b915cc0","date":1256752193,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState((AttributeSource.State) iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState((AttributeSource.State) iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f11899016a0460a7ea2e4b008d002e1e75c7d867","date":1256772085,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap<String,ArrayList<AttributeSource.State>> cache = new HashMap<String,ArrayList<AttributeSource.State>>();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList<AttributeSource.State> tokens = cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList<AttributeSource.State> tokens2 = new ArrayList<AttributeSource.State>();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator<AttributeSource.State> iter = tokens.iterator();\n\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState(iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap cache = new HashMap();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList tokens = (ArrayList) cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList tokens2 = new ArrayList();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator iter = tokens.iterator();\n\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState((AttributeSource.State) iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d57eb7c98c08c03af6e4cd83509df31c81ac16af","date":1257684312,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap<String,ArrayList<AttributeSource.State>> cache = new HashMap<String,ArrayList<AttributeSource.State>>();\n\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList<AttributeSource.State> tokens = cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList<AttributeSource.State> tokens2 = new ArrayList<AttributeSource.State>();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            @Override\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator<AttributeSource.State> iter = tokens.iterator();\n\n            @Override\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState(iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap<String,ArrayList<AttributeSource.State>> cache = new HashMap<String,ArrayList<AttributeSource.State>>();\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList<AttributeSource.State> tokens = cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList<AttributeSource.State> tokens2 = new ArrayList<AttributeSource.State>();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator<AttributeSource.State> iter = tokens.iterator();\n\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState(iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","pathOld":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getTokenCachingAnalyzer(Analyzer).mjava","sourceNew":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap<String,ArrayList<AttributeSource.State>> cache = new HashMap<String,ArrayList<AttributeSource.State>>();\n\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList<AttributeSource.State> tokens = cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList<AttributeSource.State> tokens2 = new ArrayList<AttributeSource.State>();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            @Override\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator<AttributeSource.State> iter = tokens.iterator();\n\n            @Override\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState(iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","sourceOld":"  /**\n   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's\n   * token streams, and delivers those cached tokens on subsequent calls to \n   * <code>tokenStream(String fieldName, Reader reader)</code> \n   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.\n   * <p>\n   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can \n   * help improve performance if the same document is added to multiple Lucene indexes, \n   * because the text analysis phase need not be performed more than once.\n   * <p>\n   * Caveats: \n   * <ul>\n   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> \n   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>\n   * <li>The same caching analyzer instance must not be used for more than one document\n   * because the cache is not keyed on the Reader parameter.</li>\n   * </ul>\n   * \n   * @param child\n   *            the underlying child analyzer\n   * @return a new analyzer\n   */\n  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {\n\n    if (child == null)\n      throw new IllegalArgumentException(\"child analyzer must not be null\");\n\n    return new Analyzer() {\n\n      private final HashMap<String,ArrayList<AttributeSource.State>> cache = new HashMap<String,ArrayList<AttributeSource.State>>();\n\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        final ArrayList<AttributeSource.State> tokens = cache.get(fieldName);\n        if (tokens == null) { // not yet cached\n          final ArrayList<AttributeSource.State> tokens2 = new ArrayList<AttributeSource.State>();\n          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {\n\n            @Override\n            public boolean incrementToken() throws IOException {\n              boolean hasNext = input.incrementToken();\n              if (hasNext) tokens2.add(captureState());\n              return hasNext;\n            }\n          };\n          \n          cache.put(fieldName, tokens2);\n          return tokenStream;\n        } else { // already cached\n          return new TokenStream() {\n\n            private Iterator<AttributeSource.State> iter = tokens.iterator();\n\n            @Override\n            public boolean incrementToken() {\n              if (!iter.hasNext()) return false;\n              restoreState(iter.next());\n              return true;\n            }\n          };\n        }\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d57eb7c98c08c03af6e4cd83509df31c81ac16af":["f11899016a0460a7ea2e4b008d002e1e75c7d867"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["d57eb7c98c08c03af6e4cd83509df31c81ac16af"],"f11899016a0460a7ea2e4b008d002e1e75c7d867":["d68e5c46e6a5ebdf4dafec4a123344092b915cc0"],"d68e5c46e6a5ebdf4dafec4a123344092b915cc0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d68e5c46e6a5ebdf4dafec4a123344092b915cc0"],"d57eb7c98c08c03af6e4cd83509df31c81ac16af":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"f11899016a0460a7ea2e4b008d002e1e75c7d867":["d57eb7c98c08c03af6e4cd83509df31c81ac16af"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d68e5c46e6a5ebdf4dafec4a123344092b915cc0":["f11899016a0460a7ea2e4b008d002e1e75c7d867"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}