{"path":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","commits":[{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":0,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","pathOld":"/dev/null","sourceNew":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator(null);\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    DocsEnum docsEnum = null;\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      docsEnum = termsEnum.docs(liveDocs, docsEnum, DocsEnum.FLAG_NONE);\n      for (;;) {\n        int doc = docsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n            }\n          }\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","sourceNew":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator(null);\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(liveDocs, postingsEnum, PostingsEnum.FLAG_NONE);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n            }\n          }\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","sourceOld":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator(null);\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    DocsEnum docsEnum = null;\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      docsEnum = termsEnum.docs(liveDocs, docsEnum, DocsEnum.FLAG_NONE);\n      for (;;) {\n        int doc = docsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n            }\n          }\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e73063b92d958076ef4ae8beb5f493e8ccdcecb4","date":1424177215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","sourceNew":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator(null);\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(liveDocs, postingsEnum, PostingsEnum.NONE);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n            }\n          }\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","sourceOld":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator(null);\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(liveDocs, postingsEnum, PostingsEnum.FLAG_NONE);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n            }\n          }\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","sourceNew":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(liveDocs, postingsEnum, PostingsEnum.NONE);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n            }\n          }\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","sourceOld":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator(null);\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(liveDocs, postingsEnum, PostingsEnum.NONE);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n            }\n          }\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b30fb46e65ee1ae807d474716cbb39e0c54b74f","date":1430006768,"type":3,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","sourceNew":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (ranges != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(liveDocs, postingsEnum, PostingsEnum.NONE);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","sourceOld":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(liveDocs, postingsEnum, PostingsEnum.NONE);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n            }\n          }\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","sourceNew":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (ranges != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n      postingsEnum = BitsFilteredPostingsEnum.wrap(postingsEnum, liveDocs);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","sourceOld":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (ranges != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(liveDocs, postingsEnum, PostingsEnum.NONE);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","date":1497408244,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","sourceNew":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Terms terms = reader.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (ranges != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n      postingsEnum = BitsFilteredPostingsEnum.wrap(postingsEnum, liveDocs);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","sourceOld":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (ranges != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n      postingsEnum = BitsFilteredPostingsEnum.wrap(postingsEnum, liveDocs);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","sourceNew":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Terms terms = reader.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (ranges != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n      postingsEnum = BitsFilteredPostingsEnum.wrap(postingsEnum, liveDocs);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","sourceOld":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (ranges != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n      postingsEnum = BitsFilteredPostingsEnum.wrap(postingsEnum, liveDocs);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","sourceNew":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Terms terms = reader.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (ranges != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n      postingsEnum = BitsFilteredPostingsEnum.wrap(postingsEnum, liveDocs);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","sourceOld":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Fields fields = reader.fields();\n    Terms terms = fields==null ? null : fields.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (ranges != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n      postingsEnum = BitsFilteredPostingsEnum.wrap(postingsEnum, liveDocs);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"20c968c14aace7cf49843bf2c1fafc7fd3845659","date":1533133859,"type":5,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext,int,SchemaField,DocRouter.Range[],String,HashBasedRouter,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/SolrIndexSplitter#split(LeafReaderContext).mjava","sourceNew":"  static FixedBitSet[] split(LeafReaderContext readerContext, int numPieces, SchemaField field, DocRouter.Range[] rangesArr,\n                             String splitKey, HashBasedRouter hashRouter, boolean delete) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n      if (delete) {\n        docSets[i].set(0, reader.maxDoc());\n      }\n    }\n    Bits liveDocs = reader.getLiveDocs();\n    if (liveDocs != null && delete) {\n      FixedBitSet liveDocsSet = FixedBitSet.copyOf(liveDocs);\n      for (FixedBitSet set : docSets) {\n        set.and(liveDocsSet);\n      }\n    }\n\n    Terms terms = reader.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (rangesArr != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    int partition = 0;\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n      postingsEnum = BitsFilteredPostingsEnum.wrap(postingsEnum, liveDocs);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (rangesArr == null) {\n          if (delete) {\n            docSets[partition].clear(doc);\n          } else {\n            docSets[partition].set(doc);\n          }\n          partition = (partition + 1) % numPieces;\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              if (delete) {\n                docSets[i].clear(doc);\n              } else {\n                docSets[i].set(doc);\n              }\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","sourceOld":"  FixedBitSet[] split(LeafReaderContext readerContext) throws IOException {\n    LeafReader reader = readerContext.reader();\n    FixedBitSet[] docSets = new FixedBitSet[numPieces];\n    for (int i=0; i<docSets.length; i++) {\n      docSets[i] = new FixedBitSet(reader.maxDoc());\n    }\n    Bits liveDocs = reader.getLiveDocs();\n\n    Terms terms = reader.terms(field.getName());\n    TermsEnum termsEnum = terms==null ? null : terms.iterator();\n    if (termsEnum == null) return docSets;\n\n    BytesRef term = null;\n    PostingsEnum postingsEnum = null;\n\n    int[] docsMatchingRanges = null;\n    if (ranges != null) {\n      // +1 because documents can belong to *zero*, one, several or all ranges in rangesArr\n      docsMatchingRanges = new int[rangesArr.length+1];\n    }\n\n    CharsRefBuilder idRef = new CharsRefBuilder();\n    for (;;) {\n      term = termsEnum.next();\n      if (term == null) break;\n\n      // figure out the hash for the term\n\n      // FUTURE: if conversion to strings costs too much, we could\n      // specialize and use the hash function that can work over bytes.\n      field.getType().indexedToReadable(term, idRef);\n      String idString = idRef.toString();\n\n      if (splitKey != null) {\n        // todo have composite routers support these kind of things instead\n        String part1 = getRouteKey(idString);\n        if (part1 == null)\n          continue;\n        if (!splitKey.equals(part1))  {\n          continue;\n        }\n      }\n\n      int hash = 0;\n      if (hashRouter != null) {\n        hash = hashRouter.sliceHash(idString, null, null, null);\n      }\n\n      postingsEnum = termsEnum.postings(postingsEnum, PostingsEnum.NONE);\n      postingsEnum = BitsFilteredPostingsEnum.wrap(postingsEnum, liveDocs);\n      for (;;) {\n        int doc = postingsEnum.nextDoc();\n        if (doc == DocIdSetIterator.NO_MORE_DOCS) break;\n        if (ranges == null) {\n          docSets[currPartition].set(doc);\n          currPartition = (currPartition + 1) % numPieces;\n        } else  {\n          int matchingRangesCount = 0;\n          for (int i=0; i<rangesArr.length; i++) {      // inner-loop: use array here for extra speed.\n            if (rangesArr[i].includes(hash)) {\n              docSets[i].set(doc);\n              ++matchingRangesCount;\n            }\n          }\n          docsMatchingRanges[matchingRangesCount]++;\n        }\n      }\n    }\n\n    if (docsMatchingRanges != null) {\n      for (int ii = 0; ii < docsMatchingRanges.length; ii++) {\n        if (0 == docsMatchingRanges[ii]) continue;\n        switch (ii) {\n          case 0:\n            // document loss\n            log.error(\"Splitting {}: {} documents belong to no shards and will be dropped\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          case 1:\n            // normal case, each document moves to one of the sub-shards\n            log.info(\"Splitting {}: {} documents will move into a sub-shard\",\n                reader, docsMatchingRanges[ii]);\n            break;\n          default:\n            // document duplication\n            log.error(\"Splitting {}: {} documents will be moved to multiple ({}) sub-shards\",\n                reader, docsMatchingRanges[ii], ii);\n            break;\n        }\n      }\n    }\n\n    return docSets;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["4b30fb46e65ee1ae807d474716cbb39e0c54b74f"],"4b30fb46e65ee1ae807d474716cbb39e0c54b74f":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"51f5280f31484820499077f41fcdfe92d527d9dc":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"20c968c14aace7cf49843bf2c1fafc7fd3845659":["28288370235ed02234a64753cdbf0c6ec096304a"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"28288370235ed02234a64753cdbf0c6ec096304a":["0f4464508ee83288c8c4585b533f9faaa93aa314","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["0f4464508ee83288c8c4585b533f9faaa93aa314","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["20c968c14aace7cf49843bf2c1fafc7fd3845659"]},"commit2Childs":{"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["4b30fb46e65ee1ae807d474716cbb39e0c54b74f"],"e73063b92d958076ef4ae8beb5f493e8ccdcecb4":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"4b30fb46e65ee1ae807d474716cbb39e0c54b74f":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e73063b92d958076ef4ae8beb5f493e8ccdcecb4"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["51f5280f31484820499077f41fcdfe92d527d9dc"],"20c968c14aace7cf49843bf2c1fafc7fd3845659":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"28288370235ed02234a64753cdbf0c6ec096304a":["20c968c14aace7cf49843bf2c1fafc7fd3845659"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}