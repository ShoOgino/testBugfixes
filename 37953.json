{"path":"src/java/org/apache/lucene/index/DocumentsWriter#appendPostings(DocumentsWriterFieldData[],TermInfosWriter,IndexOutput,IndexOutput).mjava","commits":[{"id":"5a0af3a442be522899177e5e11384a45a6784a3f","date":1205348952,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#appendPostings(DocumentsWriterFieldData[],TermInfosWriter,IndexOutput,IndexOutput).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#appendPostings(ThreadState.FieldData[],TermInfosWriter,IndexOutput,IndexOutput).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(DocumentsWriterFieldData[] fields,\n                      TermInfosWriter termsOut,\n                      IndexOutput freqOut,\n                      IndexOutput proxOut)\n    throws CorruptIndexException, IOException {\n\n    final int fieldNumber = fields[0].fieldInfo.number;\n    int numFields = fields.length;\n\n    final DocumentsWriterFieldMergeState[] mergeStates = new DocumentsWriterFieldMergeState[numFields];\n\n    for(int i=0;i<numFields;i++) {\n      DocumentsWriterFieldMergeState fms = mergeStates[i] = new DocumentsWriterFieldMergeState();\n      fms.field = fields[i];\n      fms.postings = fms.field.sortPostings();\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final int skipInterval = termsOut.skipInterval;\n    currentFieldStorePayloads = fields[0].fieldInfo.storePayloads;\n\n    DocumentsWriterFieldMergeState[] termStates = new DocumentsWriterFieldMergeState[numFields];\n\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      for(int i=1;i<numFields;i++) {\n        final char[] text = mergeStates[i].text;\n        final int textOffset = mergeStates[i].textOffset;\n        final int cmp = compareText(text, textOffset, termStates[0].text, termStates[0].textOffset);\n\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0)\n          termStates[numToMerge++] = mergeStates[i];\n      }\n\n      int df = 0;\n      int lastPayloadLength = -1;\n\n      int lastDoc = 0;\n\n      final char[] text = termStates[0].text;\n      final int start = termStates[0].textOffset;\n      int pos = start;\n      while(text[pos] != 0xffff)\n        pos++;\n\n      long freqPointer = freqOut.getFilePointer();\n      long proxPointer = proxOut.getFilePointer();\n\n      skipListWriter.resetSkip();\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      while(numToMerge > 0) {\n        \n        if ((++df % skipInterval) == 0) {\n          skipListWriter.setSkipData(lastDoc, currentFieldStorePayloads, lastPayloadLength);\n          skipListWriter.bufferSkip(df);\n        }\n\n        DocumentsWriterFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++)\n          if (termStates[i].docID < minState.docID)\n            minState = termStates[i];\n\n        final int doc = minState.docID;\n        final int termDocFreq = minState.termFreq;\n\n        assert doc < numDocsInRAM;\n        assert doc > lastDoc || df == 1;\n\n        final int newDocCode = (doc-lastDoc)<<1;\n        lastDoc = doc;\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        for(int j=0;j<termDocFreq;j++) {\n          final int code = prox.readVInt();\n          if (currentFieldStorePayloads) {\n            final int payloadLength;\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n            } else\n              payloadLength = 0;\n            if (payloadLength != lastPayloadLength) {\n              proxOut.writeVInt(code|1);\n              proxOut.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            } else\n              proxOut.writeVInt(code & (~1));\n            if (payloadLength > 0)\n              copyBytes(prox, proxOut, payloadLength);\n          } else {\n            assert 0 == (code & 1);\n            proxOut.writeVInt(code>>1);\n          }\n        }\n\n        if (1 == termDocFreq) {\n          freqOut.writeVInt(newDocCode|1);\n        } else {\n          freqOut.writeVInt(newDocCode);\n          freqOut.writeVInt(termDocFreq);\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          for(int i=0;i<numToMerge;i++)\n            if (termStates[i] != minState)\n              termStates[upto++] = termStates[i];\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert df > 0;\n\n      // Done merging this term\n\n      long skipPointer = skipListWriter.writeSkip(freqOut);\n\n      // Write term\n      termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));\n      termsOut.add(fieldNumber, text, start, pos-start, termInfo);\n    }\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(ThreadState.FieldData[] fields,\n                      TermInfosWriter termsOut,\n                      IndexOutput freqOut,\n                      IndexOutput proxOut)\n    throws CorruptIndexException, IOException {\n\n    final int fieldNumber = fields[0].fieldInfo.number;\n    int numFields = fields.length;\n\n    final FieldMergeState[] mergeStates = new FieldMergeState[numFields];\n\n    for(int i=0;i<numFields;i++) {\n      FieldMergeState fms = mergeStates[i] = new FieldMergeState();\n      fms.field = fields[i];\n      fms.postings = fms.field.sortPostings();\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final int skipInterval = termsOut.skipInterval;\n    currentFieldStorePayloads = fields[0].fieldInfo.storePayloads;\n\n    FieldMergeState[] termStates = new FieldMergeState[numFields];\n\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      for(int i=1;i<numFields;i++) {\n        final char[] text = mergeStates[i].text;\n        final int textOffset = mergeStates[i].textOffset;\n        final int cmp = compareText(text, textOffset, termStates[0].text, termStates[0].textOffset);\n\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0)\n          termStates[numToMerge++] = mergeStates[i];\n      }\n\n      int df = 0;\n      int lastPayloadLength = -1;\n\n      int lastDoc = 0;\n\n      final char[] text = termStates[0].text;\n      final int start = termStates[0].textOffset;\n      int pos = start;\n      while(text[pos] != 0xffff)\n        pos++;\n\n      long freqPointer = freqOut.getFilePointer();\n      long proxPointer = proxOut.getFilePointer();\n\n      skipListWriter.resetSkip();\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      while(numToMerge > 0) {\n        \n        if ((++df % skipInterval) == 0) {\n          skipListWriter.setSkipData(lastDoc, currentFieldStorePayloads, lastPayloadLength);\n          skipListWriter.bufferSkip(df);\n        }\n\n        FieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++)\n          if (termStates[i].docID < minState.docID)\n            minState = termStates[i];\n\n        final int doc = minState.docID;\n        final int termDocFreq = minState.termFreq;\n\n        assert doc < numDocsInRAM;\n        assert doc > lastDoc || df == 1;\n\n        final int newDocCode = (doc-lastDoc)<<1;\n        lastDoc = doc;\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        for(int j=0;j<termDocFreq;j++) {\n          final int code = prox.readVInt();\n          if (currentFieldStorePayloads) {\n            final int payloadLength;\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n            } else\n              payloadLength = 0;\n            if (payloadLength != lastPayloadLength) {\n              proxOut.writeVInt(code|1);\n              proxOut.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            } else\n              proxOut.writeVInt(code & (~1));\n            if (payloadLength > 0)\n              copyBytes(prox, proxOut, payloadLength);\n          } else {\n            assert 0 == (code & 1);\n            proxOut.writeVInt(code>>1);\n          }\n        }\n\n        if (1 == termDocFreq) {\n          freqOut.writeVInt(newDocCode|1);\n        } else {\n          freqOut.writeVInt(newDocCode);\n          freqOut.writeVInt(termDocFreq);\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          for(int i=0;i<numToMerge;i++)\n            if (termStates[i] != minState)\n              termStates[upto++] = termStates[i];\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert df > 0;\n\n      // Done merging this term\n\n      long skipPointer = skipListWriter.writeSkip(freqOut);\n\n      // Write term\n      termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));\n      termsOut.add(fieldNumber, text, start, pos-start, termInfo);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4","date":1206538765,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter#appendPostings(DocumentsWriterFieldData[],TermInfosWriter,IndexOutput,IndexOutput).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#appendPostings(DocumentsWriterFieldData[],TermInfosWriter,IndexOutput,IndexOutput).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(DocumentsWriterFieldData[] fields,\n                      TermInfosWriter termsOut,\n                      IndexOutput freqOut,\n                      IndexOutput proxOut)\n    throws CorruptIndexException, IOException {\n\n    final int fieldNumber = fields[0].fieldInfo.number;\n    int numFields = fields.length;\n\n    final DocumentsWriterFieldMergeState[] mergeStates = new DocumentsWriterFieldMergeState[numFields];\n\n    for(int i=0;i<numFields;i++) {\n      DocumentsWriterFieldMergeState fms = mergeStates[i] = new DocumentsWriterFieldMergeState();\n      fms.field = fields[i];\n      fms.postings = fms.field.sortPostings();\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final int skipInterval = termsOut.skipInterval;\n    currentFieldStorePayloads = fields[0].fieldInfo.storePayloads;\n\n    DocumentsWriterFieldMergeState[] termStates = new DocumentsWriterFieldMergeState[numFields];\n\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      for(int i=1;i<numFields;i++) {\n        final char[] text = mergeStates[i].text;\n        final int textOffset = mergeStates[i].textOffset;\n        final int cmp = compareText(text, textOffset, termStates[0].text, termStates[0].textOffset);\n\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0)\n          termStates[numToMerge++] = mergeStates[i];\n      }\n\n      int df = 0;\n      int lastPayloadLength = -1;\n\n      int lastDoc = 0;\n\n      final char[] text = termStates[0].text;\n      final int start = termStates[0].textOffset;\n\n      long freqPointer = freqOut.getFilePointer();\n      long proxPointer = proxOut.getFilePointer();\n\n      skipListWriter.resetSkip();\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      while(numToMerge > 0) {\n        \n        if ((++df % skipInterval) == 0) {\n          skipListWriter.setSkipData(lastDoc, currentFieldStorePayloads, lastPayloadLength);\n          skipListWriter.bufferSkip(df);\n        }\n\n        DocumentsWriterFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++)\n          if (termStates[i].docID < minState.docID)\n            minState = termStates[i];\n\n        final int doc = minState.docID;\n        final int termDocFreq = minState.termFreq;\n\n        assert doc < numDocsInRAM;\n        assert doc > lastDoc || df == 1;\n\n        final int newDocCode = (doc-lastDoc)<<1;\n        lastDoc = doc;\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        for(int j=0;j<termDocFreq;j++) {\n          final int code = prox.readVInt();\n          if (currentFieldStorePayloads) {\n            final int payloadLength;\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n            } else\n              payloadLength = 0;\n            if (payloadLength != lastPayloadLength) {\n              proxOut.writeVInt(code|1);\n              proxOut.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            } else\n              proxOut.writeVInt(code & (~1));\n            if (payloadLength > 0)\n              copyBytes(prox, proxOut, payloadLength);\n          } else {\n            assert 0 == (code & 1);\n            proxOut.writeVInt(code>>1);\n          }\n        }\n\n        if (1 == termDocFreq) {\n          freqOut.writeVInt(newDocCode|1);\n        } else {\n          freqOut.writeVInt(newDocCode);\n          freqOut.writeVInt(termDocFreq);\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          for(int i=0;i<numToMerge;i++)\n            if (termStates[i] != minState)\n              termStates[upto++] = termStates[i];\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert df > 0;\n\n      // Done merging this term\n\n      long skipPointer = skipListWriter.writeSkip(freqOut);\n\n      // Write term\n      termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));\n\n      // TODO: we could do this incrementally\n      UnicodeUtil.UTF16toUTF8(text, start, termsUTF8);\n\n      // TODO: we could save O(n) re-scan of the term by\n      // computing the shared prefix with the last term\n      // while during the UTF8 encoding\n      termsOut.add(fieldNumber,\n                   termsUTF8.result,\n                   termsUTF8.length,\n                   termInfo);\n    }\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(DocumentsWriterFieldData[] fields,\n                      TermInfosWriter termsOut,\n                      IndexOutput freqOut,\n                      IndexOutput proxOut)\n    throws CorruptIndexException, IOException {\n\n    final int fieldNumber = fields[0].fieldInfo.number;\n    int numFields = fields.length;\n\n    final DocumentsWriterFieldMergeState[] mergeStates = new DocumentsWriterFieldMergeState[numFields];\n\n    for(int i=0;i<numFields;i++) {\n      DocumentsWriterFieldMergeState fms = mergeStates[i] = new DocumentsWriterFieldMergeState();\n      fms.field = fields[i];\n      fms.postings = fms.field.sortPostings();\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final int skipInterval = termsOut.skipInterval;\n    currentFieldStorePayloads = fields[0].fieldInfo.storePayloads;\n\n    DocumentsWriterFieldMergeState[] termStates = new DocumentsWriterFieldMergeState[numFields];\n\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      for(int i=1;i<numFields;i++) {\n        final char[] text = mergeStates[i].text;\n        final int textOffset = mergeStates[i].textOffset;\n        final int cmp = compareText(text, textOffset, termStates[0].text, termStates[0].textOffset);\n\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0)\n          termStates[numToMerge++] = mergeStates[i];\n      }\n\n      int df = 0;\n      int lastPayloadLength = -1;\n\n      int lastDoc = 0;\n\n      final char[] text = termStates[0].text;\n      final int start = termStates[0].textOffset;\n      int pos = start;\n      while(text[pos] != 0xffff)\n        pos++;\n\n      long freqPointer = freqOut.getFilePointer();\n      long proxPointer = proxOut.getFilePointer();\n\n      skipListWriter.resetSkip();\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      while(numToMerge > 0) {\n        \n        if ((++df % skipInterval) == 0) {\n          skipListWriter.setSkipData(lastDoc, currentFieldStorePayloads, lastPayloadLength);\n          skipListWriter.bufferSkip(df);\n        }\n\n        DocumentsWriterFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++)\n          if (termStates[i].docID < minState.docID)\n            minState = termStates[i];\n\n        final int doc = minState.docID;\n        final int termDocFreq = minState.termFreq;\n\n        assert doc < numDocsInRAM;\n        assert doc > lastDoc || df == 1;\n\n        final int newDocCode = (doc-lastDoc)<<1;\n        lastDoc = doc;\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        for(int j=0;j<termDocFreq;j++) {\n          final int code = prox.readVInt();\n          if (currentFieldStorePayloads) {\n            final int payloadLength;\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n            } else\n              payloadLength = 0;\n            if (payloadLength != lastPayloadLength) {\n              proxOut.writeVInt(code|1);\n              proxOut.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            } else\n              proxOut.writeVInt(code & (~1));\n            if (payloadLength > 0)\n              copyBytes(prox, proxOut, payloadLength);\n          } else {\n            assert 0 == (code & 1);\n            proxOut.writeVInt(code>>1);\n          }\n        }\n\n        if (1 == termDocFreq) {\n          freqOut.writeVInt(newDocCode|1);\n        } else {\n          freqOut.writeVInt(newDocCode);\n          freqOut.writeVInt(termDocFreq);\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          for(int i=0;i<numToMerge;i++)\n            if (termStates[i] != minState)\n              termStates[upto++] = termStates[i];\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert df > 0;\n\n      // Done merging this term\n\n      long skipPointer = skipListWriter.writeSkip(freqOut);\n\n      // Write term\n      termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));\n      termsOut.add(fieldNumber, text, start, pos-start, termInfo);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/FreqProxTermsWriter#appendPostings(DocumentsWriter.FlushState,FreqProxTermsWriterPerField[],TermInfosWriter,IndexOutput,IndexOutput,DefaultSkipListWriter).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter#appendPostings(DocumentsWriterFieldData[],TermInfosWriter,IndexOutput,IndexOutput).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(final DocumentsWriter.FlushState flushState,\n                      FreqProxTermsWriterPerField[] fields,\n                      TermInfosWriter termsOut,\n                      IndexOutput freqOut,\n                      IndexOutput proxOut,\n                      DefaultSkipListWriter skipListWriter)\n    throws CorruptIndexException, IOException {\n\n    final int fieldNumber = fields[0].fieldInfo.number;\n    int numFields = fields.length;\n\n    final FreqProxFieldMergeState[] mergeStates = new FreqProxFieldMergeState[numFields];\n\n    for(int i=0;i<numFields;i++) {\n      FreqProxFieldMergeState fms = mergeStates[i] = new FreqProxFieldMergeState(fields[i]);\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final int skipInterval = termsOut.skipInterval;\n    final boolean currentFieldStorePayloads = fields[0].fieldInfo.storePayloads;\n\n    FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];\n\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      for(int i=1;i<numFields;i++) {\n        final char[] text = mergeStates[i].text;\n        final int textOffset = mergeStates[i].textOffset;\n        final int cmp = compareText(text, textOffset, termStates[0].text, termStates[0].textOffset);\n\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0)\n          termStates[numToMerge++] = mergeStates[i];\n      }\n\n      int df = 0;\n      int lastPayloadLength = -1;\n\n      int lastDoc = 0;\n\n      final char[] text = termStates[0].text;\n      final int start = termStates[0].textOffset;\n\n      long freqPointer = freqOut.getFilePointer();\n      long proxPointer = proxOut.getFilePointer();\n\n      skipListWriter.resetSkip();\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      while(numToMerge > 0) {\n        \n        if ((++df % skipInterval) == 0) {\n          skipListWriter.setSkipData(lastDoc, currentFieldStorePayloads, lastPayloadLength);\n          skipListWriter.bufferSkip(df);\n        }\n\n        FreqProxFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++)\n          if (termStates[i].docID < minState.docID)\n            minState = termStates[i];\n\n        final int doc = minState.docID;\n        final int termDocFreq = minState.termFreq;\n\n        assert doc < flushState.numDocsInRAM;\n        assert doc > lastDoc || df == 1;\n\n        final int newDocCode = (doc-lastDoc)<<1;\n\n        lastDoc = doc;\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        for(int j=0;j<termDocFreq;j++) {\n          final int code = prox.readVInt();\n          if (currentFieldStorePayloads) {\n            final int payloadLength;\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n            } else\n              payloadLength = 0;\n            if (payloadLength != lastPayloadLength) {\n              proxOut.writeVInt(code|1);\n              proxOut.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            } else\n              proxOut.writeVInt(code & (~1));\n            if (payloadLength > 0)\n              copyBytes(prox, proxOut, payloadLength);\n          } else {\n            assert 0 == (code & 1);\n            proxOut.writeVInt(code>>1);\n          }\n        }\n\n        if (1 == termDocFreq) {\n          freqOut.writeVInt(newDocCode|1);\n        } else {\n          freqOut.writeVInt(newDocCode);\n          freqOut.writeVInt(termDocFreq);\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          for(int i=0;i<numToMerge;i++)\n            if (termStates[i] != minState)\n              termStates[upto++] = termStates[i];\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert df > 0;\n\n      // Done merging this term\n\n      long skipPointer = skipListWriter.writeSkip(freqOut);\n\n      // Write term\n      termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));\n\n      // TODO: we could do this incrementally\n      UnicodeUtil.UTF16toUTF8(text, start, termsUTF8);\n\n      // TODO: we could save O(n) re-scan of the term by\n      // computing the shared prefix with the last term\n      // while during the UTF8 encoding\n      termsOut.add(fieldNumber,\n                   termsUTF8.result,\n                   termsUTF8.length,\n                   termInfo);\n    }\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void appendPostings(DocumentsWriterFieldData[] fields,\n                      TermInfosWriter termsOut,\n                      IndexOutput freqOut,\n                      IndexOutput proxOut)\n    throws CorruptIndexException, IOException {\n\n    final int fieldNumber = fields[0].fieldInfo.number;\n    int numFields = fields.length;\n\n    final DocumentsWriterFieldMergeState[] mergeStates = new DocumentsWriterFieldMergeState[numFields];\n\n    for(int i=0;i<numFields;i++) {\n      DocumentsWriterFieldMergeState fms = mergeStates[i] = new DocumentsWriterFieldMergeState();\n      fms.field = fields[i];\n      fms.postings = fms.field.sortPostings();\n\n      assert fms.field.fieldInfo == fields[0].fieldInfo;\n\n      // Should always be true\n      boolean result = fms.nextTerm();\n      assert result;\n    }\n\n    final int skipInterval = termsOut.skipInterval;\n    currentFieldStorePayloads = fields[0].fieldInfo.storePayloads;\n\n    DocumentsWriterFieldMergeState[] termStates = new DocumentsWriterFieldMergeState[numFields];\n\n    while(numFields > 0) {\n\n      // Get the next term to merge\n      termStates[0] = mergeStates[0];\n      int numToMerge = 1;\n\n      for(int i=1;i<numFields;i++) {\n        final char[] text = mergeStates[i].text;\n        final int textOffset = mergeStates[i].textOffset;\n        final int cmp = compareText(text, textOffset, termStates[0].text, termStates[0].textOffset);\n\n        if (cmp < 0) {\n          termStates[0] = mergeStates[i];\n          numToMerge = 1;\n        } else if (cmp == 0)\n          termStates[numToMerge++] = mergeStates[i];\n      }\n\n      int df = 0;\n      int lastPayloadLength = -1;\n\n      int lastDoc = 0;\n\n      final char[] text = termStates[0].text;\n      final int start = termStates[0].textOffset;\n\n      long freqPointer = freqOut.getFilePointer();\n      long proxPointer = proxOut.getFilePointer();\n\n      skipListWriter.resetSkip();\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      while(numToMerge > 0) {\n        \n        if ((++df % skipInterval) == 0) {\n          skipListWriter.setSkipData(lastDoc, currentFieldStorePayloads, lastPayloadLength);\n          skipListWriter.bufferSkip(df);\n        }\n\n        DocumentsWriterFieldMergeState minState = termStates[0];\n        for(int i=1;i<numToMerge;i++)\n          if (termStates[i].docID < minState.docID)\n            minState = termStates[i];\n\n        final int doc = minState.docID;\n        final int termDocFreq = minState.termFreq;\n\n        assert doc < numDocsInRAM;\n        assert doc > lastDoc || df == 1;\n\n        final int newDocCode = (doc-lastDoc)<<1;\n        lastDoc = doc;\n\n        final ByteSliceReader prox = minState.prox;\n\n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n        for(int j=0;j<termDocFreq;j++) {\n          final int code = prox.readVInt();\n          if (currentFieldStorePayloads) {\n            final int payloadLength;\n            if ((code & 1) != 0) {\n              // This position has a payload\n              payloadLength = prox.readVInt();\n            } else\n              payloadLength = 0;\n            if (payloadLength != lastPayloadLength) {\n              proxOut.writeVInt(code|1);\n              proxOut.writeVInt(payloadLength);\n              lastPayloadLength = payloadLength;\n            } else\n              proxOut.writeVInt(code & (~1));\n            if (payloadLength > 0)\n              copyBytes(prox, proxOut, payloadLength);\n          } else {\n            assert 0 == (code & 1);\n            proxOut.writeVInt(code>>1);\n          }\n        }\n\n        if (1 == termDocFreq) {\n          freqOut.writeVInt(newDocCode|1);\n        } else {\n          freqOut.writeVInt(newDocCode);\n          freqOut.writeVInt(termDocFreq);\n        }\n\n        if (!minState.nextDoc()) {\n\n          // Remove from termStates\n          int upto = 0;\n          for(int i=0;i<numToMerge;i++)\n            if (termStates[i] != minState)\n              termStates[upto++] = termStates[i];\n          numToMerge--;\n          assert upto == numToMerge;\n\n          // Advance this state to the next term\n\n          if (!minState.nextTerm()) {\n            // OK, no more terms, so remove from mergeStates\n            // as well\n            upto = 0;\n            for(int i=0;i<numFields;i++)\n              if (mergeStates[i] != minState)\n                mergeStates[upto++] = mergeStates[i];\n            numFields--;\n            assert upto == numFields;\n          }\n        }\n      }\n\n      assert df > 0;\n\n      // Done merging this term\n\n      long skipPointer = skipListWriter.writeSkip(freqOut);\n\n      // Write term\n      termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));\n\n      // TODO: we could do this incrementally\n      UnicodeUtil.UTF16toUTF8(text, start, termsUTF8);\n\n      // TODO: we could save O(n) re-scan of the term by\n      // computing the shared prefix with the last term\n      // while during the UTF8 encoding\n      termsOut.add(fieldNumber,\n                   termsUTF8.result,\n                   termsUTF8.length,\n                   termInfo);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4":["5a0af3a442be522899177e5e11384a45a6784a3f"],"5a0af3a442be522899177e5e11384a45a6784a3f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5350389bf83287111f7760b9e3db3af8e3648474":["dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5350389bf83287111f7760b9e3db3af8e3648474"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5a0af3a442be522899177e5e11384a45a6784a3f"],"dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4":["5350389bf83287111f7760b9e3db3af8e3648474"],"5a0af3a442be522899177e5e11384a45a6784a3f":["dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4"],"5350389bf83287111f7760b9e3db3af8e3648474":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}