{"path":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","commits":[{"id":"c45bbf26db88631f7a389cbff0f4eab70f55ec64","date":1303271007,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","pathOld":"/dev/null","sourceNew":"  public void testFilter() throws Exception {\r    TokenStream stream = new MockTokenizer(new StringReader(\r        \"short toolong evenmuchlongertext a ab toolong foo\"),\r        MockTokenizer.WHITESPACE, false);\r    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\r    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\r    \r    stream = new MockTokenizer(new StringReader(\r    \"short toolong evenmuchlongertext a ab toolong foo\"),\r    MockTokenizer.WHITESPACE, false);\r    filter = new OffsetLimitTokenFilter(stream, 12);\r    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\r    \r    stream = new MockTokenizer(new StringReader(\r        \"short toolong evenmuchlongertext a ab toolong foo\"),\r        MockTokenizer.WHITESPACE, false);\r    filter = new OffsetLimitTokenFilter(stream, 30);\r    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\r        \"evenmuchlongertext\"});\r    \r    \r    checkOneTermReuse(new Analyzer() {\r      \r      @Override\r      public TokenStream tokenStream(String fieldName, Reader reader) {\r        return new OffsetLimitTokenFilter(new MockTokenizer(reader,\r            MockTokenizer.WHITESPACE, false), 10);\r      }\r    }, \"llenges\", \"llenges\");\r  }\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d493718201f0d0c54c773fb323d87bbd2fbffe41","date":1303546048,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","pathOld":"/dev/null","sourceNew":"  public void testFilter() throws Exception {\r    TokenStream stream = new MockTokenizer(new StringReader(\r        \"short toolong evenmuchlongertext a ab toolong foo\"),\r        MockTokenizer.WHITESPACE, false);\r    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\r    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\r    \r    stream = new MockTokenizer(new StringReader(\r    \"short toolong evenmuchlongertext a ab toolong foo\"),\r    MockTokenizer.WHITESPACE, false);\r    filter = new OffsetLimitTokenFilter(stream, 12);\r    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\r    \r    stream = new MockTokenizer(new StringReader(\r        \"short toolong evenmuchlongertext a ab toolong foo\"),\r        MockTokenizer.WHITESPACE, false);\r    filter = new OffsetLimitTokenFilter(stream, 30);\r    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\r        \"evenmuchlongertext\"});\r    \r    \r    checkOneTermReuse(new Analyzer() {\r      \r      @Override\r      public TokenStream tokenStream(String fieldName, Reader reader) {\r        return new OffsetLimitTokenFilter(new MockTokenizer(reader,\r            MockTokenizer.WHITESPACE, false), 10);\r      }\r    }, \"llenges\", \"llenges\");\r  }\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5112619c0471f45787126cb808fbf19fbe1c4fd7","date":1303933218,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    TokenStream stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    \n    checkOneTermReuse(new Analyzer() {\n      \n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new OffsetLimitTokenFilter(new MockTokenizer(reader,\n            MockTokenizer.WHITESPACE, false), 10);\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\r    TokenStream stream = new MockTokenizer(new StringReader(\r        \"short toolong evenmuchlongertext a ab toolong foo\"),\r        MockTokenizer.WHITESPACE, false);\r    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\r    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\r    \r    stream = new MockTokenizer(new StringReader(\r    \"short toolong evenmuchlongertext a ab toolong foo\"),\r    MockTokenizer.WHITESPACE, false);\r    filter = new OffsetLimitTokenFilter(stream, 12);\r    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\r    \r    stream = new MockTokenizer(new StringReader(\r        \"short toolong evenmuchlongertext a ab toolong foo\"),\r        MockTokenizer.WHITESPACE, false);\r    filter = new OffsetLimitTokenFilter(stream, 30);\r    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\r        \"evenmuchlongertext\"});\r    \r    \r    checkOneTermReuse(new Analyzer() {\r      \r      @Override\r      public TokenStream tokenStream(String fieldName, Reader reader) {\r        return new OffsetLimitTokenFilter(new MockTokenizer(reader,\r            MockTokenizer.WHITESPACE, false), 10);\r      }\r    }, \"llenges\", \"llenges\");\r  }\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae4a6669d73678b396f2a88aef88690f3263bb25","date":1303934929,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    TokenStream stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    \n    checkOneTermReuse(new Analyzer() {\n      \n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new OffsetLimitTokenFilter(new MockTokenizer(reader,\n            MockTokenizer.WHITESPACE, false), 10);\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\r    TokenStream stream = new MockTokenizer(new StringReader(\r        \"short toolong evenmuchlongertext a ab toolong foo\"),\r        MockTokenizer.WHITESPACE, false);\r    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\r    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\r    \r    stream = new MockTokenizer(new StringReader(\r    \"short toolong evenmuchlongertext a ab toolong foo\"),\r    MockTokenizer.WHITESPACE, false);\r    filter = new OffsetLimitTokenFilter(stream, 12);\r    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\r    \r    stream = new MockTokenizer(new StringReader(\r        \"short toolong evenmuchlongertext a ab toolong foo\"),\r        MockTokenizer.WHITESPACE, false);\r    filter = new OffsetLimitTokenFilter(stream, 30);\r    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\r        \"evenmuchlongertext\"});\r    \r    \r    checkOneTermReuse(new Analyzer() {\r      \r      @Override\r      public TokenStream tokenStream(String fieldName, Reader reader) {\r        return new OffsetLimitTokenFilter(new MockTokenizer(reader,\r            MockTokenizer.WHITESPACE, false), 10);\r      }\r    }, \"llenges\", \"llenges\");\r  }\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","pathOld":"/dev/null","sourceNew":"  public void testFilter() throws Exception {\n    TokenStream stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    \n    checkOneTermReuse(new Analyzer() {\n      \n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new OffsetLimitTokenFilter(new MockTokenizer(reader,\n            MockTokenizer.WHITESPACE, false), 10);\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","date":1305207152,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    MockTokenizer stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    // TODO: This is not actually testing reuse! (reusableTokenStream is not implemented)\n    checkOneTermReuse(new Analyzer() {\n      \n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false);\n        return new OffsetLimitTokenFilter(tokenizer, 10);\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\n    TokenStream stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    \n    checkOneTermReuse(new Analyzer() {\n      \n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new OffsetLimitTokenFilter(new MockTokenizer(reader,\n            MockTokenizer.WHITESPACE, false), 10);\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    MockTokenizer stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    // TODO: This is not actually testing reuse! (reusableTokenStream is not implemented)\n    checkOneTermReuse(new Analyzer() {\n      \n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false);\n        return new OffsetLimitTokenFilter(tokenizer, 10);\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\n    TokenStream stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    \n    checkOneTermReuse(new Analyzer() {\n      \n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new OffsetLimitTokenFilter(new MockTokenizer(reader,\n            MockTokenizer.WHITESPACE, false), 10);\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","pathOld":"/dev/null","sourceNew":"  public void testFilter() throws Exception {\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    MockTokenizer stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    // TODO: This is not actually testing reuse! (reusableTokenStream is not implemented)\n    checkOneTermReuse(new Analyzer() {\n      \n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false);\n        return new OffsetLimitTokenFilter(tokenizer, 10);\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"53ae89cd75b0acbdfb8890710c6742f3fb80e65d","date":1315806626,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    MockTokenizer stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    checkOneTermReuse(new ReusableAnalyzerBase() {\n      \n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false);\n        return new TokenStreamComponents(tokenizer, new OffsetLimitTokenFilter(tokenizer, 10));\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    MockTokenizer stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    // TODO: This is not actually testing reuse! (reusableTokenStream is not implemented)\n    checkOneTermReuse(new Analyzer() {\n      \n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false);\n        return new OffsetLimitTokenFilter(tokenizer, 10);\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2f49143da0a5d278a72f741432047fcfa6da996e","date":1316927425,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    MockTokenizer stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    checkOneTermReuse(new Analyzer() {\n      \n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false);\n        return new TokenStreamComponents(tokenizer, new OffsetLimitTokenFilter(tokenizer, 10));\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    MockTokenizer stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    checkOneTermReuse(new ReusableAnalyzerBase() {\n      \n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false);\n        return new TokenStreamComponents(tokenizer, new OffsetLimitTokenFilter(tokenizer, 10));\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/OffsetLimitTokenFilterTest#testFilter().mjava","sourceNew":"  public void testFilter() throws Exception {\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    MockTokenizer stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    checkOneTermReuse(new Analyzer() {\n      \n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false);\n        return new TokenStreamComponents(tokenizer, new OffsetLimitTokenFilter(tokenizer, 10));\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","sourceOld":"  public void testFilter() throws Exception {\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    MockTokenizer stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    OffsetLimitTokenFilter filter = new OffsetLimitTokenFilter(stream, 10);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n    \"short toolong evenmuchlongertext a ab toolong foo\"),\n    MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 12);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\"});\n    \n    stream = new MockTokenizer(new StringReader(\n        \"short toolong evenmuchlongertext a ab toolong foo\"),\n        MockTokenizer.WHITESPACE, false);\n    stream.setEnableChecks(false);\n    filter = new OffsetLimitTokenFilter(stream, 30);\n    assertTokenStreamContents(filter, new String[] {\"short\", \"toolong\",\n        \"evenmuchlongertext\"});\n    \n    checkOneTermReuse(new Analyzer() {\n      \n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false);\n        return new TokenStreamComponents(tokenizer, new OffsetLimitTokenFilter(tokenizer, 10));\n      }\n    }, \"llenges\", \"llenges\");\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c45bbf26db88631f7a389cbff0f4eab70f55ec64":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","5112619c0471f45787126cb808fbf19fbe1c4fd7"],"5112619c0471f45787126cb808fbf19fbe1c4fd7":["c45bbf26db88631f7a389cbff0f4eab70f55ec64"],"ae4a6669d73678b396f2a88aef88690f3263bb25":["d493718201f0d0c54c773fb323d87bbd2fbffe41"],"d493718201f0d0c54c773fb323d87bbd2fbffe41":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c45bbf26db88631f7a389cbff0f4eab70f55ec64"],"53ae89cd75b0acbdfb8890710c6742f3fb80e65d":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["2f49143da0a5d278a72f741432047fcfa6da996e"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["5112619c0471f45787126cb808fbf19fbe1c4fd7"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2f49143da0a5d278a72f741432047fcfa6da996e":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"c45bbf26db88631f7a389cbff0f4eab70f55ec64":["5112619c0471f45787126cb808fbf19fbe1c4fd7","d493718201f0d0c54c773fb323d87bbd2fbffe41"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"5112619c0471f45787126cb808fbf19fbe1c4fd7":["135621f3a0670a9394eb563224a3b76cc4dddc0f","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"ae4a6669d73678b396f2a88aef88690f3263bb25":[],"d493718201f0d0c54c773fb323d87bbd2fbffe41":["ae4a6669d73678b396f2a88aef88690f3263bb25"],"53ae89cd75b0acbdfb8890710c6742f3fb80e65d":["2f49143da0a5d278a72f741432047fcfa6da996e"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c45bbf26db88631f7a389cbff0f4eab70f55ec64","135621f3a0670a9394eb563224a3b76cc4dddc0f","d493718201f0d0c54c773fb323d87bbd2fbffe41","a3776dccca01c11e7046323cfad46a3b4a471233"],"2f49143da0a5d278a72f741432047fcfa6da996e":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ae4a6669d73678b396f2a88aef88690f3263bb25","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}