{"path":"lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstRAMDir#assertAgainstRAMDirectory(MemoryIndex).mjava","commits":[{"id":"761333d77c7f29123c00c93b107b743f32f012e6","date":1411986072,"type":1,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstRAMDir#assertAgainstRAMDirectory(MemoryIndex).mjava","pathOld":"lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest#assertAgainstRAMDirectory(MemoryIndex).mjava","sourceNew":"  /**\n   * Build a randomish document for both RAMDirectory and MemoryIndex,\n   * and run all the queries against it.\n   */\n  public void assertAgainstRAMDirectory(MemoryIndex memory) throws Exception {\n    memory.reset();\n    StringBuilder fooField = new StringBuilder();\n    StringBuilder termField = new StringBuilder();\n \n    // add up to 250 terms to field \"foo\"\n    final int numFooTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numFooTerms; i++) {\n      fooField.append(\" \");\n      fooField.append(randomTerm());\n    }\n\n    // add up to 250 terms to field \"term\"\n    final int numTermTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numTermTerms; i++) {\n      termField.append(\" \");\n      termField.append(randomTerm());\n    }\n    \n    Directory ramdir = new RAMDirectory();\n    Analyzer analyzer = randomAnalyzer();\n    IndexWriter writer = new IndexWriter(ramdir,\n                                         new IndexWriterConfig(analyzer).setCodec(TestUtil.alwaysPostingsFormat(TestUtil.getDefaultPostingsFormat())));\n    Document doc = new Document();\n    Field field1 = newTextField(\"foo\", fooField.toString(), Field.Store.NO);\n    Field field2 = newTextField(\"term\", termField.toString(), Field.Store.NO);\n    doc.add(field1);\n    doc.add(field2);\n    writer.addDocument(doc);\n    writer.close();\n    \n    memory.addField(\"foo\", fooField.toString(), analyzer);\n    memory.addField(\"term\", termField.toString(), analyzer);\n    \n    LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();\n    DirectoryReader competitor = DirectoryReader.open(ramdir);\n    duellReaders(competitor, reader);\n    IOUtils.close(reader, competitor);\n    assertAllQueries(memory, ramdir, analyzer);  \n    ramdir.close();    \n  }\n\n","sourceOld":"  /**\n   * Build a randomish document for both RAMDirectory and MemoryIndex,\n   * and run all the queries against it.\n   */\n  public void assertAgainstRAMDirectory(MemoryIndex memory) throws Exception {\n    memory.reset();\n    StringBuilder fooField = new StringBuilder();\n    StringBuilder termField = new StringBuilder();\n \n    // add up to 250 terms to field \"foo\"\n    final int numFooTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numFooTerms; i++) {\n      fooField.append(\" \");\n      fooField.append(randomTerm());\n    }\n\n    // add up to 250 terms to field \"term\"\n    final int numTermTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numTermTerms; i++) {\n      termField.append(\" \");\n      termField.append(randomTerm());\n    }\n    \n    Directory ramdir = new RAMDirectory();\n    Analyzer analyzer = randomAnalyzer();\n    IndexWriter writer = new IndexWriter(ramdir,\n                                         new IndexWriterConfig(analyzer).setCodec(TestUtil.alwaysPostingsFormat(TestUtil.getDefaultPostingsFormat())));\n    Document doc = new Document();\n    Field field1 = newTextField(\"foo\", fooField.toString(), Field.Store.NO);\n    Field field2 = newTextField(\"term\", termField.toString(), Field.Store.NO);\n    doc.add(field1);\n    doc.add(field2);\n    writer.addDocument(doc);\n    writer.close();\n    \n    memory.addField(\"foo\", fooField.toString(), analyzer);\n    memory.addField(\"term\", termField.toString(), analyzer);\n    \n    LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();\n    DirectoryReader competitor = DirectoryReader.open(ramdir);\n    duellReaders(competitor, reader);\n    IOUtils.close(reader, competitor);\n    assertAllQueries(memory, ramdir, analyzer);  \n    ramdir.close();    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d9a47902d6207303f5ed3e7aaca62ca33433af66","date":1412435312,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstRAMDir#assertAgainstRAMDirectory(MemoryIndex).mjava","pathOld":"lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest#assertAgainstRAMDirectory(MemoryIndex).mjava","sourceNew":"  /**\n   * Build a randomish document for both RAMDirectory and MemoryIndex,\n   * and run all the queries against it.\n   */\n  public void assertAgainstRAMDirectory(MemoryIndex memory) throws Exception {\n    memory.reset();\n    StringBuilder fooField = new StringBuilder();\n    StringBuilder termField = new StringBuilder();\n \n    // add up to 250 terms to field \"foo\"\n    final int numFooTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numFooTerms; i++) {\n      fooField.append(\" \");\n      fooField.append(randomTerm());\n    }\n\n    // add up to 250 terms to field \"term\"\n    final int numTermTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numTermTerms; i++) {\n      termField.append(\" \");\n      termField.append(randomTerm());\n    }\n    \n    Directory ramdir = new RAMDirectory();\n    Analyzer analyzer = randomAnalyzer();\n    IndexWriter writer = new IndexWriter(ramdir,\n                                         new IndexWriterConfig(analyzer).setCodec(TestUtil.alwaysPostingsFormat(TestUtil.getDefaultPostingsFormat())));\n    Document doc = new Document();\n    Field field1 = newTextField(\"foo\", fooField.toString(), Field.Store.NO);\n    Field field2 = newTextField(\"term\", termField.toString(), Field.Store.NO);\n    doc.add(field1);\n    doc.add(field2);\n    writer.addDocument(doc);\n    writer.close();\n    \n    memory.addField(\"foo\", fooField.toString(), analyzer);\n    memory.addField(\"term\", termField.toString(), analyzer);\n    \n    LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();\n    DirectoryReader competitor = DirectoryReader.open(ramdir);\n    duellReaders(competitor, reader);\n    IOUtils.close(reader, competitor);\n    assertAllQueries(memory, ramdir, analyzer);  \n    ramdir.close();    \n  }\n\n","sourceOld":"  /**\n   * Build a randomish document for both RAMDirectory and MemoryIndex,\n   * and run all the queries against it.\n   */\n  public void assertAgainstRAMDirectory(MemoryIndex memory) throws Exception {\n    memory.reset();\n    StringBuilder fooField = new StringBuilder();\n    StringBuilder termField = new StringBuilder();\n \n    // add up to 250 terms to field \"foo\"\n    final int numFooTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numFooTerms; i++) {\n      fooField.append(\" \");\n      fooField.append(randomTerm());\n    }\n\n    // add up to 250 terms to field \"term\"\n    final int numTermTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numTermTerms; i++) {\n      termField.append(\" \");\n      termField.append(randomTerm());\n    }\n    \n    Directory ramdir = new RAMDirectory();\n    Analyzer analyzer = randomAnalyzer();\n    IndexWriter writer = new IndexWriter(ramdir,\n                                         new IndexWriterConfig(analyzer).setCodec(TestUtil.alwaysPostingsFormat(TestUtil.getDefaultPostingsFormat())));\n    Document doc = new Document();\n    Field field1 = newTextField(\"foo\", fooField.toString(), Field.Store.NO);\n    Field field2 = newTextField(\"term\", termField.toString(), Field.Store.NO);\n    doc.add(field1);\n    doc.add(field2);\n    writer.addDocument(doc);\n    writer.close();\n    \n    memory.addField(\"foo\", fooField.toString(), analyzer);\n    memory.addField(\"term\", termField.toString(), analyzer);\n    \n    LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();\n    DirectoryReader competitor = DirectoryReader.open(ramdir);\n    duellReaders(competitor, reader);\n    IOUtils.close(reader, competitor);\n    assertAllQueries(memory, ramdir, analyzer);  \n    ramdir.close();    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb","date":1420550360,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstRAMDir#assertAgainstRAMDirectory(MemoryIndex).mjava","pathOld":"lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstRAMDir#assertAgainstRAMDirectory(MemoryIndex).mjava","sourceNew":"  /**\n   * Build a randomish document for both RAMDirectory and MemoryIndex,\n   * and run all the queries against it.\n   */\n  public void assertAgainstRAMDirectory(MemoryIndex memory) throws Exception {\n    memory.reset();\n    StringBuilder fooField = new StringBuilder();\n    StringBuilder termField = new StringBuilder();\n \n    // add up to 250 terms to field \"foo\"\n    final int numFooTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numFooTerms; i++) {\n      fooField.append(\" \");\n      fooField.append(randomTerm());\n    }\n\n    // add up to 250 terms to field \"term\"\n    final int numTermTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numTermTerms; i++) {\n      termField.append(\" \");\n      termField.append(randomTerm());\n    }\n    \n    Directory ramdir = new RAMDirectory();\n    Analyzer analyzer = randomAnalyzer();\n    IndexWriter writer = new IndexWriter(ramdir,\n                                         new IndexWriterConfig(analyzer).setCodec(\n                                             TestUtil.alwaysPostingsFormat(TestUtil.getDefaultPostingsFormat())));\n    Document doc = new Document();\n    Field field1 = newTextField(\"foo\", fooField.toString(), Field.Store.NO);\n    Field field2 = newTextField(\"term\", termField.toString(), Field.Store.NO);\n    doc.add(field1);\n    doc.add(field2);\n    writer.addDocument(doc);\n    writer.close();\n    \n    memory.addField(\"foo\", fooField.toString(), analyzer);\n    memory.addField(\"term\", termField.toString(), analyzer);\n    \n    LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();\n    DirectoryReader competitor = DirectoryReader.open(ramdir);\n    duellReaders(competitor, reader);\n    IOUtils.close(reader, competitor);\n    assertAllQueries(memory, ramdir, analyzer);  \n    ramdir.close();    \n  }\n\n","sourceOld":"  /**\n   * Build a randomish document for both RAMDirectory and MemoryIndex,\n   * and run all the queries against it.\n   */\n  public void assertAgainstRAMDirectory(MemoryIndex memory) throws Exception {\n    memory.reset();\n    StringBuilder fooField = new StringBuilder();\n    StringBuilder termField = new StringBuilder();\n \n    // add up to 250 terms to field \"foo\"\n    final int numFooTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numFooTerms; i++) {\n      fooField.append(\" \");\n      fooField.append(randomTerm());\n    }\n\n    // add up to 250 terms to field \"term\"\n    final int numTermTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numTermTerms; i++) {\n      termField.append(\" \");\n      termField.append(randomTerm());\n    }\n    \n    Directory ramdir = new RAMDirectory();\n    Analyzer analyzer = randomAnalyzer();\n    IndexWriter writer = new IndexWriter(ramdir,\n                                         new IndexWriterConfig(analyzer).setCodec(TestUtil.alwaysPostingsFormat(TestUtil.getDefaultPostingsFormat())));\n    Document doc = new Document();\n    Field field1 = newTextField(\"foo\", fooField.toString(), Field.Store.NO);\n    Field field2 = newTextField(\"term\", termField.toString(), Field.Store.NO);\n    doc.add(field1);\n    doc.add(field2);\n    writer.addDocument(doc);\n    writer.close();\n    \n    memory.addField(\"foo\", fooField.toString(), analyzer);\n    memory.addField(\"term\", termField.toString(), analyzer);\n    \n    LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();\n    DirectoryReader competitor = DirectoryReader.open(ramdir);\n    duellReaders(competitor, reader);\n    IOUtils.close(reader, competitor);\n    assertAllQueries(memory, ramdir, analyzer);  \n    ramdir.close();    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb04834a792874aacf8d8b111a39603c23fbd777","date":1428406678,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstRAMDir#assertAgainstRAMDirectory(MemoryIndex).mjava","pathOld":"lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstRAMDir#assertAgainstRAMDirectory(MemoryIndex).mjava","sourceNew":"  /**\n   * Build a randomish document for both RAMDirectory and MemoryIndex,\n   * and run all the queries against it.\n   */\n  public void assertAgainstRAMDirectory(MemoryIndex memory) throws Exception {\n    memory.reset();\n    StringBuilder fooField = new StringBuilder();\n    StringBuilder termField = new StringBuilder();\n \n    // add up to 250 terms to field \"foo\"\n    final int numFooTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numFooTerms; i++) {\n      fooField.append(\" \");\n      fooField.append(randomTerm());\n    }\n\n    // add up to 250 terms to field \"term\"\n    final int numTermTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numTermTerms; i++) {\n      termField.append(\" \");\n      termField.append(randomTerm());\n    }\n    \n    Directory ramdir = new RAMDirectory();\n    Analyzer analyzer = randomAnalyzer();\n    IndexWriter writer = new IndexWriter(ramdir,\n                                         new IndexWriterConfig(analyzer).setCodec(\n                                             TestUtil.alwaysPostingsFormat(TestUtil.getDefaultPostingsFormat())));\n    Document doc = new Document();\n    Field field1 = newTextField(\"foo\", fooField.toString(), Field.Store.NO);\n    Field field2 = newTextField(\"term\", termField.toString(), Field.Store.NO);\n    doc.add(field1);\n    doc.add(field2);\n    writer.addDocument(doc);\n    writer.close();\n    \n    memory.addField(\"foo\", fooField.toString(), analyzer);\n    memory.addField(\"term\", termField.toString(), analyzer);\n    \n    LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();\n    TestUtil.checkReader(reader);\n    DirectoryReader competitor = DirectoryReader.open(ramdir);\n    duellReaders(competitor, reader);\n    IOUtils.close(reader, competitor);\n    assertAllQueries(memory, ramdir, analyzer);  \n    ramdir.close();    \n  }\n\n","sourceOld":"  /**\n   * Build a randomish document for both RAMDirectory and MemoryIndex,\n   * and run all the queries against it.\n   */\n  public void assertAgainstRAMDirectory(MemoryIndex memory) throws Exception {\n    memory.reset();\n    StringBuilder fooField = new StringBuilder();\n    StringBuilder termField = new StringBuilder();\n \n    // add up to 250 terms to field \"foo\"\n    final int numFooTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numFooTerms; i++) {\n      fooField.append(\" \");\n      fooField.append(randomTerm());\n    }\n\n    // add up to 250 terms to field \"term\"\n    final int numTermTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numTermTerms; i++) {\n      termField.append(\" \");\n      termField.append(randomTerm());\n    }\n    \n    Directory ramdir = new RAMDirectory();\n    Analyzer analyzer = randomAnalyzer();\n    IndexWriter writer = new IndexWriter(ramdir,\n                                         new IndexWriterConfig(analyzer).setCodec(\n                                             TestUtil.alwaysPostingsFormat(TestUtil.getDefaultPostingsFormat())));\n    Document doc = new Document();\n    Field field1 = newTextField(\"foo\", fooField.toString(), Field.Store.NO);\n    Field field2 = newTextField(\"term\", termField.toString(), Field.Store.NO);\n    doc.add(field1);\n    doc.add(field2);\n    writer.addDocument(doc);\n    writer.close();\n    \n    memory.addField(\"foo\", fooField.toString(), analyzer);\n    memory.addField(\"term\", termField.toString(), analyzer);\n    \n    LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();\n    DirectoryReader competitor = DirectoryReader.open(ramdir);\n    duellReaders(competitor, reader);\n    IOUtils.close(reader, competitor);\n    assertAllQueries(memory, ramdir, analyzer);  \n    ramdir.close();    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d77dafd89756a5161d244985903e3487ca109182","date":1548679743,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstDirectory#assertAgainstDirectory(MemoryIndex).mjava","pathOld":"lucene/memory/src/test/org/apache/lucene/index/memory/TestMemoryIndexAgainstRAMDir#assertAgainstRAMDirectory(MemoryIndex).mjava","sourceNew":"  /**\n   * Build a randomish document for both Directory and MemoryIndex,\n   * and run all the queries against it.\n   */\n  public void assertAgainstDirectory(MemoryIndex memory) throws Exception {\n    memory.reset();\n    StringBuilder fooField = new StringBuilder();\n    StringBuilder termField = new StringBuilder();\n \n    // add up to 250 terms to field \"foo\"\n    final int numFooTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numFooTerms; i++) {\n      fooField.append(\" \");\n      fooField.append(randomTerm());\n    }\n\n    // add up to 250 terms to field \"term\"\n    final int numTermTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numTermTerms; i++) {\n      termField.append(\" \");\n      termField.append(randomTerm());\n    }\n    \n    Directory dir = new ByteBuffersDirectory();\n    Analyzer analyzer = randomAnalyzer();\n    IndexWriter writer = new IndexWriter(dir,\n                                         new IndexWriterConfig(analyzer).setCodec(\n                                             TestUtil.alwaysPostingsFormat(TestUtil.getDefaultPostingsFormat())));\n    Document doc = new Document();\n    Field field1 = newTextField(\"foo\", fooField.toString(), Field.Store.NO);\n    Field field2 = newTextField(\"term\", termField.toString(), Field.Store.NO);\n    doc.add(field1);\n    doc.add(field2);\n    writer.addDocument(doc);\n    writer.close();\n    \n    memory.addField(\"foo\", fooField.toString(), analyzer);\n    memory.addField(\"term\", termField.toString(), analyzer);\n    \n    LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();\n    TestUtil.checkReader(reader);\n    DirectoryReader competitor = DirectoryReader.open(dir);\n    duellReaders(competitor, reader);\n    IOUtils.close(reader, competitor);\n    assertAllQueries(memory, dir, analyzer);\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Build a randomish document for both RAMDirectory and MemoryIndex,\n   * and run all the queries against it.\n   */\n  public void assertAgainstRAMDirectory(MemoryIndex memory) throws Exception {\n    memory.reset();\n    StringBuilder fooField = new StringBuilder();\n    StringBuilder termField = new StringBuilder();\n \n    // add up to 250 terms to field \"foo\"\n    final int numFooTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numFooTerms; i++) {\n      fooField.append(\" \");\n      fooField.append(randomTerm());\n    }\n\n    // add up to 250 terms to field \"term\"\n    final int numTermTerms = random().nextInt(250 * RANDOM_MULTIPLIER);\n    for (int i = 0; i < numTermTerms; i++) {\n      termField.append(\" \");\n      termField.append(randomTerm());\n    }\n    \n    Directory ramdir = new RAMDirectory();\n    Analyzer analyzer = randomAnalyzer();\n    IndexWriter writer = new IndexWriter(ramdir,\n                                         new IndexWriterConfig(analyzer).setCodec(\n                                             TestUtil.alwaysPostingsFormat(TestUtil.getDefaultPostingsFormat())));\n    Document doc = new Document();\n    Field field1 = newTextField(\"foo\", fooField.toString(), Field.Store.NO);\n    Field field2 = newTextField(\"term\", termField.toString(), Field.Store.NO);\n    doc.add(field1);\n    doc.add(field2);\n    writer.addDocument(doc);\n    writer.close();\n    \n    memory.addField(\"foo\", fooField.toString(), analyzer);\n    memory.addField(\"term\", termField.toString(), analyzer);\n    \n    LeafReader reader = (LeafReader) memory.createSearcher().getIndexReader();\n    TestUtil.checkReader(reader);\n    DirectoryReader competitor = DirectoryReader.open(ramdir);\n    duellReaders(competitor, reader);\n    IOUtils.close(reader, competitor);\n    assertAllQueries(memory, ramdir, analyzer);  \n    ramdir.close();    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bb04834a792874aacf8d8b111a39603c23fbd777":["e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d77dafd89756a5161d244985903e3487ca109182":["bb04834a792874aacf8d8b111a39603c23fbd777"],"e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb":["761333d77c7f29123c00c93b107b743f32f012e6"],"d9a47902d6207303f5ed3e7aaca62ca33433af66":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","761333d77c7f29123c00c93b107b743f32f012e6"],"761333d77c7f29123c00c93b107b743f32f012e6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d77dafd89756a5161d244985903e3487ca109182"]},"commit2Childs":{"bb04834a792874aacf8d8b111a39603c23fbd777":["d77dafd89756a5161d244985903e3487ca109182"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d9a47902d6207303f5ed3e7aaca62ca33433af66","761333d77c7f29123c00c93b107b743f32f012e6"],"e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb":["bb04834a792874aacf8d8b111a39603c23fbd777"],"d77dafd89756a5161d244985903e3487ca109182":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d9a47902d6207303f5ed3e7aaca62ca33433af66":[],"761333d77c7f29123c00c93b107b743f32f012e6":["e88ae259732b6a9caf4c8f3a2e5a19c7b54ddbcb","d9a47902d6207303f5ed3e7aaca62ca33433af66"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d9a47902d6207303f5ed3e7aaca62ca33433af66","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}