{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,CloudConfig,Supplier[List[CoreDescriptor]]).mjava","commits":[{"id":"1c560208bc8842ee884b76b08784ccb132f05b48","date":1585344697,"type":1,"author":"Mike","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,CloudConfig,Supplier[List[CoreDescriptor]]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,CloudConfig,CurrentCoreDescriptorProvider).mjava","sourceNew":"  /**\n   * @param cc Core container associated with this controller. cannot be null.\n   * @param zkServerAddress where to connect to the zk server\n   * @param zkClientConnectTimeout timeout in ms\n   * @param cloudConfig configuration for this controller. TODO: possibly redundant with CoreContainer\n   * @param descriptorsSupplier a supplier of the current core descriptors. used to know which cores to re-register on reconnect\n   */\n  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientConnectTimeout, CloudConfig cloudConfig, final Supplier<List<CoreDescriptor>> descriptorsSupplier)\n      throws InterruptedException, TimeoutException, IOException {\n\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n\n    this.cloudConfig = cloudConfig;\n\n    this.genericCoreNodeNames = cloudConfig.getGenericCoreNodeNames();\n\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in\n    // solr.xml to indicate the root context, instead of hostContext=\"\"\n    // which means the default of \"solr\"\n    String localHostContext = trimLeadingAndTrailingSlashes(cloudConfig.getSolrHostContext());\n\n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = cloudConfig.getSolrHostPort();\n    this.hostName = normalizeHostName(cloudConfig.getHost());\n    this.nodeName = generateNodeName(this.hostName, Integer.toString(this.localHostPort), localHostContext);\n    MDCLoggingContext.setNode(nodeName);\n    this.leaderVoteWait = cloudConfig.getLeaderVoteWait();\n    this.leaderConflictResolveWait = cloudConfig.getLeaderConflictResolveWait();\n\n    this.clientTimeout = cloudConfig.getZkClientTimeout();\n    DefaultConnectionStrategy strat = new DefaultConnectionStrategy();\n    String zkACLProviderClass = cloudConfig.getZkACLProviderClass();\n    ZkACLProvider zkACLProvider = null;\n    if (zkACLProviderClass != null && zkACLProviderClass.trim().length() > 0) {\n      zkACLProvider = cc.getResourceLoader().newInstance(zkACLProviderClass, ZkACLProvider.class);\n    } else {\n      zkACLProvider = new DefaultZkACLProvider();\n    }\n\n    String zkCredentialsProviderClass = cloudConfig.getZkCredentialsProviderClass();\n    if (zkCredentialsProviderClass != null && zkCredentialsProviderClass.trim().length() > 0) {\n      strat.setZkCredentialsToAddAutomatically(cc.getResourceLoader().newInstance(zkCredentialsProviderClass, ZkCredentialsProvider.class));\n    } else {\n      strat.setZkCredentialsToAddAutomatically(new DefaultZkCredentialsProvider());\n    }\n    addOnReconnectListener(getConfigDirListener());\n\n    zkClient = new SolrZkClient(zkServerAddress, clientTimeout, zkClientConnectTimeout, strat,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() throws SessionExpiredException {\n            log.info(\"ZooKeeper session re-connected ... refreshing core states after session expiration.\");\n            clearZkCollectionTerms();\n            try {\n              // recreate our watchers first so that they exist even on any problems below\n              zkStateReader.createClusterStateWatchersAndUpdate();\n\n              // this is troublesome - we dont want to kill anything the old\n              // leader accepted\n              // though I guess sync will likely get those updates back? But\n              // only if\n              // he is involved in the sync, and he certainly may not be\n              // ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n\n              // seems we dont need to do this again...\n              // Overseer.createClientNodes(zkClient, getNodeName());\n\n              // start the overseer first as following code may need it's processing\n              if (!zkRunOnly) {\n                ElectionContext context = new OverseerElectionContext(zkClient,\n                    overseer, getNodeName());\n\n                ElectionContext prevContext = overseerElector.getContext();\n                if (prevContext != null) {\n                  prevContext.cancelElection();\n                  prevContext.close();\n                }\n\n                overseerElector.setup(context);\n\n                overseerElector.joinElection(context, true);\n              }\n\n              cc.cancelCoreRecoveries();\n              \n              try {\n                registerAllCoresAsDown(descriptorsSupplier, false);\n              } catch (SessionExpiredException e) {\n                // zk has to reconnect and this will all be tried again\n                throw e;\n              } catch (Exception e) {\n                // this is really best effort - in case of races or failure cases where we now need to be the leader, if anything fails,\n                // just continue\n                log.warn(\"Exception while trying to register all cores as DOWN\", e);\n              } \n\n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n\n              List<CoreDescriptor> descriptors = descriptorsSupplier.get();\n              // re register all descriptors\n              ExecutorService executorService = (cc != null) ? cc.getCoreZkRegisterExecutorService() : null;\n              if (descriptors != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it\n                  // was\n                  // a leader that was expired - as well as what to do about\n                  // leaders/overseers\n                  // with connection loss\n                  try {\n                    // unload solrcores that have been 'failed over'\n                    throwErrorIfReplicaReplaced(descriptor);\n\n                    if (executorService != null) {\n                      executorService.submit(new RegisterCoreAsync(descriptor, true, true));\n                    } else {\n                      register(descriptor.getName(), descriptor, true, true, false);\n                    }\n                  } catch (Exception e) {\n                    SolrException.log(log, \"Error registering SolrCore\", e);\n                  }\n                }\n              }\n\n              // notify any other objects that need to know when the session was re-connected\n              HashSet<OnReconnect> clonedListeners;\n              synchronized (reconnectListeners) {\n                clonedListeners = (HashSet<OnReconnect>)reconnectListeners.clone();\n              }\n              // the OnReconnect operation can be expensive per listener, so do that async in the background\n              for (OnReconnect listener : clonedListeners) {\n                try {\n                  if (executorService != null) {\n                    executorService.submit(new OnReconnectNotifyAsync(listener));\n                  } else {\n                    listener.command();\n                  }\n                } catch (Exception exc) {\n                  // not much we can do here other than warn in the log\n                  log.warn(\"Error when notifying OnReconnect listener \" + listener + \" after session re-connected.\", exc);\n                }\n              }\n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (SessionExpiredException e) {\n              throw e;\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n          }\n\n        }, new BeforeReconnect() {\n\n      @Override\n      public void command() {\n        try {\n          ZkController.this.overseer.close();\n        } catch (Exception e) {\n          log.error(\"Error trying to stop any Overseer threads\", e);\n        }\n        closeOutstandingElections(descriptorsSupplier);\n        markAllAsNotLeader(descriptorsSupplier);\n      }\n    }, zkACLProvider, new ConnectionManager.IsClosed() {\n\n      @Override\n      public boolean isClosed() {\n        return cc.isShutDown();\n      }});\n\n\n    this.overseerRunningMap = Overseer.getRunningMap(zkClient);\n    this.overseerCompletedMap = Overseer.getCompletedMap(zkClient);\n    this.overseerFailureMap = Overseer.getFailureMap(zkClient);\n    this.asyncIdsMap = Overseer.getAsyncIdsMap(zkClient);\n\n    zkStateReader = new ZkStateReader(zkClient, () -> {\n      if (cc != null) cc.securityNodeChanged();\n    });\n\n    init();\n\n    this.overseerJobQueue = overseer.getStateUpdateQueue();\n    this.overseerCollectionQueue = overseer.getCollectionQueue(zkClient);\n    this.overseerConfigSetQueue = overseer.getConfigSetQueue(zkClient);\n    this.sysPropsCacher = new NodesSysPropsCacher(getSolrCloudManager().getNodeStateProvider(),\n        getNodeName(), zkStateReader);\n\n    assert ObjectReleaseTracker.track(this);\n  }\n\n","sourceOld":"  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientConnectTimeout, CloudConfig cloudConfig, final CurrentCoreDescriptorProvider registerOnReconnect)\n      throws InterruptedException, TimeoutException, IOException {\n\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n\n    this.cloudConfig = cloudConfig;\n\n    this.genericCoreNodeNames = cloudConfig.getGenericCoreNodeNames();\n\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in\n    // solr.xml to indicate the root context, instead of hostContext=\"\"\n    // which means the default of \"solr\"\n    String localHostContext = trimLeadingAndTrailingSlashes(cloudConfig.getSolrHostContext());\n\n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = cloudConfig.getSolrHostPort();\n    this.hostName = normalizeHostName(cloudConfig.getHost());\n    this.nodeName = generateNodeName(this.hostName, Integer.toString(this.localHostPort), localHostContext);\n    MDCLoggingContext.setNode(nodeName);\n    this.leaderVoteWait = cloudConfig.getLeaderVoteWait();\n    this.leaderConflictResolveWait = cloudConfig.getLeaderConflictResolveWait();\n\n    this.clientTimeout = cloudConfig.getZkClientTimeout();\n    DefaultConnectionStrategy strat = new DefaultConnectionStrategy();\n    String zkACLProviderClass = cloudConfig.getZkACLProviderClass();\n    ZkACLProvider zkACLProvider = null;\n    if (zkACLProviderClass != null && zkACLProviderClass.trim().length() > 0) {\n      zkACLProvider = cc.getResourceLoader().newInstance(zkACLProviderClass, ZkACLProvider.class);\n    } else {\n      zkACLProvider = new DefaultZkACLProvider();\n    }\n\n    String zkCredentialsProviderClass = cloudConfig.getZkCredentialsProviderClass();\n    if (zkCredentialsProviderClass != null && zkCredentialsProviderClass.trim().length() > 0) {\n      strat.setZkCredentialsToAddAutomatically(cc.getResourceLoader().newInstance(zkCredentialsProviderClass, ZkCredentialsProvider.class));\n    } else {\n      strat.setZkCredentialsToAddAutomatically(new DefaultZkCredentialsProvider());\n    }\n    addOnReconnectListener(getConfigDirListener());\n\n    zkClient = new SolrZkClient(zkServerAddress, clientTimeout, zkClientConnectTimeout, strat,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() throws SessionExpiredException {\n            log.info(\"ZooKeeper session re-connected ... refreshing core states after session expiration.\");\n            clearZkCollectionTerms();\n            try {\n              // recreate our watchers first so that they exist even on any problems below\n              zkStateReader.createClusterStateWatchersAndUpdate();\n\n              // this is troublesome - we dont want to kill anything the old\n              // leader accepted\n              // though I guess sync will likely get those updates back? But\n              // only if\n              // he is involved in the sync, and he certainly may not be\n              // ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n\n              // seems we dont need to do this again...\n              // Overseer.createClientNodes(zkClient, getNodeName());\n\n              // start the overseer first as following code may need it's processing\n              if (!zkRunOnly) {\n                ElectionContext context = new OverseerElectionContext(zkClient,\n                    overseer, getNodeName());\n\n                ElectionContext prevContext = overseerElector.getContext();\n                if (prevContext != null) {\n                  prevContext.cancelElection();\n                  prevContext.close();\n                }\n\n                overseerElector.setup(context);\n                overseerElector.joinElection(context, true);\n              }\n\n              cc.cancelCoreRecoveries();\n              \n              try {\n                registerAllCoresAsDown(registerOnReconnect, false);\n              } catch (SessionExpiredException e) {\n                // zk has to reconnect and this will all be tried again\n                throw e;\n              } catch (Exception e) {\n                // this is really best effort - in case of races or failure cases where we now need to be the leader, if anything fails,\n                // just continue\n                log.warn(\"Exception while trying to register all cores as DOWN\", e);\n              } \n\n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n\n              List<CoreDescriptor> descriptors = registerOnReconnect.getCurrentDescriptors();\n              // re register all descriptors\n              ExecutorService executorService = (cc != null) ? cc.getCoreZkRegisterExecutorService() : null;\n              if (descriptors != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it\n                  // was\n                  // a leader that was expired - as well as what to do about\n                  // leaders/overseers\n                  // with connection loss\n                  try {\n                    // unload solrcores that have been 'failed over'\n                    throwErrorIfReplicaReplaced(descriptor);\n\n                    if (executorService != null) {\n                      executorService.submit(new RegisterCoreAsync(descriptor, true, true));\n                    } else {\n                      register(descriptor.getName(), descriptor, true, true, false);\n                    }\n                  } catch (Exception e) {\n                    SolrException.log(log, \"Error registering SolrCore\", e);\n                  }\n                }\n              }\n\n              // notify any other objects that need to know when the session was re-connected\n              HashSet<OnReconnect> clonedListeners;\n              synchronized (reconnectListeners) {\n                clonedListeners = (HashSet<OnReconnect>)reconnectListeners.clone();\n              }\n              // the OnReconnect operation can be expensive per listener, so do that async in the background\n              for (OnReconnect listener : clonedListeners) {\n                try {\n                  if (executorService != null) {\n                    executorService.submit(new OnReconnectNotifyAsync(listener));\n                  } else {\n                    listener.command();\n                  }\n                } catch (Exception exc) {\n                  // not much we can do here other than warn in the log\n                  log.warn(\"Error when notifying OnReconnect listener \" + listener + \" after session re-connected.\", exc);\n                }\n              }\n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (SessionExpiredException e) {\n              throw e;\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n          }\n\n        }, new BeforeReconnect() {\n\n      @Override\n      public void command() {\n        try {\n          ZkController.this.overseer.close();\n        } catch (Exception e) {\n          log.error(\"Error trying to stop any Overseer threads\", e);\n        }\n        closeOutstandingElections(registerOnReconnect);\n        markAllAsNotLeader(registerOnReconnect);\n      }\n    }, zkACLProvider, new ConnectionManager.IsClosed() {\n\n      @Override\n      public boolean isClosed() {\n        return cc.isShutDown();\n      }});\n\n\n    this.overseerRunningMap = Overseer.getRunningMap(zkClient);\n    this.overseerCompletedMap = Overseer.getCompletedMap(zkClient);\n    this.overseerFailureMap = Overseer.getFailureMap(zkClient);\n    this.asyncIdsMap = Overseer.getAsyncIdsMap(zkClient);\n\n    zkStateReader = new ZkStateReader(zkClient, () -> {\n      if (cc != null) cc.securityNodeChanged();\n    });\n\n    init(registerOnReconnect);\n\n    this.overseerJobQueue = overseer.getStateUpdateQueue();\n    this.overseerCollectionQueue = overseer.getCollectionQueue(zkClient);\n    this.overseerConfigSetQueue = overseer.getConfigSetQueue(zkClient);\n    this.sysPropsCacher = new NodesSysPropsCacher(getSolrCloudManager().getNodeStateProvider(),\n        getNodeName(), zkStateReader);\n\n    assert ObjectReleaseTracker.track(this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad4957cde742defe6db19689abdc267c5d948066","date":1587990850,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,CloudConfig,Supplier[List[CoreDescriptor]]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,CloudConfig,Supplier[List[CoreDescriptor]]).mjava","sourceNew":"  /**\n   * @param cc Core container associated with this controller. cannot be null.\n   * @param zkServerAddress where to connect to the zk server\n   * @param zkClientConnectTimeout timeout in ms\n   * @param cloudConfig configuration for this controller. TODO: possibly redundant with CoreContainer\n   * @param descriptorsSupplier a supplier of the current core descriptors. used to know which cores to re-register on reconnect\n   */\n  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientConnectTimeout, CloudConfig cloudConfig, final Supplier<List<CoreDescriptor>> descriptorsSupplier)\n      throws InterruptedException, TimeoutException, IOException {\n\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n\n    this.cloudConfig = cloudConfig;\n\n    this.genericCoreNodeNames = cloudConfig.getGenericCoreNodeNames();\n\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in\n    // solr.xml to indicate the root context, instead of hostContext=\"\"\n    // which means the default of \"solr\"\n    String localHostContext = trimLeadingAndTrailingSlashes(cloudConfig.getSolrHostContext());\n\n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = cloudConfig.getSolrHostPort();\n    this.hostName = normalizeHostName(cloudConfig.getHost());\n    this.nodeName = generateNodeName(this.hostName, Integer.toString(this.localHostPort), localHostContext);\n    MDCLoggingContext.setNode(nodeName);\n    this.leaderVoteWait = cloudConfig.getLeaderVoteWait();\n    this.leaderConflictResolveWait = cloudConfig.getLeaderConflictResolveWait();\n\n    this.clientTimeout = cloudConfig.getZkClientTimeout();\n    DefaultConnectionStrategy strat = new DefaultConnectionStrategy();\n    String zkACLProviderClass = cloudConfig.getZkACLProviderClass();\n    ZkACLProvider zkACLProvider = null;\n    if (zkACLProviderClass != null && zkACLProviderClass.trim().length() > 0) {\n      zkACLProvider = cc.getResourceLoader().newInstance(zkACLProviderClass, ZkACLProvider.class);\n    } else {\n      zkACLProvider = new DefaultZkACLProvider();\n    }\n\n    String zkCredentialsProviderClass = cloudConfig.getZkCredentialsProviderClass();\n    if (zkCredentialsProviderClass != null && zkCredentialsProviderClass.trim().length() > 0) {\n      strat.setZkCredentialsToAddAutomatically(cc.getResourceLoader().newInstance(zkCredentialsProviderClass, ZkCredentialsProvider.class));\n    } else {\n      strat.setZkCredentialsToAddAutomatically(new DefaultZkCredentialsProvider());\n    }\n    addOnReconnectListener(getConfigDirListener());\n\n    zkClient = new SolrZkClient(zkServerAddress, clientTimeout, zkClientConnectTimeout, strat,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() throws SessionExpiredException {\n            log.info(\"ZooKeeper session re-connected ... refreshing core states after session expiration.\");\n            clearZkCollectionTerms();\n            try {\n              // recreate our watchers first so that they exist even on any problems below\n              zkStateReader.createClusterStateWatchersAndUpdate();\n\n              // this is troublesome - we dont want to kill anything the old\n              // leader accepted\n              // though I guess sync will likely get those updates back? But\n              // only if\n              // he is involved in the sync, and he certainly may not be\n              // ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n\n              // seems we dont need to do this again...\n              // Overseer.createClientNodes(zkClient, getNodeName());\n\n              // start the overseer first as following code may need it's processing\n              if (!zkRunOnly) {\n                ElectionContext context = new OverseerElectionContext(zkClient,\n                    overseer, getNodeName());\n\n                ElectionContext prevContext = overseerElector.getContext();\n                if (prevContext != null) {\n                  prevContext.cancelElection();\n                  prevContext.close();\n                }\n\n                overseerElector.setup(context);\n\n                overseerElector.joinElection(context, true);\n              }\n\n              cc.cancelCoreRecoveries();\n              \n              try {\n                registerAllCoresAsDown(descriptorsSupplier, false);\n              } catch (SessionExpiredException e) {\n                // zk has to reconnect and this will all be tried again\n                throw e;\n              } catch (Exception e) {\n                // this is really best effort - in case of races or failure cases where we now need to be the leader, if anything fails,\n                // just continue\n                log.warn(\"Exception while trying to register all cores as DOWN\", e);\n              } \n\n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n\n              List<CoreDescriptor> descriptors = descriptorsSupplier.get();\n              // re register all descriptors\n              ExecutorService executorService = (cc != null) ? cc.getCoreZkRegisterExecutorService() : null;\n              if (descriptors != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it\n                  // was\n                  // a leader that was expired - as well as what to do about\n                  // leaders/overseers\n                  // with connection loss\n                  try {\n                    // unload solrcores that have been 'failed over'\n                    throwErrorIfReplicaReplaced(descriptor);\n\n                    if (executorService != null) {\n                      executorService.submit(new RegisterCoreAsync(descriptor, true, true));\n                    } else {\n                      register(descriptor.getName(), descriptor, true, true, false);\n                    }\n                  } catch (Exception e) {\n                    SolrException.log(log, \"Error registering SolrCore\", e);\n                  }\n                }\n              }\n\n              // notify any other objects that need to know when the session was re-connected\n              HashSet<OnReconnect> clonedListeners;\n              synchronized (reconnectListeners) {\n                clonedListeners = (HashSet<OnReconnect>)reconnectListeners.clone();\n              }\n              // the OnReconnect operation can be expensive per listener, so do that async in the background\n              for (OnReconnect listener : clonedListeners) {\n                try {\n                  if (executorService != null) {\n                    executorService.submit(new OnReconnectNotifyAsync(listener));\n                  } else {\n                    listener.command();\n                  }\n                } catch (Exception exc) {\n                  // not much we can do here other than warn in the log\n                  log.warn(\"Error when notifying OnReconnect listener {} after session re-connected.\", listener, exc);\n                }\n              }\n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (SessionExpiredException e) {\n              throw e;\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n          }\n\n        }, new BeforeReconnect() {\n\n      @Override\n      public void command() {\n        try {\n          ZkController.this.overseer.close();\n        } catch (Exception e) {\n          log.error(\"Error trying to stop any Overseer threads\", e);\n        }\n        closeOutstandingElections(descriptorsSupplier);\n        markAllAsNotLeader(descriptorsSupplier);\n      }\n    }, zkACLProvider, new ConnectionManager.IsClosed() {\n\n      @Override\n      public boolean isClosed() {\n        return cc.isShutDown();\n      }});\n\n\n    this.overseerRunningMap = Overseer.getRunningMap(zkClient);\n    this.overseerCompletedMap = Overseer.getCompletedMap(zkClient);\n    this.overseerFailureMap = Overseer.getFailureMap(zkClient);\n    this.asyncIdsMap = Overseer.getAsyncIdsMap(zkClient);\n\n    zkStateReader = new ZkStateReader(zkClient, () -> {\n      if (cc != null) cc.securityNodeChanged();\n    });\n\n    init();\n\n    this.overseerJobQueue = overseer.getStateUpdateQueue();\n    this.overseerCollectionQueue = overseer.getCollectionQueue(zkClient);\n    this.overseerConfigSetQueue = overseer.getConfigSetQueue(zkClient);\n    this.sysPropsCacher = new NodesSysPropsCacher(getSolrCloudManager().getNodeStateProvider(),\n        getNodeName(), zkStateReader);\n\n    assert ObjectReleaseTracker.track(this);\n  }\n\n","sourceOld":"  /**\n   * @param cc Core container associated with this controller. cannot be null.\n   * @param zkServerAddress where to connect to the zk server\n   * @param zkClientConnectTimeout timeout in ms\n   * @param cloudConfig configuration for this controller. TODO: possibly redundant with CoreContainer\n   * @param descriptorsSupplier a supplier of the current core descriptors. used to know which cores to re-register on reconnect\n   */\n  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientConnectTimeout, CloudConfig cloudConfig, final Supplier<List<CoreDescriptor>> descriptorsSupplier)\n      throws InterruptedException, TimeoutException, IOException {\n\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n\n    this.cloudConfig = cloudConfig;\n\n    this.genericCoreNodeNames = cloudConfig.getGenericCoreNodeNames();\n\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in\n    // solr.xml to indicate the root context, instead of hostContext=\"\"\n    // which means the default of \"solr\"\n    String localHostContext = trimLeadingAndTrailingSlashes(cloudConfig.getSolrHostContext());\n\n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = cloudConfig.getSolrHostPort();\n    this.hostName = normalizeHostName(cloudConfig.getHost());\n    this.nodeName = generateNodeName(this.hostName, Integer.toString(this.localHostPort), localHostContext);\n    MDCLoggingContext.setNode(nodeName);\n    this.leaderVoteWait = cloudConfig.getLeaderVoteWait();\n    this.leaderConflictResolveWait = cloudConfig.getLeaderConflictResolveWait();\n\n    this.clientTimeout = cloudConfig.getZkClientTimeout();\n    DefaultConnectionStrategy strat = new DefaultConnectionStrategy();\n    String zkACLProviderClass = cloudConfig.getZkACLProviderClass();\n    ZkACLProvider zkACLProvider = null;\n    if (zkACLProviderClass != null && zkACLProviderClass.trim().length() > 0) {\n      zkACLProvider = cc.getResourceLoader().newInstance(zkACLProviderClass, ZkACLProvider.class);\n    } else {\n      zkACLProvider = new DefaultZkACLProvider();\n    }\n\n    String zkCredentialsProviderClass = cloudConfig.getZkCredentialsProviderClass();\n    if (zkCredentialsProviderClass != null && zkCredentialsProviderClass.trim().length() > 0) {\n      strat.setZkCredentialsToAddAutomatically(cc.getResourceLoader().newInstance(zkCredentialsProviderClass, ZkCredentialsProvider.class));\n    } else {\n      strat.setZkCredentialsToAddAutomatically(new DefaultZkCredentialsProvider());\n    }\n    addOnReconnectListener(getConfigDirListener());\n\n    zkClient = new SolrZkClient(zkServerAddress, clientTimeout, zkClientConnectTimeout, strat,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() throws SessionExpiredException {\n            log.info(\"ZooKeeper session re-connected ... refreshing core states after session expiration.\");\n            clearZkCollectionTerms();\n            try {\n              // recreate our watchers first so that they exist even on any problems below\n              zkStateReader.createClusterStateWatchersAndUpdate();\n\n              // this is troublesome - we dont want to kill anything the old\n              // leader accepted\n              // though I guess sync will likely get those updates back? But\n              // only if\n              // he is involved in the sync, and he certainly may not be\n              // ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n\n              // seems we dont need to do this again...\n              // Overseer.createClientNodes(zkClient, getNodeName());\n\n              // start the overseer first as following code may need it's processing\n              if (!zkRunOnly) {\n                ElectionContext context = new OverseerElectionContext(zkClient,\n                    overseer, getNodeName());\n\n                ElectionContext prevContext = overseerElector.getContext();\n                if (prevContext != null) {\n                  prevContext.cancelElection();\n                  prevContext.close();\n                }\n\n                overseerElector.setup(context);\n\n                overseerElector.joinElection(context, true);\n              }\n\n              cc.cancelCoreRecoveries();\n              \n              try {\n                registerAllCoresAsDown(descriptorsSupplier, false);\n              } catch (SessionExpiredException e) {\n                // zk has to reconnect and this will all be tried again\n                throw e;\n              } catch (Exception e) {\n                // this is really best effort - in case of races or failure cases where we now need to be the leader, if anything fails,\n                // just continue\n                log.warn(\"Exception while trying to register all cores as DOWN\", e);\n              } \n\n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n\n              List<CoreDescriptor> descriptors = descriptorsSupplier.get();\n              // re register all descriptors\n              ExecutorService executorService = (cc != null) ? cc.getCoreZkRegisterExecutorService() : null;\n              if (descriptors != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it\n                  // was\n                  // a leader that was expired - as well as what to do about\n                  // leaders/overseers\n                  // with connection loss\n                  try {\n                    // unload solrcores that have been 'failed over'\n                    throwErrorIfReplicaReplaced(descriptor);\n\n                    if (executorService != null) {\n                      executorService.submit(new RegisterCoreAsync(descriptor, true, true));\n                    } else {\n                      register(descriptor.getName(), descriptor, true, true, false);\n                    }\n                  } catch (Exception e) {\n                    SolrException.log(log, \"Error registering SolrCore\", e);\n                  }\n                }\n              }\n\n              // notify any other objects that need to know when the session was re-connected\n              HashSet<OnReconnect> clonedListeners;\n              synchronized (reconnectListeners) {\n                clonedListeners = (HashSet<OnReconnect>)reconnectListeners.clone();\n              }\n              // the OnReconnect operation can be expensive per listener, so do that async in the background\n              for (OnReconnect listener : clonedListeners) {\n                try {\n                  if (executorService != null) {\n                    executorService.submit(new OnReconnectNotifyAsync(listener));\n                  } else {\n                    listener.command();\n                  }\n                } catch (Exception exc) {\n                  // not much we can do here other than warn in the log\n                  log.warn(\"Error when notifying OnReconnect listener \" + listener + \" after session re-connected.\", exc);\n                }\n              }\n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (SessionExpiredException e) {\n              throw e;\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n          }\n\n        }, new BeforeReconnect() {\n\n      @Override\n      public void command() {\n        try {\n          ZkController.this.overseer.close();\n        } catch (Exception e) {\n          log.error(\"Error trying to stop any Overseer threads\", e);\n        }\n        closeOutstandingElections(descriptorsSupplier);\n        markAllAsNotLeader(descriptorsSupplier);\n      }\n    }, zkACLProvider, new ConnectionManager.IsClosed() {\n\n      @Override\n      public boolean isClosed() {\n        return cc.isShutDown();\n      }});\n\n\n    this.overseerRunningMap = Overseer.getRunningMap(zkClient);\n    this.overseerCompletedMap = Overseer.getCompletedMap(zkClient);\n    this.overseerFailureMap = Overseer.getFailureMap(zkClient);\n    this.asyncIdsMap = Overseer.getAsyncIdsMap(zkClient);\n\n    zkStateReader = new ZkStateReader(zkClient, () -> {\n      if (cc != null) cc.securityNodeChanged();\n    });\n\n    init();\n\n    this.overseerJobQueue = overseer.getStateUpdateQueue();\n    this.overseerCollectionQueue = overseer.getCollectionQueue(zkClient);\n    this.overseerConfigSetQueue = overseer.getConfigSetQueue(zkClient);\n    this.sysPropsCacher = new NodesSysPropsCacher(getSolrCloudManager().getNodeStateProvider(),\n        getNodeName(), zkStateReader);\n\n    assert ObjectReleaseTracker.track(this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5ad9c35f926b4bf8da0336d1300efc709c8d5a56","date":1591729157,"type":3,"author":"murblanc","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,CloudConfig,Supplier[List[CoreDescriptor]]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,CloudConfig,Supplier[List[CoreDescriptor]]).mjava","sourceNew":"  /**\n   * @param cc Core container associated with this controller. cannot be null.\n   * @param zkServerAddress where to connect to the zk server\n   * @param zkClientConnectTimeout timeout in ms\n   * @param cloudConfig configuration for this controller. TODO: possibly redundant with CoreContainer\n   * @param descriptorsSupplier a supplier of the current core descriptors. used to know which cores to re-register on reconnect\n   */\n  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientConnectTimeout, CloudConfig cloudConfig, final Supplier<List<CoreDescriptor>> descriptorsSupplier)\n      throws InterruptedException, TimeoutException, IOException {\n\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n\n    this.cloudConfig = cloudConfig;\n\n    this.genericCoreNodeNames = cloudConfig.getGenericCoreNodeNames();\n\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in\n    // solr.xml to indicate the root context, instead of hostContext=\"\"\n    // which means the default of \"solr\"\n    String localHostContext = trimLeadingAndTrailingSlashes(cloudConfig.getSolrHostContext());\n\n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = cloudConfig.getSolrHostPort();\n    this.hostName = normalizeHostName(cloudConfig.getHost());\n    this.nodeName = generateNodeName(this.hostName, Integer.toString(this.localHostPort), localHostContext);\n    MDCLoggingContext.setNode(nodeName);\n    this.leaderVoteWait = cloudConfig.getLeaderVoteWait();\n    this.leaderConflictResolveWait = cloudConfig.getLeaderConflictResolveWait();\n\n    this.clientTimeout = cloudConfig.getZkClientTimeout();\n    DefaultConnectionStrategy strat = new DefaultConnectionStrategy();\n    String zkACLProviderClass = cloudConfig.getZkACLProviderClass();\n    ZkACLProvider zkACLProvider = null;\n    if (zkACLProviderClass != null && zkACLProviderClass.trim().length() > 0) {\n      zkACLProvider = cc.getResourceLoader().newInstance(zkACLProviderClass, ZkACLProvider.class);\n    } else {\n      zkACLProvider = new DefaultZkACLProvider();\n    }\n\n    String zkCredentialsProviderClass = cloudConfig.getZkCredentialsProviderClass();\n    if (zkCredentialsProviderClass != null && zkCredentialsProviderClass.trim().length() > 0) {\n      strat.setZkCredentialsToAddAutomatically(cc.getResourceLoader().newInstance(zkCredentialsProviderClass, ZkCredentialsProvider.class));\n    } else {\n      strat.setZkCredentialsToAddAutomatically(new DefaultZkCredentialsProvider());\n    }\n    addOnReconnectListener(getConfigDirListener());\n\n    zkClient = new SolrZkClient(zkServerAddress, clientTimeout, zkClientConnectTimeout, strat,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() throws SessionExpiredException {\n            log.info(\"ZooKeeper session re-connected ... refreshing core states after session expiration.\");\n            clearZkCollectionTerms();\n            try {\n              // recreate our watchers first so that they exist even on any problems below\n              zkStateReader.createClusterStateWatchersAndUpdate();\n\n              // this is troublesome - we dont want to kill anything the old\n              // leader accepted\n              // though I guess sync will likely get those updates back? But\n              // only if\n              // he is involved in the sync, and he certainly may not be\n              // ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n\n              // seems we dont need to do this again...\n              // Overseer.createClientNodes(zkClient, getNodeName());\n\n              // start the overseer first as following code may need it's processing\n              if (!zkRunOnly) {\n                ElectionContext context = new OverseerElectionContext(zkClient,\n                    overseer, getNodeName());\n\n                ElectionContext prevContext = overseerElector.getContext();\n                if (prevContext != null) {\n                  prevContext.cancelElection();\n                  prevContext.close();\n                }\n\n                overseerElector.setup(context);\n\n                overseerElector.joinElection(context, true);\n              }\n\n              cc.cancelCoreRecoveries();\n              \n              try {\n                registerAllCoresAsDown(descriptorsSupplier, false);\n              } catch (SessionExpiredException e) {\n                // zk has to reconnect and this will all be tried again\n                throw e;\n              } catch (Exception e) {\n                // this is really best effort - in case of races or failure cases where we now need to be the leader, if anything fails,\n                // just continue\n                log.warn(\"Exception while trying to register all cores as DOWN\", e);\n              } \n\n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n\n              List<CoreDescriptor> descriptors = descriptorsSupplier.get();\n              // re register all descriptors\n              ExecutorService executorService = (cc != null) ? cc.getCoreZkRegisterExecutorService() : null;\n              if (descriptors != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it\n                  // was\n                  // a leader that was expired - as well as what to do about\n                  // leaders/overseers\n                  // with connection loss\n                  try {\n                    // unload solrcores that have been 'failed over'\n                    throwErrorIfReplicaReplaced(descriptor);\n\n                    if (executorService != null) {\n                      executorService.submit(new RegisterCoreAsync(descriptor, true, true));\n                    } else {\n                      register(descriptor.getName(), descriptor, true, true, false);\n                    }\n                  } catch (Exception e) {\n                    SolrException.log(log, \"Error registering SolrCore\", e);\n                  }\n                }\n              }\n\n              // notify any other objects that need to know when the session was re-connected\n              HashSet<OnReconnect> clonedListeners;\n              synchronized (reconnectListeners) {\n                clonedListeners = (HashSet<OnReconnect>)reconnectListeners.clone();\n              }\n              // the OnReconnect operation can be expensive per listener, so do that async in the background\n              for (OnReconnect listener : clonedListeners) {\n                try {\n                  if (executorService != null) {\n                    executorService.submit(new OnReconnectNotifyAsync(listener));\n                  } else {\n                    listener.command();\n                  }\n                } catch (Exception exc) {\n                  // not much we can do here other than warn in the log\n                  log.warn(\"Error when notifying OnReconnect listener {} after session re-connected.\", listener, exc);\n                }\n              }\n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (SessionExpiredException e) {\n              throw e;\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n          }\n\n        }, new BeforeReconnect() {\n\n      @Override\n      public void command() {\n        try {\n          ZkController.this.overseer.close();\n        } catch (Exception e) {\n          log.error(\"Error trying to stop any Overseer threads\", e);\n        }\n        closeOutstandingElections(descriptorsSupplier);\n        markAllAsNotLeader(descriptorsSupplier);\n      }\n    }, zkACLProvider, new ConnectionManager.IsClosed() {\n\n      @Override\n      public boolean isClosed() {\n        return cc.isShutDown();\n      }});\n\n    // Refuse to start if ZK has a non empty /clusterstate.json\n    checkNoOldClusterstate(zkClient);\n\n    this.overseerRunningMap = Overseer.getRunningMap(zkClient);\n    this.overseerCompletedMap = Overseer.getCompletedMap(zkClient);\n    this.overseerFailureMap = Overseer.getFailureMap(zkClient);\n    this.asyncIdsMap = Overseer.getAsyncIdsMap(zkClient);\n\n    zkStateReader = new ZkStateReader(zkClient, () -> {\n      if (cc != null) cc.securityNodeChanged();\n    });\n\n    init();\n\n    this.overseerJobQueue = overseer.getStateUpdateQueue();\n    this.overseerCollectionQueue = overseer.getCollectionQueue(zkClient);\n    this.overseerConfigSetQueue = overseer.getConfigSetQueue(zkClient);\n    this.sysPropsCacher = new NodesSysPropsCacher(getSolrCloudManager().getNodeStateProvider(),\n        getNodeName(), zkStateReader);\n\n    assert ObjectReleaseTracker.track(this);\n  }\n\n","sourceOld":"  /**\n   * @param cc Core container associated with this controller. cannot be null.\n   * @param zkServerAddress where to connect to the zk server\n   * @param zkClientConnectTimeout timeout in ms\n   * @param cloudConfig configuration for this controller. TODO: possibly redundant with CoreContainer\n   * @param descriptorsSupplier a supplier of the current core descriptors. used to know which cores to re-register on reconnect\n   */\n  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientConnectTimeout, CloudConfig cloudConfig, final Supplier<List<CoreDescriptor>> descriptorsSupplier)\n      throws InterruptedException, TimeoutException, IOException {\n\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n\n    this.cloudConfig = cloudConfig;\n\n    this.genericCoreNodeNames = cloudConfig.getGenericCoreNodeNames();\n\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in\n    // solr.xml to indicate the root context, instead of hostContext=\"\"\n    // which means the default of \"solr\"\n    String localHostContext = trimLeadingAndTrailingSlashes(cloudConfig.getSolrHostContext());\n\n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = cloudConfig.getSolrHostPort();\n    this.hostName = normalizeHostName(cloudConfig.getHost());\n    this.nodeName = generateNodeName(this.hostName, Integer.toString(this.localHostPort), localHostContext);\n    MDCLoggingContext.setNode(nodeName);\n    this.leaderVoteWait = cloudConfig.getLeaderVoteWait();\n    this.leaderConflictResolveWait = cloudConfig.getLeaderConflictResolveWait();\n\n    this.clientTimeout = cloudConfig.getZkClientTimeout();\n    DefaultConnectionStrategy strat = new DefaultConnectionStrategy();\n    String zkACLProviderClass = cloudConfig.getZkACLProviderClass();\n    ZkACLProvider zkACLProvider = null;\n    if (zkACLProviderClass != null && zkACLProviderClass.trim().length() > 0) {\n      zkACLProvider = cc.getResourceLoader().newInstance(zkACLProviderClass, ZkACLProvider.class);\n    } else {\n      zkACLProvider = new DefaultZkACLProvider();\n    }\n\n    String zkCredentialsProviderClass = cloudConfig.getZkCredentialsProviderClass();\n    if (zkCredentialsProviderClass != null && zkCredentialsProviderClass.trim().length() > 0) {\n      strat.setZkCredentialsToAddAutomatically(cc.getResourceLoader().newInstance(zkCredentialsProviderClass, ZkCredentialsProvider.class));\n    } else {\n      strat.setZkCredentialsToAddAutomatically(new DefaultZkCredentialsProvider());\n    }\n    addOnReconnectListener(getConfigDirListener());\n\n    zkClient = new SolrZkClient(zkServerAddress, clientTimeout, zkClientConnectTimeout, strat,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() throws SessionExpiredException {\n            log.info(\"ZooKeeper session re-connected ... refreshing core states after session expiration.\");\n            clearZkCollectionTerms();\n            try {\n              // recreate our watchers first so that they exist even on any problems below\n              zkStateReader.createClusterStateWatchersAndUpdate();\n\n              // this is troublesome - we dont want to kill anything the old\n              // leader accepted\n              // though I guess sync will likely get those updates back? But\n              // only if\n              // he is involved in the sync, and he certainly may not be\n              // ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n\n              // seems we dont need to do this again...\n              // Overseer.createClientNodes(zkClient, getNodeName());\n\n              // start the overseer first as following code may need it's processing\n              if (!zkRunOnly) {\n                ElectionContext context = new OverseerElectionContext(zkClient,\n                    overseer, getNodeName());\n\n                ElectionContext prevContext = overseerElector.getContext();\n                if (prevContext != null) {\n                  prevContext.cancelElection();\n                  prevContext.close();\n                }\n\n                overseerElector.setup(context);\n\n                overseerElector.joinElection(context, true);\n              }\n\n              cc.cancelCoreRecoveries();\n              \n              try {\n                registerAllCoresAsDown(descriptorsSupplier, false);\n              } catch (SessionExpiredException e) {\n                // zk has to reconnect and this will all be tried again\n                throw e;\n              } catch (Exception e) {\n                // this is really best effort - in case of races or failure cases where we now need to be the leader, if anything fails,\n                // just continue\n                log.warn(\"Exception while trying to register all cores as DOWN\", e);\n              } \n\n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n\n              List<CoreDescriptor> descriptors = descriptorsSupplier.get();\n              // re register all descriptors\n              ExecutorService executorService = (cc != null) ? cc.getCoreZkRegisterExecutorService() : null;\n              if (descriptors != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it\n                  // was\n                  // a leader that was expired - as well as what to do about\n                  // leaders/overseers\n                  // with connection loss\n                  try {\n                    // unload solrcores that have been 'failed over'\n                    throwErrorIfReplicaReplaced(descriptor);\n\n                    if (executorService != null) {\n                      executorService.submit(new RegisterCoreAsync(descriptor, true, true));\n                    } else {\n                      register(descriptor.getName(), descriptor, true, true, false);\n                    }\n                  } catch (Exception e) {\n                    SolrException.log(log, \"Error registering SolrCore\", e);\n                  }\n                }\n              }\n\n              // notify any other objects that need to know when the session was re-connected\n              HashSet<OnReconnect> clonedListeners;\n              synchronized (reconnectListeners) {\n                clonedListeners = (HashSet<OnReconnect>)reconnectListeners.clone();\n              }\n              // the OnReconnect operation can be expensive per listener, so do that async in the background\n              for (OnReconnect listener : clonedListeners) {\n                try {\n                  if (executorService != null) {\n                    executorService.submit(new OnReconnectNotifyAsync(listener));\n                  } else {\n                    listener.command();\n                  }\n                } catch (Exception exc) {\n                  // not much we can do here other than warn in the log\n                  log.warn(\"Error when notifying OnReconnect listener {} after session re-connected.\", listener, exc);\n                }\n              }\n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (SessionExpiredException e) {\n              throw e;\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n          }\n\n        }, new BeforeReconnect() {\n\n      @Override\n      public void command() {\n        try {\n          ZkController.this.overseer.close();\n        } catch (Exception e) {\n          log.error(\"Error trying to stop any Overseer threads\", e);\n        }\n        closeOutstandingElections(descriptorsSupplier);\n        markAllAsNotLeader(descriptorsSupplier);\n      }\n    }, zkACLProvider, new ConnectionManager.IsClosed() {\n\n      @Override\n      public boolean isClosed() {\n        return cc.isShutDown();\n      }});\n\n\n    this.overseerRunningMap = Overseer.getRunningMap(zkClient);\n    this.overseerCompletedMap = Overseer.getCompletedMap(zkClient);\n    this.overseerFailureMap = Overseer.getFailureMap(zkClient);\n    this.asyncIdsMap = Overseer.getAsyncIdsMap(zkClient);\n\n    zkStateReader = new ZkStateReader(zkClient, () -> {\n      if (cc != null) cc.securityNodeChanged();\n    });\n\n    init();\n\n    this.overseerJobQueue = overseer.getStateUpdateQueue();\n    this.overseerCollectionQueue = overseer.getCollectionQueue(zkClient);\n    this.overseerConfigSetQueue = overseer.getConfigSetQueue(zkClient);\n    this.sysPropsCacher = new NodesSysPropsCacher(getSolrCloudManager().getNodeStateProvider(),\n        getNodeName(), zkStateReader);\n\n    assert ObjectReleaseTracker.track(this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06a8891f085f71282bb3ece1b1732b68f07813a3","date":1591912889,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,CloudConfig,Supplier[List[CoreDescriptor]]).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#ZkController(CoreContainer,String,int,CloudConfig,Supplier[List[CoreDescriptor]]).mjava","sourceNew":"  /**\n   * @param cc Core container associated with this controller. cannot be null.\n   * @param zkServerAddress where to connect to the zk server\n   * @param zkClientConnectTimeout timeout in ms\n   * @param cloudConfig configuration for this controller. TODO: possibly redundant with CoreContainer\n   * @param descriptorsSupplier a supplier of the current core descriptors. used to know which cores to re-register on reconnect\n   */\n  @SuppressWarnings({\"unchecked\"})\n  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientConnectTimeout, CloudConfig cloudConfig, final Supplier<List<CoreDescriptor>> descriptorsSupplier)\n      throws InterruptedException, TimeoutException, IOException {\n\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n\n    this.cloudConfig = cloudConfig;\n\n    this.genericCoreNodeNames = cloudConfig.getGenericCoreNodeNames();\n\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in\n    // solr.xml to indicate the root context, instead of hostContext=\"\"\n    // which means the default of \"solr\"\n    String localHostContext = trimLeadingAndTrailingSlashes(cloudConfig.getSolrHostContext());\n\n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = cloudConfig.getSolrHostPort();\n    this.hostName = normalizeHostName(cloudConfig.getHost());\n    this.nodeName = generateNodeName(this.hostName, Integer.toString(this.localHostPort), localHostContext);\n    MDCLoggingContext.setNode(nodeName);\n    this.leaderVoteWait = cloudConfig.getLeaderVoteWait();\n    this.leaderConflictResolveWait = cloudConfig.getLeaderConflictResolveWait();\n\n    this.clientTimeout = cloudConfig.getZkClientTimeout();\n    DefaultConnectionStrategy strat = new DefaultConnectionStrategy();\n    String zkACLProviderClass = cloudConfig.getZkACLProviderClass();\n    ZkACLProvider zkACLProvider = null;\n    if (zkACLProviderClass != null && zkACLProviderClass.trim().length() > 0) {\n      zkACLProvider = cc.getResourceLoader().newInstance(zkACLProviderClass, ZkACLProvider.class);\n    } else {\n      zkACLProvider = new DefaultZkACLProvider();\n    }\n\n    String zkCredentialsProviderClass = cloudConfig.getZkCredentialsProviderClass();\n    if (zkCredentialsProviderClass != null && zkCredentialsProviderClass.trim().length() > 0) {\n      strat.setZkCredentialsToAddAutomatically(cc.getResourceLoader().newInstance(zkCredentialsProviderClass, ZkCredentialsProvider.class));\n    } else {\n      strat.setZkCredentialsToAddAutomatically(new DefaultZkCredentialsProvider());\n    }\n    addOnReconnectListener(getConfigDirListener());\n\n    zkClient = new SolrZkClient(zkServerAddress, clientTimeout, zkClientConnectTimeout, strat,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() throws SessionExpiredException {\n            log.info(\"ZooKeeper session re-connected ... refreshing core states after session expiration.\");\n            clearZkCollectionTerms();\n            try {\n              // recreate our watchers first so that they exist even on any problems below\n              zkStateReader.createClusterStateWatchersAndUpdate();\n\n              // this is troublesome - we dont want to kill anything the old\n              // leader accepted\n              // though I guess sync will likely get those updates back? But\n              // only if\n              // he is involved in the sync, and he certainly may not be\n              // ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n\n              // seems we dont need to do this again...\n              // Overseer.createClientNodes(zkClient, getNodeName());\n\n              // start the overseer first as following code may need it's processing\n              if (!zkRunOnly) {\n                ElectionContext context = new OverseerElectionContext(zkClient,\n                    overseer, getNodeName());\n\n                ElectionContext prevContext = overseerElector.getContext();\n                if (prevContext != null) {\n                  prevContext.cancelElection();\n                  prevContext.close();\n                }\n\n                overseerElector.setup(context);\n\n                overseerElector.joinElection(context, true);\n              }\n\n              cc.cancelCoreRecoveries();\n              \n              try {\n                registerAllCoresAsDown(descriptorsSupplier, false);\n              } catch (SessionExpiredException e) {\n                // zk has to reconnect and this will all be tried again\n                throw e;\n              } catch (Exception e) {\n                // this is really best effort - in case of races or failure cases where we now need to be the leader, if anything fails,\n                // just continue\n                log.warn(\"Exception while trying to register all cores as DOWN\", e);\n              } \n\n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n\n              List<CoreDescriptor> descriptors = descriptorsSupplier.get();\n              // re register all descriptors\n              ExecutorService executorService = (cc != null) ? cc.getCoreZkRegisterExecutorService() : null;\n              if (descriptors != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it\n                  // was\n                  // a leader that was expired - as well as what to do about\n                  // leaders/overseers\n                  // with connection loss\n                  try {\n                    // unload solrcores that have been 'failed over'\n                    throwErrorIfReplicaReplaced(descriptor);\n\n                    if (executorService != null) {\n                      executorService.submit(new RegisterCoreAsync(descriptor, true, true));\n                    } else {\n                      register(descriptor.getName(), descriptor, true, true, false);\n                    }\n                  } catch (Exception e) {\n                    SolrException.log(log, \"Error registering SolrCore\", e);\n                  }\n                }\n              }\n\n              // notify any other objects that need to know when the session was re-connected\n              HashSet<OnReconnect> clonedListeners;\n              synchronized (reconnectListeners) {\n                clonedListeners = (HashSet<OnReconnect>)reconnectListeners.clone();\n              }\n              // the OnReconnect operation can be expensive per listener, so do that async in the background\n              for (OnReconnect listener : clonedListeners) {\n                try {\n                  if (executorService != null) {\n                    executorService.submit(new OnReconnectNotifyAsync(listener));\n                  } else {\n                    listener.command();\n                  }\n                } catch (Exception exc) {\n                  // not much we can do here other than warn in the log\n                  log.warn(\"Error when notifying OnReconnect listener {} after session re-connected.\", listener, exc);\n                }\n              }\n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (SessionExpiredException e) {\n              throw e;\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n          }\n\n        }, new BeforeReconnect() {\n\n      @Override\n      public void command() {\n        try {\n          ZkController.this.overseer.close();\n        } catch (Exception e) {\n          log.error(\"Error trying to stop any Overseer threads\", e);\n        }\n        closeOutstandingElections(descriptorsSupplier);\n        markAllAsNotLeader(descriptorsSupplier);\n      }\n    }, zkACLProvider, new ConnectionManager.IsClosed() {\n\n      @Override\n      public boolean isClosed() {\n        return cc.isShutDown();\n      }});\n\n    // Refuse to start if ZK has a non empty /clusterstate.json\n    checkNoOldClusterstate(zkClient);\n\n    this.overseerRunningMap = Overseer.getRunningMap(zkClient);\n    this.overseerCompletedMap = Overseer.getCompletedMap(zkClient);\n    this.overseerFailureMap = Overseer.getFailureMap(zkClient);\n    this.asyncIdsMap = Overseer.getAsyncIdsMap(zkClient);\n\n    zkStateReader = new ZkStateReader(zkClient, () -> {\n      if (cc != null) cc.securityNodeChanged();\n    });\n\n    init();\n\n    this.overseerJobQueue = overseer.getStateUpdateQueue();\n    this.overseerCollectionQueue = overseer.getCollectionQueue(zkClient);\n    this.overseerConfigSetQueue = overseer.getConfigSetQueue(zkClient);\n    this.sysPropsCacher = new NodesSysPropsCacher(getSolrCloudManager().getNodeStateProvider(),\n        getNodeName(), zkStateReader);\n\n    assert ObjectReleaseTracker.track(this);\n  }\n\n","sourceOld":"  /**\n   * @param cc Core container associated with this controller. cannot be null.\n   * @param zkServerAddress where to connect to the zk server\n   * @param zkClientConnectTimeout timeout in ms\n   * @param cloudConfig configuration for this controller. TODO: possibly redundant with CoreContainer\n   * @param descriptorsSupplier a supplier of the current core descriptors. used to know which cores to re-register on reconnect\n   */\n  public ZkController(final CoreContainer cc, String zkServerAddress, int zkClientConnectTimeout, CloudConfig cloudConfig, final Supplier<List<CoreDescriptor>> descriptorsSupplier)\n      throws InterruptedException, TimeoutException, IOException {\n\n    if (cc == null) throw new IllegalArgumentException(\"CoreContainer cannot be null.\");\n    this.cc = cc;\n\n    this.cloudConfig = cloudConfig;\n\n    this.genericCoreNodeNames = cloudConfig.getGenericCoreNodeNames();\n\n    // be forgiving and strip this off leading/trailing slashes\n    // this allows us to support users specifying hostContext=\"/\" in\n    // solr.xml to indicate the root context, instead of hostContext=\"\"\n    // which means the default of \"solr\"\n    String localHostContext = trimLeadingAndTrailingSlashes(cloudConfig.getSolrHostContext());\n\n    this.zkServerAddress = zkServerAddress;\n    this.localHostPort = cloudConfig.getSolrHostPort();\n    this.hostName = normalizeHostName(cloudConfig.getHost());\n    this.nodeName = generateNodeName(this.hostName, Integer.toString(this.localHostPort), localHostContext);\n    MDCLoggingContext.setNode(nodeName);\n    this.leaderVoteWait = cloudConfig.getLeaderVoteWait();\n    this.leaderConflictResolveWait = cloudConfig.getLeaderConflictResolveWait();\n\n    this.clientTimeout = cloudConfig.getZkClientTimeout();\n    DefaultConnectionStrategy strat = new DefaultConnectionStrategy();\n    String zkACLProviderClass = cloudConfig.getZkACLProviderClass();\n    ZkACLProvider zkACLProvider = null;\n    if (zkACLProviderClass != null && zkACLProviderClass.trim().length() > 0) {\n      zkACLProvider = cc.getResourceLoader().newInstance(zkACLProviderClass, ZkACLProvider.class);\n    } else {\n      zkACLProvider = new DefaultZkACLProvider();\n    }\n\n    String zkCredentialsProviderClass = cloudConfig.getZkCredentialsProviderClass();\n    if (zkCredentialsProviderClass != null && zkCredentialsProviderClass.trim().length() > 0) {\n      strat.setZkCredentialsToAddAutomatically(cc.getResourceLoader().newInstance(zkCredentialsProviderClass, ZkCredentialsProvider.class));\n    } else {\n      strat.setZkCredentialsToAddAutomatically(new DefaultZkCredentialsProvider());\n    }\n    addOnReconnectListener(getConfigDirListener());\n\n    zkClient = new SolrZkClient(zkServerAddress, clientTimeout, zkClientConnectTimeout, strat,\n        // on reconnect, reload cloud info\n        new OnReconnect() {\n\n          @Override\n          public void command() throws SessionExpiredException {\n            log.info(\"ZooKeeper session re-connected ... refreshing core states after session expiration.\");\n            clearZkCollectionTerms();\n            try {\n              // recreate our watchers first so that they exist even on any problems below\n              zkStateReader.createClusterStateWatchersAndUpdate();\n\n              // this is troublesome - we dont want to kill anything the old\n              // leader accepted\n              // though I guess sync will likely get those updates back? But\n              // only if\n              // he is involved in the sync, and he certainly may not be\n              // ExecutorUtil.shutdownAndAwaitTermination(cc.getCmdDistribExecutor());\n              // we need to create all of our lost watches\n\n              // seems we dont need to do this again...\n              // Overseer.createClientNodes(zkClient, getNodeName());\n\n              // start the overseer first as following code may need it's processing\n              if (!zkRunOnly) {\n                ElectionContext context = new OverseerElectionContext(zkClient,\n                    overseer, getNodeName());\n\n                ElectionContext prevContext = overseerElector.getContext();\n                if (prevContext != null) {\n                  prevContext.cancelElection();\n                  prevContext.close();\n                }\n\n                overseerElector.setup(context);\n\n                overseerElector.joinElection(context, true);\n              }\n\n              cc.cancelCoreRecoveries();\n              \n              try {\n                registerAllCoresAsDown(descriptorsSupplier, false);\n              } catch (SessionExpiredException e) {\n                // zk has to reconnect and this will all be tried again\n                throw e;\n              } catch (Exception e) {\n                // this is really best effort - in case of races or failure cases where we now need to be the leader, if anything fails,\n                // just continue\n                log.warn(\"Exception while trying to register all cores as DOWN\", e);\n              } \n\n              // we have to register as live first to pick up docs in the buffer\n              createEphemeralLiveNode();\n\n              List<CoreDescriptor> descriptors = descriptorsSupplier.get();\n              // re register all descriptors\n              ExecutorService executorService = (cc != null) ? cc.getCoreZkRegisterExecutorService() : null;\n              if (descriptors != null) {\n                for (CoreDescriptor descriptor : descriptors) {\n                  // TODO: we need to think carefully about what happens when it\n                  // was\n                  // a leader that was expired - as well as what to do about\n                  // leaders/overseers\n                  // with connection loss\n                  try {\n                    // unload solrcores that have been 'failed over'\n                    throwErrorIfReplicaReplaced(descriptor);\n\n                    if (executorService != null) {\n                      executorService.submit(new RegisterCoreAsync(descriptor, true, true));\n                    } else {\n                      register(descriptor.getName(), descriptor, true, true, false);\n                    }\n                  } catch (Exception e) {\n                    SolrException.log(log, \"Error registering SolrCore\", e);\n                  }\n                }\n              }\n\n              // notify any other objects that need to know when the session was re-connected\n              HashSet<OnReconnect> clonedListeners;\n              synchronized (reconnectListeners) {\n                clonedListeners = (HashSet<OnReconnect>)reconnectListeners.clone();\n              }\n              // the OnReconnect operation can be expensive per listener, so do that async in the background\n              for (OnReconnect listener : clonedListeners) {\n                try {\n                  if (executorService != null) {\n                    executorService.submit(new OnReconnectNotifyAsync(listener));\n                  } else {\n                    listener.command();\n                  }\n                } catch (Exception exc) {\n                  // not much we can do here other than warn in the log\n                  log.warn(\"Error when notifying OnReconnect listener {} after session re-connected.\", listener, exc);\n                }\n              }\n            } catch (InterruptedException e) {\n              // Restore the interrupted status\n              Thread.currentThread().interrupt();\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            } catch (SessionExpiredException e) {\n              throw e;\n            } catch (Exception e) {\n              SolrException.log(log, \"\", e);\n              throw new ZooKeeperException(\n                  SolrException.ErrorCode.SERVER_ERROR, \"\", e);\n            }\n          }\n\n        }, new BeforeReconnect() {\n\n      @Override\n      public void command() {\n        try {\n          ZkController.this.overseer.close();\n        } catch (Exception e) {\n          log.error(\"Error trying to stop any Overseer threads\", e);\n        }\n        closeOutstandingElections(descriptorsSupplier);\n        markAllAsNotLeader(descriptorsSupplier);\n      }\n    }, zkACLProvider, new ConnectionManager.IsClosed() {\n\n      @Override\n      public boolean isClosed() {\n        return cc.isShutDown();\n      }});\n\n    // Refuse to start if ZK has a non empty /clusterstate.json\n    checkNoOldClusterstate(zkClient);\n\n    this.overseerRunningMap = Overseer.getRunningMap(zkClient);\n    this.overseerCompletedMap = Overseer.getCompletedMap(zkClient);\n    this.overseerFailureMap = Overseer.getFailureMap(zkClient);\n    this.asyncIdsMap = Overseer.getAsyncIdsMap(zkClient);\n\n    zkStateReader = new ZkStateReader(zkClient, () -> {\n      if (cc != null) cc.securityNodeChanged();\n    });\n\n    init();\n\n    this.overseerJobQueue = overseer.getStateUpdateQueue();\n    this.overseerCollectionQueue = overseer.getCollectionQueue(zkClient);\n    this.overseerConfigSetQueue = overseer.getConfigSetQueue(zkClient);\n    this.sysPropsCacher = new NodesSysPropsCacher(getSolrCloudManager().getNodeStateProvider(),\n        getNodeName(), zkStateReader);\n\n    assert ObjectReleaseTracker.track(this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5ad9c35f926b4bf8da0336d1300efc709c8d5a56":["ad4957cde742defe6db19689abdc267c5d948066"],"1c560208bc8842ee884b76b08784ccb132f05b48":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"06a8891f085f71282bb3ece1b1732b68f07813a3":["5ad9c35f926b4bf8da0336d1300efc709c8d5a56"],"ad4957cde742defe6db19689abdc267c5d948066":["1c560208bc8842ee884b76b08784ccb132f05b48"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["06a8891f085f71282bb3ece1b1732b68f07813a3"]},"commit2Childs":{"5ad9c35f926b4bf8da0336d1300efc709c8d5a56":["06a8891f085f71282bb3ece1b1732b68f07813a3"],"1c560208bc8842ee884b76b08784ccb132f05b48":["ad4957cde742defe6db19689abdc267c5d948066"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1c560208bc8842ee884b76b08784ccb132f05b48"],"06a8891f085f71282bb3ece1b1732b68f07813a3":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ad4957cde742defe6db19689abdc267c5d948066":["5ad9c35f926b4bf8da0336d1300efc709c8d5a56"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}