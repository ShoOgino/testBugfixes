{"path":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testChunkCleanup().mjava","commits":[{"id":"bd7962f4da329a4e559727022b752c5cefaee5da","date":1421356185,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testChunkCleanup().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw, true);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      doc.add(new StoredField(\"text\", \"not very long at all\"));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingStoredFieldsReader reader = (CompressingStoredFieldsReader)sr.getFieldsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingStoredFieldsReader reader = (CompressingStoredFieldsReader)sr.getFieldsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b52491e71f0d5d0f0160d6ed0d39e0dd661be68a","date":1429550638,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testChunkCleanup().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testChunkCleanup().mjava","sourceNew":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw, true);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      doc.add(new StoredField(\"text\", \"not very long at all\"));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingStoredFieldsReader reader = (CompressingStoredFieldsReader)sr.getFieldsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingStoredFieldsReader reader = (CompressingStoredFieldsReader)sr.getFieldsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * writes some tiny segments with incomplete compressed blocks,\n   * and ensures merge recompresses them.\n   */\n  public void testChunkCleanup() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMergePolicy(NoMergePolicy.INSTANCE);\n    \n    // we have to enforce certain things like maxDocsPerChunk to cause dirty chunks to be created\n    // by this test.\n    iwConf.setCodec(CompressingCodec.randomInstance(random(), 4*1024, 100, false, 8));\n    IndexWriter iw = new IndexWriter(dir, iwConf);\n    DirectoryReader ir = DirectoryReader.open(iw, true);\n    for (int i = 0; i < 5; i++) {\n      Document doc = new Document();\n      doc.add(new StoredField(\"text\", \"not very long at all\"));\n      iw.addDocument(doc);\n      // force flush\n      DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n      assertNotNull(ir2);\n      ir.close();\n      ir = ir2;\n      // examine dirty counts:\n      for (LeafReaderContext leaf : ir2.leaves()) {\n        CodecReader sr = (CodecReader) leaf.reader();\n        CompressingStoredFieldsReader reader = (CompressingStoredFieldsReader)sr.getFieldsReader();\n        assertEquals(1, reader.getNumChunks());\n        assertEquals(1, reader.getNumDirtyChunks());\n      }\n    }\n    iw.getConfig().setMergePolicy(newLogMergePolicy());\n    iw.forceMerge(1);\n    DirectoryReader ir2 = DirectoryReader.openIfChanged(ir);\n    assertNotNull(ir2);\n    ir.close();\n    ir = ir2;\n    CodecReader sr = getOnlySegmentReader(ir);\n    CompressingStoredFieldsReader reader = (CompressingStoredFieldsReader)sr.getFieldsReader();\n    // we could get lucky, and have zero, but typically one.\n    assertTrue(reader.getNumDirtyChunks() <= 1);\n    ir.close();\n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"bd7962f4da329a4e559727022b752c5cefaee5da":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b52491e71f0d5d0f0160d6ed0d39e0dd661be68a":["bd7962f4da329a4e559727022b752c5cefaee5da"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b52491e71f0d5d0f0160d6ed0d39e0dd661be68a"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["bd7962f4da329a4e559727022b752c5cefaee5da"],"bd7962f4da329a4e559727022b752c5cefaee5da":["b52491e71f0d5d0f0160d6ed0d39e0dd661be68a"],"b52491e71f0d5d0f0160d6ed0d39e0dd661be68a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}