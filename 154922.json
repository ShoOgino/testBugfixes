{"path":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","commits":[{"id":"770281b8a8459cafcdd2354b6a06078fea2d83c9","date":1077308096,"type":0,"author":"Doug Cutting","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"/dev/null","sourceNew":"  protected void setUp() {\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true);\n    }\n    \n    try {\n      Arrays.sort(testTerms);\n      for (int j = 0; j < 5; j++) {\n        writer = new TermVectorsWriter(dir, seg, fieldInfos);\n        writer.openDocument();\n\n        for (int k = 0; k < testFields.length; k++) {\n          writer.openField(testFields[k]);\n          for (int i = 0; i < testTerms.length; i++) {\n            writer.addTerm(testTerms[i], i);      \n          }\n          writer.closeField();\n        }\n        writer.closeDocument();\n        writer.close();\n      }\n\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["11764865fb318bf86302eab36bdf9cd00c50c110"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0","date":1096997448,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  protected void setUp() {\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    \n    for (int i = 0; i < testTerms.length; i++)\n    {\n      positions[i] = new int[3];\n      for (int j = 0; j < positions[i].length; j++) {\n        // poditions are always sorted in increasing order\n        positions[i][j] = (int)(j * 10 + Math.random() * 10);\n      }\n      offsets[i] = new TermVectorOffsetInfo[3];\n      for (int j = 0; j < offsets[i].length; j++){\n        // ofsets are alway sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n      }        \n    }\n    try {\n      Arrays.sort(testTerms);\n      for (int j = 0; j < 5; j++) {\n        writer = new TermVectorsWriter(dir, seg, fieldInfos);\n        writer.openDocument();\n\n        for (int k = 0; k < testFields.length; k++) {\n          writer.openField(testFields[k]);\n          for (int i = 0; i < testTerms.length; i++) {\n            writer.addTerm(testTerms[i], 3, positions[i], offsets[i]);      \n          }\n          writer.closeField();\n        }\n        writer.closeDocument();\n        writer.close();\n      }\n\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }    \n  }\n\n","sourceOld":"  protected void setUp() {\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true);\n    }\n    \n    try {\n      Arrays.sort(testTerms);\n      for (int j = 0; j < 5; j++) {\n        writer = new TermVectorsWriter(dir, seg, fieldInfos);\n        writer.openDocument();\n\n        for (int k = 0; k < testFields.length; k++) {\n          writer.openField(testFields[k]);\n          for (int i = 0; i < testTerms.length; i++) {\n            writer.addTerm(testTerms[i], i);      \n          }\n          writer.closeField();\n        }\n        writer.closeDocument();\n        writer.close();\n      }\n\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }    \n  }\n\n","bugFix":null,"bugIntro":["11764865fb318bf86302eab36bdf9cd00c50c110"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1229e4509cad6bf268c0fe18199b63a72216c66b","date":1120256033,"type":3,"author":"Daniel Naber","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  protected void setUp() throws IOException {\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    \n    for (int i = 0; i < testTerms.length; i++)\n    {\n      positions[i] = new int[3];\n      for (int j = 0; j < positions[i].length; j++) {\n        // poditions are always sorted in increasing order\n        positions[i][j] = (int)(j * 10 + Math.random() * 10);\n      }\n      offsets[i] = new TermVectorOffsetInfo[3];\n      for (int j = 0; j < offsets[i].length; j++){\n        // ofsets are alway sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n      }        \n    }\n    Arrays.sort(testTerms);\n    for (int j = 0; j < 5; j++) {\n      writer = new TermVectorsWriter(dir, seg, fieldInfos);\n      writer.openDocument();\n\n      for (int k = 0; k < testFields.length; k++) {\n        writer.openField(testFields[k]);\n        for (int i = 0; i < testTerms.length; i++) {\n          writer.addTerm(testTerms[i], 3, positions[i], offsets[i]);      \n        }\n        writer.closeField();\n      }\n      writer.closeDocument();\n      writer.close();\n    }\n  }\n\n","sourceOld":"  protected void setUp() {\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    \n    for (int i = 0; i < testTerms.length; i++)\n    {\n      positions[i] = new int[3];\n      for (int j = 0; j < positions[i].length; j++) {\n        // poditions are always sorted in increasing order\n        positions[i][j] = (int)(j * 10 + Math.random() * 10);\n      }\n      offsets[i] = new TermVectorOffsetInfo[3];\n      for (int j = 0; j < offsets[i].length; j++){\n        // ofsets are alway sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n      }        \n    }\n    try {\n      Arrays.sort(testTerms);\n      for (int j = 0; j < 5; j++) {\n        writer = new TermVectorsWriter(dir, seg, fieldInfos);\n        writer.openDocument();\n\n        for (int k = 0; k < testFields.length; k++) {\n          writer.openField(testFields[k]);\n          for (int i = 0; i < testTerms.length; i++) {\n            writer.addTerm(testTerms[i], 3, positions[i], offsets[i]);      \n          }\n          writer.closeField();\n        }\n        writer.closeDocument();\n        writer.close();\n      }\n\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }    \n  }\n\n","bugFix":null,"bugIntro":["11764865fb318bf86302eab36bdf9cd00c50c110","290c401c31db375e771805c3ba7ac5f64c7370dc"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"352bfe1fae83b92d1562f01c057bfbe6f5af3ddb","date":1185160645,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  protected void setUp() throws IOException {\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[3];\n      for (int j = 0; j < positions[i].length; j++) {\n        // poditions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n      }\n      offsets[i] = new TermVectorOffsetInfo[3];\n      for (int j = 0; j < offsets[i].length; j++) {\n        // ofsets are alway sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n      }\n    }\n    Arrays.sort(testTerms);\n    //Create 5 documents for testing, they all have the same terms\n    writer = new TermVectorsWriter(dir, seg, fieldInfos);\n    for (int j = 0; j < 5; j++) {\n\n      writer.openDocument();\n\n      for (int k = 0; k < testFields.length; k++) {\n        writer.openField(testFields[k]);\n        for (int i = 0; i < testTerms.length; i++) {\n          writer.addTerm(testTerms[i], 3, positions[i], offsets[i]);\n        }\n        writer.closeField();\n      }\n      writer.closeDocument();\n\n    }\n    writer.close();\n  }\n\n","sourceOld":"  protected void setUp() throws IOException {\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    \n    for (int i = 0; i < testTerms.length; i++)\n    {\n      positions[i] = new int[3];\n      for (int j = 0; j < positions[i].length; j++) {\n        // poditions are always sorted in increasing order\n        positions[i][j] = (int)(j * 10 + Math.random() * 10);\n      }\n      offsets[i] = new TermVectorOffsetInfo[3];\n      for (int j = 0; j < offsets[i].length; j++){\n        // ofsets are alway sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n      }        \n    }\n    Arrays.sort(testTerms);\n    for (int j = 0; j < 5; j++) {\n      writer = new TermVectorsWriter(dir, seg, fieldInfos);\n      writer.openDocument();\n\n      for (int k = 0; k < testFields.length; k++) {\n        writer.openField(testFields[k]);\n        for (int i = 0; i < testTerms.length; i++) {\n          writer.addTerm(testTerms[i], 3, positions[i], offsets[i]);      \n        }\n        writer.closeField();\n      }\n      writer.closeDocument();\n      writer.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["11764865fb318bf86302eab36bdf9cd00c50c110"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"11764865fb318bf86302eab36bdf9cd00c50c110","date":1190109214,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  protected void setUp() throws IOException {\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.TOKENIZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.flush();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, seg + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n  }\n\n","sourceOld":"  protected void setUp() throws IOException {\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[3];\n      for (int j = 0; j < positions[i].length; j++) {\n        // poditions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n      }\n      offsets[i] = new TermVectorOffsetInfo[3];\n      for (int j = 0; j < offsets[i].length; j++) {\n        // ofsets are alway sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n      }\n    }\n    Arrays.sort(testTerms);\n    //Create 5 documents for testing, they all have the same terms\n    writer = new TermVectorsWriter(dir, seg, fieldInfos);\n    for (int j = 0; j < 5; j++) {\n\n      writer.openDocument();\n\n      for (int k = 0; k < testFields.length; k++) {\n        writer.openField(testFields[k]);\n        for (int i = 0; i < testTerms.length; i++) {\n          writer.addTerm(testTerms[i], 3, positions[i], offsets[i]);\n        }\n        writer.closeField();\n      }\n      writer.closeDocument();\n\n    }\n    writer.close();\n  }\n\n","bugFix":["770281b8a8459cafcdd2354b6a06078fea2d83c9","352bfe1fae83b92d1562f01c057bfbe6f5af3ddb","6177f0f28ace66d1538b1e6ac5f1773e5449a0b0","1229e4509cad6bf268c0fe18199b63a72216c66b"],"bugIntro":["017bea7152e5f8c54b62fc8048a90e95117a626b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"290c401c31db375e771805c3ba7ac5f64c7370dc","date":1192034795,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.TOKENIZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.flush();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, seg + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n  }\n\n","sourceOld":"  protected void setUp() throws IOException {\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.TOKENIZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.flush();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, seg + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n  }\n\n","bugFix":["1229e4509cad6bf268c0fe18199b63a72216c66b"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0018e7a0579df5d3de71d0bd878322a7abef04d9","date":1202242049,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.TOKENIZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.flush();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, seg + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n  }\n\n","sourceOld":"  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.TOKENIZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.flush();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, seg + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a","date":1221082732,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.flush();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, seg + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n  }\n\n","sourceOld":"  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.TOKENIZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.flush();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, seg + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1","date":1255502337,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, seg + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n  }\n\n","sourceOld":"  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.flush();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, seg + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1326054a8d3aa66382d49decc7f330955c9c6f71","date":1257386139,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, seg + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n  }\n\n","sourceOld":"  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, seg + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"775efee7f959e0dd3df7960b93767d9e00b78751","date":1267203159,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, IndexFileNames.segmentFileName(seg, IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, seg + \".\" + IndexFileNames.FIELD_INFOS_EXTENSION);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1cedb00d2dd44640194401179358a2e3ba6051bf","date":1268243626,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT).setAnalyzer(new MyAnalyzer()));\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundDocStore(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, IndexFileNames.segmentFileName(seg, IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, IndexFileNames.segmentFileName(seg, IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e52fea2c4081a1e552b98506691990be59503168","date":1268250331,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, IndexFileNames.segmentFileName(seg, IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT).setAnalyzer(new MyAnalyzer()));\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundDocStore(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, IndexFileNames.segmentFileName(seg, IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8","date":1268494368,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()));\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundDocStore(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, IndexFileNames.segmentFileName(seg, IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new MyAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    writer.setUseCompoundFile(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, IndexFileNames.segmentFileName(seg, IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","pathOld":"src/test/org/apache/lucene/index/TestTermVectorsReader#setUp().mjava","sourceNew":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()));\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundDocStore(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, IndexFileNames.segmentFileName(seg, IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","sourceOld":"  @Override\n  protected void setUp() throws Exception {\n    super.setUp();\n    /*\n    for (int i = 0; i < testFields.length; i++) {\n      fieldInfos.add(testFields[i], true, true, testFieldsStorePos[i], testFieldsStoreOff[i]);\n    }\n    */\n\n    Arrays.sort(testTerms);\n    int tokenUpto = 0;\n    for (int i = 0; i < testTerms.length; i++) {\n      positions[i] = new int[TERM_FREQ];\n      offsets[i] = new TermVectorOffsetInfo[TERM_FREQ];\n      // first position must be 0\n      for (int j = 0; j < TERM_FREQ; j++) {\n        // positions are always sorted in increasing order\n        positions[i][j] = (int) (j * 10 + Math.random() * 10);\n        // offsets are always sorted in increasing order\n        offsets[i][j] = new TermVectorOffsetInfo(j * 10, j * 10 + testTerms[i].length());\n        TestToken token = tokens[tokenUpto++] = new TestToken();\n        token.text = testTerms[i];\n        token.pos = positions[i][j];\n        token.startOffset = offsets[i][j].getStartOffset();\n        token.endOffset = offsets[i][j].getEndOffset();\n      }\n    }\n    Arrays.sort(tokens);\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MyAnalyzer()));\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundFile(false);\n    ((LogMergePolicy) writer.getMergePolicy()).setUseCompoundDocStore(false);\n    Document doc = new Document();\n    for(int i=0;i<testFields.length;i++) {\n      final Field.TermVector tv;\n      if (testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS_OFFSETS;\n      else if (testFieldsStorePos[i] && !testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_POSITIONS;\n      else if (!testFieldsStorePos[i] && testFieldsStoreOff[i])\n        tv = Field.TermVector.WITH_OFFSETS;\n      else\n        tv = Field.TermVector.YES;\n      doc.add(new Field(testFields[i], \"\", Field.Store.NO, Field.Index.ANALYZED, tv));\n    }\n\n    //Create 5 documents for testing, they all have the same\n    //terms\n    for(int j=0;j<5;j++)\n      writer.addDocument(doc);\n    writer.commit();\n    seg = writer.newestSegment().name;\n    writer.close();\n\n    fieldInfos = new FieldInfos(dir, IndexFileNames.segmentFileName(seg, IndexFileNames.FIELD_INFOS_EXTENSION));\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"775efee7f959e0dd3df7960b93767d9e00b78751":["1326054a8d3aa66382d49decc7f330955c9c6f71"],"11764865fb318bf86302eab36bdf9cd00c50c110":["352bfe1fae83b92d1562f01c057bfbe6f5af3ddb"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["775efee7f959e0dd3df7960b93767d9e00b78751"],"be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["0018e7a0579df5d3de71d0bd878322a7abef04d9"],"e52fea2c4081a1e552b98506691990be59503168":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"0018e7a0579df5d3de71d0bd878322a7abef04d9":["290c401c31db375e771805c3ba7ac5f64c7370dc"],"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1326054a8d3aa66382d49decc7f330955c9c6f71":["be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["e52fea2c4081a1e552b98506691990be59503168"],"352bfe1fae83b92d1562f01c057bfbe6f5af3ddb":["1229e4509cad6bf268c0fe18199b63a72216c66b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"290c401c31db375e771805c3ba7ac5f64c7370dc":["11764865fb318bf86302eab36bdf9cd00c50c110"],"1229e4509cad6bf268c0fe18199b63a72216c66b":["6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"]},"commit2Childs":{"775efee7f959e0dd3df7960b93767d9e00b78751":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"11764865fb318bf86302eab36bdf9cd00c50c110":["290c401c31db375e771805c3ba7ac5f64c7370dc"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["e52fea2c4081a1e552b98506691990be59503168"],"be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1":["1326054a8d3aa66382d49decc7f330955c9c6f71"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["be2419774ad2eb3c65ca1cb035c3a2ccc6ae7da1"],"0018e7a0579df5d3de71d0bd878322a7abef04d9":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"e52fea2c4081a1e552b98506691990be59503168":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"],"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0":["1229e4509cad6bf268c0fe18199b63a72216c66b"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"1326054a8d3aa66382d49decc7f330955c9c6f71":["775efee7f959e0dd3df7960b93767d9e00b78751"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"352bfe1fae83b92d1562f01c057bfbe6f5af3ddb":["11764865fb318bf86302eab36bdf9cd00c50c110"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"290c401c31db375e771805c3ba7ac5f64c7370dc":["0018e7a0579df5d3de71d0bd878322a7abef04d9"],"1229e4509cad6bf268c0fe18199b63a72216c66b":["352bfe1fae83b92d1562f01c057bfbe6f5af3ddb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}