{"path":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","commits":[{"id":"eee9156bf08d7eaae5e8d8ab5f855ba61012e257","date":1285443157,"type":0,"author":"Ryan McKinley","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","pathOld":"/dev/null","sourceNew":"  @Override\r\n  public T create(IndexReader reader) throws IOException {\r\n\r\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\r\n    Terms terms = MultiFields.getTerms(reader, field);\r\n\r\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\r\n    final int termCountHardLimit = reader.maxDoc();\r\n\r\n    // Holds the actual term data, expanded.\r\n    final PagedBytes bytes = new PagedBytes(15);\r\n\r\n    int startBPV;\r\n\r\n    if (terms != null) {\r\n      // Try for coarse estimate for number of bits; this\r\n      // should be an underestimate most of the time, which\r\n      // is fine -- GrowableWriter will reallocate as needed\r\n      long numUniqueTerms = 0;\r\n      try {\r\n        numUniqueTerms = terms.getUniqueTermCount();\r\n      } catch (UnsupportedOperationException uoe) {\r\n        numUniqueTerms = -1;\r\n      }\r\n      if (numUniqueTerms != -1) {\r\n        if (numUniqueTerms > termCountHardLimit) {\r\n          numUniqueTerms = termCountHardLimit;\r\n        }\r\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\r\n      } else {\r\n        startBPV = 1;\r\n      }\r\n    } else {\r\n      startBPV = 1;\r\n    }\r\n\r\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\r\n\r\n    // pointer==0 means not set\r\n    bytes.copyUsingLengthPrefix(new BytesRef());\r\n\r\n    if (terms != null) {\r\n      int termCount = 0;\r\n      final TermsEnum termsEnum = terms.iterator();\r\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\r\n      DocsEnum docs = null;\r\n      while(true) {\r\n        if (termCount++ == termCountHardLimit) {\r\n          // app is misusing the API (there is more than\r\n          // one term per doc); in this case we make best\r\n          // effort to load what we can (see LUCENE-2142)\r\n          break;\r\n        }\r\n\r\n        final BytesRef term = termsEnum.next();\r\n        if (term == null) {\r\n          break;\r\n        }\r\n        final long pointer = bytes.copyUsingLengthPrefix(term);\r\n        docs = termsEnum.docs(delDocs, docs);\r\n        while (true) {\r\n          final int docID = docs.nextDoc();\r\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\r\n            break;\r\n          }\r\n          docToOffset.set(docID, pointer);\r\n        }\r\n      }\r\n    }\r\n\r\n    // maybe an int-only impl?\r\n    return (T)new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\r\n  }\r\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5637938a7dc36e7ff09a5d9398957bd46b15129a","date":1285538458,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public T create(IndexReader reader) throws IOException {\n\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return (T)new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","sourceOld":"  @Override\r\n  public T create(IndexReader reader) throws IOException {\r\n\r\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\r\n    Terms terms = MultiFields.getTerms(reader, field);\r\n\r\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\r\n    final int termCountHardLimit = reader.maxDoc();\r\n\r\n    // Holds the actual term data, expanded.\r\n    final PagedBytes bytes = new PagedBytes(15);\r\n\r\n    int startBPV;\r\n\r\n    if (terms != null) {\r\n      // Try for coarse estimate for number of bits; this\r\n      // should be an underestimate most of the time, which\r\n      // is fine -- GrowableWriter will reallocate as needed\r\n      long numUniqueTerms = 0;\r\n      try {\r\n        numUniqueTerms = terms.getUniqueTermCount();\r\n      } catch (UnsupportedOperationException uoe) {\r\n        numUniqueTerms = -1;\r\n      }\r\n      if (numUniqueTerms != -1) {\r\n        if (numUniqueTerms > termCountHardLimit) {\r\n          numUniqueTerms = termCountHardLimit;\r\n        }\r\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\r\n      } else {\r\n        startBPV = 1;\r\n      }\r\n    } else {\r\n      startBPV = 1;\r\n    }\r\n\r\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\r\n\r\n    // pointer==0 means not set\r\n    bytes.copyUsingLengthPrefix(new BytesRef());\r\n\r\n    if (terms != null) {\r\n      int termCount = 0;\r\n      final TermsEnum termsEnum = terms.iterator();\r\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\r\n      DocsEnum docs = null;\r\n      while(true) {\r\n        if (termCount++ == termCountHardLimit) {\r\n          // app is misusing the API (there is more than\r\n          // one term per doc); in this case we make best\r\n          // effort to load what we can (see LUCENE-2142)\r\n          break;\r\n        }\r\n\r\n        final BytesRef term = termsEnum.next();\r\n        if (term == null) {\r\n          break;\r\n        }\r\n        final long pointer = bytes.copyUsingLengthPrefix(term);\r\n        docs = termsEnum.docs(delDocs, docs);\r\n        while (true) {\r\n          final int docID = docs.nextDoc();\r\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\r\n            break;\r\n          }\r\n          docToOffset.set(docID, pointer);\r\n        }\r\n      }\r\n    }\r\n\r\n    // maybe an int-only impl?\r\n    return (T)new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\r\n  }\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eda01daf042abfe36d82ad9ecad499220a1f3ecf","date":1287094261,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public T create(IndexReader reader) throws IOException {\n\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    @SuppressWarnings(\"unchecked\") final T t =\n      (T)new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n    return t;\n  }\n\n","sourceOld":"  @Override\n  public T create(IndexReader reader) throws IOException {\n\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return (T)new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8f684ff0d559fa047f3bfa01bcbc78fc826a7817","date":1287264459,"type":3,"author":"Ryan McKinley","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","sourceOld":"  @Override\n  public T create(IndexReader reader) throws IOException {\n\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    @SuppressWarnings(\"unchecked\") final T t =\n      (T)new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n    return t;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ecea1664e8617d82eca3b8055a3c37cb4da8511","date":1287578668,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","sourceOld":"  @Override\n  public T create(IndexReader reader) throws IOException {\n\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return (T)new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","date":1308670974,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","sourceOld":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","sourceOld":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(liveDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","sourceOld":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(liveDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","sourceOld":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","sourceNew":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(liveDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","sourceOld":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    String field = StringHelper.intern(this.field); // TODO?? necessary?\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits delDocs = MultiFields.getDeletedDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(delDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"382fe3a6ca9745891afebda9b9a57cc158305545","date":1320952430,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/search/cache/DocTermsCreator#create(IndexReader).mjava","sourceNew":null,"sourceOld":"  @Override\n  public DocTerms create(IndexReader reader) throws IOException {\n\n    Terms terms = MultiFields.getTerms(reader, field);\n\n    final boolean fasterButMoreRAM = hasOption( FASTER_BUT_MORE_RAM );\n    final int termCountHardLimit = reader.maxDoc();\n\n    // Holds the actual term data, expanded.\n    final PagedBytes bytes = new PagedBytes(15);\n\n    int startBPV;\n\n    if (terms != null) {\n      // Try for coarse estimate for number of bits; this\n      // should be an underestimate most of the time, which\n      // is fine -- GrowableWriter will reallocate as needed\n      long numUniqueTerms = 0;\n      try {\n        numUniqueTerms = terms.getUniqueTermCount();\n      } catch (UnsupportedOperationException uoe) {\n        numUniqueTerms = -1;\n      }\n      if (numUniqueTerms != -1) {\n        if (numUniqueTerms > termCountHardLimit) {\n          numUniqueTerms = termCountHardLimit;\n        }\n        startBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n      } else {\n        startBPV = 1;\n      }\n    } else {\n      startBPV = 1;\n    }\n\n    final GrowableWriter docToOffset = new GrowableWriter(startBPV, reader.maxDoc(), fasterButMoreRAM);\n\n    // pointer==0 means not set\n    bytes.copyUsingLengthPrefix(new BytesRef());\n\n    if (terms != null) {\n      int termCount = 0;\n      final TermsEnum termsEnum = terms.iterator();\n      final Bits liveDocs = MultiFields.getLiveDocs(reader);\n      DocsEnum docs = null;\n      while(true) {\n        if (termCount++ == termCountHardLimit) {\n          // app is misusing the API (there is more than\n          // one term per doc); in this case we make best\n          // effort to load what we can (see LUCENE-2142)\n          break;\n        }\n\n        final BytesRef term = termsEnum.next();\n        if (term == null) {\n          break;\n        }\n        final long pointer = bytes.copyUsingLengthPrefix(term);\n        docs = termsEnum.docs(liveDocs, docs);\n        while (true) {\n          final int docID = docs.nextDoc();\n          if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n            break;\n          }\n          docToOffset.set(docID, pointer);\n        }\n      }\n    }\n\n    // maybe an int-only impl?\n    return new DocTermsImpl(bytes.freeze(true), docToOffset.getMutable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"eee9156bf08d7eaae5e8d8ab5f855ba61012e257":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["8f684ff0d559fa047f3bfa01bcbc78fc826a7817"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"2553b00f699380c64959ccb27991289aae87be2e":["8f684ff0d559fa047f3bfa01bcbc78fc826a7817","fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","8f684ff0d559fa047f3bfa01bcbc78fc826a7817"],"382fe3a6ca9745891afebda9b9a57cc158305545":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["8f684ff0d559fa047f3bfa01bcbc78fc826a7817","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["2553b00f699380c64959ccb27991289aae87be2e","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"8f684ff0d559fa047f3bfa01bcbc78fc826a7817":["eda01daf042abfe36d82ad9ecad499220a1f3ecf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"eda01daf042abfe36d82ad9ecad499220a1f3ecf":["5637938a7dc36e7ff09a5d9398957bd46b15129a"],"5637938a7dc36e7ff09a5d9398957bd46b15129a":["eee9156bf08d7eaae5e8d8ab5f855ba61012e257"],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":["5637938a7dc36e7ff09a5d9398957bd46b15129a","8f684ff0d559fa047f3bfa01bcbc78fc826a7817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["382fe3a6ca9745891afebda9b9a57cc158305545"]},"commit2Childs":{"eee9156bf08d7eaae5e8d8ab5f855ba61012e257":["5637938a7dc36e7ff09a5d9398957bd46b15129a"],"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","2553b00f699380c64959ccb27991289aae87be2e"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["382fe3a6ca9745891afebda9b9a57cc158305545","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"2553b00f699380c64959ccb27991289aae87be2e":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"382fe3a6ca9745891afebda9b9a57cc158305545":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"8f684ff0d559fa047f3bfa01bcbc78fc826a7817":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","2553b00f699380c64959ccb27991289aae87be2e","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","d083e83f225b11e5fdd900e83d26ddb385b6955c","4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["eee9156bf08d7eaae5e8d8ab5f855ba61012e257","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"eda01daf042abfe36d82ad9ecad499220a1f3ecf":["8f684ff0d559fa047f3bfa01bcbc78fc826a7817"],"5637938a7dc36e7ff09a5d9398957bd46b15129a":["eda01daf042abfe36d82ad9ecad499220a1f3ecf","4ecea1664e8617d82eca3b8055a3c37cb4da8511"],"4ecea1664e8617d82eca3b8055a3c37cb4da8511":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","4ecea1664e8617d82eca3b8055a3c37cb4da8511","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}