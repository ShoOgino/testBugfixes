{"path":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","commits":[{"id":"c024a3e8fec0a081cbf9539845db12f0dc84d029","date":1376654698,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setCodec(new Facet42Codec());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    FacetFields facetFields = new FacetFields(taxoWriter);\n\n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for(int i=0;i<numLabels;i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for(FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n\n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2ceca04c06658aeb20e0a319ade784ad9a0576dd","date":1376662287,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setCodec(new Facet42Codec());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    FacetFields facetFields = new FacetFields(taxoWriter);\n\n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for(int i=0;i<numLabels;i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for(FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n\n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"09a42395865f791464f0bd5f6118a4abbfa3eb8a","date":1376920143,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","sourceNew":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setCodec(new Facet45Codec());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    FacetFields facetFields = new FacetFields(taxoWriter);\n\n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for(int i=0;i<numLabels;i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for(FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n\n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","sourceOld":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setCodec(new Facet42Codec());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    FacetFields facetFields = new FacetFields(taxoWriter);\n\n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for(int i=0;i<numLabels;i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for(FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n\n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff","date":1377034255,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","sourceNew":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setCodec(new Facet45Codec());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    FacetFields facetFields = new FacetFields(taxoWriter);\n\n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for(int i=0;i<numLabels;i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for(FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n\n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","sourceOld":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setCodec(new Facet42Codec());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    FacetFields facetFields = new FacetFields(taxoWriter);\n\n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for(int i=0;i<numLabels;i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for(FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n\n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setCodec(new Facet45Codec());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    FacetFields facetFields = new FacetFields(taxoWriter);\n\n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for(int i=0;i<numLabels;i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for(FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n\n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8435160e9702b19398118ddf76b61c846612b6a4","date":1380349140,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","sourceNew":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setCodec(new Facet46Codec());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    FacetFields facetFields = new FacetFields(taxoWriter);\n\n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for(int i=0;i<numLabels;i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for(FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n\n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","sourceOld":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setCodec(new Facet45Codec());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    FacetFields facetFields = new FacetFields(taxoWriter);\n\n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for(int i=0;i<numLabels;i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for(FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n\n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84becf9f21f25fe1b9b54cf2a176015749af0283","date":1383417412,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","sourceNew":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    assumeTrue(\"default Codec doesn't support huge BinaryDocValues\", _TestUtil.fieldSupportsHugeBinaryDocValues(CategoryListParams.DEFAULT_FIELD));\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n    \n    FacetFields facetFields = new FacetFields(taxoWriter);\n    \n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n    \n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for (int i = 0; i < numLabels; i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n    \n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n    \n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n    \n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n    \n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for (FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n    \n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","sourceOld":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwc.setCodec(new Facet46Codec());\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n\n    FacetFields facetFields = new FacetFields(taxoWriter);\n\n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n\n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for(int i=0;i<numLabels;i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n\n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n\n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n\n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n\n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for(FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n\n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c190847801a50f4dd20fd639bdc29b54ea3b288b","date":1384461522,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","sourceNew":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    assumeTrue(\"default Codec doesn't support huge BinaryDocValues\", _TestUtil.fieldSupportsHugeBinaryDocValues(CategoryListParams.DEFAULT_FIELD));\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n    \n    FacetFields facetFields = new FacetFields(taxoWriter);\n    \n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n    \n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<FacetLabel> paths = new ArrayList<FacetLabel>();\n    for (int i = 0; i < numLabels; i++) {\n      paths.add(new FacetLabel(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n    \n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n    \n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new FacetLabel(\"dim\"), Integer.MAX_VALUE));\n    \n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n    \n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for (FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n    \n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","sourceOld":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    assumeTrue(\"default Codec doesn't support huge BinaryDocValues\", _TestUtil.fieldSupportsHugeBinaryDocValues(CategoryListParams.DEFAULT_FIELD));\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n    \n    FacetFields facetFields = new FacetFields(taxoWriter);\n    \n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n    \n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for (int i = 0; i < numLabels; i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n    \n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n    \n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n    \n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n    \n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for (FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n    \n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19f5022544a8fc895776356d1b35a4b46d05945c","date":1385063323,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","sourceNew":null,"sourceOld":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    assumeTrue(\"default Codec doesn't support huge BinaryDocValues\", _TestUtil.fieldSupportsHugeBinaryDocValues(CategoryListParams.DEFAULT_FIELD));\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n    \n    FacetFields facetFields = new FacetFields(taxoWriter);\n    \n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n    \n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<FacetLabel> paths = new ArrayList<FacetLabel>();\n    for (int i = 0; i < numLabels; i++) {\n      paths.add(new FacetLabel(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n    \n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n    \n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new FacetLabel(\"dim\"), Integer.MAX_VALUE));\n    \n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n    \n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for (FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n    \n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc728b07df73b197e6d940d27f9b08b63918f13","date":1388834348,"type":4,"author":"Michael McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets#testManyFacetsInOneDocument().mjava","sourceNew":null,"sourceOld":"  // LUCENE-4583: make sure if we require > 32 KB for one\n  // document, we don't hit exc when using Facet42DocValuesFormat\n  public void testManyFacetsInOneDocument() throws Exception {\n    assumeTrue(\"default Codec doesn't support huge BinaryDocValues\", _TestUtil.fieldSupportsHugeBinaryDocValues(CategoryListParams.DEFAULT_FIELD));\n    Directory dir = newDirectory();\n    Directory taxoDir = newDirectory();\n    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);\n    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);\n    \n    FacetFields facetFields = new FacetFields(taxoWriter);\n    \n    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);\n    \n    Document doc = new Document();\n    doc.add(newTextField(\"field\", \"text\", Field.Store.NO));\n    List<CategoryPath> paths = new ArrayList<CategoryPath>();\n    for (int i = 0; i < numLabels; i++) {\n      paths.add(new CategoryPath(\"dim\", \"\" + i));\n    }\n    facetFields.addFields(doc, paths);\n    writer.addDocument(doc);\n    \n    // NRT open\n    IndexSearcher searcher = newSearcher(writer.getReader());\n    writer.close();\n    \n    // NRT open\n    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);\n    taxoWriter.close();\n    \n    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath(\"dim\"), Integer.MAX_VALUE));\n    \n    // Aggregate the facet counts:\n    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);\n    \n    // MatchAllDocsQuery is for \"browsing\" (counts facets\n    // for all non-deleted docs in the index); normally\n    // you'd use a \"normal\" query, and use MultiCollector to\n    // wrap collecting the \"normal\" hits and also facets:\n    searcher.search(new MatchAllDocsQuery(), c);\n    List<FacetResult> results = c.getFacetResults();\n    assertEquals(1, results.size());\n    FacetResultNode root = results.get(0).getFacetResultNode();\n    assertEquals(numLabels, root.subResults.size());\n    Set<String> allLabels = new HashSet<String>();\n    for (FacetResultNode childNode : root.subResults) {\n      assertEquals(2, childNode.label.length);\n      allLabels.add(childNode.label.components[1]);\n      assertEquals(1, (int) childNode.value);\n    }\n    assertEquals(numLabels, allLabels.size());\n    \n    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"84becf9f21f25fe1b9b54cf2a176015749af0283":["8435160e9702b19398118ddf76b61c846612b6a4"],"19f5022544a8fc895776356d1b35a4b46d05945c":["c190847801a50f4dd20fd639bdc29b54ea3b288b"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8435160e9702b19398118ddf76b61c846612b6a4":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"3cc728b07df73b197e6d940d27f9b08b63918f13":["84becf9f21f25fe1b9b54cf2a176015749af0283","19f5022544a8fc895776356d1b35a4b46d05945c"],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["c024a3e8fec0a081cbf9539845db12f0dc84d029","09a42395865f791464f0bd5f6118a4abbfa3eb8a"],"c024a3e8fec0a081cbf9539845db12f0dc84d029":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"09a42395865f791464f0bd5f6118a4abbfa3eb8a":["2ceca04c06658aeb20e0a319ade784ad9a0576dd"],"2ceca04c06658aeb20e0a319ade784ad9a0576dd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c024a3e8fec0a081cbf9539845db12f0dc84d029"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"c190847801a50f4dd20fd639bdc29b54ea3b288b":["84becf9f21f25fe1b9b54cf2a176015749af0283"]},"commit2Childs":{"84becf9f21f25fe1b9b54cf2a176015749af0283":["3cc728b07df73b197e6d940d27f9b08b63918f13","c190847801a50f4dd20fd639bdc29b54ea3b288b"],"19f5022544a8fc895776356d1b35a4b46d05945c":["3cc728b07df73b197e6d940d27f9b08b63918f13"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","c024a3e8fec0a081cbf9539845db12f0dc84d029","2ceca04c06658aeb20e0a319ade784ad9a0576dd"],"8435160e9702b19398118ddf76b61c846612b6a4":["84becf9f21f25fe1b9b54cf2a176015749af0283"],"3cc728b07df73b197e6d940d27f9b08b63918f13":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","8435160e9702b19398118ddf76b61c846612b6a4"],"c024a3e8fec0a081cbf9539845db12f0dc84d029":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff","2ceca04c06658aeb20e0a319ade784ad9a0576dd"],"09a42395865f791464f0bd5f6118a4abbfa3eb8a":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"2ceca04c06658aeb20e0a319ade784ad9a0576dd":["09a42395865f791464f0bd5f6118a4abbfa3eb8a"],"c190847801a50f4dd20fd639bdc29b54ea3b288b":["19f5022544a8fc895776356d1b35a4b46d05945c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}