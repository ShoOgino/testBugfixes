{"path":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/src/test-framework/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random());\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random());\n      TokenStream ts = analyzer.tokenStream(\"fake\", term);\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", term);\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random());\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random());\n      TokenStream ts = analyzer.tokenStream(\"fake\", term);\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", term);\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random());\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"99c9d8533c954f661481ae44273622957dbf572f","date":1380991288,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random());\n      TokenStream ts = analyzer.tokenStream(\"fake\", term);\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n      assertFalse(ts.incrementToken());\n      ts.end();\n      ts.close();\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", term);\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n              assertFalse(ts.incrementToken());\n              ts.end();\n              ts.close();\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random());\n      TokenStream ts = analyzer.tokenStream(\"fake\", term);\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", term);\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"782ed6a4b4ba50ec19734fc8db4e570ee193d627","date":1381127065,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random());\n      try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n        TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        BytesRef bytes = termAtt.getBytesRef();\n        ts.reset();\n        assertTrue(ts.incrementToken());\n        termAtt.fillBytesRef();\n        // ensure we make a copy of the actual bytes too\n        map.put(term, BytesRef.deepCopyOf(bytes));\n        assertFalse(ts.incrementToken());\n        ts.end();\n      }\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n                TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n                BytesRef bytes = termAtt.getBytesRef();\n                ts.reset();\n                assertTrue(ts.incrementToken());\n                termAtt.fillBytesRef();\n                assertEquals(expected, bytes);\n                assertFalse(ts.incrementToken());\n                ts.end();\n              }\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random());\n      TokenStream ts = analyzer.tokenStream(\"fake\", term);\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, BytesRef.deepCopyOf(bytes));\n      assertFalse(ts.incrementToken());\n      ts.end();\n      ts.close();\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", term);\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n              assertFalse(ts.incrementToken());\n              ts.end();\n              ts.close();\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":["e6e919043fa85ee891123768dd655a98edbbf63c","99c9d8533c954f661481ae44273622957dbf572f","b3d07f1ae3b58102f36f3393c397d78ba4e547a4","c83d6c4335f31cae14f625a222bc842f20073dcd","d15f7215dbedf8f3258d977583979d3164ae8cf9"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = TestUtil.randomSimpleString(random());\n      try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n        TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        BytesRef bytes = termAtt.getBytesRef();\n        ts.reset();\n        assertTrue(ts.incrementToken());\n        termAtt.fillBytesRef();\n        // ensure we make a copy of the actual bytes too\n        map.put(term, BytesRef.deepCopyOf(bytes));\n        assertFalse(ts.incrementToken());\n        ts.end();\n      }\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n                TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n                BytesRef bytes = termAtt.getBytesRef();\n                ts.reset();\n                assertTrue(ts.incrementToken());\n                termAtt.fillBytesRef();\n                assertEquals(expected, bytes);\n                assertFalse(ts.incrementToken());\n                ts.end();\n              }\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random());\n      try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n        TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        BytesRef bytes = termAtt.getBytesRef();\n        ts.reset();\n        assertTrue(ts.incrementToken());\n        termAtt.fillBytesRef();\n        // ensure we make a copy of the actual bytes too\n        map.put(term, BytesRef.deepCopyOf(bytes));\n        assertFalse(ts.incrementToken());\n        ts.end();\n      }\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n                TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n                BytesRef bytes = termAtt.getBytesRef();\n                ts.reset();\n                assertTrue(ts.incrementToken());\n                termAtt.fillBytesRef();\n                assertEquals(expected, bytes);\n                assertFalse(ts.incrementToken());\n                ts.end();\n              }\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = TestUtil.randomSimpleString(random());\n      try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n        TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        BytesRef bytes = termAtt.getBytesRef();\n        ts.reset();\n        assertTrue(ts.incrementToken());\n        termAtt.fillBytesRef();\n        // ensure we make a copy of the actual bytes too\n        map.put(term, BytesRef.deepCopyOf(bytes));\n        assertFalse(ts.incrementToken());\n        ts.end();\n      }\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n                TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n                BytesRef bytes = termAtt.getBytesRef();\n                ts.reset();\n                assertTrue(ts.incrementToken());\n                termAtt.fillBytesRef();\n                assertEquals(expected, bytes);\n                assertFalse(ts.incrementToken());\n                ts.end();\n              }\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = TestUtil.randomSimpleString(random());\n      try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n        TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        BytesRef bytes = termAtt.getBytesRef();\n        ts.reset();\n        assertTrue(ts.incrementToken());\n        termAtt.fillBytesRef();\n        // ensure we make a copy of the actual bytes too\n        map.put(term, BytesRef.deepCopyOf(bytes));\n        assertFalse(ts.incrementToken());\n        ts.end();\n      }\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n                TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n                BytesRef bytes = termAtt.getBytesRef();\n                ts.reset();\n                assertTrue(ts.incrementToken());\n                termAtt.fillBytesRef();\n                assertEquals(expected, bytes);\n                assertFalse(ts.incrementToken());\n                ts.end();\n              }\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"804b857d1066ab5185b3b9101bde41b0b71426ec","date":1435846169,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = TestUtil.randomSimpleString(random());\n      try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n        TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        ts.reset();\n        assertTrue(ts.incrementToken());\n        // ensure we make a copy of the actual bytes too\n        map.put(term, BytesRef.deepCopyOf(termAtt.getBytesRef()));\n        assertFalse(ts.incrementToken());\n        ts.end();\n      }\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n                TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n                ts.reset();\n                assertTrue(ts.incrementToken());\n                assertEquals(expected, termAtt.getBytesRef());\n                assertFalse(ts.incrementToken());\n                ts.end();\n              }\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = TestUtil.nextInt(random(), 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = TestUtil.randomSimpleString(random());\n      try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n        TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n        BytesRef bytes = termAtt.getBytesRef();\n        ts.reset();\n        assertTrue(ts.incrementToken());\n        termAtt.fillBytesRef();\n        // ensure we make a copy of the actual bytes too\n        map.put(term, BytesRef.deepCopyOf(bytes));\n        assertFalse(ts.incrementToken());\n        ts.end();\n      }\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              try (TokenStream ts = analyzer.tokenStream(\"fake\", term)) {\n                TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n                BytesRef bytes = termAtt.getBytesRef();\n                ts.reset();\n                assertTrue(ts.incrementToken());\n                termAtt.fillBytesRef();\n                assertEquals(expected, bytes);\n                assertFalse(ts.incrementToken());\n                ts.end();\n              }\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"804b857d1066ab5185b3b9101bde41b0b71426ec":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"99c9d8533c954f661481ae44273622957dbf572f":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6613659748fe4411a7dcf85266e55db1f95f7315"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["629c38c4ae4e303d0617e05fbfe508140b32f0a3","c83d6c4335f31cae14f625a222bc842f20073dcd"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"6613659748fe4411a7dcf85266e55db1f95f7315":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["99c9d8533c954f661481ae44273622957dbf572f"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["804b857d1066ab5185b3b9101bde41b0b71426ec"]},"commit2Childs":{"804b857d1066ab5185b3b9101bde41b0b71426ec":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"99c9d8533c954f661481ae44273622957dbf572f":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["804b857d1066ab5185b3b9101bde41b0b71426ec"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"c83d6c4335f31cae14f625a222bc842f20073dcd":["99c9d8533c954f661481ae44273622957dbf572f","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"6613659748fe4411a7dcf85266e55db1f95f7315":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["6613659748fe4411a7dcf85266e55db1f95f7315"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}