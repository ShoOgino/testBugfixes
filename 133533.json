{"path":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashNumeric#calcFacets().mjava","commits":[{"id":"79759974460bc59933cd169acc94f5c6b16368d5","date":1471318443,"type":1,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashNumeric#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","sourceNew":"  private SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList<>(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n    SimpleOrderedMap<Object> res = new SimpleOrderedMap<>();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap<Object> map = new SimpleOrderedMap<>(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", (long) numBuckets);\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList<SimpleOrderedMap> bucketList = new ArrayList<>(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", new Long(numBuckets));\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashNumeric#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","sourceNew":"  private SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList<>(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n    SimpleOrderedMap<Object> res = new SimpleOrderedMap<>();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap<Object> map = new SimpleOrderedMap<>(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", (long) numBuckets);\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList<SimpleOrderedMap> bucketList = new ArrayList<>(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", new Long(numBuckets));\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashNumeric#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorNumeric#calcFacets().mjava","sourceNew":"  private SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList<>(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n    SimpleOrderedMap<Object> res = new SimpleOrderedMap<>();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap<Object> map = new SimpleOrderedMap<>(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", (long) numBuckets);\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList<SimpleOrderedMap> bucketList = new ArrayList<>(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n    return res;\n  }\n\n","sourceOld":"  public SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n\n    SimpleOrderedMap res = new SimpleOrderedMap();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap map = new SimpleOrderedMap(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", new Long(numBuckets));\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList bucketList = new ArrayList(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3661d6742eed69ff6cc30ea2538d572624a7cdf8","date":1472676864,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashNumeric#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashNumeric#calcFacets().mjava","sourceNew":"  private SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n    if (sf.getType().getNumericType() != null) {\n      calc = FacetRangeProcessor.getNumericCalc(sf);\n    } else {\n      calc = new TermOrdCalc(); // kind of a hack\n    }\n\n    // TODO: Use the number of indexed terms, if present, as an estimate!\n    //    Even for NumericDocValues, we could check for a terms index for an estimate.\n    //    Our estimation should aim high to avoid expensive rehashes.\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    // note: these methods/phases align with FacetFieldProcessorByArray's\n\n    createCollectAcc();\n\n    collectDocs();\n\n    return super.findTopSlots(table.numSlots(), table.cardinality(),\n        slotNum -> calc.bitsToValue(table.vals[slotNum]), // getBucketValFromSlotNum\n        val -> calc.formatValue(val)); // getFieldQueryVal\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList<>(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n    SimpleOrderedMap<Object> res = new SimpleOrderedMap<>();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap<Object> map = new SimpleOrderedMap<>(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", (long) numBuckets);\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList<SimpleOrderedMap> bucketList = new ArrayList<>(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"602cca3f75af03832471d8324bbc5b977a02969c","date":1472676981,"type":5,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashDV#calcFacets().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashNumeric#calcFacets().mjava","sourceNew":"  private SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n    if (sf.getType().getNumericType() != null) {\n      calc = FacetRangeProcessor.getNumericCalc(sf);\n    } else {\n      calc = new TermOrdCalc(); // kind of a hack\n    }\n\n    // TODO: Use the number of indexed terms, if present, as an estimate!\n    //    Even for NumericDocValues, we could check for a terms index for an estimate.\n    //    Our estimation should aim high to avoid expensive rehashes.\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    // note: these methods/phases align with FacetFieldProcessorByArray's\n\n    createCollectAcc();\n\n    collectDocs();\n\n    return super.findTopSlots(table.numSlots(), table.cardinality(),\n        slotNum -> calc.bitsToValue(table.vals[slotNum]), // getBucketValFromSlotNum\n        val -> calc.formatValue(val)); // getFieldQueryVal\n  }\n\n","sourceOld":"  private SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n    if (sf.getType().getNumericType() != null) {\n      calc = FacetRangeProcessor.getNumericCalc(sf);\n    } else {\n      calc = new TermOrdCalc(); // kind of a hack\n    }\n\n    // TODO: Use the number of indexed terms, if present, as an estimate!\n    //    Even for NumericDocValues, we could check for a terms index for an estimate.\n    //    Our estimation should aim high to avoid expensive rehashes.\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    // note: these methods/phases align with FacetFieldProcessorByArray's\n\n    createCollectAcc();\n\n    collectDocs();\n\n    return super.findTopSlots(table.numSlots(), table.cardinality(),\n        slotNum -> calc.bitsToValue(table.vals[slotNum]), // getBucketValFromSlotNum\n        val -> calc.formatValue(val)); // getFieldQueryVal\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bc8f206328a706450934717bec7ccc22ad166fc0","date":1473142172,"type":4,"author":"Noble Paul","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashNumeric#calcFacets().mjava","sourceNew":null,"sourceOld":"  private SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList<>(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n    SimpleOrderedMap<Object> res = new SimpleOrderedMap<>();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap<Object> map = new SimpleOrderedMap<>(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", (long) numBuckets);\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList<SimpleOrderedMap> bucketList = new ArrayList<>(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":4,"author":"Karl Wright","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByHashNumeric#calcFacets().mjava","sourceNew":null,"sourceOld":"  private SimpleOrderedMap<Object> calcFacets() throws IOException {\n\n    final FacetRangeProcessor.Calc calc = FacetRangeProcessor.getNumericCalc(sf);\n\n    // TODO: it would be really nice to know the number of unique values!!!!\n\n    int possibleValues = fcontext.base.size();\n    // size smaller tables so that no resize will be necessary\n    int currHashSize = BitUtil.nextHighestPowerOfTwo((int) (possibleValues * (1 / LongCounts.LOAD_FACTOR) + 1));\n    currHashSize = Math.min(currHashSize, MAXIMUM_STARTING_TABLE_SIZE);\n    final LongCounts table = new LongCounts(currHashSize) {\n      @Override\n      protected void rehash() {\n        super.rehash();\n        doRehash(this);\n        oldToNewMapping = null; // allow for gc\n      }\n    };\n\n    int numSlots = currHashSize;\n\n    int numMissing = 0;\n\n    if (freq.allBuckets) {\n      allBucketsSlot = numSlots++;\n    }\n\n    indexOrderAcc = new SlotAcc(fcontext) {\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        long s1 = calc.bitsToSortableBits(table.vals[slotA]);\n        long s2 = calc.bitsToSortableBits(table.vals[slotB]);\n        return Long.compare(s1, s2);\n      }\n\n      @Override\n      public Object getValue(int slotNum) throws IOException {\n        return null;\n      }\n\n      @Override\n      public void reset() {\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n      }\n    };\n\n    countAcc = new CountSlotAcc(fcontext) {\n      @Override\n      public void incrementCount(int slot, int count) {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int getCount(int slot) {\n        return table.counts[slot];\n      }\n\n      @Override\n      public Object getValue(int slotNum) {\n        return getCount(slotNum);\n      }\n\n      @Override\n      public void reset() {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public void collect(int doc, int slot) throws IOException {\n        throw new UnsupportedOperationException();\n      }\n\n      @Override\n      public int compare(int slotA, int slotB) {\n        return Integer.compare( table.counts[slotA], table.counts[slotB] );\n      }\n\n      @Override\n      public void resize(Resizer resizer) {\n        throw new UnsupportedOperationException();\n      }\n    };\n\n    // we set the countAcc & indexAcc first so generic ones won't be created for us.\n    createCollectAcc(fcontext.base.size(), numSlots);\n\n    if (freq.allBuckets) {\n      allBucketsAcc = new SpecialSlotAcc(fcontext, collectAcc, allBucketsSlot, otherAccs, 0);\n    }\n\n    NumericDocValues values = null;\n    Bits docsWithField = null;\n\n    // TODO: factor this code out so it can be shared...\n    final List<LeafReaderContext> leaves = fcontext.searcher.getIndexReader().leaves();\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    int segBase = 0;\n    int segMax;\n    int adjustedMax = 0;\n    for (DocIterator docsIt = fcontext.base.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (doc >= adjustedMax) {\n        do {\n          ctx = ctxIt.next();\n          segBase = ctx.docBase;\n          segMax = ctx.reader().maxDoc();\n          adjustedMax = segBase + segMax;\n        } while (doc >= adjustedMax);\n        assert doc >= ctx.docBase;\n        setNextReaderFirstPhase(ctx);\n\n        values = DocValues.getNumeric(ctx.reader(), sf.getName());\n        docsWithField = DocValues.getDocsWithField(ctx.reader(), sf.getName());\n      }\n\n      int segDoc = doc - segBase;\n      long val = values.get(segDoc);\n      if (val != 0 || docsWithField.get(segDoc)) {\n        int slot = table.add(val);  // this can trigger a rehash rehash\n\n        // countAcc.incrementCount(slot, 1);\n        // our countAcc is virtual, so this is not needed\n\n        collectFirstPhase(segDoc, slot);\n      }\n    }\n\n    //\n    // collection done, time to find the top slots\n    //\n\n    int numBuckets = 0;\n    List<Object> bucketVals = null;\n    if (freq.numBuckets && fcontext.isShard()) {\n      bucketVals = new ArrayList<>(100);\n    }\n\n    int off = fcontext.isShard() ? 0 : (int) freq.offset;\n    // add a modest amount of over-request if this is a shard request\n    int lim = freq.limit >= 0 ? (fcontext.isShard() ? (int)(freq.limit*1.1+4) : (int)freq.limit) : Integer.MAX_VALUE;\n\n    int maxsize = (int)(freq.limit >= 0 ?  freq.offset + lim : Integer.MAX_VALUE - 1);\n    maxsize = Math.min(maxsize, table.cardinality);\n\n    final int sortMul = freq.sortDirection.getMultiplier();\n\n    PriorityQueue<Slot> queue = new PriorityQueue<Slot>(maxsize) {\n      @Override\n      protected boolean lessThan(Slot a, Slot b) {\n        // TODO: sort-by-index-order\n        int cmp = sortAcc.compare(a.slot, b.slot) * sortMul;\n        return cmp == 0 ? (indexOrderAcc.compare(a.slot, b.slot) > 0) : cmp < 0;\n      }\n    };\n\n    // TODO: create a countAcc that wrapps the table so we can reuse more code?\n\n    Slot bottom = null;\n    for (int i=0; i<table.counts.length; i++) {\n      int count = table.counts[i];\n      if (count < effectiveMincount) {\n        // either not a valid slot, or count not high enough\n        continue;\n      }\n      numBuckets++;  // can be different from the table cardinality if mincount > 1\n\n      long val = table.vals[i];\n      if (bucketVals != null && bucketVals.size()<100) {\n        bucketVals.add( calc.bitsToValue(val) );\n      }\n\n      if (bottom == null) {\n        bottom = new Slot();\n      }\n      bottom.slot = i;\n\n      bottom = queue.insertWithOverflow(bottom);\n    }\n\n    SimpleOrderedMap<Object> res = new SimpleOrderedMap<>();\n    if (freq.numBuckets) {\n      if (!fcontext.isShard()) {\n        res.add(\"numBuckets\", numBuckets);\n      } else {\n        SimpleOrderedMap<Object> map = new SimpleOrderedMap<>(2);\n        map.add(\"numBuckets\", numBuckets);\n        map.add(\"vals\", bucketVals);\n        res.add(\"numBuckets\", map);\n      }\n    }\n\n    FacetDebugInfo fdebug = fcontext.getDebugInfo();\n    if (fdebug != null) fdebug.putInfoItem(\"numBuckets\", (long) numBuckets);\n\n    if (freq.allBuckets) {\n      SimpleOrderedMap<Object> allBuckets = new SimpleOrderedMap<>();\n      // countAcc.setValues(allBuckets, allBucketsSlot);\n      allBuckets.add(\"count\", table.numAdds);\n      allBucketsAcc.setValues(allBuckets, -1);\n      // allBuckets currently doesn't execute sub-facets (because it doesn't change the domain?)\n      res.add(\"allBuckets\", allBuckets);\n    }\n\n    if (freq.missing) {\n      // TODO: it would be more efficient to buid up a missing DocSet if we need it here anyway.\n\n      SimpleOrderedMap<Object> missingBucket = new SimpleOrderedMap<>();\n      fillBucket(missingBucket, getFieldMissingQuery(fcontext.searcher, freq.field), null);\n      res.add(\"missing\", missingBucket);\n    }\n\n    // if we are deep paging, we don't have to order the highest \"offset\" counts.\n    int collectCount = Math.max(0, queue.size() - off);\n    assert collectCount <= lim;\n    int[] sortedSlots = new int[collectCount];\n    for (int i = collectCount - 1; i >= 0; i--) {\n      sortedSlots[i] = queue.pop().slot;\n    }\n\n    ArrayList<SimpleOrderedMap> bucketList = new ArrayList<>(collectCount);\n    res.add(\"buckets\", bucketList);\n\n    boolean needFilter = deferredAggs != null || freq.getSubFacets().size() > 0;\n\n    for (int slotNum : sortedSlots) {\n      SimpleOrderedMap<Object> bucket = new SimpleOrderedMap<>();\n      Comparable val = calc.bitsToValue(table.vals[slotNum]);\n      bucket.add(\"val\", val);\n\n      Query filter = needFilter ? sf.getType().getFieldQuery(null, sf, calc.formatValue(val)) : null;\n\n      fillBucket(bucket, table.counts[slotNum], slotNum, null, filter);\n\n      bucketList.add(bucket);\n    }\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"403d05f7f8d69b65659157eff1bc1d2717f04c66":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"602cca3f75af03832471d8324bbc5b977a02969c":["3661d6742eed69ff6cc30ea2538d572624a7cdf8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["403d05f7f8d69b65659157eff1bc1d2717f04c66","602cca3f75af03832471d8324bbc5b977a02969c"],"3661d6742eed69ff6cc30ea2538d572624a7cdf8":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","79759974460bc59933cd169acc94f5c6b16368d5"],"79759974460bc59933cd169acc94f5c6b16368d5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"bc8f206328a706450934717bec7ccc22ad166fc0":["403d05f7f8d69b65659157eff1bc1d2717f04c66","602cca3f75af03832471d8324bbc5b977a02969c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"]},"commit2Childs":{"403d05f7f8d69b65659157eff1bc1d2717f04c66":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","3661d6742eed69ff6cc30ea2538d572624a7cdf8","bc8f206328a706450934717bec7ccc22ad166fc0"],"602cca3f75af03832471d8324bbc5b977a02969c":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","bc8f206328a706450934717bec7ccc22ad166fc0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["403d05f7f8d69b65659157eff1bc1d2717f04c66","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","79759974460bc59933cd169acc94f5c6b16368d5"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3661d6742eed69ff6cc30ea2538d572624a7cdf8":["602cca3f75af03832471d8324bbc5b977a02969c"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"79759974460bc59933cd169acc94f5c6b16368d5":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"bc8f206328a706450934717bec7ccc22ad166fc0":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["bc8f206328a706450934717bec7ccc22ad166fc0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}