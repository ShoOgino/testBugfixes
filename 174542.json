{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testBoth().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testBoth().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testBoth().mjava","sourceNew":"  public void testBoth() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"a\", \"b\", \"c\", \"d\", \"e f g\", \"e\", \"f\", \"g\",\n          \"link\", \"here\", \"link\", \"there\", \"italics here\", \"italics\", \"here\",\n          \"something\", \"more italics\", \"more\", \"italics\", \"h   i   j\", \"h\", \"i\", \"j\" },\n        new int[] { 11, 11, 13, 15, 17, 32, 32, 34, 36, 42, 47, 56, 61, 71, 71, 79, 86, 98,  98,  103, 124, 124, 128, 132 },\n        new int[] { 18, 12, 14, 16, 18, 37, 33, 35, 37, 46, 51, 60, 66, 83, 78, 83, 95, 110, 102, 110, 133, 125, 129, 133 },\n        new int[] { 1,  0,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,   0,   1,   1,   0,   1,   1 }\n       );\n    \n    // now check the flags, TODO: add way to check flags from BaseTokenStreamTestCase?\n    tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    int expectedFlags[] = new int[] { UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, \n        0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0 };\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    tf.reset();\n    for (int i = 0; i < expectedFlags.length; i++) {\n      assertTrue(tf.incrementToken());\n      assertEquals(\"flags \" + i, expectedFlags[i], flagsAtt.getFlags());\n    }\n    assertFalse(tf.incrementToken());\n    tf.close();\n  }\n\n","sourceOld":"  public void testBoth() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"a\", \"b\", \"c\", \"d\", \"e f g\", \"e\", \"f\", \"g\",\n          \"link\", \"here\", \"link\", \"there\", \"italics here\", \"italics\", \"here\",\n          \"something\", \"more italics\", \"more\", \"italics\", \"h   i   j\", \"h\", \"i\", \"j\" },\n        new int[] { 11, 11, 13, 15, 17, 32, 32, 34, 36, 42, 47, 56, 61, 71, 71, 79, 86, 98,  98,  103, 124, 124, 128, 132 },\n        new int[] { 18, 12, 14, 16, 18, 37, 33, 35, 37, 46, 51, 60, 66, 83, 78, 83, 95, 110, 102, 110, 133, 125, 129, 133 },\n        new int[] { 1,  0,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,   0,   1,   1,   0,   1,   1 }\n       );\n    \n    // now check the flags, TODO: add way to check flags from BaseTokenStreamTestCase?\n    tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    int expectedFlags[] = new int[] { UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, \n        0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0 };\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    tf.reset();\n    for (int i = 0; i < expectedFlags.length; i++) {\n      assertTrue(tf.incrementToken());\n      assertEquals(\"flags \" + i, expectedFlags[i], flagsAtt.getFlags());\n    }\n    assertFalse(tf.incrementToken());\n    tf.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testBoth().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testBoth().mjava","sourceNew":"  public void testBoth() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(WikipediaTokenizer.BOTH, untoks);\n    tf.setReader(new StringReader(test));\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"a\", \"b\", \"c\", \"d\", \"e f g\", \"e\", \"f\", \"g\",\n          \"link\", \"here\", \"link\", \"there\", \"italics here\", \"italics\", \"here\",\n          \"something\", \"more italics\", \"more\", \"italics\", \"h   i   j\", \"h\", \"i\", \"j\" },\n        new int[] { 11, 11, 13, 15, 17, 32, 32, 34, 36, 42, 47, 56, 61, 71, 71, 79, 86, 98,  98,  103, 124, 124, 128, 132 },\n        new int[] { 18, 12, 14, 16, 18, 37, 33, 35, 37, 46, 51, 60, 66, 83, 78, 83, 95, 110, 102, 110, 133, 125, 129, 133 },\n        new int[] { 1,  0,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,   0,   1,   1,   0,   1,   1 }\n       );\n    \n    // now check the flags, TODO: add way to check flags from BaseTokenStreamTestCase?\n    tf = new WikipediaTokenizer(WikipediaTokenizer.BOTH, untoks);\n    tf.setReader(new StringReader(test));\n    int expectedFlags[] = new int[] { UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, \n        0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0 };\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    tf.reset();\n    for (int i = 0; i < expectedFlags.length; i++) {\n      assertTrue(tf.incrementToken());\n      assertEquals(\"flags \" + i, expectedFlags[i], flagsAtt.getFlags());\n    }\n    assertFalse(tf.incrementToken());\n    tf.close();\n  }\n\n","sourceOld":"  public void testBoth() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"a\", \"b\", \"c\", \"d\", \"e f g\", \"e\", \"f\", \"g\",\n          \"link\", \"here\", \"link\", \"there\", \"italics here\", \"italics\", \"here\",\n          \"something\", \"more italics\", \"more\", \"italics\", \"h   i   j\", \"h\", \"i\", \"j\" },\n        new int[] { 11, 11, 13, 15, 17, 32, 32, 34, 36, 42, 47, 56, 61, 71, 71, 79, 86, 98,  98,  103, 124, 124, 128, 132 },\n        new int[] { 18, 12, 14, 16, 18, 37, 33, 35, 37, 46, 51, 60, 66, 83, 78, 83, 95, 110, 102, 110, 133, 125, 129, 133 },\n        new int[] { 1,  0,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,   0,   1,   1,   0,   1,   1 }\n       );\n    \n    // now check the flags, TODO: add way to check flags from BaseTokenStreamTestCase?\n    tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);\n    int expectedFlags[] = new int[] { UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, \n        0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0 };\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    tf.reset();\n    for (int i = 0; i < expectedFlags.length; i++) {\n      assertTrue(tf.incrementToken());\n      assertEquals(\"flags \" + i, expectedFlags[i], flagsAtt.getFlags());\n    }\n    assertFalse(tf.incrementToken());\n    tf.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testBoth().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testBoth().mjava","sourceNew":"  public void testBoth() throws Exception {\n    Set<String> untoks = new HashSet<>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(WikipediaTokenizer.BOTH, untoks);\n    tf.setReader(new StringReader(test));\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"a\", \"b\", \"c\", \"d\", \"e f g\", \"e\", \"f\", \"g\",\n          \"link\", \"here\", \"link\", \"there\", \"italics here\", \"italics\", \"here\",\n          \"something\", \"more italics\", \"more\", \"italics\", \"h   i   j\", \"h\", \"i\", \"j\" },\n        new int[] { 11, 11, 13, 15, 17, 32, 32, 34, 36, 42, 47, 56, 61, 71, 71, 79, 86, 98,  98,  103, 124, 124, 128, 132 },\n        new int[] { 18, 12, 14, 16, 18, 37, 33, 35, 37, 46, 51, 60, 66, 83, 78, 83, 95, 110, 102, 110, 133, 125, 129, 133 },\n        new int[] { 1,  0,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,   0,   1,   1,   0,   1,   1 }\n       );\n    \n    // now check the flags, TODO: add way to check flags from BaseTokenStreamTestCase?\n    tf = new WikipediaTokenizer(WikipediaTokenizer.BOTH, untoks);\n    tf.setReader(new StringReader(test));\n    int expectedFlags[] = new int[] { UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, \n        0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0 };\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    tf.reset();\n    for (int i = 0; i < expectedFlags.length; i++) {\n      assertTrue(tf.incrementToken());\n      assertEquals(\"flags \" + i, expectedFlags[i], flagsAtt.getFlags());\n    }\n    assertFalse(tf.incrementToken());\n    tf.close();\n  }\n\n","sourceOld":"  public void testBoth() throws Exception {\n    Set<String> untoks = new HashSet<String>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(WikipediaTokenizer.BOTH, untoks);\n    tf.setReader(new StringReader(test));\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"a\", \"b\", \"c\", \"d\", \"e f g\", \"e\", \"f\", \"g\",\n          \"link\", \"here\", \"link\", \"there\", \"italics here\", \"italics\", \"here\",\n          \"something\", \"more italics\", \"more\", \"italics\", \"h   i   j\", \"h\", \"i\", \"j\" },\n        new int[] { 11, 11, 13, 15, 17, 32, 32, 34, 36, 42, 47, 56, 61, 71, 71, 79, 86, 98,  98,  103, 124, 124, 128, 132 },\n        new int[] { 18, 12, 14, 16, 18, 37, 33, 35, 37, 46, 51, 60, 66, 83, 78, 83, 95, 110, 102, 110, 133, 125, 129, 133 },\n        new int[] { 1,  0,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,   0,   1,   1,   0,   1,   1 }\n       );\n    \n    // now check the flags, TODO: add way to check flags from BaseTokenStreamTestCase?\n    tf = new WikipediaTokenizer(WikipediaTokenizer.BOTH, untoks);\n    tf.setReader(new StringReader(test));\n    int expectedFlags[] = new int[] { UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, \n        0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0 };\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    tf.reset();\n    for (int i = 0; i < expectedFlags.length; i++) {\n      assertTrue(tf.incrementToken());\n      assertEquals(\"flags \" + i, expectedFlags[i], flagsAtt.getFlags());\n    }\n    assertFalse(tf.incrementToken());\n    tf.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75","date":1399205975,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testBoth().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest#testBoth().mjava","sourceNew":"  public void testBoth() throws Exception {\n    Set<String> untoks = new HashSet<>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(newAttributeFactory(), WikipediaTokenizer.BOTH, untoks);\n    tf.setReader(new StringReader(test));\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"a\", \"b\", \"c\", \"d\", \"e f g\", \"e\", \"f\", \"g\",\n          \"link\", \"here\", \"link\", \"there\", \"italics here\", \"italics\", \"here\",\n          \"something\", \"more italics\", \"more\", \"italics\", \"h   i   j\", \"h\", \"i\", \"j\" },\n        new int[] { 11, 11, 13, 15, 17, 32, 32, 34, 36, 42, 47, 56, 61, 71, 71, 79, 86, 98,  98,  103, 124, 124, 128, 132 },\n        new int[] { 18, 12, 14, 16, 18, 37, 33, 35, 37, 46, 51, 60, 66, 83, 78, 83, 95, 110, 102, 110, 133, 125, 129, 133 },\n        new int[] { 1,  0,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,   0,   1,   1,   0,   1,   1 }\n       );\n    \n    // now check the flags, TODO: add way to check flags from BaseTokenStreamTestCase?\n    tf = new WikipediaTokenizer(newAttributeFactory(), WikipediaTokenizer.BOTH, untoks);\n    tf.setReader(new StringReader(test));\n    int expectedFlags[] = new int[] { UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, \n        0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0 };\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    tf.reset();\n    for (int i = 0; i < expectedFlags.length; i++) {\n      assertTrue(tf.incrementToken());\n      assertEquals(\"flags \" + i, expectedFlags[i], flagsAtt.getFlags());\n    }\n    assertFalse(tf.incrementToken());\n    tf.close();\n  }\n\n","sourceOld":"  public void testBoth() throws Exception {\n    Set<String> untoks = new HashSet<>();\n    untoks.add(WikipediaTokenizer.CATEGORY);\n    untoks.add(WikipediaTokenizer.ITALICS);\n    String test = \"[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]\";\n    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens\n    WikipediaTokenizer tf = new WikipediaTokenizer(WikipediaTokenizer.BOTH, untoks);\n    tf.setReader(new StringReader(test));\n    assertTokenStreamContents(tf,\n        new String[] { \"a b c d\", \"a\", \"b\", \"c\", \"d\", \"e f g\", \"e\", \"f\", \"g\",\n          \"link\", \"here\", \"link\", \"there\", \"italics here\", \"italics\", \"here\",\n          \"something\", \"more italics\", \"more\", \"italics\", \"h   i   j\", \"h\", \"i\", \"j\" },\n        new int[] { 11, 11, 13, 15, 17, 32, 32, 34, 36, 42, 47, 56, 61, 71, 71, 79, 86, 98,  98,  103, 124, 124, 128, 132 },\n        new int[] { 18, 12, 14, 16, 18, 37, 33, 35, 37, 46, 51, 60, 66, 83, 78, 83, 95, 110, 102, 110, 133, 125, 129, 133 },\n        new int[] { 1,  0,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,   0,   1,   1,   0,   1,   1 }\n       );\n    \n    // now check the flags, TODO: add way to check flags from BaseTokenStreamTestCase?\n    tf = new WikipediaTokenizer(WikipediaTokenizer.BOTH, untoks);\n    tf.setReader(new StringReader(test));\n    int expectedFlags[] = new int[] { UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, 0, \n        0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, UNTOKENIZED_TOKEN_FLAG, 0, 0, 0 };\n    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);\n    tf.reset();\n    for (int i = 0; i < expectedFlags.length; i++) {\n      assertTrue(tf.incrementToken());\n      assertEquals(\"flags \" + i, expectedFlags[i], flagsAtt.getFlags());\n    }\n    assertFalse(tf.incrementToken());\n    tf.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["923f36bb0db6f793cf62dbb68723ae3bfbaf1d75"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["923f36bb0db6f793cf62dbb68723ae3bfbaf1d75"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}