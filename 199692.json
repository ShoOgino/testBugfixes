{"path":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      TermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(TermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setTermBuffer(token.term());\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          token.setTermBuffer(terms[t]);\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      TermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(TermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setTermBuffer(token.term());\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          token.setTermBuffer(terms[t]);\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9924ae8182f54b91a3776337f6aff2358372ce1c","date":1270982258,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      TermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(TermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setTermBuffer(token.term());\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          token.setTermBuffer(terms[t]);\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      TermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(TermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setTermBuffer(token.term());\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          token.setTermBuffer(terms[t]);\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a7347509fad0711ac30cb15a746e9a3830a38ebd","date":1275388513,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      TermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(TermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setTermBuffer(token.term());\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          token.setTermBuffer(terms[t]);\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":["be4c30aa8665022511179534b83b929f65ec86f5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    String[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t], offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b","date":1288192616,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.quickSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          return t1.startOffset() - t2.endOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":["f52e903c0f884fd81afca4714ccb4203d7178429"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ca0ffea399542e8aac8ed7608f34f8ec4cb8904d","date":1288424244,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.quickSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          return t1.startOffset() - t2.endOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.quickSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          return t1.startOffset() - t2.endOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      Arrays.sort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() > t2.endOffset())\n            return 1;\n          if (t1.startOffset() < t2.startOffset())\n            return -1;\n          return 0;\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f52e903c0f884fd81afca4714ccb4203d7178429","date":1295438700,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.quickSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset())\n            return t1.endOffset() - t2.endOffset();\n          else\n            return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.quickSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          return t1.startOffset() - t2.endOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":["ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b"],"bugIntro":["be4c30aa8665022511179534b83b929f65ec86f5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e79a6d080bdd5b2a8f56342cf571b5476de04180","date":1295638686,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.quickSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset())\n            return t1.endOffset() - t2.endOffset();\n          else\n            return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.quickSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          return t1.startOffset() - t2.endOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.quickSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset())\n            return t1.endOffset() - t2.endOffset();\n          else\n            return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.quickSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          return t1.startOffset() - t2.endOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2a0d986f42c7320fce5b6ba6a767c160289c738a","date":1304428044,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset())\n            return t1.endOffset() - t2.endOffset();\n          else\n            return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.quickSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset())\n            return t1.endOffset() - t2.endOffset();\n          else\n            return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"be4c30aa8665022511179534b83b929f65ec86f5","date":1305219285,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset())\n            return t1.endOffset() - t2.endOffset();\n          else\n            return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":["f52e903c0f884fd81afca4714ccb4203d7178429","fafd002a407d38098f1f0edf4365f971102ae0ef","4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.quickSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset())\n            return t1.endOffset() - t2.endOffset();\n          else\n            return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(), offsets[tp]\n              .getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(), offsets[tp].getStartOffset(),\n              offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.quickSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset())\n            return t1.endOffset() - t2.endOffset();\n          else\n            return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c8477254b0c13b59b07ac961d5eab631b2a126ab","date":1307456028,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = (PositionIncrementAttribute) addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(Terms,boolean).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(Terms tpv,\n      boolean tokenPositionsGuaranteedContiguous) \n  throws IOException {\n    if (!tokenPositionsGuaranteedContiguous && hasPositions(tpv)) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    TermsEnum termsEnum = tpv.iterator(null);\n    int totalTokens = 0;\n    while(termsEnum.next() != null) {\n      totalTokens += (int) termsEnum.totalTermFreq();\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    termsEnum = tpv.iterator(null);\n    BytesRef text;\n    DocsAndPositionsEnum dpEnum = null;\n    while ((text = termsEnum.next()) != null) {\n\n      dpEnum = termsEnum.docsAndPositions(null, dpEnum);\n      if (dpEnum == null || (!dpEnum.attributes().hasAttribute(OffsetAttribute.class))) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      final String term = text.utf8ToString();\n\n      final OffsetAttribute offsetAtt = dpEnum.attributes().getAttribute(OffsetAttribute.class);\n      dpEnum.nextDoc();\n      final int freq = dpEnum.freq();\n      for(int posUpto=0;posUpto<freq;posUpto++) {\n        final int pos = dpEnum.nextPosition();\n        final Token token = new Token(term,\n                                      offsetAtt.startOffset(),\n                                      offsetAtt.endOffset());\n        if (tokenPositionsGuaranteedContiguous && pos != -1) {\n          // We have positions stored and a guarantee that the token position\n          // information is contiguous\n\n          // This may be fast BUT wont work if Tokenizers used which create >1\n          // token in same position or\n          // creates jumps in position numbers - this code would fail under those\n          // circumstances\n\n          // tokens stored with positions - can use this to index straight into\n          // sorted array\n          tokensInOriginalOrder[pos] = token;\n        } else {\n          // tokens NOT stored with positions or not guaranteed contiguous - must\n          // add to list and sort later\n          if (unsortedTokens == null) {\n            unsortedTokens = new ArrayList<Token>();\n          }\n          unsortedTokens.add(token);\n        }\n      }\n    }\n\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","sourceOld":"  /**\n   * Low level api. Returns a token stream or null if no offset info available\n   * in index. This can be used to feed the highlighter with a pre-parsed token\n   * stream\n   * \n   * In my tests the speeds to recreate 1000 token streams using this method\n   * are: - with TermVector offset only data stored - 420 milliseconds - with\n   * TermVector offset AND position data stored - 271 milliseconds (nb timings\n   * for TermVector with position data are based on a tokenizer with contiguous\n   * positions - no overlaps or gaps) The cost of not using TermPositionVector\n   * to store pre-parsed content and using an analyzer to re-parse the original\n   * content: - reanalyzing the original content - 980 milliseconds\n   * \n   * The re-analyze timings will typically vary depending on - 1) The complexity\n   * of the analyzer code (timings above were using a\n   * stemmer/lowercaser/stopword combo) 2) The number of other fields (Lucene\n   * reads ALL fields off the disk when accessing just one document field - can\n   * cost dear!) 3) Use of compression on field storage - could be faster due to\n   * compression (less disk IO) or slower (more CPU burn) depending on the\n   * content.\n   * \n   * @param tpv\n   * @param tokenPositionsGuaranteedContiguous true if the token position\n   *        numbers have no overlaps or gaps. If looking to eek out the last\n   *        drops of performance, set to true. If in doubt, set to false.\n   */\n  public static TokenStream getTokenStream(TermPositionVector tpv,\n      boolean tokenPositionsGuaranteedContiguous) {\n    if (!tokenPositionsGuaranteedContiguous && tpv.getTermPositions(0) != null) {\n      return new TokenStreamFromTermPositionVector(tpv);\n    }\n\n    // an object used to iterate across an array of tokens\n    final class StoredTokenStream extends TokenStream {\n      Token tokens[];\n\n      int currentToken = 0;\n\n      CharTermAttribute termAtt;\n\n      OffsetAttribute offsetAtt;\n\n      PositionIncrementAttribute posincAtt;\n\n      StoredTokenStream(Token tokens[]) {\n        this.tokens = tokens;\n        termAtt = addAttribute(CharTermAttribute.class);\n        offsetAtt = addAttribute(OffsetAttribute.class);\n        posincAtt = addAttribute(PositionIncrementAttribute.class);\n      }\n\n      @Override\n      public boolean incrementToken() throws IOException {\n        if (currentToken >= tokens.length) {\n          return false;\n        }\n        Token token = tokens[currentToken++];\n        clearAttributes();\n        termAtt.setEmpty().append(token);\n        offsetAtt.setOffset(token.startOffset(), token.endOffset());\n        posincAtt\n            .setPositionIncrement(currentToken <= 1\n                || tokens[currentToken - 1].startOffset() > tokens[currentToken - 2]\n                    .startOffset() ? 1 : 0);\n        return true;\n      }\n    }\n    // code to reconstruct the original sequence of Tokens\n    BytesRef[] terms = tpv.getTerms();\n    int[] freq = tpv.getTermFrequencies();\n    int totalTokens = 0;\n    for (int t = 0; t < freq.length; t++) {\n      totalTokens += freq[t];\n    }\n    Token tokensInOriginalOrder[] = new Token[totalTokens];\n    ArrayList<Token> unsortedTokens = null;\n    for (int t = 0; t < freq.length; t++) {\n      TermVectorOffsetInfo[] offsets = tpv.getOffsets(t);\n      if (offsets == null) {\n        throw new IllegalArgumentException(\n            \"Required TermVector Offset information was not found\");\n      }\n\n      int[] pos = null;\n      if (tokenPositionsGuaranteedContiguous) {\n        // try get the token position info to speed up assembly of tokens into\n        // sorted sequence\n        pos = tpv.getTermPositions(t);\n      }\n      if (pos == null) {\n        // tokens NOT stored with positions or not guaranteed contiguous - must\n        // add to list and sort later\n        if (unsortedTokens == null) {\n          unsortedTokens = new ArrayList<Token>();\n        }\n        for (int tp = 0; tp < offsets.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          unsortedTokens.add(token);\n        }\n      } else {\n        // We have positions stored and a guarantee that the token position\n        // information is contiguous\n\n        // This may be fast BUT wont work if Tokenizers used which create >1\n        // token in same position or\n        // creates jumps in position numbers - this code would fail under those\n        // circumstances\n\n        // tokens stored with positions - can use this to index straight into\n        // sorted array\n        for (int tp = 0; tp < pos.length; tp++) {\n          Token token = new Token(terms[t].utf8ToString(),\n              offsets[tp].getStartOffset(), offsets[tp].getEndOffset());\n          tokensInOriginalOrder[pos[tp]] = token;\n        }\n      }\n    }\n    // If the field has been stored without position data we must perform a sort\n    if (unsortedTokens != null) {\n      tokensInOriginalOrder = unsortedTokens.toArray(new Token[unsortedTokens\n          .size()]);\n      ArrayUtil.mergeSort(tokensInOriginalOrder, new Comparator<Token>() {\n        public int compare(Token t1, Token t2) {\n          if (t1.startOffset() == t2.startOffset()) return t1.endOffset()\n              - t2.endOffset();\n          else return t1.startOffset() - t2.startOffset();\n        }\n      });\n    }\n    return new StoredTokenStream(tokensInOriginalOrder);\n  }\n\n","bugFix":null,"bugIntro":["4d3e8520fd031bab31fd0e4d480e55958bc45efe","000498895a9d8c442dd10d03121bd753ec00bc0e","815287248ca7a77db68038baad5698c5767f36a7"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"9924ae8182f54b91a3776337f6aff2358372ce1c":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["a3776dccca01c11e7046323cfad46a3b4a471233","c8477254b0c13b59b07ac961d5eab631b2a126ab"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"3cc749c053615f5871f3b95715fe292f34e70a53":["c8477254b0c13b59b07ac961d5eab631b2a126ab"],"5f4e87790277826a2aea119328600dfb07761f32":["a7347509fad0711ac30cb15a746e9a3830a38ebd","4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"f52e903c0f884fd81afca4714ccb4203d7178429":["ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","f52e903c0f884fd81afca4714ccb4203d7178429"],"ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"a3776dccca01c11e7046323cfad46a3b4a471233":["f52e903c0f884fd81afca4714ccb4203d7178429","be4c30aa8665022511179534b83b929f65ec86f5"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["ca0ffea399542e8aac8ed7608f34f8ec4cb8904d","f52e903c0f884fd81afca4714ccb4203d7178429"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","c8477254b0c13b59b07ac961d5eab631b2a126ab"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["29ef99d61cda9641b6250bf9567329a6e65f901d","be4c30aa8665022511179534b83b929f65ec86f5"],"c8477254b0c13b59b07ac961d5eab631b2a126ab":["be4c30aa8665022511179534b83b929f65ec86f5"],"ca0ffea399542e8aac8ed7608f34f8ec4cb8904d":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b"],"be4c30aa8665022511179534b83b929f65ec86f5":["2a0d986f42c7320fce5b6ba6a767c160289c738a"],"2a0d986f42c7320fce5b6ba6a767c160289c738a":["f52e903c0f884fd81afca4714ccb4203d7178429"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["9924ae8182f54b91a3776337f6aff2358372ce1c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc749c053615f5871f3b95715fe292f34e70a53"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"9924ae8182f54b91a3776337f6aff2358372ce1c":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":[],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["5f4e87790277826a2aea119328600dfb07761f32","ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b","ca0ffea399542e8aac8ed7608f34f8ec4cb8904d"],"3cc749c053615f5871f3b95715fe292f34e70a53":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"f52e903c0f884fd81afca4714ccb4203d7178429":["e79a6d080bdd5b2a8f56342cf571b5476de04180","a3776dccca01c11e7046323cfad46a3b4a471233","29ef99d61cda9641b6250bf9567329a6e65f901d","2a0d986f42c7320fce5b6ba6a767c160289c738a"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["e79a6d080bdd5b2a8f56342cf571b5476de04180"],"e79a6d080bdd5b2a8f56342cf571b5476de04180":[],"ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b":["f52e903c0f884fd81afca4714ccb4203d7178429","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","ca0ffea399542e8aac8ed7608f34f8ec4cb8904d"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"c8477254b0c13b59b07ac961d5eab631b2a126ab":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","3cc749c053615f5871f3b95715fe292f34e70a53","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"ca0ffea399542e8aac8ed7608f34f8ec4cb8904d":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","5f4e87790277826a2aea119328600dfb07761f32"],"be4c30aa8665022511179534b83b929f65ec86f5":["a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","c8477254b0c13b59b07ac961d5eab631b2a126ab"],"2a0d986f42c7320fce5b6ba6a767c160289c738a":["be4c30aa8665022511179534b83b929f65ec86f5"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["9924ae8182f54b91a3776337f6aff2358372ce1c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","e79a6d080bdd5b2a8f56342cf571b5476de04180","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}