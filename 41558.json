{"path":"modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","commits":[{"id":"44d6f0ab53c1962856b9f48dedb7a2a6cc18905c","date":1310389132,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","pathOld":"/dev/null","sourceNew":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      CharsRef inputs[];\n      CharsRef outputs[];\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(analyzer, unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(analyzer, unescape(outputStrings[i]).trim(), new CharsRef());\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(analyzer, unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        if (expand) {\n          outputs = inputs;\n        } else {\n          outputs = new CharsRef[1];\n          outputs[0] = inputs[0];\n        }\n      }\n      \n      // currently we include the term itself in the map,\n      // and use includeOrig = false always.\n      // this is how the existing filter does it, but its actually a bug,\n      // especially if combined with ignoreCase = true\n      for (int i = 0; i < inputs.length; i++) {\n        for (int j = 0; j < outputs.length; j++) {\n          add(inputs[i], outputs[j], false);\n        }\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["927d09add12b0fa3c10f6f9ae564d85bef5dc12c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser#addInternal(BufferedReader).mjava","sourceNew":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      CharsRef inputs[];\n      CharsRef outputs[];\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(analyzer, unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(analyzer, unescape(outputStrings[i]).trim(), new CharsRef());\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(analyzer, unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        if (expand) {\n          outputs = inputs;\n        } else {\n          outputs = new CharsRef[1];\n          outputs[0] = inputs[0];\n        }\n      }\n      \n      // currently we include the term itself in the map,\n      // and use includeOrig = false always.\n      // this is how the existing filter does it, but its actually a bug,\n      // especially if combined with ignoreCase = true\n      for (int i = 0; i < inputs.length; i++) {\n        for (int j = 0; j < outputs.length; j++) {\n          add(inputs[i], outputs[j], false);\n        }\n      }\n    }\n  }\n\n","sourceOld":"  private void addInternal(BufferedReader in) throws IOException {\n    String line = null;\n    while ((line = in.readLine()) != null) {\n      if (line.length() == 0 || line.charAt(0) == '#') {\n        continue; // ignore empty lines and comments\n      }\n      \n      CharsRef inputs[];\n      CharsRef outputs[];\n      \n      // TODO: we could process this more efficiently.\n      String sides[] = split(line, \"=>\");\n      if (sides.length > 1) { // explicit mapping\n        if (sides.length != 2) {\n          throw new IllegalArgumentException(\"more than one explicit mapping specified on the same line\");\n        }\n        String inputStrings[] = split(sides[0], \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(analyzer, unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        \n        String outputStrings[] = split(sides[1], \",\");\n        outputs = new CharsRef[outputStrings.length];\n        for (int i = 0; i < outputs.length; i++) {\n          outputs[i] = analyze(analyzer, unescape(outputStrings[i]).trim(), new CharsRef());\n        }\n      } else {\n        String inputStrings[] = split(line, \",\");\n        inputs = new CharsRef[inputStrings.length];\n        for (int i = 0; i < inputs.length; i++) {\n          inputs[i] = analyze(analyzer, unescape(inputStrings[i]).trim(), new CharsRef());\n        }\n        if (expand) {\n          outputs = inputs;\n        } else {\n          outputs = new CharsRef[1];\n          outputs[0] = inputs[0];\n        }\n      }\n      \n      // currently we include the term itself in the map,\n      // and use includeOrig = false always.\n      // this is how the existing filter does it, but its actually a bug,\n      // especially if combined with ignoreCase = true\n      for (int i = 0; i < inputs.length; i++) {\n        for (int j = 0; j < outputs.length; j++) {\n          add(inputs[i], outputs[j], false);\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["44d6f0ab53c1962856b9f48dedb7a2a6cc18905c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"44d6f0ab53c1962856b9f48dedb7a2a6cc18905c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["44d6f0ab53c1962856b9f48dedb7a2a6cc18905c"],"44d6f0ab53c1962856b9f48dedb7a2a6cc18905c":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}