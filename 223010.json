{"path":"solr/core/src/java/org/apache/solr/update/processor/DistributedZkUpdateProcessor#doFinish().mjava","commits":[{"id":"9d70e774cb25c8a8d2c3e5e84200f235f9168d87","date":1553016391,"type":1,"author":"Bar Rotstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedZkUpdateProcessor#doFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<SolrCmdDistributor.Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<SolrCmdDistributor.Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n\n      if (error.req.node instanceof SolrCmdDistributor.ForwardNode) {\n        // if it's a forward, any fail is a problem -\n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded\n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n\n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof SolrCmdDistributor.StdNode) {\n        SolrCmdDistributor.StdNode stdNode = (SolrCmdDistributor.StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n          leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n            replicasShouldBeInLowerTerms.add(coreNodeName);\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    handleReplicationFactor();\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();    \n    List<Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n      \n      if (error.req.node instanceof ForwardNode) {\n        // if it's a forward, any fail is a problem - \n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n      \n      // for now we don't error - we assume if it was added locally, we\n      // succeeded \n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n      \n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof StdNode) {\n        StdNode stdNode = (StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n            leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n            replicasShouldBeInLowerTerms.add(coreNodeName);\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    handleReplicationFactor();\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6af93a959465efb96e77e5e3143b530b1c5fba8","date":1571867341,"type":5,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/processor/DistributedZkUpdateProcessor#doDistribFinish().mjava","pathOld":"solr/core/src/java/org/apache/solr/update/processor/DistributedZkUpdateProcessor#doFinish().mjava","sourceNew":"  // TODO: optionally fail if n replicas are not reached...\n  protected void doDistribFinish() {\n    clusterState = zkController.getClusterState();\n\n    boolean shouldUpdateTerms = isLeader && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<SolrCmdDistributor.Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<SolrCmdDistributor.Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n\n      if (error.req.node instanceof SolrCmdDistributor.ForwardNode) {\n        // if it's a forward, any fail is a problem -\n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded\n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n\n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof SolrCmdDistributor.StdNode) {\n        SolrCmdDistributor.StdNode stdNode = (SolrCmdDistributor.StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n          leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n            replicasShouldBeInLowerTerms.add(coreNodeName);\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    handleReplicationFactor();\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n\n  }\n\n","sourceOld":"  // TODO: optionally fail if n replicas are not reached...\n  private void doFinish() {\n    boolean shouldUpdateTerms = isLeader && isIndexChanged;\n    if (shouldUpdateTerms) {\n      ZkShardTerms zkShardTerms = zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId());\n      if (skippedCoreNodeNames != null) {\n        zkShardTerms.ensureTermsIsHigher(cloudDesc.getCoreNodeName(), skippedCoreNodeNames);\n      }\n      zkController.getShardTerms(collection, cloudDesc.getShardId()).ensureHighestTermsAreNotZero();\n    }\n    // TODO: if not a forward and replication req is not specified, we could\n    // send in a background thread\n\n    cmdDistrib.finish();\n    List<SolrCmdDistributor.Error> errors = cmdDistrib.getErrors();\n    // TODO - we may need to tell about more than one error...\n\n    List<SolrCmdDistributor.Error> errorsForClient = new ArrayList<>(errors.size());\n    Set<String> replicasShouldBeInLowerTerms = new HashSet<>();\n    for (final SolrCmdDistributor.Error error : errors) {\n\n      if (error.req.node instanceof SolrCmdDistributor.ForwardNode) {\n        // if it's a forward, any fail is a problem -\n        // otherwise we assume things are fine if we got it locally\n        // until we start allowing min replication param\n        errorsForClient.add(error);\n        continue;\n      }\n\n      // else...\n\n      // for now we don't error - we assume if it was added locally, we\n      // succeeded\n      if (log.isWarnEnabled()) {\n        log.warn(\"Error sending update to \" + error.req.node.getBaseUrl(), error.e);\n      }\n\n      // Since it is not a forward request, for each fail, try to tell them to\n      // recover - the doc was already added locally, so it should have been\n      // legit\n\n      DistribPhase phase = DistribPhase.parseParam(error.req.uReq.getParams().get(DISTRIB_UPDATE_PARAM));\n      if (phase != DistribPhase.FROMLEADER)\n        continue; // don't have non-leaders try to recovery other nodes\n\n      // commits are special -- they can run on any node irrespective of whether it is a leader or not\n      // we don't want to run recovery on a node which missed a commit command\n      if (error.req.uReq.getParams().get(COMMIT_END_POINT) != null)\n        continue;\n\n      final String replicaUrl = error.req.node.getUrl();\n\n      // if the remote replica failed the request because of leader change (SOLR-6511), then fail the request\n      String cause = (error.e instanceof SolrException) ? ((SolrException)error.e).getMetadata(\"cause\") : null;\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        continue;\n      }\n\n      String collection = null;\n      String shardId = null;\n\n      if (error.req.node instanceof SolrCmdDistributor.StdNode) {\n        SolrCmdDistributor.StdNode stdNode = (SolrCmdDistributor.StdNode)error.req.node;\n        collection = stdNode.getCollection();\n        shardId = stdNode.getShardId();\n\n        // before we go setting other replicas to down, make sure we're still the leader!\n        String leaderCoreNodeName = null;\n        Exception getLeaderExc = null;\n        Replica leaderProps = null;\n        try {\n          leaderProps = zkController.getZkStateReader().getLeader(collection, shardId);\n          if (leaderProps != null) {\n            leaderCoreNodeName = leaderProps.getName();\n          }\n        } catch (Exception exc) {\n          getLeaderExc = exc;\n        }\n        if (leaderCoreNodeName == null) {\n          log.warn(\"Failed to determine if {} is still the leader for collection={} shardId={} \" +\n                  \"before putting {} into leader-initiated recovery\",\n              cloudDesc.getCoreNodeName(), collection, shardId, replicaUrl, getLeaderExc);\n        }\n\n        List<ZkCoreNodeProps> myReplicas = zkController.getZkStateReader().getReplicaProps(collection,\n            cloudDesc.getShardId(), cloudDesc.getCoreNodeName());\n        boolean foundErrorNodeInReplicaList = false;\n        if (myReplicas != null) {\n          for (ZkCoreNodeProps replicaProp : myReplicas) {\n            if (((Replica) replicaProp.getNodeProps()).getName().equals(((Replica)stdNode.getNodeProps().getNodeProps()).getName()))  {\n              foundErrorNodeInReplicaList = true;\n              break;\n            }\n          }\n        }\n\n        if (leaderCoreNodeName != null && cloudDesc.getCoreNodeName().equals(leaderCoreNodeName) // we are still same leader\n            && foundErrorNodeInReplicaList // we found an error for one of replicas\n            && !stdNode.getNodeProps().getCoreUrl().equals(leaderProps.getCoreUrl())) { // we do not want to put ourself into LIR\n          try {\n            String coreNodeName = ((Replica) stdNode.getNodeProps().getNodeProps()).getName();\n            // if false, then the node is probably not \"live\" anymore\n            // and we do not need to send a recovery message\n            Throwable rootCause = SolrException.getRootCause(error.e);\n            log.error(\"Setting up to try to start recovery on replica {} with url {} by increasing leader term\", coreNodeName, replicaUrl, rootCause);\n            replicasShouldBeInLowerTerms.add(coreNodeName);\n          } catch (Exception exc) {\n            Throwable setLirZnodeFailedCause = SolrException.getRootCause(exc);\n            log.error(\"Leader failed to set replica \" +\n                error.req.node.getUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n          }\n        } else {\n          // not the leader anymore maybe or the error'd node is not my replica?\n          if (!foundErrorNodeInReplicaList) {\n            log.warn(\"Core \"+cloudDesc.getCoreNodeName()+\" belonging to \"+collection+\" \"+\n                cloudDesc.getShardId()+\", does not have error'd node \" + stdNode.getNodeProps().getCoreUrl() + \" as a replica. \" +\n                \"No request recovery command will be sent!\");\n            if (!shardId.equals(cloudDesc.getShardId())) {\n              // some replicas on other shard did not receive the updates (ex: during splitshard),\n              // exception must be notified to clients\n              errorsForClient.add(error);\n            }\n          } else {\n            log.warn(\"Core \" + cloudDesc.getCoreNodeName() + \" is no longer the leader for \" + collection + \" \"\n                + shardId + \" or we tried to put ourself into LIR, no request recovery command will be sent!\");\n          }\n        }\n      }\n    }\n    if (!replicasShouldBeInLowerTerms.isEmpty()) {\n      zkController.getShardTerms(cloudDesc.getCollectionName(), cloudDesc.getShardId())\n          .ensureTermsIsHigher(cloudDesc.getCoreNodeName(), replicasShouldBeInLowerTerms);\n    }\n    handleReplicationFactor();\n    if (0 < errorsForClient.size()) {\n      throw new DistributedUpdatesAsyncException(errorsForClient);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d6af93a959465efb96e77e5e3143b530b1c5fba8":["9d70e774cb25c8a8d2c3e5e84200f235f9168d87"],"9d70e774cb25c8a8d2c3e5e84200f235f9168d87":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d6af93a959465efb96e77e5e3143b530b1c5fba8"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9d70e774cb25c8a8d2c3e5e84200f235f9168d87"],"d6af93a959465efb96e77e5e3143b530b1c5fba8":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9d70e774cb25c8a8d2c3e5e84200f235f9168d87":["d6af93a959465efb96e77e5e3143b530b1c5fba8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}