{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexTooManyDocs#testIndexTooManyDocs().mjava","commits":[{"id":"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef","date":1512420564,"type":0,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexTooManyDocs#testIndexTooManyDocs().mjava","pathOld":"/dev/null","sourceNew":"  /*\n   * This test produces a boat load of very small segments with lot of deletes which are likely deleting\n   * the entire segment. see https://issues.apache.org/jira/browse/LUCENE-8043\n   */\n  public void testIndexTooManyDocs() throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    int numMaxDoc = 25;\n    IndexWriterConfig config = new IndexWriterConfig();\n    config.setRAMBufferSizeMB(0.000001); // force lots of small segments and logs of concurrent deletes\n    IndexWriter writer = new IndexWriter(dir, config);\n    try {\n      IndexWriter.setMaxDocs(numMaxDoc);\n      int numThreads = 5 + random().nextInt(5);\n      Thread[] threads = new Thread[numThreads];\n      CountDownLatch latch = new CountDownLatch(numThreads);\n      CountDownLatch indexingDone = new CountDownLatch(numThreads - 2);\n      AtomicBoolean done = new AtomicBoolean(false);\n      for (int i = 0; i < numThreads; i++) {\n        if (i >= 2) {\n          threads[i] = new Thread(() -> {\n            latch.countDown();\n            try {\n              latch.await();\n            } catch (InterruptedException e) {\n              throw new AssertionError(e);\n            }\n            for (int d = 0; d < 100; d++) {\n              Document doc = new Document();\n              String id = Integer.toString(random().nextInt(numMaxDoc * 2));\n              doc.add(new StringField(\"id\", id, Field.Store.NO));\n              try {\n                Term t = new Term(\"id\", id);\n                if (random().nextInt(5) == 0) {\n                  writer.deleteDocuments(new TermQuery(t));\n                }\n                writer.updateDocument(t, doc);\n              } catch (IOException e) {\n                throw new AssertionError(e);\n              } catch (IllegalArgumentException e) {\n                assertEquals(\"number of documents in the index cannot exceed \" + IndexWriter.getActualMaxDocs(), e.getMessage());\n              }\n            }\n            indexingDone.countDown();\n          });\n        } else {\n          threads[i] = new Thread(() -> {\n            try {\n              latch.countDown();\n              latch.await();\n              DirectoryReader open = DirectoryReader.open(writer, true, true);\n              while (done.get() == false) {\n                DirectoryReader directoryReader = DirectoryReader.openIfChanged(open);\n                if (directoryReader != null) {\n                  open.close();\n                  open = directoryReader;\n                }\n              }\n              IOUtils.closeWhileHandlingException(open);\n            } catch (Exception e) {\n              throw new AssertionError(e);\n            }\n          });\n        }\n        threads[i].start();\n      }\n\n      indexingDone.await();\n      done.set(true);\n\n\n      for (int i = 0; i < numThreads; i++) {\n        threads[i].join();\n      }\n      writer.close();\n      dir.close();\n    } finally {\n      IndexWriter.setMaxDocs(IndexWriter.MAX_DOCS);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"963e10261f7f652e15240c710d46b5a282039bdd","date":1559033718,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexTooManyDocs#testIndexTooManyDocs().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexTooManyDocs#testIndexTooManyDocs().mjava","sourceNew":"  /*\n   * This test produces a boat load of very small segments with lot of deletes which are likely deleting\n   * the entire segment. see https://issues.apache.org/jira/browse/LUCENE-8043\n   */\n  public void testIndexTooManyDocs() throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    int numMaxDoc = 25;\n    IndexWriterConfig config = new IndexWriterConfig();\n    config.setRAMBufferSizeMB(0.000001); // force lots of small segments and logs of concurrent deletes\n    IndexWriter writer = new IndexWriter(dir, config);\n    try {\n      IndexWriter.setMaxDocs(numMaxDoc);\n      int numThreads = 5 + random().nextInt(5);\n      Thread[] threads = new Thread[numThreads];\n      CountDownLatch latch = new CountDownLatch(numThreads);\n      CountDownLatch indexingDone = new CountDownLatch(numThreads - 2);\n      AtomicBoolean done = new AtomicBoolean(false);\n      for (int i = 0; i < numThreads; i++) {\n        if (i >= 2) {\n          threads[i] = new Thread(() -> {\n            latch.countDown();\n            try {\n              try {\n                latch.await();\n              } catch (InterruptedException e) {\n                throw new AssertionError(e);\n              }\n              for (int d = 0; d < 100; d++) {\n                Document doc = new Document();\n                String id = Integer.toString(random().nextInt(numMaxDoc * 2));\n                doc.add(new StringField(\"id\", id, Field.Store.NO));\n                try {\n                  Term t = new Term(\"id\", id);\n                  if (random().nextInt(5) == 0) {\n                    writer.deleteDocuments(new TermQuery(t));\n                  }\n                  writer.updateDocument(t, doc);\n                } catch (IOException e) {\n                  throw new AssertionError(e);\n                } catch (IllegalArgumentException e) {\n                  assertEquals(\"number of documents in the index cannot exceed \" + IndexWriter.getActualMaxDocs(), e.getMessage());\n                }\n              }\n            } finally {\n              indexingDone.countDown();\n            }\n          });\n        } else {\n          threads[i] = new Thread(() -> {\n            try {\n              latch.countDown();\n              latch.await();\n              DirectoryReader open = DirectoryReader.open(writer, true, true);\n              while (done.get() == false) {\n                DirectoryReader directoryReader = DirectoryReader.openIfChanged(open);\n                if (directoryReader != null) {\n                  open.close();\n                  open = directoryReader;\n                }\n              }\n              IOUtils.closeWhileHandlingException(open);\n            } catch (Exception e) {\n              throw new AssertionError(e);\n            }\n          });\n        }\n        threads[i].start();\n      }\n\n      indexingDone.await();\n      done.set(true);\n\n\n      for (int i = 0; i < numThreads; i++) {\n        threads[i].join();\n      }\n      writer.close();\n      dir.close();\n    } finally {\n      IndexWriter.setMaxDocs(IndexWriter.MAX_DOCS);\n    }\n  }\n\n","sourceOld":"  /*\n   * This test produces a boat load of very small segments with lot of deletes which are likely deleting\n   * the entire segment. see https://issues.apache.org/jira/browse/LUCENE-8043\n   */\n  public void testIndexTooManyDocs() throws IOException, InterruptedException {\n    Directory dir = newDirectory();\n    int numMaxDoc = 25;\n    IndexWriterConfig config = new IndexWriterConfig();\n    config.setRAMBufferSizeMB(0.000001); // force lots of small segments and logs of concurrent deletes\n    IndexWriter writer = new IndexWriter(dir, config);\n    try {\n      IndexWriter.setMaxDocs(numMaxDoc);\n      int numThreads = 5 + random().nextInt(5);\n      Thread[] threads = new Thread[numThreads];\n      CountDownLatch latch = new CountDownLatch(numThreads);\n      CountDownLatch indexingDone = new CountDownLatch(numThreads - 2);\n      AtomicBoolean done = new AtomicBoolean(false);\n      for (int i = 0; i < numThreads; i++) {\n        if (i >= 2) {\n          threads[i] = new Thread(() -> {\n            latch.countDown();\n            try {\n              latch.await();\n            } catch (InterruptedException e) {\n              throw new AssertionError(e);\n            }\n            for (int d = 0; d < 100; d++) {\n              Document doc = new Document();\n              String id = Integer.toString(random().nextInt(numMaxDoc * 2));\n              doc.add(new StringField(\"id\", id, Field.Store.NO));\n              try {\n                Term t = new Term(\"id\", id);\n                if (random().nextInt(5) == 0) {\n                  writer.deleteDocuments(new TermQuery(t));\n                }\n                writer.updateDocument(t, doc);\n              } catch (IOException e) {\n                throw new AssertionError(e);\n              } catch (IllegalArgumentException e) {\n                assertEquals(\"number of documents in the index cannot exceed \" + IndexWriter.getActualMaxDocs(), e.getMessage());\n              }\n            }\n            indexingDone.countDown();\n          });\n        } else {\n          threads[i] = new Thread(() -> {\n            try {\n              latch.countDown();\n              latch.await();\n              DirectoryReader open = DirectoryReader.open(writer, true, true);\n              while (done.get() == false) {\n                DirectoryReader directoryReader = DirectoryReader.openIfChanged(open);\n                if (directoryReader != null) {\n                  open.close();\n                  open = directoryReader;\n                }\n              }\n              IOUtils.closeWhileHandlingException(open);\n            } catch (Exception e) {\n              throw new AssertionError(e);\n            }\n          });\n        }\n        threads[i].start();\n      }\n\n      indexingDone.await();\n      done.set(true);\n\n\n      for (int i = 0; i < numThreads; i++) {\n        threads[i].join();\n      }\n      writer.close();\n      dir.close();\n    } finally {\n      IndexWriter.setMaxDocs(IndexWriter.MAX_DOCS);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"963e10261f7f652e15240c710d46b5a282039bdd":["fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["963e10261f7f652e15240c710d46b5a282039bdd"]},"commit2Childs":{"963e10261f7f652e15240c710d46b5a282039bdd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef":["963e10261f7f652e15240c710d46b5a282039bdd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}