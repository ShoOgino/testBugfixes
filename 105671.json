{"path":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"modules/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cd659803551ebd8ca09b9e4ad7abd18d3d558f9d","date":1336650316,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      DocValuesField idvGroupField = new DocValuesField(\"group\", new BytesRef(), Type.BYTES_VAR_SORTED);\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":["fa0f44f887719e97183771e977cfc4bfb485b766"],"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"57ae3024996ccdb3c36c42cb890e1efb37df4ce8","date":1338343651,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      final boolean preFlex = \"Lucene3x\".equals(w.w.getConfig().getCodec().getName());\n      boolean canUseIDV = !preFlex;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = !preFlex;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, preFlex, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newField(\"group\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(group);\n      Field sort1 = newField(\"sort1\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newField(\"sort2\", \"\", StringField.TYPE_UNSTORED);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newField(\"content\", \"\", TextField.TYPE_UNSTORED);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":["a78a90fc9701e511308346ea29f4f5e548bb39fe","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b6a0e3c1c21aac8ecf75706605133012833585c7","date":1347535263,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(\"content\", new BytesRef(searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(\"content\", new BytesRef(searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f45457a742a53533c348c4b990b1c579ff364467","date":1353197071,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"854f97cd3613b9579fba83755c80b697e2f3993f","date":1353527621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e13078ebcbc41380853f4612578b706f40699cf5","date":1358203044,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0837ab0472feecb3a54260729d845f839e1cbd72","date":1358283639,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          // nocommit: what is going on here\n          boolean idvBasedImplsUsed = random().nextBoolean();\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e3eb88edd735aec1f42cbe41c478fb4f8d41f0ec","date":1358544193,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          //randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          // nocommit: what is going on here\n          boolean idvBasedImplsUsed = random().nextBoolean();\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b9d5080ee18db9b6f0dec906913eea6414758ec7","date":1358710681,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          //randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"423d89a2b3cc419b647c07c2b3fdbc54311d07f9","date":1358836612,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d4d69c535930b5cce125cff868d40f6373dc27d4","date":1360270101,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedBytesDocValuesField(\"group\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final int[] docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID[hit.doc]];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID[hit.doc]);\n            //System.out.println(\"  score=\" + hit.score + \" id=\" + docIDToID[hit.doc]);\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final int[] docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(\"group\", groupSort, groupOffset+topNGroups, canUseIDV);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, \"group\");\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, \"group\", groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          boolean idvBasedImplsUsed = DVFirstPassGroupingCollector.class.isAssignableFrom(c1.getClass());\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, idvBasedImplsUsed);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, true, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":["6b861c0fdfa4d005c70848c9121655e9dc704f96","cd659803551ebd8ca09b9e4ad7abd18d3d558f9d","7528ec8c6e88061e2e6af98c4ae1f72a30f180b2","4739c84c362b9673ab5ed3e038ff760c718c30c8","6005b05c19356dfca18f39979caeeb6b85bc88bb","8b48c85d1bf438ef65fbc1abe44f4e2c04a43e00","7cb194976386e349893169fee3c2aa6de3a83fd1"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"97d4692d0c601ff773f0a2231967312428a904e4","date":1366026608,"type":3,"author":"Martijn van Groningen","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<BytesRef>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<String,Map<Float,Float>>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<Float,Float>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<Boolean>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<BytesRef>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<BytesRef>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<Boolean>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"66d4c05d2724c63d6dcbdb32aab67299d77e3ca1","date":1370803313,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392","date":1377503666,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(new SlowCompositeReaderWrapper(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n          //final int groupOffset = 0;\n\n          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = _TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = _TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = _TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = _TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[_TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = _TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = _TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = _TestUtil.nextInt(random(), 0, (topNGroups-1)/2);\n          //final int groupOffset = 0;\n\n          final int docOffset = _TestUtil.nextInt(random(), 0, docsPerGroup-1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n          //final int groupOffset = 0;\n\n          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n          //final int groupOffset = 0;\n\n          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","date":1399816179,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n          //final int groupOffset = 0;\n\n          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group_dv\";\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n          //final int groupOffset = 0;\n\n          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b36581872266f87caefe066d71f76c81cf1b636e","date":1399817565,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n          //final int groupOffset = 0;\n\n          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(true));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n          //final int groupOffset = 0;\n\n          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group_dv\";\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9f72b4e0953a7ed14ab0430053c1bb65f2ef529","date":1399818681,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n        assertNotNull(docIDToIDBlocks);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n          //final int groupOffset = 0;\n\n          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n          //final int groupOffset = 0;\n\n          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(true));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":["9274621789ce990dbfef455dabdf026bb3184821"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9274621789ce990dbfef455dabdf026bb3184821","date":1400046684,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n        assertNotNull(docIDToIDBlocks);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n          //final int groupOffset = 0;\n\n          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":["c5f61d6a2927b52517a31a8bf022549d33b1dfec","1fa60a501961bce2ff07ee1cde7c78699025547e","e13078ebcbc41380853f4612578b706f40699cf5","854f97cd3613b9579fba83755c80b697e2f3993f","669ab70ac675a9fdd757b28a1a6ca63f667c7188","6005b05c19356dfca18f39979caeeb6b85bc88bb","7cb194976386e349893169fee3c2aa6de3a83fd1","b6a0e3c1c21aac8ecf75706605133012833585c7","f45457a742a53533c348c4b990b1c579ff364467","97d4692d0c601ff773f0a2231967312428a904e4","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","4739c84c362b9673ab5ed3e038ff760c718c30c8","66d4c05d2724c63d6dcbdb32aab67299d77e3ca1","b36581872266f87caefe066d71f76c81cf1b636e","8b48c85d1bf438ef65fbc1abe44f4e2c04a43e00","6b861c0fdfa4d005c70848c9121655e9dc704f96","307cff5af2b00f126fdf9d3435b75d5ed4d0f402","d4aa50b979cd392e00e5bc0f23f78cbd106cb968","090a0320e4de4a3674376aef96b9701f47564f86","6b588d7000deacb0a01f30746b91644112b94326","6613659748fe4411a7dcf85266e55db1f95f7315","f4c236b7b2d1cb61683f5ba02d09249fdb42a7fd","629c38c4ae4e303d0617e05fbfe508140b32f0a3","e3eb88edd735aec1f42cbe41c478fb4f8d41f0ec","8fa38e5ecc85303dce7ded93b3cc9a48b3d546d9","b9d5080ee18db9b6f0dec906913eea6414758ec7","3ce36a160d1241ae9c70e109dc3fdfdfb009674a","c9f72b4e0953a7ed14ab0430053c1bb65f2ef529","d4d1b712a120ae5e1d1a6fddf8eaa6f642264ace"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n          //final int groupOffset = 0;\n\n          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = _TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      boolean canUseIDV = true;\n\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group_dv\", new BytesRef());\n      if (canUseIDV) {\n        doc.add(idvGroupField);\n        docNoGroup.add(idvGroupField);\n      }\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = newStringField(\"sort1\", \"\", Field.Store.NO);\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = newStringField(\"sort2\", \"\", Field.Store.NO);\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      docNoGroup.add(id);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          if (canUseIDV) {\n            idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n          }\n        } else if (canUseIDV) {\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setStringValue(groupDoc.sort1.utf8ToString());\n        sort2.setStringValue(groupDoc.sort2.utf8ToString());\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      // NOTE: intentional but temporary field cache insanity!\n      final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(r), \"id\", false);\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      try {\n        final IndexSearcher s = newSearcher(r);\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searcher=\" + s);\n        }\n\n        if (SlowCompositeReaderWrapper.class.isAssignableFrom(s.getIndexReader().getClass())) {\n          canUseIDV = false;\n        } else {\n          canUseIDV = true;\n        }\n        final ShardState shards = new ShardState(s);\n\n        for(int contentID=0;contentID<3;contentID++) {\n          final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocs[docIDToID.get(hit.doc)];\n            assertTrue(gd.score == 0.0);\n            gd.score = hit.score;\n            assertEquals(gd.id, docIDToID.get(hit.doc));\n          }\n        }\n\n        for(GroupDoc gd : groupDocs) {\n          assertTrue(gd.score != 0.0);\n        }\n\n        // Build 2nd index, where docs are added in blocks by\n        // group, so we can use single pass collector\n        dirBlocks = newDirectory();\n        rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n        final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n        final FieldCache.Ints docIDToIDBlocks = FieldCache.DEFAULT.getInts(SlowCompositeReaderWrapper.wrap(rBlocks), \"id\", false);\n\n        final IndexSearcher sBlocks = newSearcher(rBlocks);\n        final ShardState shardsBlocks = new ShardState(sBlocks);\n\n        // ReaderBlocks only increases maxDoc() vs reader, which\n        // means a monotonic shift in scores, so we can\n        // reliably remap them w/ Map:\n        final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n        // Tricky: must separately set .score2, because the doc\n        // block index was created with possible deletions!\n        //System.out.println(\"fixup score2\");\n        for(int contentID=0;contentID<3;contentID++) {\n          //System.out.println(\"  term=real\" + contentID);\n          final Map<Float,Float> termScoreMap = new HashMap<>();\n          scoreMap.put(\"real\"+contentID, termScoreMap);\n          //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n          //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n          final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n          for(ScoreDoc hit : hits) {\n            final GroupDoc gd = groupDocsByID[docIDToIDBlocks.get(hit.doc)];\n            assertTrue(gd.score2 == 0.0);\n            gd.score2 = hit.score;\n            assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n            //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n            termScoreMap.put(gd.score, gd.score2);\n          }\n        }\n\n        for(int searchIter=0;searchIter<100;searchIter++) {\n\n          if (VERBOSE) {\n            System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n          }\n\n          final String searchTerm = \"real\" + random().nextInt(3);\n          final boolean fillFields = random().nextBoolean();\n          boolean getScores = random().nextBoolean();\n          final boolean getMaxScores = random().nextBoolean();\n          final Sort groupSort = getRandomSort();\n          //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n          // TODO: also test null (= sort by relevance)\n          final Sort docSort = getRandomSort();\n\n          for(SortField sf : docSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          for(SortField sf : groupSort.getSort()) {\n            if (sf.getType() == SortField.Type.SCORE) {\n              getScores = true;\n              break;\n            }\n          }\n\n          final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n          //final int topNGroups = 10;\n          final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n\n          final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n          //final int groupOffset = 0;\n\n          final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n          //final int docOffset = 0;\n\n          final boolean doCache = random().nextBoolean();\n          final boolean doAllGroups = random().nextBoolean();\n          if (VERBOSE) {\n            System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n          }\n\n          String groupField = \"group\";\n          if (canUseIDV && random().nextBoolean()) {\n            groupField += \"_dv\";\n          }\n          if (VERBOSE) {\n            System.out.println(\"  groupField=\" + groupField);\n          }\n          final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n          final CachingCollector cCache;\n          final Collector c;\n\n          final AbstractAllGroupsCollector<?> allGroupsCollector;\n          if (doAllGroups) {\n            allGroupsCollector = createAllGroupsCollector(c1, groupField);\n          } else {\n            allGroupsCollector = null;\n          }\n\n          final boolean useWrappingCollector = random().nextBoolean();\n\n          if (doCache) {\n            final double maxCacheMB = random().nextDouble();\n            if (VERBOSE) {\n              System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n            }\n\n            if (useWrappingCollector) {\n              if (doAllGroups) {\n                cCache = CachingCollector.create(c1, true, maxCacheMB);\n                c = MultiCollector.wrap(cCache, allGroupsCollector);\n              } else {\n                c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n              }\n            } else {\n              // Collect only into cache, then replay multiple times:\n              c = cCache = CachingCollector.create(false, true, maxCacheMB);\n            }\n          } else {\n            cCache = null;\n            if (doAllGroups) {\n              c = MultiCollector.wrap(c1, allGroupsCollector);\n            } else {\n              c = c1;\n            }\n          }\n\n          // Search top reader:\n          final Query query = new TermQuery(new Term(\"content\", searchTerm));\n\n          s.search(query, c);\n\n          if (doCache && !useWrappingCollector) {\n            if (cCache.isCached()) {\n              // Replay for first-pass grouping\n              cCache.replay(c1);\n              if (doAllGroups) {\n                // Replay for all groups:\n                cCache.replay(allGroupsCollector);\n              }\n            } else {\n              // Replay by re-running search:\n              s.search(query, c1);\n              if (doAllGroups) {\n                s.search(query, allGroupsCollector);\n              }\n            }\n          }\n\n          // Get 1st pass top groups\n          final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n          final TopGroups<BytesRef> groupsResult;\n          if (VERBOSE) {\n            System.out.println(\"TEST: first pass topGroups\");\n            if (topGroups == null) {\n              System.out.println(\"  null\");\n            } else {\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n          }\n\n          // Get 1st pass top groups using shards\n\n          ValueHolder<Boolean> idvBasedImplsUsedSharded = new ValueHolder<>(false);\n          final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n              groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, canUseIDV, false, idvBasedImplsUsedSharded);\n          final AbstractSecondPassGroupingCollector<?> c2;\n          if (topGroups != null) {\n\n            if (VERBOSE) {\n              System.out.println(\"TEST: topGroups\");\n              for (SearchGroup<BytesRef> searchGroup : topGroups) {\n                System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n              }\n            }\n\n            c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n            if (doCache) {\n              if (cCache.isCached()) {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache is intact\");\n                }\n                cCache.replay(c2);\n              } else {\n                if (VERBOSE) {\n                  System.out.println(\"TEST: cache was too large\");\n                }\n                s.search(query, c2);\n              }\n            } else {\n              s.search(query, c2);\n            }\n\n            if (doAllGroups) {\n              TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n              groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n            } else {\n              groupsResult = getTopGroups(c2, docOffset);\n            }\n          } else {\n            c2 = null;\n            groupsResult = null;\n            if (VERBOSE) {\n              System.out.println(\"TEST:   no results\");\n            }\n          }\n\n          final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n\n          if (VERBOSE) {\n            if (expectedGroups == null) {\n              System.out.println(\"TEST: no expected groups\");\n            } else {\n              System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n                }\n              }\n            }\n\n            if (groupsResult == null) {\n              System.out.println(\"TEST: no matched groups\");\n            } else {\n              System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n\n              if (searchIter == 14) {\n                for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                  System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n                }\n              }\n            }\n\n            if (topGroupsShards == null) {\n              System.out.println(\"TEST: no matched-merged groups\");\n            } else {\n              System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n              for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, groupField.endsWith(\"_dv\"));\n\n          // Confirm merged shards match:\n          assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, idvBasedImplsUsedSharded.value);\n          if (topGroupsShards != null) {\n            verifyShards(shards.docStarts, topGroupsShards);\n          }\n\n          final boolean needsScores = getScores || getMaxScores || docSort == null;\n          final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n          final TermAllGroupsCollector allGroupsCollector2;\n          final Collector c4;\n          if (doAllGroups) {\n            // NOTE: must be \"group\" and not \"group_dv\"\n            // (groupField) because we didn't index doc\n            // values in the block index:\n            allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n            c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n          } else {\n            allGroupsCollector2 = null;\n            c4 = c3;\n          }\n          // Get block grouping result:\n          sBlocks.search(query, c4);\n          @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n          final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n          final TopGroups<BytesRef> groupsResultBlocks;\n          if (doAllGroups && tempTopGroupsBlocks != null) {\n            assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n            groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n          } else {\n            groupsResultBlocks = tempTopGroupsBlocks;\n          }\n\n          if (VERBOSE) {\n            if (groupsResultBlocks == null) {\n              System.out.println(\"TEST: no block groups\");\n            } else {\n              System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n              boolean first = true;\n              for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n                System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n                for(ScoreDoc sd : gd.scoreDocs) {\n                  System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                  if (first) {\n                    System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                    first = false;\n                  }\n                }\n              }\n            }\n          }\n\n          // Get shard'd block grouping result:\n          // Block index does not index DocValues so we pass\n          // false for canUseIDV:\n          final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n              groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false, new ValueHolder<>(false));\n\n          if (expectedGroups != null) {\n            // Fixup scores for reader2\n            for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n              for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n                final GroupDoc gd = groupDocsByID[hit.doc];\n                assertEquals(gd.id, hit.doc);\n                //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n                hit.score = gd.score2;\n              }\n            }\n\n            final SortField[] sortFields = groupSort.getSort();\n            final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n            for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n              if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  if (groupDocsHits.groupSortValues != null) {\n                    //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                    groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                    assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                  }\n                }\n              }\n            }\n\n            final SortField[] docSortFields = docSort.getSort();\n            for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n              if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n                for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                  for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                    FieldDoc hit = (FieldDoc) _hit;\n                    if (hit.fields != null) {\n                      hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                      assertNotNull(hit.fields[docSortIDX]);\n                    }\n                  }\n                }\n              }\n            }\n          }\n\n          assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n          assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n        }\n      } finally {\n        QueryUtils.purgeFieldCache(r);\n        if (rBlocks != null) {\n          QueryUtils.purgeFieldCache(rBlocks);\n        }\n      }\n\n      r.close();\n      dir.close();\n\n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(TEST_VERSION_CURRENT,\n                                                                       new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.shutdown();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ac5d789c4e320734d65e4c2e8542ae0bc8d19f7","date":1421315622,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(false, true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60596f28be69b10c37a56a303c2dbea07b2ca4ba","date":1425060541,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\")));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\")));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new CachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\"))));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"41e0d5825f76d6bd3636a0dbaf6aa020cb357334","date":1435740067,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Filter lastDocInBlock = new QueryWrapperFilter(new TermQuery(new Term(\"groupend\", \"x\")));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, lastDocInBlock);\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e7cf486535cf187cb3745154ca5dd3de3bd2999","date":1449256632,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        // TODO: also test null (= sort by relevance)\n        final Sort docSort = getRandomSort();\n        \n        for(SortField sf : docSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        for(SortField sf : groupSort.getSort()) {\n          if (sf.getType() == SortField.Type.SCORE) {\n            getScores = true;\n            break;\n          }\n        }\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n        \n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"770342641f7b505eaa8dccdc666158bff2419109","date":1449868421,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      IntField id = new IntField(\"id\", 0, Field.Store.NO);\n      doc.add(id);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(id);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        id.setIntValue(groupDoc.id);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5e3e97dbceff4180d72bfc95d4d9facd37dcd8bd","date":1453499687,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = (int) docIDToID.get(hit.doc);\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocs[(int) docIDToID.get(hit.doc)];\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, docIDToID.get(hit.doc));\n        }\n      }\n      \n      for(GroupDoc gd : groupDocs) {\n        assertTrue(gd.score != 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = (int) docIDToID.get(hit.doc);\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = (int) docIDToID.get(hit.doc);\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n\n      final NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = (int) docIDToID.get(hit.doc);\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      final NumericDocValues docIDToIDBlocks = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(docIDToIDBlocks);\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[(int) docIDToIDBlocks.get(hit.doc)];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks.get(hit.doc));\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks.get(hit.doc));\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID.get(docIDX) + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID.get(sd.doc) + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks.get(sd.doc) + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"20e94e61fe5291647346b70437617e6b6c370408","date":1483783127,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final SecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","date":1484239864,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final SecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final AbstractFirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AbstractAllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final AbstractSecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7ae958a739da1866696f442384393ba2f13e33e5","date":1491819018,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final SecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54ca69905c5d9d1529286f06ab1d12c68f6c13cb","date":1492683554,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, false);\n        final SecondPassGroupingCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupField, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final TermAllGroupsCollector allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new TermAllGroupsCollector(\"group\");\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9fc47cb7b4346802411bb432f501ed0673d7119e","date":1512640179,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, ScoreMode.COMPLETE_NO_SCORES));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"417142ff08fda9cf0b72d5133e63097a166c6458","date":1512729693,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, ScoreMode.COMPLETE_NO_SCORES));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, false));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"efac7e9b9b43e45142de4b73db16d889bd4004b7","date":1521039576,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, ScoreMode.COMPLETE_NO_SCORES));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, ScoreMode.COMPLETE_NO_SCORES));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"475584d5e08a22ad3fc7babefe006d77bc744567","date":1523282824,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores,\n            sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, ScoreMode.COMPLETE_NO_SCORES));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d58e44159788900f4a2113b84463dc3fbbf80f20","date":1523319203,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores,\n            sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores, sBlocks.createNormalizedWeight(lastDocInBlock, ScoreMode.COMPLETE_NO_SCORES));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04c370507e5521b2eb998530736f1c19b851ed5a","date":1531911305,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores,\n            sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean fillFields = random().nextBoolean();\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset, fillFields);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores, fillFields);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, fillFields, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, fillFields, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores,\n            sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, fillFields, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1d3f7ab1a502671bbdb03bcced21e764d2483221","date":1532329609,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getMaxScores);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups,\n            groupSort.needsScores() || docSort.needsScores(), sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        boolean getScores = random().nextBoolean();\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        getScores |= (groupSort.needsScores() || docSort.needsScores());\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getScores=\" + getScores + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getScores, getMaxScores);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, getScores, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, getScores, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, getScores, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n        \n        final boolean needsScores = getScores || getMaxScores || docSort == null;\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups, needsScores,\n            sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getScores, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, getScores, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, getScores, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83788ad129a5154d5c6562c4e8ce3db48793aada","date":1532961485,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getMaxScores);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups,\n            groupSort.needsScores() || docSort.needsScores(), sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getMaxScores);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups,\n            groupSort.needsScores() || docSort.needsScores(), sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9","date":1574619880,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getMaxScores);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups,\n            groupSort.needsScores() || docSort.needsScores(), sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getMaxScores);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups,\n            groupSort.needsScores() || docSort.needsScores(), sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb9c3baacabd473e8ecd6c4948aabacead49b88e","date":1574700980,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getMaxScores);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups,\n            groupSort.needsScores() || docSort.needsScores(), sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getMaxScores);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups,\n            groupSort.needsScores() || docSort.needsScores(), sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f","date":1579652839,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","pathOld":"lucene/grouping/src/test/org/apache/lucene/search/grouping/TestGrouping#testRandom().mjava","sourceNew":"  public void testRandom() throws Exception {\n    int numberOfRuns = atLeast(1);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = atLeast(100);\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getMaxScores);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups,\n            groupSort.needsScores() || docSort.needsScores(), sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","sourceOld":"  public void testRandom() throws Exception {\n    int numberOfRuns = TestUtil.nextInt(random(), 3, 6);\n    for (int iter=0; iter<numberOfRuns; iter++) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter=\" + iter);\n      }\n\n      final int numDocs = TestUtil.nextInt(random(), 100, 1000) * RANDOM_MULTIPLIER;\n      //final int numDocs = _TestUtil.nextInt(random, 5, 20);\n\n      final int numGroups = TestUtil.nextInt(random(), 1, numDocs);\n\n      if (VERBOSE) {\n        System.out.println(\"TEST: numDocs=\" + numDocs + \" numGroups=\" + numGroups);\n      }\n\n      final List<BytesRef> groups = new ArrayList<>();\n      for(int i=0;i<numGroups;i++) {\n        String randomValue;\n        do {\n          // B/c of DV based impl we can't see the difference between an empty string and a null value.\n          // For that reason we don't generate empty string\n          // groups.\n          randomValue = TestUtil.randomRealisticUnicodeString(random());\n          //randomValue = TestUtil.randomSimpleString(random());\n        } while (\"\".equals(randomValue));\n\n        groups.add(new BytesRef(randomValue));\n      }\n      final String[] contentStrings = new String[TestUtil.nextInt(random(), 2, 20)];\n      if (VERBOSE) {\n        System.out.println(\"TEST: create fake content\");\n      }\n      for(int contentIDX=0;contentIDX<contentStrings.length;contentIDX++) {\n        final StringBuilder sb = new StringBuilder();\n        sb.append(\"real\").append(random().nextInt(3)).append(' ');\n        final int fakeCount = random().nextInt(10);\n        for(int fakeIDX=0;fakeIDX<fakeCount;fakeIDX++) {\n          sb.append(\"fake \");\n        }\n        contentStrings[contentIDX] = sb.toString();\n        if (VERBOSE) {\n          System.out.println(\"  content=\" + sb.toString());\n        }\n      }\n\n      Directory dir = newDirectory();\n      RandomIndexWriter w = new RandomIndexWriter(\n                                                  random(),\n                                                  dir,\n                                                  newIndexWriterConfig(new MockAnalyzer(random())));\n      Document doc = new Document();\n      Document docNoGroup = new Document();\n      Field idvGroupField = new SortedDocValuesField(\"group\", new BytesRef());\n      doc.add(idvGroupField);\n      docNoGroup.add(idvGroupField);\n\n      Field group = newStringField(\"group\", \"\", Field.Store.NO);\n      doc.add(group);\n      Field sort1 = new SortedDocValuesField(\"sort1\", new BytesRef());\n      doc.add(sort1);\n      docNoGroup.add(sort1);\n      Field sort2 = new SortedDocValuesField(\"sort2\", new BytesRef());\n      doc.add(sort2);\n      docNoGroup.add(sort2);\n      Field content = newTextField(\"content\", \"\", Field.Store.NO);\n      doc.add(content);\n      docNoGroup.add(content);\n      NumericDocValuesField idDV = new NumericDocValuesField(\"id\", 0);\n      doc.add(idDV);\n      docNoGroup.add(idDV);\n      final GroupDoc[] groupDocs = new GroupDoc[numDocs];\n      for(int i=0;i<numDocs;i++) {\n        final BytesRef groupValue;\n        if (random().nextInt(24) == 17) {\n          // So we test the \"doc doesn't have the group'd\n          // field\" case:\n          groupValue = null;\n        } else {\n          groupValue = groups.get(random().nextInt(groups.size()));\n        }\n        final GroupDoc groupDoc = new GroupDoc(i,\n                                               groupValue,\n                                               groups.get(random().nextInt(groups.size())),\n                                               groups.get(random().nextInt(groups.size())),\n                                               contentStrings[random().nextInt(contentStrings.length)]);\n        if (VERBOSE) {\n          System.out.println(\"  doc content=\" + groupDoc.content + \" id=\" + i + \" group=\" + (groupDoc.group == null ? \"null\" : groupDoc.group.utf8ToString()) + \" sort1=\" + groupDoc.sort1.utf8ToString() + \" sort2=\" + groupDoc.sort2.utf8ToString());\n        }\n\n        groupDocs[i] = groupDoc;\n        if (groupDoc.group != null) {\n          group.setStringValue(groupDoc.group.utf8ToString());\n          idvGroupField.setBytesValue(BytesRef.deepCopyOf(groupDoc.group));\n        } else {\n          // TODO: not true\n          // Must explicitly set empty string, else eg if\n          // the segment has all docs missing the field then\n          // we get null back instead of empty BytesRef:\n          idvGroupField.setBytesValue(new BytesRef());\n        }\n        sort1.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort1));\n        sort2.setBytesValue(BytesRef.deepCopyOf(groupDoc.sort2));\n        content.setStringValue(groupDoc.content);\n        idDV.setLongValue(groupDoc.id);\n        if (groupDoc.group == null) {\n          w.addDocument(docNoGroup);\n        } else {\n          w.addDocument(doc);\n        }\n      }\n\n      final GroupDoc[] groupDocsByID = new GroupDoc[groupDocs.length];\n      System.arraycopy(groupDocs, 0, groupDocsByID, 0, groupDocs.length);\n\n      final DirectoryReader r = w.getReader();\n      w.close();\n      \n      NumericDocValues values = MultiDocValues.getNumericValues(r, \"id\");\n      int[] docIDToID = new int[r.maxDoc()];\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToID[i] = (int) values.longValue();\n      }\n      DirectoryReader rBlocks = null;\n      Directory dirBlocks = null;\n\n      final IndexSearcher s = newSearcher(r);\n      // This test relies on the fact that longer fields produce lower scores\n      s.setSimilarity(new BM25Similarity());\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: searcher=\" + s);\n      }\n      \n      final ShardState shards = new ShardState(s);\n      \n      Set<Integer> seenIDs = new HashSet<>();\n      for(int contentID=0;contentID<3;contentID++) {\n        final ScoreDoc[] hits = s.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          int idValue = docIDToID[hit.doc];\n\n          final GroupDoc gd = groupDocs[idValue];\n          seenIDs.add(idValue);\n          assertTrue(gd.score == 0.0);\n          gd.score = hit.score;\n          assertEquals(gd.id, idValue);\n        }\n      }\n      \n      // make sure all groups were seen across the hits\n      assertEquals(groupDocs.length, seenIDs.size());\n\n      for(GroupDoc gd : groupDocs) {\n        assertTrue(Float.isFinite(gd.score));\n        assertTrue(gd.score >= 0.0);\n      }\n      \n      // Build 2nd index, where docs are added in blocks by\n      // group, so we can use single pass collector\n      dirBlocks = newDirectory();\n      rBlocks = getDocBlockReader(dirBlocks, groupDocs);\n      final Query lastDocInBlock = new TermQuery(new Term(\"groupend\", \"x\"));\n      \n      final IndexSearcher sBlocks = newSearcher(rBlocks);\n      // This test relies on the fact that longer fields produce lower scores\n      sBlocks.setSimilarity(new BM25Similarity());\n\n      final ShardState shardsBlocks = new ShardState(sBlocks);\n      \n      // ReaderBlocks only increases maxDoc() vs reader, which\n      // means a monotonic shift in scores, so we can\n      // reliably remap them w/ Map:\n      final Map<String,Map<Float,Float>> scoreMap = new HashMap<>();\n\n      values = MultiDocValues.getNumericValues(rBlocks, \"id\");\n      assertNotNull(values);\n      int[] docIDToIDBlocks = new int[rBlocks.maxDoc()];\n      for(int i=0;i<rBlocks.maxDoc();i++) {\n        assertEquals(i, values.nextDoc());\n        docIDToIDBlocks[i] = (int) values.longValue();\n      }\n      \n      // Tricky: must separately set .score2, because the doc\n      // block index was created with possible deletions!\n      //System.out.println(\"fixup score2\");\n      for(int contentID=0;contentID<3;contentID++) {\n        //System.out.println(\"  term=real\" + contentID);\n        final Map<Float,Float> termScoreMap = new HashMap<>();\n        scoreMap.put(\"real\"+contentID, termScoreMap);\n        //System.out.println(\"term=real\" + contentID + \" dfold=\" + s.docFreq(new Term(\"content\", \"real\"+contentID)) +\n        //\" dfnew=\" + sBlocks.docFreq(new Term(\"content\", \"real\"+contentID)));\n        final ScoreDoc[] hits = sBlocks.search(new TermQuery(new Term(\"content\", \"real\"+contentID)), numDocs).scoreDocs;\n        for(ScoreDoc hit : hits) {\n          final GroupDoc gd = groupDocsByID[docIDToIDBlocks[hit.doc]];\n          assertTrue(gd.score2 == 0.0);\n          gd.score2 = hit.score;\n          assertEquals(gd.id, docIDToIDBlocks[hit.doc]);\n          //System.out.println(\"    score=\" + gd.score + \" score2=\" + hit.score + \" id=\" + docIDToIDBlocks[hit.doc]);\n          termScoreMap.put(gd.score, gd.score2);\n        }\n      }\n      \n      for(int searchIter=0;searchIter<100;searchIter++) {\n        \n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: searchIter=\" + searchIter);\n        }\n        \n        final String searchTerm = \"real\" + random().nextInt(3);\n        final boolean getMaxScores = random().nextBoolean();\n        final Sort groupSort = getRandomSort();\n        //final Sort groupSort = new Sort(new SortField[] {new SortField(\"sort1\", SortField.STRING), new SortField(\"id\", SortField.INT)});\n        final Sort docSort = getRandomSort();\n        \n        final int topNGroups = TestUtil.nextInt(random(), 1, 30);\n        //final int topNGroups = 10;\n        final int docsPerGroup = TestUtil.nextInt(random(), 1, 50);\n        \n        final int groupOffset = TestUtil.nextInt(random(), 0, (topNGroups - 1) / 2);\n        //final int groupOffset = 0;\n        \n        final int docOffset = TestUtil.nextInt(random(), 0, docsPerGroup - 1);\n        //final int docOffset = 0;\n\n        final boolean doCache = random().nextBoolean();\n        final boolean doAllGroups = random().nextBoolean();\n        if (VERBOSE) {\n          System.out.println(\"TEST: groupSort=\" + groupSort + \" docSort=\" + docSort + \" searchTerm=\" + searchTerm + \" dF=\" + r.docFreq(new Term(\"content\", searchTerm))  +\" dFBlock=\" + rBlocks.docFreq(new Term(\"content\", searchTerm)) + \" topNGroups=\" + topNGroups + \" groupOffset=\" + groupOffset + \" docOffset=\" + docOffset + \" doCache=\" + doCache + \" docsPerGroup=\" + docsPerGroup + \" doAllGroups=\" + doAllGroups + \" getMaxScores=\" + getMaxScores);\n        }\n        \n        String groupField = \"group\";\n        if (VERBOSE) {\n          System.out.println(\"  groupField=\" + groupField);\n        }\n        final FirstPassGroupingCollector<?> c1 = createRandomFirstPassCollector(groupField, groupSort, groupOffset+topNGroups);\n        final CachingCollector cCache;\n        final Collector c;\n        \n        final AllGroupsCollector<?> allGroupsCollector;\n        if (doAllGroups) {\n          allGroupsCollector = createAllGroupsCollector(c1, groupField);\n        } else {\n          allGroupsCollector = null;\n        }\n        \n        final boolean useWrappingCollector = random().nextBoolean();\n        \n        if (doCache) {\n          final double maxCacheMB = random().nextDouble();\n          if (VERBOSE) {\n            System.out.println(\"TEST: maxCacheMB=\" + maxCacheMB);\n          }\n          \n          if (useWrappingCollector) {\n            if (doAllGroups) {\n              cCache = CachingCollector.create(c1, true, maxCacheMB);\n              c = MultiCollector.wrap(cCache, allGroupsCollector);\n            } else {\n              c = cCache = CachingCollector.create(c1, true, maxCacheMB);\n            }\n          } else {\n            // Collect only into cache, then replay multiple times:\n            c = cCache = CachingCollector.create(true, maxCacheMB);\n          }\n        } else {\n          cCache = null;\n          if (doAllGroups) {\n            c = MultiCollector.wrap(c1, allGroupsCollector);\n          } else {\n            c = c1;\n          }\n        }\n        \n        // Search top reader:\n        final Query query = new TermQuery(new Term(\"content\", searchTerm));\n        \n        s.search(query, c);\n        \n        if (doCache && !useWrappingCollector) {\n          if (cCache.isCached()) {\n            // Replay for first-pass grouping\n            cCache.replay(c1);\n            if (doAllGroups) {\n              // Replay for all groups:\n              cCache.replay(allGroupsCollector);\n            }\n          } else {\n            // Replay by re-running search:\n            s.search(query, c1);\n            if (doAllGroups) {\n              s.search(query, allGroupsCollector);\n            }\n          }\n        }\n        \n        // Get 1st pass top groups\n        final Collection<SearchGroup<BytesRef>> topGroups = getSearchGroups(c1, groupOffset);\n        final TopGroups<BytesRef> groupsResult;\n        if (VERBOSE) {\n          System.out.println(\"TEST: first pass topGroups\");\n          if (topGroups == null) {\n            System.out.println(\"  null\");\n          } else {\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n        }\n        \n        // Get 1st pass top groups using shards\n        \n        final TopGroups<BytesRef> topGroupsShards = searchShards(s, shards.subSearchers, query, groupSort, docSort,\n            groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, true, true);\n        final TopGroupsCollector<?> c2;\n        if (topGroups != null) {\n          \n          if (VERBOSE) {\n            System.out.println(\"TEST: topGroups\");\n            for (SearchGroup<BytesRef> searchGroup : topGroups) {\n              System.out.println(\"  \" + (searchGroup.groupValue == null ? \"null\" : searchGroup.groupValue.utf8ToString()) + \": \" + Arrays.deepToString(searchGroup.sortValues));\n            }\n          }\n          \n          c2 = createSecondPassCollector(c1, groupSort, docSort, groupOffset, docOffset + docsPerGroup, getMaxScores);\n          if (doCache) {\n            if (cCache.isCached()) {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache is intact\");\n              }\n              cCache.replay(c2);\n            } else {\n              if (VERBOSE) {\n                System.out.println(\"TEST: cache was too large\");\n              }\n              s.search(query, c2);\n            }\n          } else {\n            s.search(query, c2);\n          }\n          \n          if (doAllGroups) {\n            TopGroups<BytesRef> tempTopGroups = getTopGroups(c2, docOffset);\n            groupsResult = new TopGroups<>(tempTopGroups, allGroupsCollector.getGroupCount());\n          } else {\n            groupsResult = getTopGroups(c2, docOffset);\n          }\n        } else {\n          c2 = null;\n          groupsResult = null;\n          if (VERBOSE) {\n            System.out.println(\"TEST:   no results\");\n          }\n        }\n        \n        final TopGroups<BytesRef> expectedGroups = slowGrouping(groupDocs, searchTerm, getMaxScores, doAllGroups, groupSort, docSort, topNGroups, docsPerGroup, groupOffset, docOffset);\n        \n        if (VERBOSE) {\n          if (expectedGroups == null) {\n            System.out.println(\"TEST: no expected groups\");\n          } else {\n            System.out.println(\"TEST: expected groups totalGroupedHitCount=\" + expectedGroups.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : expectedGroups.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value + \" scoreDocs.len=\" + gd.scoreDocs.length);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + sd.doc + \" score=\" + sd.score);\n              }\n            }\n          }\n          \n          if (groupsResult == null) {\n            System.out.println(\"TEST: no matched groups\");\n          } else {\n            System.out.println(\"TEST: matched groups totalGroupedHitCount=\" + groupsResult.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : groupsResult.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n            \n            if (searchIter == 14) {\n              for(int docIDX=0;docIDX<s.getIndexReader().maxDoc();docIDX++) {\n                System.out.println(\"ID=\" + docIDToID[docIDX] + \" explain=\" + s.explain(query, docIDX));\n              }\n            }\n          }\n          \n          if (topGroupsShards == null) {\n            System.out.println(\"TEST: no matched-merged groups\");\n          } else {\n            System.out.println(\"TEST: matched-merged groups totalGroupedHitCount=\" + topGroupsShards.totalGroupedHitCount);\n            for(GroupDocs<BytesRef> gd : topGroupsShards.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToID[sd.doc] + \" score=\" + sd.score);\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToID, expectedGroups, groupsResult, true, true, true);\n        \n        // Confirm merged shards match:\n        assertEquals(docIDToID, expectedGroups, topGroupsShards, true, false, true);\n        if (topGroupsShards != null) {\n          verifyShards(shards.docStarts, topGroupsShards);\n        }\n\n        final BlockGroupingCollector c3 = new BlockGroupingCollector(groupSort, groupOffset+topNGroups,\n            groupSort.needsScores() || docSort.needsScores(), sBlocks.createWeight(sBlocks.rewrite(lastDocInBlock), ScoreMode.COMPLETE_NO_SCORES, 1));\n        final AllGroupsCollector<BytesRef> allGroupsCollector2;\n        final Collector c4;\n        if (doAllGroups) {\n          // NOTE: must be \"group\" and not \"group_dv\"\n          // (groupField) because we didn't index doc\n          // values in the block index:\n          allGroupsCollector2 = new AllGroupsCollector<>(new TermGroupSelector(\"group\"));\n          c4 = MultiCollector.wrap(c3, allGroupsCollector2);\n        } else {\n          allGroupsCollector2 = null;\n          c4 = c3;\n        }\n        // Get block grouping result:\n        sBlocks.search(query, c4);\n        @SuppressWarnings({\"unchecked\",\"rawtypes\"})\n        final TopGroups<BytesRef> tempTopGroupsBlocks = (TopGroups<BytesRef>) c3.getTopGroups(docSort, groupOffset, docOffset, docOffset+docsPerGroup);\n        final TopGroups<BytesRef> groupsResultBlocks;\n        if (doAllGroups && tempTopGroupsBlocks != null) {\n          assertEquals((int) tempTopGroupsBlocks.totalGroupCount, allGroupsCollector2.getGroupCount());\n          groupsResultBlocks = new TopGroups<>(tempTopGroupsBlocks, allGroupsCollector2.getGroupCount());\n        } else {\n          groupsResultBlocks = tempTopGroupsBlocks;\n        }\n        \n        if (VERBOSE) {\n          if (groupsResultBlocks == null) {\n            System.out.println(\"TEST: no block groups\");\n          } else {\n            System.out.println(\"TEST: block groups totalGroupedHitCount=\" + groupsResultBlocks.totalGroupedHitCount);\n            boolean first = true;\n            for(GroupDocs<BytesRef> gd : groupsResultBlocks.groups) {\n              System.out.println(\"  group=\" + (gd.groupValue == null ? \"null\" : gd.groupValue.utf8ToString()) + \" totalHits=\" + gd.totalHits.value);\n              for(ScoreDoc sd : gd.scoreDocs) {\n                System.out.println(\"    id=\" + docIDToIDBlocks[sd.doc] + \" score=\" + sd.score);\n                if (first) {\n                  System.out.println(\"explain: \" + sBlocks.explain(query, sd.doc));\n                  first = false;\n                }\n              }\n            }\n          }\n        }\n        \n        // Get shard'd block grouping result:\n        final TopGroups<BytesRef> topGroupsBlockShards = searchShards(sBlocks, shardsBlocks.subSearchers, query,\n            groupSort, docSort, groupOffset, topNGroups, docOffset, docsPerGroup, getMaxScores, false, false);\n        \n        if (expectedGroups != null) {\n          // Fixup scores for reader2\n          for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n            for(ScoreDoc hit : groupDocsHits.scoreDocs) {\n              final GroupDoc gd = groupDocsByID[hit.doc];\n              assertEquals(gd.id, hit.doc);\n              //System.out.println(\"fixup score \" + hit.score + \" to \" + gd.score2 + \" vs \" + gd.score);\n              hit.score = gd.score2;\n            }\n          }\n          \n          final SortField[] sortFields = groupSort.getSort();\n          final Map<Float,Float> termScoreMap = scoreMap.get(searchTerm);\n          for(int groupSortIDX=0;groupSortIDX<sortFields.length;groupSortIDX++) {\n            if (sortFields[groupSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                if (groupDocsHits.groupSortValues != null) {\n                  //System.out.println(\"remap \" + groupDocsHits.groupSortValues[groupSortIDX] + \" to \" + termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]));\n                  groupDocsHits.groupSortValues[groupSortIDX] = termScoreMap.get(groupDocsHits.groupSortValues[groupSortIDX]);\n                  assertNotNull(groupDocsHits.groupSortValues[groupSortIDX]);\n                }\n              }\n            }\n          }\n          \n          final SortField[] docSortFields = docSort.getSort();\n          for(int docSortIDX=0;docSortIDX<docSortFields.length;docSortIDX++) {\n            if (docSortFields[docSortIDX].getType() == SortField.Type.SCORE) {\n              for (GroupDocs<?> groupDocsHits : expectedGroups.groups) {\n                for(ScoreDoc _hit : groupDocsHits.scoreDocs) {\n                  FieldDoc hit = (FieldDoc) _hit;\n                  if (hit.fields != null) {\n                    hit.fields[docSortIDX] = termScoreMap.get(hit.fields[docSortIDX]);\n                    assertNotNull(hit.fields[docSortIDX]);\n                  }\n                }\n              }\n            }\n          }\n        }\n        \n        assertEquals(docIDToIDBlocks, expectedGroups, groupsResultBlocks, false, true, false);\n        assertEquals(docIDToIDBlocks, expectedGroups, topGroupsBlockShards, false, false, false);\n      }\n      \n      r.close();\n      dir.close();\n      \n      rBlocks.close();\n      dirBlocks.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"475584d5e08a22ad3fc7babefe006d77bc744567":["efac7e9b9b43e45142de4b73db16d889bd4004b7"],"9274621789ce990dbfef455dabdf026bb3184821":["c9f72b4e0953a7ed14ab0430053c1bb65f2ef529"],"e3eb88edd735aec1f42cbe41c478fb4f8d41f0ec":["0837ab0472feecb3a54260729d845f839e1cbd72"],"f45457a742a53533c348c4b990b1c579ff364467":["b6a0e3c1c21aac8ecf75706605133012833585c7"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["cd659803551ebd8ca09b9e4ad7abd18d3d558f9d"],"d58e44159788900f4a2113b84463dc3fbbf80f20":["efac7e9b9b43e45142de4b73db16d889bd4004b7","475584d5e08a22ad3fc7babefe006d77bc744567"],"66d4c05d2724c63d6dcbdb32aab67299d77e3ca1":["97d4692d0c601ff773f0a2231967312428a904e4"],"97d4692d0c601ff773f0a2231967312428a904e4":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"e13078ebcbc41380853f4612578b706f40699cf5":["854f97cd3613b9579fba83755c80b697e2f3993f"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["7ae958a739da1866696f442384393ba2f13e33e5"],"a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"b36581872266f87caefe066d71f76c81cf1b636e":["b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c"],"770342641f7b505eaa8dccdc666158bff2419109":["7e7cf486535cf187cb3745154ca5dd3de3bd2999"],"1d3f7ab1a502671bbdb03bcced21e764d2483221":["04c370507e5521b2eb998530736f1c19b851ed5a"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["b6a0e3c1c21aac8ecf75706605133012833585c7","423d89a2b3cc419b647c07c2b3fdbc54311d07f9"],"4ac5d789c4e320734d65e4c2e8542ae0bc8d19f7":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"b6a0e3c1c21aac8ecf75706605133012833585c7":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"7ae958a739da1866696f442384393ba2f13e33e5":["20e94e61fe5291647346b70437617e6b6c370408"],"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":["20e94e61fe5291647346b70437617e6b6c370408"],"04c370507e5521b2eb998530736f1c19b851ed5a":["d58e44159788900f4a2113b84463dc3fbbf80f20"],"56572ec06f1407c066d6b7399413178b33176cd8":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","93dd449115a9247533e44bab47e8429e5dccbc6d"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["4ac5d789c4e320734d65e4c2e8542ae0bc8d19f7","60596f28be69b10c37a56a303c2dbea07b2ca4ba"],"41e0d5825f76d6bd3636a0dbaf6aa020cb357334":["60596f28be69b10c37a56a303c2dbea07b2ca4ba"],"c9f72b4e0953a7ed14ab0430053c1bb65f2ef529":["b36581872266f87caefe066d71f76c81cf1b636e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["5e3e97dbceff4180d72bfc95d4d9facd37dcd8bd","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"0837ab0472feecb3a54260729d845f839e1cbd72":["e13078ebcbc41380853f4612578b706f40699cf5"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["1d3f7ab1a502671bbdb03bcced21e764d2483221"],"cd659803551ebd8ca09b9e4ad7abd18d3d558f9d":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f":["bb9c3baacabd473e8ecd6c4948aabacead49b88e"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"20e94e61fe5291647346b70437617e6b6c370408":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"60596f28be69b10c37a56a303c2dbea07b2ca4ba":["4ac5d789c4e320734d65e4c2e8542ae0bc8d19f7"],"6613659748fe4411a7dcf85266e55db1f95f7315":["df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["66d4c05d2724c63d6dcbdb32aab67299d77e3ca1","df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392"],"efac7e9b9b43e45142de4b73db16d889bd4004b7":["417142ff08fda9cf0b72d5133e63097a166c6458"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","20e94e61fe5291647346b70437617e6b6c370408"],"b9d5080ee18db9b6f0dec906913eea6414758ec7":["e3eb88edd735aec1f42cbe41c478fb4f8d41f0ec"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["5e3e97dbceff4180d72bfc95d4d9facd37dcd8bd","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"417142ff08fda9cf0b72d5133e63097a166c6458":["7ae958a739da1866696f442384393ba2f13e33e5","9fc47cb7b4346802411bb432f501ed0673d7119e"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","9274621789ce990dbfef455dabdf026bb3184821"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["5e3e97dbceff4180d72bfc95d4d9facd37dcd8bd"],"bb9c3baacabd473e8ecd6c4948aabacead49b88e":["a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392":["66d4c05d2724c63d6dcbdb32aab67299d77e3ca1"],"5e3e97dbceff4180d72bfc95d4d9facd37dcd8bd":["770342641f7b505eaa8dccdc666158bff2419109"],"7e7cf486535cf187cb3745154ca5dd3de3bd2999":["41e0d5825f76d6bd3636a0dbaf6aa020cb357334"],"423d89a2b3cc419b647c07c2b3fdbc54311d07f9":["b9d5080ee18db9b6f0dec906913eea6414758ec7"],"854f97cd3613b9579fba83755c80b697e2f3993f":["f45457a742a53533c348c4b990b1c579ff364467"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f"]},"commit2Childs":{"475584d5e08a22ad3fc7babefe006d77bc744567":["d58e44159788900f4a2113b84463dc3fbbf80f20"],"9274621789ce990dbfef455dabdf026bb3184821":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"e3eb88edd735aec1f42cbe41c478fb4f8d41f0ec":["b9d5080ee18db9b6f0dec906913eea6414758ec7"],"f45457a742a53533c348c4b990b1c579ff364467":["854f97cd3613b9579fba83755c80b697e2f3993f"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["b6a0e3c1c21aac8ecf75706605133012833585c7"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"d58e44159788900f4a2113b84463dc3fbbf80f20":["04c370507e5521b2eb998530736f1c19b851ed5a"],"66d4c05d2724c63d6dcbdb32aab67299d77e3ca1":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392"],"97d4692d0c601ff773f0a2231967312428a904e4":["66d4c05d2724c63d6dcbdb32aab67299d77e3ca1"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["56572ec06f1407c066d6b7399413178b33176cd8","b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","93dd449115a9247533e44bab47e8429e5dccbc6d"],"e13078ebcbc41380853f4612578b706f40699cf5":["0837ab0472feecb3a54260729d845f839e1cbd72"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["417142ff08fda9cf0b72d5133e63097a166c6458"],"a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9":["bb9c3baacabd473e8ecd6c4948aabacead49b88e"],"b36581872266f87caefe066d71f76c81cf1b636e":["c9f72b4e0953a7ed14ab0430053c1bb65f2ef529"],"770342641f7b505eaa8dccdc666158bff2419109":["5e3e97dbceff4180d72bfc95d4d9facd37dcd8bd"],"1d3f7ab1a502671bbdb03bcced21e764d2483221":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd659803551ebd8ca09b9e4ad7abd18d3d558f9d"],"d4d69c535930b5cce125cff868d40f6373dc27d4":["97d4692d0c601ff773f0a2231967312428a904e4"],"b6a0e3c1c21aac8ecf75706605133012833585c7":["f45457a742a53533c348c4b990b1c579ff364467","d4d69c535930b5cce125cff868d40f6373dc27d4"],"4ac5d789c4e320734d65e4c2e8542ae0bc8d19f7":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","60596f28be69b10c37a56a303c2dbea07b2ca4ba"],"7ae958a739da1866696f442384393ba2f13e33e5":["9fc47cb7b4346802411bb432f501ed0673d7119e","417142ff08fda9cf0b72d5133e63097a166c6458"],"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":[],"04c370507e5521b2eb998530736f1c19b851ed5a":["1d3f7ab1a502671bbdb03bcced21e764d2483221"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"41e0d5825f76d6bd3636a0dbaf6aa020cb357334":["7e7cf486535cf187cb3745154ca5dd3de3bd2999"],"c9f72b4e0953a7ed14ab0430053c1bb65f2ef529":["9274621789ce990dbfef455dabdf026bb3184821"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["20e94e61fe5291647346b70437617e6b6c370408","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"0837ab0472feecb3a54260729d845f839e1cbd72":["e3eb88edd735aec1f42cbe41c478fb4f8d41f0ec"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["a5df378a6155dcc1f4d4ecdcbd8ea5bc058560e9"],"cd659803551ebd8ca09b9e4ad7abd18d3d558f9d":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"20e94e61fe5291647346b70437617e6b6c370408":["7ae958a739da1866696f442384393ba2f13e33e5","54ca69905c5d9d1529286f06ab1d12c68f6c13cb","09ab8ee44ca898536770d0106a7c0ee4be4f0eb7"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"60596f28be69b10c37a56a303c2dbea07b2ca4ba":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","41e0d5825f76d6bd3636a0dbaf6aa020cb357334"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"efac7e9b9b43e45142de4b73db16d889bd4004b7":["475584d5e08a22ad3fc7babefe006d77bc744567","d58e44159788900f4a2113b84463dc3fbbf80f20"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["b36581872266f87caefe066d71f76c81cf1b636e"],"09ab8ee44ca898536770d0106a7c0ee4be4f0eb7":[],"b9d5080ee18db9b6f0dec906913eea6414758ec7":["423d89a2b3cc419b647c07c2b3fdbc54311d07f9"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["09ab8ee44ca898536770d0106a7c0ee4be4f0eb7"],"417142ff08fda9cf0b72d5133e63097a166c6458":["efac7e9b9b43e45142de4b73db16d889bd4004b7"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["56572ec06f1407c066d6b7399413178b33176cd8","54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["4ac5d789c4e320734d65e4c2e8542ae0bc8d19f7"],"bb9c3baacabd473e8ecd6c4948aabacead49b88e":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f"],"df9bf66ed405ee5c7d32b47bdb36c2e36d2c1392":["6613659748fe4411a7dcf85266e55db1f95f7315","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"5e3e97dbceff4180d72bfc95d4d9facd37dcd8bd":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"7e7cf486535cf187cb3745154ca5dd3de3bd2999":["770342641f7b505eaa8dccdc666158bff2419109"],"423d89a2b3cc419b647c07c2b3fdbc54311d07f9":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"854f97cd3613b9579fba83755c80b697e2f3993f":["e13078ebcbc41380853f4612578b706f40699cf5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb","56572ec06f1407c066d6b7399413178b33176cd8","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","09ab8ee44ca898536770d0106a7c0ee4be4f0eb7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}