{"path":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","commits":[{"id":"938d1493cf2f269d3b9e66e932c07ee784e00022","date":1161902835,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram segments.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n    //\n    // In current addIndexes(Directory[]), segment infos in S are added to\n    // T's \"segmentInfos\" upfront. Then segments in S are merged to T several\n    // at a time. Every merge is committed with T's \"segmentInfos\". So if\n    // a reader is opened on T while addIndexes() is going on, it could see\n    // an inconsistent index. AddIndexesNoOptimize() has a similar behaviour.\n\n    // 1 flush ram segments\n    flushRamSegments();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int start = segmentInfos.size();\n    int startUpperBound = minMergeDocs;\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n\n          while (startUpperBound < info.docCount) {\n            startUpperBound *= mergeFactor; // find the highest level from dirs\n            if (startUpperBound > maxMergeDocs) {\n              // upper bound cannot exceed maxMergeDocs\n              throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n            }\n          }\n        }\n      }\n    } catch (IllegalArgumentException e) {\n      for (int i = segmentInfos.size() - 1; i >= start; i--) {\n        segmentInfos.remove(i);\n      }\n      throw e;\n    }\n\n    // 3 maybe merge segments starting from the highest level from dirs\n    maybeMergeSegments(startUpperBound);\n\n    // get the tail segments whose levels <= h\n    int segmentCount = segmentInfos.size();\n    int numTailSegments = 0;\n    while (numTailSegments < segmentCount\n        && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n      numTailSegments++;\n    }\n    if (numTailSegments == 0) {\n      return;\n    }\n\n    // 4 make sure invariants hold for the tail segments whose levels <= h\n    if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n      // identify the segments from S to be copied (not merged in 3)\n      int numSegmentsToCopy = 0;\n      while (numSegmentsToCopy < segmentCount\n          && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n        numSegmentsToCopy++;\n      }\n      if (numSegmentsToCopy == 0) {\n        return;\n      }\n\n      // copy those segments from S\n      for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n        mergeSegments(segmentInfos, i, i + 1);\n      }\n      if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n        return;\n      }\n    }\n\n    // invariants do not hold, simply merge those segments\n    mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);\n\n    // maybe merge segments again if necessary\n    if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n      maybeMergeSegments(startUpperBound * mergeFactor);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["dbb18b6a222f2507f22fab7cc7eed06658d59772","1b54a9bc667895a2095a886184bf69a3179e63df"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"eeefd99c477417e5c7c574228461ebafe92469d4","date":1166460329,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram segments.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n\n    // 1 flush ram segments\n\n    flushRamSegments();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int start = segmentInfos.size();\n    int startUpperBound = minMergeDocs;\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      try {\n        for (int i = 0; i < dirs.length; i++) {\n          if (directory == dirs[i]) {\n            // cannot add this index: segments may be deleted in merge before added\n            throw new IllegalArgumentException(\"Cannot add this index to itself\");\n          }\n\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dirs[i]);\n          for (int j = 0; j < sis.size(); j++) {\n            SegmentInfo info = sis.info(j);\n            segmentInfos.addElement(info); // add each info\n\n            while (startUpperBound < info.docCount) {\n              startUpperBound *= mergeFactor; // find the highest level from dirs\n              if (startUpperBound > maxMergeDocs) {\n                // upper bound cannot exceed maxMergeDocs\n                throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n              }\n            }\n          }\n        }\n      } catch (IllegalArgumentException e) {\n        for (int i = segmentInfos.size() - 1; i >= start; i--) {\n          segmentInfos.remove(i);\n        }\n        throw e;\n      }\n\n      // 3 maybe merge segments starting from the highest level from dirs\n      maybeMergeSegments(startUpperBound);\n\n      // get the tail segments whose levels <= h\n      int segmentCount = segmentInfos.size();\n      int numTailSegments = 0;\n      while (numTailSegments < segmentCount\n             && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n        numTailSegments++;\n      }\n      if (numTailSegments == 0) {\n        success = true;\n        return;\n      }\n\n      // 4 make sure invariants hold for the tail segments whose levels <= h\n      if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n        // identify the segments from S to be copied (not merged in 3)\n        int numSegmentsToCopy = 0;\n        while (numSegmentsToCopy < segmentCount\n               && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n          numSegmentsToCopy++;\n        }\n        if (numSegmentsToCopy == 0) {\n          success = true;\n          return;\n        }\n\n        // copy those segments from S\n        for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n          mergeSegments(segmentInfos, i, i + 1);\n        }\n        if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n          success = true;\n          return;\n        }\n      }\n\n      // invariants do not hold, simply merge those segments\n      mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);\n\n      // maybe merge segments again if necessary\n      if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n        maybeMergeSegments(startUpperBound * mergeFactor);\n      }\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram segments.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n    //\n    // In current addIndexes(Directory[]), segment infos in S are added to\n    // T's \"segmentInfos\" upfront. Then segments in S are merged to T several\n    // at a time. Every merge is committed with T's \"segmentInfos\". So if\n    // a reader is opened on T while addIndexes() is going on, it could see\n    // an inconsistent index. AddIndexesNoOptimize() has a similar behaviour.\n\n    // 1 flush ram segments\n    flushRamSegments();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int start = segmentInfos.size();\n    int startUpperBound = minMergeDocs;\n\n    try {\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n\n          while (startUpperBound < info.docCount) {\n            startUpperBound *= mergeFactor; // find the highest level from dirs\n            if (startUpperBound > maxMergeDocs) {\n              // upper bound cannot exceed maxMergeDocs\n              throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n            }\n          }\n        }\n      }\n    } catch (IllegalArgumentException e) {\n      for (int i = segmentInfos.size() - 1; i >= start; i--) {\n        segmentInfos.remove(i);\n      }\n      throw e;\n    }\n\n    // 3 maybe merge segments starting from the highest level from dirs\n    maybeMergeSegments(startUpperBound);\n\n    // get the tail segments whose levels <= h\n    int segmentCount = segmentInfos.size();\n    int numTailSegments = 0;\n    while (numTailSegments < segmentCount\n        && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n      numTailSegments++;\n    }\n    if (numTailSegments == 0) {\n      return;\n    }\n\n    // 4 make sure invariants hold for the tail segments whose levels <= h\n    if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n      // identify the segments from S to be copied (not merged in 3)\n      int numSegmentsToCopy = 0;\n      while (numSegmentsToCopy < segmentCount\n          && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n        numSegmentsToCopy++;\n      }\n      if (numSegmentsToCopy == 0) {\n        return;\n      }\n\n      // copy those segments from S\n      for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n        mergeSegments(segmentInfos, i, i + 1);\n      }\n      if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n        return;\n      }\n    }\n\n    // invariants do not hold, simply merge those segments\n    mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);\n\n    // maybe merge segments again if necessary\n    if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n      maybeMergeSegments(startUpperBound * mergeFactor);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1b54a9bc667895a2095a886184bf69a3179e63df","date":1172088096,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram segments.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n\n    // 1 flush ram segments\n\n    flushRamSegments();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int start = segmentInfos.size();\n    int startUpperBound = minMergeDocs;\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      try {\n        for (int i = 0; i < dirs.length; i++) {\n          if (directory == dirs[i]) {\n            // cannot add this index: segments may be deleted in merge before added\n            throw new IllegalArgumentException(\"Cannot add this index to itself\");\n          }\n\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dirs[i]);\n          for (int j = 0; j < sis.size(); j++) {\n            SegmentInfo info = sis.info(j);\n            segmentInfos.addElement(info); // add each info\n\n            while (startUpperBound < info.docCount) {\n              startUpperBound *= mergeFactor; // find the highest level from dirs\n              if (startUpperBound > maxMergeDocs) {\n                // upper bound cannot exceed maxMergeDocs\n                throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n              }\n            }\n          }\n        }\n      } catch (IllegalArgumentException e) {\n        for (int i = segmentInfos.size() - 1; i >= start; i--) {\n          segmentInfos.remove(i);\n        }\n        throw e;\n      }\n\n      // 3 maybe merge segments starting from the highest level from dirs\n      maybeMergeSegments(startUpperBound);\n\n      // get the tail segments whose levels <= h\n      int segmentCount = segmentInfos.size();\n      int numTailSegments = 0;\n      while (numTailSegments < segmentCount\n             && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n        numTailSegments++;\n      }\n      if (numTailSegments == 0) {\n        success = true;\n        return;\n      }\n\n      // 4 make sure invariants hold for the tail segments whose levels <= h\n      if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n        // identify the segments from S to be copied (not merged in 3)\n        int numSegmentsToCopy = 0;\n        while (numSegmentsToCopy < segmentCount\n               && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n          numSegmentsToCopy++;\n        }\n        if (numSegmentsToCopy == 0) {\n          success = true;\n          return;\n        }\n\n        // copy those segments from S\n        for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n          mergeSegments(segmentInfos, i, i + 1);\n        }\n        if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n          success = true;\n          return;\n        }\n      }\n\n      // invariants do not hold, simply merge those segments\n      mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);\n\n      // maybe merge segments again if necessary\n      if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n        maybeMergeSegments(startUpperBound * mergeFactor);\n      }\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram segments.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n\n    // 1 flush ram segments\n\n    flushRamSegments();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int start = segmentInfos.size();\n    int startUpperBound = minMergeDocs;\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      try {\n        for (int i = 0; i < dirs.length; i++) {\n          if (directory == dirs[i]) {\n            // cannot add this index: segments may be deleted in merge before added\n            throw new IllegalArgumentException(\"Cannot add this index to itself\");\n          }\n\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dirs[i]);\n          for (int j = 0; j < sis.size(); j++) {\n            SegmentInfo info = sis.info(j);\n            segmentInfos.addElement(info); // add each info\n\n            while (startUpperBound < info.docCount) {\n              startUpperBound *= mergeFactor; // find the highest level from dirs\n              if (startUpperBound > maxMergeDocs) {\n                // upper bound cannot exceed maxMergeDocs\n                throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n              }\n            }\n          }\n        }\n      } catch (IllegalArgumentException e) {\n        for (int i = segmentInfos.size() - 1; i >= start; i--) {\n          segmentInfos.remove(i);\n        }\n        throw e;\n      }\n\n      // 3 maybe merge segments starting from the highest level from dirs\n      maybeMergeSegments(startUpperBound);\n\n      // get the tail segments whose levels <= h\n      int segmentCount = segmentInfos.size();\n      int numTailSegments = 0;\n      while (numTailSegments < segmentCount\n             && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n        numTailSegments++;\n      }\n      if (numTailSegments == 0) {\n        success = true;\n        return;\n      }\n\n      // 4 make sure invariants hold for the tail segments whose levels <= h\n      if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n        // identify the segments from S to be copied (not merged in 3)\n        int numSegmentsToCopy = 0;\n        while (numSegmentsToCopy < segmentCount\n               && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n          numSegmentsToCopy++;\n        }\n        if (numSegmentsToCopy == 0) {\n          success = true;\n          return;\n        }\n\n        // copy those segments from S\n        for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n          mergeSegments(segmentInfos, i, i + 1);\n        }\n        if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n          success = true;\n          return;\n        }\n      }\n\n      // invariants do not hold, simply merge those segments\n      mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);\n\n      // maybe merge segments again if necessary\n      if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n        maybeMergeSegments(startUpperBound * mergeFactor);\n      }\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":["938d1493cf2f269d3b9e66e932c07ee784e00022"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0cb4e0840ad0d45706ad7fe5806b6d3e6624335b","date":1173256348,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram segments.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n\n    // 1 flush ram segments\n\n    flushRamSegments();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int start = segmentInfos.size();\n    int startUpperBound = minMergeDocs;\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n          \n          while (startUpperBound < info.docCount) {\n            startUpperBound *= mergeFactor; // find the highest level from dirs\n            if (startUpperBound > maxMergeDocs) {\n              // upper bound cannot exceed maxMergeDocs\n              throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n            }\n          }\n        }\n      }\n\n      // 3 maybe merge segments starting from the highest level from dirs\n      maybeMergeSegments(startUpperBound);\n\n      // get the tail segments whose levels <= h\n      int segmentCount = segmentInfos.size();\n      int numTailSegments = 0;\n      while (numTailSegments < segmentCount\n             && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n        numTailSegments++;\n      }\n      if (numTailSegments == 0) {\n        success = true;\n        return;\n      }\n\n      // 4 make sure invariants hold for the tail segments whose levels <= h\n      if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n        // identify the segments from S to be copied (not merged in 3)\n        int numSegmentsToCopy = 0;\n        while (numSegmentsToCopy < segmentCount\n               && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n          numSegmentsToCopy++;\n        }\n        if (numSegmentsToCopy == 0) {\n          success = true;\n          return;\n        }\n\n        // copy those segments from S\n        for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n          mergeSegments(segmentInfos, i, i + 1);\n        }\n        if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n          success = true;\n          return;\n        }\n      }\n\n      // invariants do not hold, simply merge those segments\n      mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);\n\n      // maybe merge segments again if necessary\n      if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n        maybeMergeSegments(startUpperBound * mergeFactor);\n      }\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram segments.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n\n    // 1 flush ram segments\n\n    flushRamSegments();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int start = segmentInfos.size();\n    int startUpperBound = minMergeDocs;\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      try {\n        for (int i = 0; i < dirs.length; i++) {\n          if (directory == dirs[i]) {\n            // cannot add this index: segments may be deleted in merge before added\n            throw new IllegalArgumentException(\"Cannot add this index to itself\");\n          }\n\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dirs[i]);\n          for (int j = 0; j < sis.size(); j++) {\n            SegmentInfo info = sis.info(j);\n            segmentInfos.addElement(info); // add each info\n\n            while (startUpperBound < info.docCount) {\n              startUpperBound *= mergeFactor; // find the highest level from dirs\n              if (startUpperBound > maxMergeDocs) {\n                // upper bound cannot exceed maxMergeDocs\n                throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n              }\n            }\n          }\n        }\n      } catch (IllegalArgumentException e) {\n        for (int i = segmentInfos.size() - 1; i >= start; i--) {\n          segmentInfos.remove(i);\n        }\n        throw e;\n      }\n\n      // 3 maybe merge segments starting from the highest level from dirs\n      maybeMergeSegments(startUpperBound);\n\n      // get the tail segments whose levels <= h\n      int segmentCount = segmentInfos.size();\n      int numTailSegments = 0;\n      while (numTailSegments < segmentCount\n             && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n        numTailSegments++;\n      }\n      if (numTailSegments == 0) {\n        success = true;\n        return;\n      }\n\n      // 4 make sure invariants hold for the tail segments whose levels <= h\n      if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n        // identify the segments from S to be copied (not merged in 3)\n        int numSegmentsToCopy = 0;\n        while (numSegmentsToCopy < segmentCount\n               && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n          numSegmentsToCopy++;\n        }\n        if (numSegmentsToCopy == 0) {\n          success = true;\n          return;\n        }\n\n        // copy those segments from S\n        for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n          mergeSegments(segmentInfos, i, i + 1);\n        }\n        if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n          success = true;\n          return;\n        }\n      }\n\n      // invariants do not hold, simply merge those segments\n      mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);\n\n      // maybe merge segments again if necessary\n      if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n        maybeMergeSegments(startUpperBound * mergeFactor);\n      }\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"328c1568e471f0c6eaa49ec00334ca59e573710f","date":1173897963,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram segments.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n\n    // 1 flush ram segments\n\n    ensureOpen();\n    flushRamSegments();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int start = segmentInfos.size();\n    int startUpperBound = minMergeDocs;\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n          \n          while (startUpperBound < info.docCount) {\n            startUpperBound *= mergeFactor; // find the highest level from dirs\n            if (startUpperBound > maxMergeDocs) {\n              // upper bound cannot exceed maxMergeDocs\n              throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n            }\n          }\n        }\n      }\n\n      // 3 maybe merge segments starting from the highest level from dirs\n      maybeMergeSegments(startUpperBound);\n\n      // get the tail segments whose levels <= h\n      int segmentCount = segmentInfos.size();\n      int numTailSegments = 0;\n      while (numTailSegments < segmentCount\n             && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n        numTailSegments++;\n      }\n      if (numTailSegments == 0) {\n        success = true;\n        return;\n      }\n\n      // 4 make sure invariants hold for the tail segments whose levels <= h\n      if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n        // identify the segments from S to be copied (not merged in 3)\n        int numSegmentsToCopy = 0;\n        while (numSegmentsToCopy < segmentCount\n               && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n          numSegmentsToCopy++;\n        }\n        if (numSegmentsToCopy == 0) {\n          success = true;\n          return;\n        }\n\n        // copy those segments from S\n        for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n          mergeSegments(segmentInfos, i, i + 1);\n        }\n        if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n          success = true;\n          return;\n        }\n      }\n\n      // invariants do not hold, simply merge those segments\n      mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);\n\n      // maybe merge segments again if necessary\n      if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n        maybeMergeSegments(startUpperBound * mergeFactor);\n      }\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram segments.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n\n    // 1 flush ram segments\n\n    flushRamSegments();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int start = segmentInfos.size();\n    int startUpperBound = minMergeDocs;\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n          \n          while (startUpperBound < info.docCount) {\n            startUpperBound *= mergeFactor; // find the highest level from dirs\n            if (startUpperBound > maxMergeDocs) {\n              // upper bound cannot exceed maxMergeDocs\n              throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n            }\n          }\n        }\n      }\n\n      // 3 maybe merge segments starting from the highest level from dirs\n      maybeMergeSegments(startUpperBound);\n\n      // get the tail segments whose levels <= h\n      int segmentCount = segmentInfos.size();\n      int numTailSegments = 0;\n      while (numTailSegments < segmentCount\n             && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n        numTailSegments++;\n      }\n      if (numTailSegments == 0) {\n        success = true;\n        return;\n      }\n\n      // 4 make sure invariants hold for the tail segments whose levels <= h\n      if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n        // identify the segments from S to be copied (not merged in 3)\n        int numSegmentsToCopy = 0;\n        while (numSegmentsToCopy < segmentCount\n               && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n          numSegmentsToCopy++;\n        }\n        if (numSegmentsToCopy == 0) {\n          success = true;\n          return;\n        }\n\n        // copy those segments from S\n        for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n          mergeSegments(segmentInfos, i, i + 1);\n        }\n        if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n          success = true;\n          return;\n        }\n      }\n\n      // invariants do not hold, simply merge those segments\n      mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);\n\n      // maybe merge segments again if necessary\n      if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n        maybeMergeSegments(startUpperBound * mergeFactor);\n      }\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b685e89f45e34ea31f9ca89912e4f29038818ff6","date":1173986546,"type":3,"author":"Doron Cohen","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram segments.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n\n    // 1 flush ram segments\n\n    ensureOpen();\n    flushRamSegments();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int startUpperBound = minMergeDocs;\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n          \n          while (startUpperBound < info.docCount) {\n            startUpperBound *= mergeFactor; // find the highest level from dirs\n            if (startUpperBound > maxMergeDocs) {\n              // upper bound cannot exceed maxMergeDocs\n              throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n            }\n          }\n        }\n      }\n\n      // 3 maybe merge segments starting from the highest level from dirs\n      maybeMergeSegments(startUpperBound);\n\n      // get the tail segments whose levels <= h\n      int segmentCount = segmentInfos.size();\n      int numTailSegments = 0;\n      while (numTailSegments < segmentCount\n             && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n        numTailSegments++;\n      }\n      if (numTailSegments == 0) {\n        success = true;\n        return;\n      }\n\n      // 4 make sure invariants hold for the tail segments whose levels <= h\n      if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n        // identify the segments from S to be copied (not merged in 3)\n        int numSegmentsToCopy = 0;\n        while (numSegmentsToCopy < segmentCount\n               && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n          numSegmentsToCopy++;\n        }\n        if (numSegmentsToCopy == 0) {\n          success = true;\n          return;\n        }\n\n        // copy those segments from S\n        for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n          mergeSegments(segmentInfos, i, i + 1);\n        }\n        if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n          success = true;\n          return;\n        }\n      }\n\n      // invariants do not hold, simply merge those segments\n      mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);\n\n      // maybe merge segments again if necessary\n      if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n        maybeMergeSegments(startUpperBound * mergeFactor);\n      }\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram segments.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n\n    // 1 flush ram segments\n\n    ensureOpen();\n    flushRamSegments();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int start = segmentInfos.size();\n    int startUpperBound = minMergeDocs;\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n          \n          while (startUpperBound < info.docCount) {\n            startUpperBound *= mergeFactor; // find the highest level from dirs\n            if (startUpperBound > maxMergeDocs) {\n              // upper bound cannot exceed maxMergeDocs\n              throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n            }\n          }\n        }\n      }\n\n      // 3 maybe merge segments starting from the highest level from dirs\n      maybeMergeSegments(startUpperBound);\n\n      // get the tail segments whose levels <= h\n      int segmentCount = segmentInfos.size();\n      int numTailSegments = 0;\n      while (numTailSegments < segmentCount\n             && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n        numTailSegments++;\n      }\n      if (numTailSegments == 0) {\n        success = true;\n        return;\n      }\n\n      // 4 make sure invariants hold for the tail segments whose levels <= h\n      if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n        // identify the segments from S to be copied (not merged in 3)\n        int numSegmentsToCopy = 0;\n        while (numSegmentsToCopy < segmentCount\n               && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n          numSegmentsToCopy++;\n        }\n        if (numSegmentsToCopy == 0) {\n          success = true;\n          return;\n        }\n\n        // copy those segments from S\n        for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n          mergeSegments(segmentInfos, i, i + 1);\n        }\n        if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n          success = true;\n          return;\n        }\n      }\n\n      // invariants do not hold, simply merge those segments\n      mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);\n\n      // maybe merge segments again if necessary\n      if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n        maybeMergeSegments(startUpperBound * mergeFactor);\n      }\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n\n    // 1 flush ram\n\n    ensureOpen();\n    flush();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int startUpperBound = docWriter.getMaxBufferedDocs();\n\n    /* new merge policy\n    if (startUpperBound == 0)\n      startUpperBound = 10;\n    */\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n          \n          while (startUpperBound < info.docCount) {\n            startUpperBound *= mergeFactor; // find the highest level from dirs\n            if (startUpperBound > maxMergeDocs) {\n              // upper bound cannot exceed maxMergeDocs\n              throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n            }\n          }\n        }\n      }\n\n      // 3 maybe merge segments starting from the highest level from dirs\n      maybeMergeSegments(startUpperBound);\n\n      // get the tail segments whose levels <= h\n      int segmentCount = segmentInfos.size();\n      int numTailSegments = 0;\n      while (numTailSegments < segmentCount\n             && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n        numTailSegments++;\n      }\n      if (numTailSegments == 0) {\n        success = true;\n        return;\n      }\n\n      // 4 make sure invariants hold for the tail segments whose levels <= h\n      if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n        // identify the segments from S to be copied (not merged in 3)\n        int numSegmentsToCopy = 0;\n        while (numSegmentsToCopy < segmentCount\n               && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n          numSegmentsToCopy++;\n        }\n        if (numSegmentsToCopy == 0) {\n          success = true;\n          return;\n        }\n\n        // copy those segments from S\n        for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n          mergeSegments(i, i + 1);\n        }\n        if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n          success = true;\n          return;\n        }\n      }\n\n      // invariants do not hold, simply merge those segments\n      mergeSegments(segmentCount - numTailSegments, segmentCount);\n\n      // maybe merge segments again if necessary\n      if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n        maybeMergeSegments(startUpperBound * mergeFactor);\n      }\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram segments.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n\n    // 1 flush ram segments\n\n    ensureOpen();\n    flushRamSegments();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int startUpperBound = minMergeDocs;\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n          \n          while (startUpperBound < info.docCount) {\n            startUpperBound *= mergeFactor; // find the highest level from dirs\n            if (startUpperBound > maxMergeDocs) {\n              // upper bound cannot exceed maxMergeDocs\n              throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n            }\n          }\n        }\n      }\n\n      // 3 maybe merge segments starting from the highest level from dirs\n      maybeMergeSegments(startUpperBound);\n\n      // get the tail segments whose levels <= h\n      int segmentCount = segmentInfos.size();\n      int numTailSegments = 0;\n      while (numTailSegments < segmentCount\n             && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n        numTailSegments++;\n      }\n      if (numTailSegments == 0) {\n        success = true;\n        return;\n      }\n\n      // 4 make sure invariants hold for the tail segments whose levels <= h\n      if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n        // identify the segments from S to be copied (not merged in 3)\n        int numSegmentsToCopy = 0;\n        while (numSegmentsToCopy < segmentCount\n               && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n          numSegmentsToCopy++;\n        }\n        if (numSegmentsToCopy == 0) {\n          success = true;\n          return;\n        }\n\n        // copy those segments from S\n        for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n          mergeSegments(segmentInfos, i, i + 1);\n        }\n        if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n          success = true;\n          return;\n        }\n      }\n\n      // invariants do not hold, simply merge those segments\n      mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);\n\n      // maybe merge segments again if necessary\n      if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n        maybeMergeSegments(startUpperBound * mergeFactor);\n      }\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b1405362241b561f5590ff4a87d5d6e173bcd9cf","date":1190107634,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    flush();\n\n    /* new merge policy\n    if (startUpperBound == 0)\n      startUpperBound = 10;\n    */\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n        }\n      }\n\n      maybeMerge();\n\n      // If after merging there remain segments in the index\n      // that are in a different directory, just copy these\n      // over into our index.  This is necessary (before\n      // finishing the transaction) to avoid leaving the\n      // index in an unusable (inconsistent) state.\n      copyExternalSegments();\n\n      success = true;\n\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n    // Adding indexes can be viewed as adding a sequence of segments S to\n    // a sequence of segments T. Segments in T follow the invariants but\n    // segments in S may not since they could come from multiple indexes.\n    // Here is the merge algorithm for addIndexesNoOptimize():\n    //\n    // 1 Flush ram.\n    // 2 Consider a combined sequence with segments from T followed\n    //   by segments from S (same as current addIndexes(Directory[])).\n    // 3 Assume the highest level for segments in S is h. Call\n    //   maybeMergeSegments(), but instead of starting w/ lowerBound = -1\n    //   and upperBound = maxBufferedDocs, start w/ lowerBound = -1 and\n    //   upperBound = upperBound of level h. After this, the invariants\n    //   are guaranteed except for the last < M segments whose levels <= h.\n    // 4 If the invariants hold for the last < M segments whose levels <= h,\n    //   if some of those < M segments are from S (not merged in step 3),\n    //   properly copy them over*, otherwise done.\n    //   Otherwise, simply merge those segments. If the merge results in\n    //   a segment of level <= h, done. Otherwise, it's of level h+1 and call\n    //   maybeMergeSegments() starting w/ upperBound = upperBound of level h+1.\n    //\n    // * Ideally, we want to simply copy a segment. However, directory does\n    // not support copy yet. In addition, source may use compound file or not\n    // and target may use compound file or not. So we use mergeSegments() to\n    // copy a segment, which may cause doc count to change because deleted\n    // docs are garbage collected.\n\n    // 1 flush ram\n\n    ensureOpen();\n    flush();\n\n    // 2 copy segment infos and find the highest level from dirs\n    int startUpperBound = docWriter.getMaxBufferedDocs();\n\n    /* new merge policy\n    if (startUpperBound == 0)\n      startUpperBound = 10;\n    */\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n          \n          while (startUpperBound < info.docCount) {\n            startUpperBound *= mergeFactor; // find the highest level from dirs\n            if (startUpperBound > maxMergeDocs) {\n              // upper bound cannot exceed maxMergeDocs\n              throw new IllegalArgumentException(\"Upper bound cannot exceed maxMergeDocs\");\n            }\n          }\n        }\n      }\n\n      // 3 maybe merge segments starting from the highest level from dirs\n      maybeMergeSegments(startUpperBound);\n\n      // get the tail segments whose levels <= h\n      int segmentCount = segmentInfos.size();\n      int numTailSegments = 0;\n      while (numTailSegments < segmentCount\n             && startUpperBound >= segmentInfos.info(segmentCount - 1 - numTailSegments).docCount) {\n        numTailSegments++;\n      }\n      if (numTailSegments == 0) {\n        success = true;\n        return;\n      }\n\n      // 4 make sure invariants hold for the tail segments whose levels <= h\n      if (checkNonDecreasingLevels(segmentCount - numTailSegments)) {\n        // identify the segments from S to be copied (not merged in 3)\n        int numSegmentsToCopy = 0;\n        while (numSegmentsToCopy < segmentCount\n               && directory != segmentInfos.info(segmentCount - 1 - numSegmentsToCopy).dir) {\n          numSegmentsToCopy++;\n        }\n        if (numSegmentsToCopy == 0) {\n          success = true;\n          return;\n        }\n\n        // copy those segments from S\n        for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {\n          mergeSegments(i, i + 1);\n        }\n        if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {\n          success = true;\n          return;\n        }\n      }\n\n      // invariants do not hold, simply merge those segments\n      mergeSegments(segmentCount - numTailSegments, segmentCount);\n\n      // maybe merge segments again if necessary\n      if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {\n        maybeMergeSegments(startUpperBound * mergeFactor);\n      }\n\n      success = true;\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"67006a60923e2124212d3baa0d29b444bcbd8373","date":1191425052,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexesNoOptimize\");\n    flush();\n\n    /* new merge policy\n    if (startUpperBound == 0)\n      startUpperBound = 10;\n    */\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n        }\n      }\n\n      maybeMerge();\n\n      // If after merging there remain segments in the index\n      // that are in a different directory, just copy these\n      // over into our index.  This is necessary (before\n      // finishing the transaction) to avoid leaving the\n      // index in an unusable (inconsistent) state.\n      copyExternalSegments();\n\n      success = true;\n\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    flush();\n\n    /* new merge policy\n    if (startUpperBound == 0)\n      startUpperBound = 10;\n    */\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n        }\n      }\n\n      maybeMerge();\n\n      // If after merging there remain segments in the index\n      // that are in a different directory, just copy these\n      // over into our index.  This is necessary (before\n      // finishing the transaction) to avoid leaving the\n      // index in an unusable (inconsistent) state.\n      copyExternalSegments();\n\n      success = true;\n\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2607e6466921754dbb3ebd17f098a4e9f1fc4877","date":1192019178,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexesNoOptimize\");\n    flush();\n\n    /* new merge policy\n    if (startUpperBound == 0)\n      startUpperBound = 10;\n    */\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n        }\n      }\n\n      maybeMerge();\n\n      // If after merging there remain segments in the index\n      // that are in a different directory, just copy these\n      // over into our index.  This is necessary (before\n      // finishing the transaction) to avoid leaving the\n      // index in an unusable (inconsistent) state.\n      copyExternalSegments();\n\n      success = true;\n\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexesNoOptimize\");\n    flush();\n\n    /* new merge policy\n    if (startUpperBound == 0)\n      startUpperBound = 10;\n    */\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n        }\n      }\n\n      maybeMerge();\n\n      // If after merging there remain segments in the index\n      // that are in a different directory, just copy these\n      // over into our index.  This is necessary (before\n      // finishing the transaction) to avoid leaving the\n      // index in an unusable (inconsistent) state.\n      copyExternalSegments();\n\n      success = true;\n\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3b248d9b80f6b2fceade80b3c8683d1cca6e4c98","date":1194520024,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexesNoOptimize\");\n    flush();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n        }\n      }\n\n      maybeMerge();\n\n      // If after merging there remain segments in the index\n      // that are in a different directory, just copy these\n      // over into our index.  This is necessary (before\n      // finishing the transaction) to avoid leaving the\n      // index in an unusable (inconsistent) state.\n      copyExternalSegments();\n\n      success = true;\n\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexesNoOptimize\");\n    flush();\n\n    /* new merge policy\n    if (startUpperBound == 0)\n      startUpperBound = 10;\n    */\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n        }\n      }\n\n      maybeMerge();\n\n      // If after merging there remain segments in the index\n      // that are in a different directory, just copy these\n      // over into our index.  This is necessary (before\n      // finishing the transaction) to avoid leaving the\n      // index in an unusable (inconsistent) state.\n      copyExternalSegments();\n\n      success = true;\n\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e82780afe6097066eb5befb86e9432f077667e3d","date":1202756169,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexesNoOptimize\");\n    flush(true, false);\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n        }\n      }\n\n      maybeMerge();\n\n      // If after merging there remain segments in the index\n      // that are in a different directory, just copy these\n      // over into our index.  This is necessary (before\n      // finishing the transaction) to avoid leaving the\n      // index in an unusable (inconsistent) state.\n      copyExternalSegments();\n\n      success = true;\n\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexesNoOptimize\");\n    flush();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n        }\n      }\n\n      maybeMerge();\n\n      // If after merging there remain segments in the index\n      // that are in a different directory, just copy these\n      // over into our index.  This is necessary (before\n      // finishing the transaction) to avoid leaving the\n      // index in an unusable (inconsistent) state.\n      copyExternalSegments();\n\n      success = true;\n\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63","date":1204234542,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n\n        for (int i = 0; i < dirs.length; i++) {\n          if (directory == dirs[i]) {\n            // cannot add this index: segments may be deleted in merge before added\n            throw new IllegalArgumentException(\"Cannot add this index to itself\");\n          }\n\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dirs[i]);\n          for (int j = 0; j < sis.size(); j++) {\n            SegmentInfo info = sis.info(j);\n            segmentInfos.addElement(info); // add each info\n          }\n        }\n\n        maybeMerge();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        copyExternalSegments();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    if (infoStream != null)\n      message(\"flush at addIndexesNoOptimize\");\n    flush(true, false);\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n\n      for (int i = 0; i < dirs.length; i++) {\n        if (directory == dirs[i]) {\n          // cannot add this index: segments may be deleted in merge before added\n          throw new IllegalArgumentException(\"Cannot add this index to itself\");\n        }\n\n        SegmentInfos sis = new SegmentInfos(); // read infos from dir\n        sis.read(dirs[i]);\n        for (int j = 0; j < sis.size(); j++) {\n          SegmentInfo info = sis.info(j);\n          segmentInfos.addElement(info); // add each info\n        }\n      }\n\n      maybeMerge();\n\n      // If after merging there remain segments in the index\n      // that are in a different directory, just copy these\n      // over into our index.  This is necessary (before\n      // finishing the transaction) to avoid leaving the\n      // index in an unusable (inconsistent) state.\n      copyExternalSegments();\n\n      success = true;\n\n    } finally {\n      if (success) {\n        commitTransaction();\n      } else {\n        rollbackTransaction();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["dbb18b6a222f2507f22fab7cc7eed06658d59772","cd488f50316362b01a7f67b11a96796b9652e3e5"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be","date":1204801324,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n\n        int docCount = 0;\n        for (int i = 0; i < dirs.length; i++) {\n          if (directory == dirs[i]) {\n            // cannot add this index: segments may be deleted in merge before added\n            throw new IllegalArgumentException(\"Cannot add this index to itself\");\n          }\n\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dirs[i]);\n          for (int j = 0; j < sis.size(); j++) {\n            SegmentInfo info = sis.info(j);\n            docCount += info.docCount;\n            segmentInfos.addElement(info); // add each info\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        copyExternalSegments();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n\n        for (int i = 0; i < dirs.length; i++) {\n          if (directory == dirs[i]) {\n            // cannot add this index: segments may be deleted in merge before added\n            throw new IllegalArgumentException(\"Cannot add this index to itself\");\n          }\n\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dirs[i]);\n          for (int j = 0; j < sis.size(); j++) {\n            SegmentInfo info = sis.info(j);\n            segmentInfos.addElement(info); // add each info\n          }\n        }\n\n        maybeMerge();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        copyExternalSegments();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    }\n  }\n\n","bugFix":null,"bugIntro":["dbb18b6a222f2507f22fab7cc7eed06658d59772"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dbb18b6a222f2507f22fab7cc7eed06658d59772","date":1204804366,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              segmentInfos.addElement(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        copyExternalSegments();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n\n        int docCount = 0;\n        for (int i = 0; i < dirs.length; i++) {\n          if (directory == dirs[i]) {\n            // cannot add this index: segments may be deleted in merge before added\n            throw new IllegalArgumentException(\"Cannot add this index to itself\");\n          }\n\n          SegmentInfos sis = new SegmentInfos(); // read infos from dir\n          sis.read(dirs[i]);\n          for (int j = 0; j < sis.size(); j++) {\n            SegmentInfo info = sis.info(j);\n            docCount += info.docCount;\n            segmentInfos.addElement(info); // add each info\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        copyExternalSegments();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    }\n  }\n\n","bugFix":["938d1493cf2f269d3b9e66e932c07ee784e00022","01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63","a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"268d26e9bb23249b6a418a01d52fcbe19ee33a1f","date":1219498325,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              segmentInfos.addElement(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        copyExternalSegments();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this index.\n   * <p>\n   * This is similar to addIndexes(Directory[]). However, no optimize()\n   * is called either at the beginning or at the end. Instead, merges\n   * are carried out as necessary.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>\n   * This requires this index not be among those to be added, and the\n   * upper bound* of those segment doc counts not exceed maxMergeDocs.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              segmentInfos.addElement(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        copyExternalSegments();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2586f96f60332eb97ecd2934b0763791462568b2","date":1220116589,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              assert !segmentInfos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n              docCount += info.docCount;\n              segmentInfos.addElement(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        ensureOpen();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        resolveExternalSegments();\n\n        ensureOpen();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              docCount += info.docCount;\n              segmentInfos.addElement(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        copyExternalSegments();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7b6cdc70e097da94da79a655ed8f94477ff69f5","date":1220815360,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              assert !segmentInfos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n              docCount += info.docCount;\n              segmentInfos.add(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        ensureOpen();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        resolveExternalSegments();\n\n        ensureOpen();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              assert !segmentInfos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n              docCount += info.docCount;\n              segmentInfos.addElement(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        ensureOpen();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        resolveExternalSegments();\n\n        ensureOpen();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9665d17707cc21b1db995118ff36129723139ab","date":1225384420,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              assert !segmentInfos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n              docCount += info.docCount;\n              segmentInfos.add(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        ensureOpen();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        resolveExternalSegments();\n\n        ensureOpen();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              assert !segmentInfos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n              docCount += info.docCount;\n              segmentInfos.add(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        ensureOpen();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        resolveExternalSegments();\n\n        ensureOpen();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c4ff8864209d2e972cb4393600c26082f9a6533d","date":1239297466,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              assert !segmentInfos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n              docCount += info.docCount;\n              segmentInfos.add(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        ensureOpen();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        resolveExternalSegments();\n\n        ensureOpen();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              assert !segmentInfos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n              docCount += info.docCount;\n              segmentInfos.add(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        ensureOpen();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        resolveExternalSegments();\n\n        ensureOpen();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cd488f50316362b01a7f67b11a96796b9652e3e5","date":1241121034,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              assert !segmentInfos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n              docCount += info.docCount;\n              segmentInfos.add(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        ensureOpen();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        resolveExternalSegments();\n\n        ensureOpen();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexesNoOptimize\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              assert !segmentInfos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n              docCount += info.docCount;\n              segmentInfos.add(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        ensureOpen();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        resolveExternalSegments();\n\n        ensureOpen();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":["01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7","date":1255555265,"type":5,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory...).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexesNoOptimize(Directory[]).mjava","sourceNew":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory... dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              assert !segmentInfos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n              docCount += info.docCount;\n              segmentInfos.add(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        ensureOpen();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        resolveExternalSegments();\n\n        ensureOpen();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexesNoOptimize\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Merges all segments from an array of indexes into this\n   * index.\n   *\n   * <p>This may be used to parallelize batch indexing.  A large document\n   * collection can be broken into sub-collections.  Each sub-collection can be\n   * indexed in parallel, on a different thread, process or machine.  The\n   * complete index can then be created by merging sub-collection indexes\n   * with this method.\n   *\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>This method is transactional in how Exceptions are\n   * handled: it does not commit a new segments_N file until\n   * all indexes are added.  This means if an Exception\n   * occurs (for example disk full), then either no indexes\n   * will have been added or they all will have been.</p>\n   *\n   * <p>Note that this requires temporary free space in the\n   * Directory up to 2X the sum of all input indexes\n   * (including the starting index).  If readers/searchers\n   * are open against the starting index, then temporary\n   * free space required will be higher by the size of the\n   * starting index (see {@link #optimize()} for details).\n   * </p>\n   *\n   * <p>Once this completes, the final size of the index\n   * will be less than the sum of all input index sizes\n   * (including the starting index).  It could be quite a\n   * bit smaller (if there were many pending deletes) or\n   * just slightly smaller.</p>\n   * \n   * <p>\n   * This requires this index not be among those to be added.\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexesNoOptimize(Directory[] dirs)\n      throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    noDupDirs(dirs);\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      if (infoStream != null)\n        message(\"flush at addIndexesNoOptimize\");\n      flush(true, false, true);\n\n      boolean success = false;\n\n      startTransaction(false);\n\n      try {\n\n        int docCount = 0;\n        synchronized(this) {\n          ensureOpen();\n\n          for (int i = 0; i < dirs.length; i++) {\n            if (directory == dirs[i]) {\n              // cannot add this index: segments may be deleted in merge before added\n              throw new IllegalArgumentException(\"Cannot add this index to itself\");\n            }\n\n            SegmentInfos sis = new SegmentInfos(); // read infos from dir\n            sis.read(dirs[i]);\n            for (int j = 0; j < sis.size(); j++) {\n              SegmentInfo info = sis.info(j);\n              assert !segmentInfos.contains(info): \"dup info dir=\" + info.dir + \" name=\" + info.name;\n              docCount += info.docCount;\n              segmentInfos.add(info); // add each info\n            }\n          }\n        }\n\n        // Notify DocumentsWriter that the flushed count just increased\n        docWriter.updateFlushedDocCount(docCount);\n\n        maybeMerge();\n\n        ensureOpen();\n\n        // If after merging there remain segments in the index\n        // that are in a different directory, just copy these\n        // over into our index.  This is necessary (before\n        // finishing the transaction) to avoid leaving the\n        // index in an unusable (inconsistent) state.\n        resolveExternalSegments();\n\n        ensureOpen();\n\n        success = true;\n\n      } finally {\n        if (success) {\n          commitTransaction();\n        } else {\n          rollbackTransaction();\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexesNoOptimize\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"268d26e9bb23249b6a418a01d52fcbe19ee33a1f":["dbb18b6a222f2507f22fab7cc7eed06658d59772"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"328c1568e471f0c6eaa49ec00334ca59e573710f":["0cb4e0840ad0d45706ad7fe5806b6d3e6624335b"],"b685e89f45e34ea31f9ca89912e4f29038818ff6":["328c1568e471f0c6eaa49ec00334ca59e573710f"],"1b54a9bc667895a2095a886184bf69a3179e63df":["eeefd99c477417e5c7c574228461ebafe92469d4"],"67006a60923e2124212d3baa0d29b444bcbd8373":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"c4ff8864209d2e972cb4393600c26082f9a6533d":["e9665d17707cc21b1db995118ff36129723139ab"],"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7":["cd488f50316362b01a7f67b11a96796b9652e3e5"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["2586f96f60332eb97ecd2934b0763791462568b2"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["b685e89f45e34ea31f9ca89912e4f29038818ff6"],"e9665d17707cc21b1db995118ff36129723139ab":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"dbb18b6a222f2507f22fab7cc7eed06658d59772":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"cd488f50316362b01a7f67b11a96796b9652e3e5":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63":["e82780afe6097066eb5befb86e9432f077667e3d"],"eeefd99c477417e5c7c574228461ebafe92469d4":["938d1493cf2f269d3b9e66e932c07ee784e00022"],"938d1493cf2f269d3b9e66e932c07ee784e00022":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3b248d9b80f6b2fceade80b3c8683d1cca6e4c98":["2607e6466921754dbb3ebd17f098a4e9f1fc4877"],"2607e6466921754dbb3ebd17f098a4e9f1fc4877":["67006a60923e2124212d3baa0d29b444bcbd8373"],"e82780afe6097066eb5befb86e9432f077667e3d":["3b248d9b80f6b2fceade80b3c8683d1cca6e4c98"],"2586f96f60332eb97ecd2934b0763791462568b2":["268d26e9bb23249b6a418a01d52fcbe19ee33a1f"],"0cb4e0840ad0d45706ad7fe5806b6d3e6624335b":["1b54a9bc667895a2095a886184bf69a3179e63df"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7"]},"commit2Childs":{"268d26e9bb23249b6a418a01d52fcbe19ee33a1f":["2586f96f60332eb97ecd2934b0763791462568b2"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["67006a60923e2124212d3baa0d29b444bcbd8373"],"328c1568e471f0c6eaa49ec00334ca59e573710f":["b685e89f45e34ea31f9ca89912e4f29038818ff6"],"b685e89f45e34ea31f9ca89912e4f29038818ff6":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"1b54a9bc667895a2095a886184bf69a3179e63df":["0cb4e0840ad0d45706ad7fe5806b6d3e6624335b"],"67006a60923e2124212d3baa0d29b444bcbd8373":["2607e6466921754dbb3ebd17f098a4e9f1fc4877"],"c4ff8864209d2e972cb4393600c26082f9a6533d":["cd488f50316362b01a7f67b11a96796b9652e3e5"],"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["e9665d17707cc21b1db995118ff36129723139ab"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["dbb18b6a222f2507f22fab7cc7eed06658d59772"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"dbb18b6a222f2507f22fab7cc7eed06658d59772":["268d26e9bb23249b6a418a01d52fcbe19ee33a1f"],"e9665d17707cc21b1db995118ff36129723139ab":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"cd488f50316362b01a7f67b11a96796b9652e3e5":["560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7"],"01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"eeefd99c477417e5c7c574228461ebafe92469d4":["1b54a9bc667895a2095a886184bf69a3179e63df"],"938d1493cf2f269d3b9e66e932c07ee784e00022":["eeefd99c477417e5c7c574228461ebafe92469d4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["938d1493cf2f269d3b9e66e932c07ee784e00022"],"3b248d9b80f6b2fceade80b3c8683d1cca6e4c98":["e82780afe6097066eb5befb86e9432f077667e3d"],"2607e6466921754dbb3ebd17f098a4e9f1fc4877":["3b248d9b80f6b2fceade80b3c8683d1cca6e4c98"],"e82780afe6097066eb5befb86e9432f077667e3d":["01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63"],"2586f96f60332eb97ecd2934b0763791462568b2":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"0cb4e0840ad0d45706ad7fe5806b6d3e6624335b":["328c1568e471f0c6eaa49ec00334ca59e573710f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}