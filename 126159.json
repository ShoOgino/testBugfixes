{"path":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","commits":[{"id":"50e7972fe4865715af8951d4ba15555e3426fc5d","date":1115024647,"type":0,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","pathOld":"/dev/null","sourceNew":"\t/**\n\t * Convenience method; Tokenizes the given field text and adds the resulting\n\t * terms to the index; Equivalent to adding a tokenized, indexed,\n\t * termVectorStored, unstored, non-keyword Lucene\n\t * {@link org.apache.lucene.document.Field}.\n\t * \n\t * @param fieldName\n\t *            a name to be associated with the text\n\t * @param text\n\t *            the text to tokenize and index.\n\t * @param analyzer\n\t *            the analyzer to use for tokenization\n\t */\n\tpublic void addField(String fieldName, String text, Analyzer analyzer) {\n\t\tif (fieldName == null)\n\t\t\tthrow new IllegalArgumentException(\"fieldName must not be null\");\n\t\tif (text == null)\n\t\t\tthrow new IllegalArgumentException(\"text must not be null\");\n\t\tif (analyzer == null)\n\t\t\tthrow new IllegalArgumentException(\"analyzer must not be null\");\n\t\t\n\t\tTokenStream stream;\n\t\tif (analyzer instanceof PatternAnalyzer) {\n\t\t\tstream = ((PatternAnalyzer) analyzer).tokenStream(fieldName, text);\n\t\t} else {\n\t\t\tstream = analyzer.tokenStream(fieldName, new StringReader(text));\n\t\t}\n\t\taddField(fieldName, stream);\n\t}\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c8f14489323057ef6de92ba5ea2d0cfe6e34755f","date":1120167605,"type":3,"author":"Mark Harwood","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","sourceNew":"\t/**\r\n\t * Convenience method; Tokenizes the given field text and adds the resulting\r\n\t * terms to the index; Equivalent to adding a tokenized, indexed,\r\n\t * termVectorStored, unstored, non-keyword Lucene\r\n\t * {@link org.apache.lucene.document.Field}.\r\n\t * \r\n\t * @param fieldName\r\n\t *            a name to be associated with the text\r\n\t * @param text\r\n\t *            the text to tokenize and index.\r\n\t * @param analyzer\r\n\t *            the analyzer to use for tokenization\r\n\t */\r\n\tpublic void addField(String fieldName, String text, Analyzer analyzer) {\r\n\t\tif (fieldName == null)\r\n\t\t\tthrow new IllegalArgumentException(\"fieldName must not be null\");\r\n\t\tif (text == null)\r\n\t\t\tthrow new IllegalArgumentException(\"text must not be null\");\r\n\t\tif (analyzer == null)\r\n\t\t\tthrow new IllegalArgumentException(\"analyzer must not be null\");\r\n\t\t\r\n\t\tTokenStream stream;\r\n\t\tif (analyzer instanceof PatternAnalyzer) {\r\n\t\t\tstream = ((PatternAnalyzer) analyzer).tokenStream(fieldName, text);\r\n\t\t} else {\r\n\t\t\tstream = analyzer.tokenStream(fieldName, \r\n\t\t\t\t\tnew PatternAnalyzer.FastStringReader(text));\r\n\t\t}\r\n\t\taddField(fieldName, stream);\r\n\t}\r\n\n","sourceOld":"\t/**\n\t * Convenience method; Tokenizes the given field text and adds the resulting\n\t * terms to the index; Equivalent to adding a tokenized, indexed,\n\t * termVectorStored, unstored, non-keyword Lucene\n\t * {@link org.apache.lucene.document.Field}.\n\t * \n\t * @param fieldName\n\t *            a name to be associated with the text\n\t * @param text\n\t *            the text to tokenize and index.\n\t * @param analyzer\n\t *            the analyzer to use for tokenization\n\t */\n\tpublic void addField(String fieldName, String text, Analyzer analyzer) {\n\t\tif (fieldName == null)\n\t\t\tthrow new IllegalArgumentException(\"fieldName must not be null\");\n\t\tif (text == null)\n\t\t\tthrow new IllegalArgumentException(\"text must not be null\");\n\t\tif (analyzer == null)\n\t\t\tthrow new IllegalArgumentException(\"analyzer must not be null\");\n\t\t\n\t\tTokenStream stream;\n\t\tif (analyzer instanceof PatternAnalyzer) {\n\t\t\tstream = ((PatternAnalyzer) analyzer).tokenStream(fieldName, text);\n\t\t} else {\n\t\t\tstream = analyzer.tokenStream(fieldName, new StringReader(text));\n\t\t}\n\t\taddField(fieldName, stream);\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"35dd40ede4dd66fa47506858c4a073d295c5a76e","date":1133587328,"type":4,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"/dev/null","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","sourceNew":null,"sourceOld":"\t/**\r\n\t * Convenience method; Tokenizes the given field text and adds the resulting\r\n\t * terms to the index; Equivalent to adding a tokenized, indexed,\r\n\t * termVectorStored, unstored, non-keyword Lucene\r\n\t * {@link org.apache.lucene.document.Field}.\r\n\t * \r\n\t * @param fieldName\r\n\t *            a name to be associated with the text\r\n\t * @param text\r\n\t *            the text to tokenize and index.\r\n\t * @param analyzer\r\n\t *            the analyzer to use for tokenization\r\n\t */\r\n\tpublic void addField(String fieldName, String text, Analyzer analyzer) {\r\n\t\tif (fieldName == null)\r\n\t\t\tthrow new IllegalArgumentException(\"fieldName must not be null\");\r\n\t\tif (text == null)\r\n\t\t\tthrow new IllegalArgumentException(\"text must not be null\");\r\n\t\tif (analyzer == null)\r\n\t\t\tthrow new IllegalArgumentException(\"analyzer must not be null\");\r\n\t\t\r\n\t\tTokenStream stream;\r\n\t\tif (analyzer instanceof PatternAnalyzer) {\r\n\t\t\tstream = ((PatternAnalyzer) analyzer).tokenStream(fieldName, text);\r\n\t\t} else {\r\n\t\t\tstream = analyzer.tokenStream(fieldName, \r\n\t\t\t\t\tnew PatternAnalyzer.FastStringReader(text));\r\n\t\t}\r\n\t\taddField(fieldName, stream);\r\n\t}\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a93e1e4a21be8ebb98e53e6933412a363931faa1","date":1133587471,"type":0,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","pathOld":"/dev/null","sourceNew":"\t/**\n\t * Convenience method; Tokenizes the given field text and adds the resulting\n\t * terms to the index; Equivalent to adding a tokenized, indexed,\n\t * termVectorStored, unstored, non-keyword Lucene\n\t * {@link org.apache.lucene.document.Field}.\n\t * \n\t * @param fieldName\n\t *            a name to be associated with the text\n\t * @param text\n\t *            the text to tokenize and index.\n\t * @param analyzer\n\t *            the analyzer to use for tokenization\n\t */\n\tpublic void addField(String fieldName, String text, Analyzer analyzer) {\n\t\tif (fieldName == null)\n\t\t\tthrow new IllegalArgumentException(\"fieldName must not be null\");\n\t\tif (text == null)\n\t\t\tthrow new IllegalArgumentException(\"text must not be null\");\n\t\tif (analyzer == null)\n\t\t\tthrow new IllegalArgumentException(\"analyzer must not be null\");\n\t\t\n\t\tTokenStream stream;\n\t\tif (analyzer instanceof PatternAnalyzer) {\n\t\t\tstream = ((PatternAnalyzer) analyzer).tokenStream(fieldName, text);\n\t\t} else {\n\t\t\tstream = analyzer.tokenStream(fieldName, \n\t\t\t\t\tnew PatternAnalyzer.FastStringReader(text));\n\t\t}\n\t\taddField(fieldName, stream);\n\t}\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f68e24227d5556d33ee6d586fd9010cd9ff8bec","date":1150091176,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","sourceNew":"  /**\n   * Convenience method; Tokenizes the given field text and adds the resulting\n   * terms to the index; Equivalent to adding a tokenized, indexed,\n   * termVectorStored, unstored, non-keyword Lucene\n   * {@link org.apache.lucene.document.Field}.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param text\n   *            the text to tokenize and index.\n   * @param analyzer\n   *            the analyzer to use for tokenization\n   */\n  public void addField(String fieldName, String text, Analyzer analyzer) {\n    if (fieldName == null)\n      throw new IllegalArgumentException(\"fieldName must not be null\");\n    if (text == null)\n      throw new IllegalArgumentException(\"text must not be null\");\n    if (analyzer == null)\n      throw new IllegalArgumentException(\"analyzer must not be null\");\n    \n    TokenStream stream;\n    if (analyzer instanceof PatternAnalyzer) {\n      stream = ((PatternAnalyzer) analyzer).tokenStream(fieldName, text);\n    } else {\n      stream = analyzer.tokenStream(fieldName, \n          new PatternAnalyzer.FastStringReader(text));\n    }\n    addField(fieldName, stream);\n  }\n\n","sourceOld":"\t/**\n\t * Convenience method; Tokenizes the given field text and adds the resulting\n\t * terms to the index; Equivalent to adding a tokenized, indexed,\n\t * termVectorStored, unstored, non-keyword Lucene\n\t * {@link org.apache.lucene.document.Field}.\n\t * \n\t * @param fieldName\n\t *            a name to be associated with the text\n\t * @param text\n\t *            the text to tokenize and index.\n\t * @param analyzer\n\t *            the analyzer to use for tokenization\n\t */\n\tpublic void addField(String fieldName, String text, Analyzer analyzer) {\n\t\tif (fieldName == null)\n\t\t\tthrow new IllegalArgumentException(\"fieldName must not be null\");\n\t\tif (text == null)\n\t\t\tthrow new IllegalArgumentException(\"text must not be null\");\n\t\tif (analyzer == null)\n\t\t\tthrow new IllegalArgumentException(\"analyzer must not be null\");\n\t\t\n\t\tTokenStream stream;\n\t\tif (analyzer instanceof PatternAnalyzer) {\n\t\t\tstream = ((PatternAnalyzer) analyzer).tokenStream(fieldName, text);\n\t\t} else {\n\t\t\tstream = analyzer.tokenStream(fieldName, \n\t\t\t\t\tnew PatternAnalyzer.FastStringReader(text));\n\t\t}\n\t\taddField(fieldName, stream);\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a45b4b9cfa660dfbe65b606f7ca1e8f498b7e160","date":1164224545,"type":3,"author":"Wolfgang Hoschek","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","sourceNew":"  /**\n   * Convenience method; Tokenizes the given field text and adds the resulting\n   * terms to the index; Equivalent to adding an indexed non-keyword Lucene\n   * {@link org.apache.lucene.document.Field} that is\n   * {@link org.apache.lucene.document.Field.Index#TOKENIZED tokenized},\n   * {@link org.apache.lucene.document.Field.Store#NO not stored},\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions} (or\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions and offsets}),\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param text\n   *            the text to tokenize and index.\n   * @param analyzer\n   *            the analyzer to use for tokenization\n   */\n  public void addField(String fieldName, String text, Analyzer analyzer) {\n    if (fieldName == null)\n      throw new IllegalArgumentException(\"fieldName must not be null\");\n    if (text == null)\n      throw new IllegalArgumentException(\"text must not be null\");\n    if (analyzer == null)\n      throw new IllegalArgumentException(\"analyzer must not be null\");\n    \n    TokenStream stream;\n    if (analyzer instanceof PatternAnalyzer) {\n      stream = ((PatternAnalyzer) analyzer).tokenStream(fieldName, text);\n    } else {\n      stream = analyzer.tokenStream(fieldName, \n          new PatternAnalyzer.FastStringReader(text));\n    }\n    addField(fieldName, stream);\n  }\n\n","sourceOld":"  /**\n   * Convenience method; Tokenizes the given field text and adds the resulting\n   * terms to the index; Equivalent to adding a tokenized, indexed,\n   * termVectorStored, unstored, non-keyword Lucene\n   * {@link org.apache.lucene.document.Field}.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param text\n   *            the text to tokenize and index.\n   * @param analyzer\n   *            the analyzer to use for tokenization\n   */\n  public void addField(String fieldName, String text, Analyzer analyzer) {\n    if (fieldName == null)\n      throw new IllegalArgumentException(\"fieldName must not be null\");\n    if (text == null)\n      throw new IllegalArgumentException(\"text must not be null\");\n    if (analyzer == null)\n      throw new IllegalArgumentException(\"analyzer must not be null\");\n    \n    TokenStream stream;\n    if (analyzer instanceof PatternAnalyzer) {\n      stream = ((PatternAnalyzer) analyzer).tokenStream(fieldName, text);\n    } else {\n      stream = analyzer.tokenStream(fieldName, \n          new PatternAnalyzer.FastStringReader(text));\n    }\n    addField(fieldName, stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a","date":1221082732,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","sourceNew":"  /**\n   * Convenience method; Tokenizes the given field text and adds the resulting\n   * terms to the index; Equivalent to adding an indexed non-keyword Lucene\n   * {@link org.apache.lucene.document.Field} that is\n   * {@link org.apache.lucene.document.Field.Index#ANALYZED tokenized},\n   * {@link org.apache.lucene.document.Field.Store#NO not stored},\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions} (or\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions and offsets}),\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param text\n   *            the text to tokenize and index.\n   * @param analyzer\n   *            the analyzer to use for tokenization\n   */\n  public void addField(String fieldName, String text, Analyzer analyzer) {\n    if (fieldName == null)\n      throw new IllegalArgumentException(\"fieldName must not be null\");\n    if (text == null)\n      throw new IllegalArgumentException(\"text must not be null\");\n    if (analyzer == null)\n      throw new IllegalArgumentException(\"analyzer must not be null\");\n    \n    TokenStream stream;\n    if (analyzer instanceof PatternAnalyzer) {\n      stream = ((PatternAnalyzer) analyzer).tokenStream(fieldName, text);\n    } else {\n      stream = analyzer.tokenStream(fieldName, \n          new PatternAnalyzer.FastStringReader(text));\n    }\n    addField(fieldName, stream);\n  }\n\n","sourceOld":"  /**\n   * Convenience method; Tokenizes the given field text and adds the resulting\n   * terms to the index; Equivalent to adding an indexed non-keyword Lucene\n   * {@link org.apache.lucene.document.Field} that is\n   * {@link org.apache.lucene.document.Field.Index#TOKENIZED tokenized},\n   * {@link org.apache.lucene.document.Field.Store#NO not stored},\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions} (or\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions and offsets}),\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param text\n   *            the text to tokenize and index.\n   * @param analyzer\n   *            the analyzer to use for tokenization\n   */\n  public void addField(String fieldName, String text, Analyzer analyzer) {\n    if (fieldName == null)\n      throw new IllegalArgumentException(\"fieldName must not be null\");\n    if (text == null)\n      throw new IllegalArgumentException(\"text must not be null\");\n    if (analyzer == null)\n      throw new IllegalArgumentException(\"analyzer must not be null\");\n    \n    TokenStream stream;\n    if (analyzer instanceof PatternAnalyzer) {\n      stream = ((PatternAnalyzer) analyzer).tokenStream(fieldName, text);\n    } else {\n      stream = analyzer.tokenStream(fieldName, \n          new PatternAnalyzer.FastStringReader(text));\n    }\n    addField(fieldName, stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0d8c222875ac4ef7e08f3e25d40508b821d711ff","date":1257374221,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","sourceNew":"  /**\n   * Convenience method; Tokenizes the given field text and adds the resulting\n   * terms to the index; Equivalent to adding an indexed non-keyword Lucene\n   * {@link org.apache.lucene.document.Field} that is\n   * {@link org.apache.lucene.document.Field.Index#ANALYZED tokenized},\n   * {@link org.apache.lucene.document.Field.Store#NO not stored},\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions} (or\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions and offsets}),\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param text\n   *            the text to tokenize and index.\n   * @param analyzer\n   *            the analyzer to use for tokenization\n   */\n  public void addField(String fieldName, String text, Analyzer analyzer) {\n    if (fieldName == null)\n      throw new IllegalArgumentException(\"fieldName must not be null\");\n    if (text == null)\n      throw new IllegalArgumentException(\"text must not be null\");\n    if (analyzer == null)\n      throw new IllegalArgumentException(\"analyzer must not be null\");\n    \n    TokenStream stream = analyzer.tokenStream(fieldName, \n    \t\tnew StringReader(text));\n\n    addField(fieldName, stream);\n  }\n\n","sourceOld":"  /**\n   * Convenience method; Tokenizes the given field text and adds the resulting\n   * terms to the index; Equivalent to adding an indexed non-keyword Lucene\n   * {@link org.apache.lucene.document.Field} that is\n   * {@link org.apache.lucene.document.Field.Index#ANALYZED tokenized},\n   * {@link org.apache.lucene.document.Field.Store#NO not stored},\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions} (or\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions and offsets}),\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param text\n   *            the text to tokenize and index.\n   * @param analyzer\n   *            the analyzer to use for tokenization\n   */\n  public void addField(String fieldName, String text, Analyzer analyzer) {\n    if (fieldName == null)\n      throw new IllegalArgumentException(\"fieldName must not be null\");\n    if (text == null)\n      throw new IllegalArgumentException(\"text must not be null\");\n    if (analyzer == null)\n      throw new IllegalArgumentException(\"analyzer must not be null\");\n    \n    TokenStream stream;\n    if (analyzer instanceof PatternAnalyzer) {\n      stream = ((PatternAnalyzer) analyzer).tokenStream(fieldName, text);\n    } else {\n      stream = analyzer.tokenStream(fieldName, \n          new PatternAnalyzer.FastStringReader(text));\n    }\n    addField(fieldName, stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,String,Analyzer).mjava","sourceNew":"  /**\n   * Convenience method; Tokenizes the given field text and adds the resulting\n   * terms to the index; Equivalent to adding an indexed non-keyword Lucene\n   * {@link org.apache.lucene.document.Field} that is\n   * {@link org.apache.lucene.document.Field.Index#ANALYZED tokenized},\n   * {@link org.apache.lucene.document.Field.Store#NO not stored},\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions} (or\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions and offsets}),\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param text\n   *            the text to tokenize and index.\n   * @param analyzer\n   *            the analyzer to use for tokenization\n   */\n  public void addField(String fieldName, String text, Analyzer analyzer) {\n    if (fieldName == null)\n      throw new IllegalArgumentException(\"fieldName must not be null\");\n    if (text == null)\n      throw new IllegalArgumentException(\"text must not be null\");\n    if (analyzer == null)\n      throw new IllegalArgumentException(\"analyzer must not be null\");\n    \n    TokenStream stream = analyzer.tokenStream(fieldName, \n    \t\tnew StringReader(text));\n\n    addField(fieldName, stream);\n  }\n\n","sourceOld":"  /**\n   * Convenience method; Tokenizes the given field text and adds the resulting\n   * terms to the index; Equivalent to adding an indexed non-keyword Lucene\n   * {@link org.apache.lucene.document.Field} that is\n   * {@link org.apache.lucene.document.Field.Index#ANALYZED tokenized},\n   * {@link org.apache.lucene.document.Field.Store#NO not stored},\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions} (or\n   * {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS termVectorStored with positions and offsets}),\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param text\n   *            the text to tokenize and index.\n   * @param analyzer\n   *            the analyzer to use for tokenization\n   */\n  public void addField(String fieldName, String text, Analyzer analyzer) {\n    if (fieldName == null)\n      throw new IllegalArgumentException(\"fieldName must not be null\");\n    if (text == null)\n      throw new IllegalArgumentException(\"text must not be null\");\n    if (analyzer == null)\n      throw new IllegalArgumentException(\"analyzer must not be null\");\n    \n    TokenStream stream = analyzer.tokenStream(fieldName, \n    \t\tnew StringReader(text));\n\n    addField(fieldName, stream);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"35dd40ede4dd66fa47506858c4a073d295c5a76e":["c8f14489323057ef6de92ba5ea2d0cfe6e34755f"],"0d8c222875ac4ef7e08f3e25d40508b821d711ff":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"c8f14489323057ef6de92ba5ea2d0cfe6e34755f":["50e7972fe4865715af8951d4ba15555e3426fc5d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a45b4b9cfa660dfbe65b606f7ca1e8f498b7e160":["7f68e24227d5556d33ee6d586fd9010cd9ff8bec"],"7f68e24227d5556d33ee6d586fd9010cd9ff8bec":["a93e1e4a21be8ebb98e53e6933412a363931faa1"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["a45b4b9cfa660dfbe65b606f7ca1e8f498b7e160"],"50e7972fe4865715af8951d4ba15555e3426fc5d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a93e1e4a21be8ebb98e53e6933412a363931faa1":["35dd40ede4dd66fa47506858c4a073d295c5a76e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["0d8c222875ac4ef7e08f3e25d40508b821d711ff"]},"commit2Childs":{"35dd40ede4dd66fa47506858c4a073d295c5a76e":["a93e1e4a21be8ebb98e53e6933412a363931faa1"],"0d8c222875ac4ef7e08f3e25d40508b821d711ff":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"c8f14489323057ef6de92ba5ea2d0cfe6e34755f":["35dd40ede4dd66fa47506858c4a073d295c5a76e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["50e7972fe4865715af8951d4ba15555e3426fc5d"],"a45b4b9cfa660dfbe65b606f7ca1e8f498b7e160":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"7f68e24227d5556d33ee6d586fd9010cd9ff8bec":["a45b4b9cfa660dfbe65b606f7ca1e8f498b7e160"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["0d8c222875ac4ef7e08f3e25d40508b821d711ff"],"50e7972fe4865715af8951d4ba15555e3426fc5d":["c8f14489323057ef6de92ba5ea2d0cfe6e34755f"],"a93e1e4a21be8ebb98e53e6933412a363931faa1":["7f68e24227d5556d33ee6d586fd9010cd9ff8bec"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}