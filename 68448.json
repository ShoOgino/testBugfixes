{"path":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark#execute(Judge,SubmissionReport,PrintWriter).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark#execute(Judge,SubmissionReport,PrintWriter).mjava","pathOld":"contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark#execute(Judge,SubmissionReport,PrintWriter).mjava","sourceNew":"  /**\n   * Run the quality benchmark.\n   * @param judge the judge that can tell if a certain result doc is relevant for a certain quality query. \n   *        If null, no judgements would be made. Usually null for a submission run. \n   * @param submitRep submission report is created if non null.\n   * @param qualityLog If not null, quality run data would be printed for each query.\n   * @return QualityStats of each quality query that was executed.\n   * @throws Exception if quality benchmark failed to run.\n   */\n  public  QualityStats [] execute(Judge judge, SubmissionReport submitRep, \n                                  PrintWriter qualityLog) throws Exception {\n    int nQueries = Math.min(maxQueries, qualityQueries.length);\n    QualityStats stats[] = new QualityStats[nQueries]; \n    for (int i=0; i<nQueries; i++) {\n      QualityQuery qq = qualityQueries[i];\n      // generate query\n      Query q = qqParser.parse(qq);\n      // search with this query \n      long t1 = System.currentTimeMillis();\n      TopDocs td = searcher.search(q,null,maxResults);\n      long searchTime = System.currentTimeMillis()-t1;\n      //most likely we either submit or judge, but check both \n      if (judge!=null) {\n        stats[i] = analyzeQueryResults(qq, q, td, judge, qualityLog, searchTime);\n      }\n      if (submitRep!=null) {\n        submitRep.report(qq,td,docNameField,searcher);\n      }\n    } \n    if (submitRep!=null) {\n      submitRep.flush();\n    }\n    return stats;\n  }\n\n","sourceOld":"  /**\n   * Run the quality benchmark.\n   * @param judge the judge that can tell if a certain result doc is relevant for a certain quality query. \n   *        If null, no judgements would be made. Usually null for a submission run. \n   * @param submitRep submission report is created if non null.\n   * @param qualityLog If not null, quality run data would be printed for each query.\n   * @return QualityStats of each quality query that was executed.\n   * @throws Exception if quality benchmark failed to run.\n   */\n  public  QualityStats [] execute(Judge judge, SubmissionReport submitRep, \n                                  PrintWriter qualityLog) throws Exception {\n    int nQueries = Math.min(maxQueries, qualityQueries.length);\n    QualityStats stats[] = new QualityStats[nQueries]; \n    for (int i=0; i<nQueries; i++) {\n      QualityQuery qq = qualityQueries[i];\n      // generate query\n      Query q = qqParser.parse(qq);\n      // search with this query \n      long t1 = System.currentTimeMillis();\n      TopDocs td = searcher.search(q,null,maxResults);\n      long searchTime = System.currentTimeMillis()-t1;\n      //most likely we either submit or judge, but check both \n      if (judge!=null) {\n        stats[i] = analyzeQueryResults(qq, q, td, judge, qualityLog, searchTime);\n      }\n      if (submitRep!=null) {\n        submitRep.report(qq,td,docNameField,searcher);\n      }\n    } \n    if (submitRep!=null) {\n      submitRep.flush();\n    }\n    return stats;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ecc11368dc265bfdad90214f8bf5da99016ab1e2","date":1294144090,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark#execute(Judge,SubmissionReport,PrintWriter).mjava","pathOld":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark#execute(Judge,SubmissionReport,PrintWriter).mjava","sourceNew":"  /**\n   * Run the quality benchmark.\n   * @param judge the judge that can tell if a certain result doc is relevant for a certain quality query. \n   *        If null, no judgements would be made. Usually null for a submission run. \n   * @param submitRep submission report is created if non null.\n   * @param qualityLog If not null, quality run data would be printed for each query.\n   * @return QualityStats of each quality query that was executed.\n   * @throws Exception if quality benchmark failed to run.\n   */\n  public  QualityStats [] execute(Judge judge, SubmissionReport submitRep, \n                                  PrintWriter qualityLog) throws Exception {\n    int nQueries = Math.min(maxQueries, qualityQueries.length);\n    QualityStats stats[] = new QualityStats[nQueries]; \n    for (int i=0; i<nQueries; i++) {\n      QualityQuery qq = qualityQueries[i];\n      // generate query\n      Query q = qqParser.parse(qq);\n      // search with this query \n      long t1 = System.currentTimeMillis();\n      TopDocs td = searcher.search(q,null,maxResults);\n      long searchTime = System.currentTimeMillis()-t1;\n      //most likely we either submit or judge, but check both \n      if (judge!=null) {\n        stats[i] = analyzeQueryResults(qq, q, td, judge, qualityLog, searchTime);\n      }\n      if (submitRep!=null) {\n        submitRep.report(qq,td,docNameField,searcher);\n      }\n    } \n    if (submitRep!=null) {\n      submitRep.flush();\n    }\n    return stats;\n  }\n\n","sourceOld":"  /**\n   * Run the quality benchmark.\n   * @param judge the judge that can tell if a certain result doc is relevant for a certain quality query. \n   *        If null, no judgements would be made. Usually null for a submission run. \n   * @param submitRep submission report is created if non null.\n   * @param qualityLog If not null, quality run data would be printed for each query.\n   * @return QualityStats of each quality query that was executed.\n   * @throws Exception if quality benchmark failed to run.\n   */\n  public  QualityStats [] execute(Judge judge, SubmissionReport submitRep, \n                                  PrintWriter qualityLog) throws Exception {\n    int nQueries = Math.min(maxQueries, qualityQueries.length);\n    QualityStats stats[] = new QualityStats[nQueries]; \n    for (int i=0; i<nQueries; i++) {\n      QualityQuery qq = qualityQueries[i];\n      // generate query\n      Query q = qqParser.parse(qq);\n      // search with this query \n      long t1 = System.currentTimeMillis();\n      TopDocs td = searcher.search(q,null,maxResults);\n      long searchTime = System.currentTimeMillis()-t1;\n      //most likely we either submit or judge, but check both \n      if (judge!=null) {\n        stats[i] = analyzeQueryResults(qq, q, td, judge, qualityLog, searchTime);\n      }\n      if (submitRep!=null) {\n        submitRep.report(qq,td,docNameField,searcher);\n      }\n    } \n    if (submitRep!=null) {\n      submitRep.flush();\n    }\n    return stats;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":5,"author":"Simon Willnauer","isMerge":true,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark#execute(Judge,SubmissionReport,PrintWriter).mjava","pathOld":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark#execute(Judge,SubmissionReport,PrintWriter).mjava","sourceNew":"  /**\n   * Run the quality benchmark.\n   * @param judge the judge that can tell if a certain result doc is relevant for a certain quality query. \n   *        If null, no judgements would be made. Usually null for a submission run. \n   * @param submitRep submission report is created if non null.\n   * @param qualityLog If not null, quality run data would be printed for each query.\n   * @return QualityStats of each quality query that was executed.\n   * @throws Exception if quality benchmark failed to run.\n   */\n  public  QualityStats [] execute(Judge judge, SubmissionReport submitRep, \n                                  PrintWriter qualityLog) throws Exception {\n    int nQueries = Math.min(maxQueries, qualityQueries.length);\n    QualityStats stats[] = new QualityStats[nQueries]; \n    for (int i=0; i<nQueries; i++) {\n      QualityQuery qq = qualityQueries[i];\n      // generate query\n      Query q = qqParser.parse(qq);\n      // search with this query \n      long t1 = System.currentTimeMillis();\n      TopDocs td = searcher.search(q,null,maxResults);\n      long searchTime = System.currentTimeMillis()-t1;\n      //most likely we either submit or judge, but check both \n      if (judge!=null) {\n        stats[i] = analyzeQueryResults(qq, q, td, judge, qualityLog, searchTime);\n      }\n      if (submitRep!=null) {\n        submitRep.report(qq,td,docNameField,searcher);\n      }\n    } \n    if (submitRep!=null) {\n      submitRep.flush();\n    }\n    return stats;\n  }\n\n","sourceOld":"  /**\n   * Run the quality benchmark.\n   * @param judge the judge that can tell if a certain result doc is relevant for a certain quality query. \n   *        If null, no judgements would be made. Usually null for a submission run. \n   * @param submitRep submission report is created if non null.\n   * @param qualityLog If not null, quality run data would be printed for each query.\n   * @return QualityStats of each quality query that was executed.\n   * @throws Exception if quality benchmark failed to run.\n   */\n  public  QualityStats [] execute(Judge judge, SubmissionReport submitRep, \n                                  PrintWriter qualityLog) throws Exception {\n    int nQueries = Math.min(maxQueries, qualityQueries.length);\n    QualityStats stats[] = new QualityStats[nQueries]; \n    for (int i=0; i<nQueries; i++) {\n      QualityQuery qq = qualityQueries[i];\n      // generate query\n      Query q = qqParser.parse(qq);\n      // search with this query \n      long t1 = System.currentTimeMillis();\n      TopDocs td = searcher.search(q,null,maxResults);\n      long searchTime = System.currentTimeMillis()-t1;\n      //most likely we either submit or judge, but check both \n      if (judge!=null) {\n        stats[i] = analyzeQueryResults(qq, q, td, judge, qualityLog, searchTime);\n      }\n      if (submitRep!=null) {\n        submitRep.report(qq,td,docNameField,searcher);\n      }\n    } \n    if (submitRep!=null) {\n      submitRep.flush();\n    }\n    return stats;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":5,"author":"Michael Busch","isMerge":true,"pathNew":"modules/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark#execute(Judge,SubmissionReport,PrintWriter).mjava","pathOld":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityBenchmark#execute(Judge,SubmissionReport,PrintWriter).mjava","sourceNew":"  /**\n   * Run the quality benchmark.\n   * @param judge the judge that can tell if a certain result doc is relevant for a certain quality query. \n   *        If null, no judgements would be made. Usually null for a submission run. \n   * @param submitRep submission report is created if non null.\n   * @param qualityLog If not null, quality run data would be printed for each query.\n   * @return QualityStats of each quality query that was executed.\n   * @throws Exception if quality benchmark failed to run.\n   */\n  public  QualityStats [] execute(Judge judge, SubmissionReport submitRep, \n                                  PrintWriter qualityLog) throws Exception {\n    int nQueries = Math.min(maxQueries, qualityQueries.length);\n    QualityStats stats[] = new QualityStats[nQueries]; \n    for (int i=0; i<nQueries; i++) {\n      QualityQuery qq = qualityQueries[i];\n      // generate query\n      Query q = qqParser.parse(qq);\n      // search with this query \n      long t1 = System.currentTimeMillis();\n      TopDocs td = searcher.search(q,null,maxResults);\n      long searchTime = System.currentTimeMillis()-t1;\n      //most likely we either submit or judge, but check both \n      if (judge!=null) {\n        stats[i] = analyzeQueryResults(qq, q, td, judge, qualityLog, searchTime);\n      }\n      if (submitRep!=null) {\n        submitRep.report(qq,td,docNameField,searcher);\n      }\n    } \n    if (submitRep!=null) {\n      submitRep.flush();\n    }\n    return stats;\n  }\n\n","sourceOld":"  /**\n   * Run the quality benchmark.\n   * @param judge the judge that can tell if a certain result doc is relevant for a certain quality query. \n   *        If null, no judgements would be made. Usually null for a submission run. \n   * @param submitRep submission report is created if non null.\n   * @param qualityLog If not null, quality run data would be printed for each query.\n   * @return QualityStats of each quality query that was executed.\n   * @throws Exception if quality benchmark failed to run.\n   */\n  public  QualityStats [] execute(Judge judge, SubmissionReport submitRep, \n                                  PrintWriter qualityLog) throws Exception {\n    int nQueries = Math.min(maxQueries, qualityQueries.length);\n    QualityStats stats[] = new QualityStats[nQueries]; \n    for (int i=0; i<nQueries; i++) {\n      QualityQuery qq = qualityQueries[i];\n      // generate query\n      Query q = qqParser.parse(qq);\n      // search with this query \n      long t1 = System.currentTimeMillis();\n      TopDocs td = searcher.search(q,null,maxResults);\n      long searchTime = System.currentTimeMillis()-t1;\n      //most likely we either submit or judge, but check both \n      if (judge!=null) {\n        stats[i] = analyzeQueryResults(qq, q, td, judge, qualityLog, searchTime);\n      }\n      if (submitRep!=null) {\n        submitRep.report(qq,td,docNameField,searcher);\n      }\n    } \n    if (submitRep!=null) {\n      submitRep.flush();\n    }\n    return stats;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70ad682703b8585f5d0a637efec044d57ec05efb":["9454a6510e2db155fb01faa5c049b06ece95fab9","ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"ecc11368dc265bfdad90214f8bf5da99016ab1e2":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"868da859b43505d9d2a023bfeae6dd0c795f5295":["9454a6510e2db155fb01faa5c049b06ece95fab9","ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ecc11368dc265bfdad90214f8bf5da99016ab1e2"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"70ad682703b8585f5d0a637efec044d57ec05efb":[],"ecc11368dc265bfdad90214f8bf5da99016ab1e2":["70ad682703b8585f5d0a637efec044d57ec05efb","868da859b43505d9d2a023bfeae6dd0c795f5295","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"868da859b43505d9d2a023bfeae6dd0c795f5295":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["70ad682703b8585f5d0a637efec044d57ec05efb","ecc11368dc265bfdad90214f8bf5da99016ab1e2","868da859b43505d9d2a023bfeae6dd0c795f5295"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["70ad682703b8585f5d0a637efec044d57ec05efb","868da859b43505d9d2a023bfeae6dd0c795f5295","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}