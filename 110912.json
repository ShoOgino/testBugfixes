{"path":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/SolrRecordWriter#write(K,V).mjava","commits":[{"id":"d6e604e9030fb0cabf0c5a85ae6039921a81419c","date":1386009743,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/SolrRecordWriter#write(K,V).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Write a record. This method accumulates records in to a batch, and when\n   * {@link #batchSize} items are present flushes it to the indexer. The writes\n   * can take a substantial amount of time, depending on {@link #batchSize}. If\n   * there is heavy disk contention the writes may take more than the 600 second\n   * default timeout.\n   */\n  @Override\n  public void write(K key, V value) throws IOException {\n    heartBeater.needHeartBeat();\n    try {\n      try {\n        SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value;\n        batch.add(sidw.getSolrInputDocument());\n        if (batch.size() >= batchSize) {\n          batchWriter.queueBatch(batch);\n          numDocsWritten += batch.size();\n          if (System.currentTimeMillis() >= nextLogTime) {\n            LOG.info(\"docsWritten: {}\", numDocsWritten);\n            nextLogTime += 10000;\n          }\n          batch.clear();\n        }\n      } catch (SolrServerException e) {\n        throw new IOException(e);\n      }\n    } finally {\n      heartBeater.cancelHeartBeat();\n    }\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/java/org/apache/solr/hadoop/SolrRecordWriter#write(K,V).mjava","pathOld":"solr/contrib/solr-mr/src/java/org/apache/solr/hadoop/SolrRecordWriter#write(K,V).mjava","sourceNew":"  /**\n   * Write a record. This method accumulates records in to a batch, and when\n   * {@link #batchSize} items are present flushes it to the indexer. The writes\n   * can take a substantial amount of time, depending on {@link #batchSize}. If\n   * there is heavy disk contention the writes may take more than the 600 second\n   * default timeout.\n   */\n  @Override\n  public void write(K key, V value) throws IOException {\n    heartBeater.needHeartBeat();\n    try {\n      try {\n        SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value;\n        batch.add(sidw.getSolrInputDocument());\n        if (batch.size() >= batchSize) {\n          batchWriter.queueBatch(batch);\n          numDocsWritten += batch.size();\n          if (System.currentTimeMillis() >= nextLogTime) {\n            LOG.info(\"docsWritten: {}\", numDocsWritten);\n            nextLogTime += 10000;\n          }\n          batch.clear();\n        }\n      } catch (SolrServerException e) {\n        throw new IOException(e);\n      }\n    } finally {\n      heartBeater.cancelHeartBeat();\n    }\n\n  }\n\n","sourceOld":"  /**\n   * Write a record. This method accumulates records in to a batch, and when\n   * {@link #batchSize} items are present flushes it to the indexer. The writes\n   * can take a substantial amount of time, depending on {@link #batchSize}. If\n   * there is heavy disk contention the writes may take more than the 600 second\n   * default timeout.\n   */\n  @Override\n  public void write(K key, V value) throws IOException {\n    heartBeater.needHeartBeat();\n    try {\n      try {\n        SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value;\n        batch.add(sidw.getSolrInputDocument());\n        if (batch.size() >= batchSize) {\n          batchWriter.queueBatch(batch);\n          numDocsWritten += batch.size();\n          if (System.currentTimeMillis() >= nextLogTime) {\n            LOG.info(\"docsWritten: {}\", numDocsWritten);\n            nextLogTime += 10000;\n          }\n          batch.clear();\n        }\n      } catch (SolrServerException e) {\n        throw new IOException(e);\n      }\n    } finally {\n      heartBeater.cancelHeartBeat();\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"d6e604e9030fb0cabf0c5a85ae6039921a81419c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"]},"commit2Childs":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d6e604e9030fb0cabf0c5a85ae6039921a81419c":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d6e604e9030fb0cabf0c5a85ae6039921a81419c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}