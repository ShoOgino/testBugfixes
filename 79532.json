{"path":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","commits":[{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int).mjava","sourceNew":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      final String fieldsSegment;\n      final Directory dir;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                        si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n\n        // Verify # deletes does not exceed maxDoc for this segment:\n        if (deletedDocs.count() > maxDoc()) {\n          throw new CorruptIndexException(\"number of deletes (\" + deletedDocs.count() + \") exceeds max doc (\" + maxDoc() + \") for segment \" + si.name);\n        }\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si, int readBufferSize) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".cfs\", readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos, readBufferSize);\n\n      // Verify two sources of \"maxDoc\" agree:\n      if (fieldsReader.size() != si.docCount) {\n        throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n\n        // Verify # deletes does not exceed maxDoc for this segment:\n        if (deletedDocs.count() > maxDoc()) {\n          throw new CorruptIndexException(\"number of deletes (\" + deletedDocs.count() + \") exceeds max doc (\" + maxDoc() + \") for segment \" + si.name);\n        }\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (fieldInfos.hasVectors()) { // open term vector files only as needed\n        termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos, readBufferSize);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3b9d7142a399ac70a71ce5b40ee66695eda5b7e8","date":1195335263,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","sourceNew":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      final String fieldsSegment;\n      final Directory dir;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                        si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      final String fieldsSegment;\n      final Directory dir;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                        si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      // NOTE: the bitvector is stored using the regular directory, not cfs\n      if (hasDeletions(si)) {\n        deletedDocs = new BitVector(directory(), si.getDelFileName());\n\n        // Verify # deletes does not exceed maxDoc for this segment:\n        if (deletedDocs.count() > maxDoc()) {\n          throw new CorruptIndexException(\"number of deletes (\" + deletedDocs.count() + \") exceeds max doc (\" + maxDoc() + \") for segment \" + si.name);\n        }\n      }\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d7a119dd14350e2cfe6ba6b5e71d48ee6c356482","date":1196863177,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","sourceNew":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      final String fieldsSegment;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                        si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      final String fieldsSegment;\n      final Directory dir;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                        si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"385e2d9c17fe505794d82f2776a4151e9a47c7ca","date":1214999847,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","sourceNew":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      final String fieldsSegment;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                        si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      // No compound file exists - use the multi-file format\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      final String fieldsSegment;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                        si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"902ba79f4590a41c663c447756d2e5041cbbdda9","date":1217956662,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","sourceNew":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      boolean anyProx = false;\n      final int numFields = fieldInfos.size();\n      for(int i=0;!anyProx && i<numFields;i++)\n        if (!fieldInfos.fieldInfo(i).omitTf)\n          anyProx = true;\n\n      final String fieldsSegment;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                        si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      if (anyProx)\n        proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      final String fieldsSegment;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                        si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"de9999a287e59e079489cd445c376fa0dab501ba","date":1229510190,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","sourceNew":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      boolean anyProx = false;\n      final int numFields = fieldInfos.size();\n      for(int i=0;!anyProx && i<numFields;i++)\n        if (!fieldInfos.fieldInfo(i).omitTf)\n          anyProx = true;\n\n      final String fieldsSegment;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReaderOrig = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      if (anyProx)\n        proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      boolean anyProx = false;\n      final int numFields = fieldInfos.size();\n      for(int i=0;!anyProx && i<numFields;i++)\n        if (!fieldInfos.fieldInfo(i).omitTf)\n          anyProx = true;\n\n      final String fieldsSegment;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                        si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReader.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      if (anyProx)\n        proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"066b6ff5a08e35c3b6880e7c3ddda79526acdab1","date":1237569961,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","sourceNew":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      boolean anyProx = false;\n      final int numFields = fieldInfos.size();\n      for(int i=0;!anyProx && i<numFields;i++)\n        if (!fieldInfos.fieldInfo(i).omitTermFreqAndPositions)\n          anyProx = true;\n\n      final String fieldsSegment;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReaderOrig = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      if (anyProx)\n        proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      boolean anyProx = false;\n      final int numFields = fieldInfos.size();\n      for(int i=0;!anyProx && i<numFields;i++)\n        if (!fieldInfos.fieldInfo(i).omitTf)\n          anyProx = true;\n\n      final String fieldsSegment;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReaderOrig = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      if (anyProx)\n        proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c4ff8864209d2e972cb4393600c26082f9a6533d","date":1239297466,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","sourceNew":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      if (doOpenStores) {\n        openDocStores();\n      }\n\n      boolean anyProx = false;\n      final int numFields = fieldInfos.size();\n      for(int i=0;!anyProx && i<numFields;i++)\n        if (!fieldInfos.fieldInfo(i).omitTermFreqAndPositions)\n          anyProx = true;\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      if (anyProx)\n        proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      final Directory storeDir;\n\n      if (doOpenStores) {\n        if (si.getDocStoreOffset() != -1) {\n          if (si.getDocStoreIsCompoundFile()) {\n            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + \".\" + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);\n            storeDir = storeCFSReader;\n          } else {\n            storeDir = directory();\n          }\n        } else {\n          storeDir = cfsDir;\n        }\n      } else\n        storeDir = null;\n\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      boolean anyProx = false;\n      final int numFields = fieldInfos.size();\n      for(int i=0;!anyProx && i<numFields;i++)\n        if (!fieldInfos.fieldInfo(i).omitTermFreqAndPositions)\n          anyProx = true;\n\n      final String fieldsSegment;\n\n      if (si.getDocStoreOffset() != -1)\n        fieldsSegment = si.getDocStoreSegment();\n      else\n        fieldsSegment = segment;\n\n      if (doOpenStores) {\n        fieldsReaderOrig = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,\n                                            si.getDocStoreOffset(), si.docCount);\n\n        // Verify two sources of \"maxDoc\" agree:\n        if (si.getDocStoreOffset() == -1 && fieldsReaderOrig.size() != si.docCount) {\n          throw new CorruptIndexException(\"doc counts differ for segment \" + si.name + \": fieldsReader shows \" + fieldsReaderOrig.size() + \" but segmentInfo shows \" + si.docCount);\n        }\n      }\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      if (anyProx)\n        proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed\n        final String vectorsSegment;\n        if (si.getDocStoreOffset() != -1)\n          vectorsSegment = si.getDocStoreSegment();\n        else\n          vectorsSegment = segment;\n        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);\n      }\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4b0a12e9aa5d9fd24932c99a893fb6a04c74c1","date":1244392278,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/SegmentReader#get(boolean,Directory,SegmentInfo,int,boolean).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentReader#initialize(SegmentInfo,int,boolean).mjava","sourceNew":"  /**\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public static SegmentReader get(boolean readOnly,\n                                  Directory dir,\n                                  SegmentInfo si,\n                                  int readBufferSize,\n                                  boolean doOpenStores)\n    throws CorruptIndexException, IOException {\n    SegmentReader instance;\n    try {\n      if (readOnly)\n        instance = (SegmentReader)READONLY_IMPL.newInstance();\n      else\n        instance = (SegmentReader)IMPL.newInstance();\n    } catch (Exception e) {\n      throw new RuntimeException(\"cannot load SegmentReader class: \" + e, e);\n    }\n    instance.directory = dir;\n    instance.readOnly = readOnly;\n    instance.segment = si.name;\n    instance.si = si;\n    instance.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = instance.directory();\n      if (si.getUseCompoundFile()) {\n        instance.cfsReader = new CompoundFileReader(instance.directory(), instance.segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = instance.cfsReader;\n      }\n\n      instance.fieldInfos = new FieldInfos(cfsDir, instance.segment + \".fnm\");\n\n      if (doOpenStores) {\n        instance.openDocStores();\n      }\n\n      boolean anyProx = false;\n      final int numFields = instance.fieldInfos.size();\n      for(int i=0;!anyProx && i<numFields;i++)\n        if (!instance.fieldInfos.fieldInfo(i).omitTermFreqAndPositions)\n          anyProx = true;\n\n      instance.tis = new TermInfosReader(cfsDir, instance.segment, instance.fieldInfos, readBufferSize);\n\n      instance.loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      instance.freqStream = cfsDir.openInput(instance.segment + \".frq\", readBufferSize);\n      if (anyProx)\n        instance.proxStream = cfsDir.openInput(instance.segment + \".prx\", readBufferSize);\n      instance.openNorms(cfsDir, readBufferSize);\n\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        instance.doClose();\n      }\n    }\n    return instance;\n  }\n\n","sourceOld":"  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {\n    segment = si.name;\n    this.si = si;\n    this.readBufferSize = readBufferSize;\n\n    boolean success = false;\n\n    try {\n      // Use compound file directory for some files, if it exists\n      Directory cfsDir = directory();\n      if (si.getUseCompoundFile()) {\n        cfsReader = new CompoundFileReader(directory(), segment + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);\n        cfsDir = cfsReader;\n      }\n\n      fieldInfos = new FieldInfos(cfsDir, segment + \".fnm\");\n\n      if (doOpenStores) {\n        openDocStores();\n      }\n\n      boolean anyProx = false;\n      final int numFields = fieldInfos.size();\n      for(int i=0;!anyProx && i<numFields;i++)\n        if (!fieldInfos.fieldInfo(i).omitTermFreqAndPositions)\n          anyProx = true;\n\n      tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);\n      \n      loadDeletedDocs();\n\n      // make sure that all index files have been read or are kept open\n      // so that if an index update removes them we'll still have them\n      freqStream = cfsDir.openInput(segment + \".frq\", readBufferSize);\n      if (anyProx)\n        proxStream = cfsDir.openInput(segment + \".prx\", readBufferSize);\n      openNorms(cfsDir, readBufferSize);\n\n      success = true;\n    } finally {\n\n      // With lock-less commits, it's entirely possible (and\n      // fine) to hit a FileNotFound exception above.  In\n      // this case, we want to explicitly close any subset\n      // of things that were opened so that we don't have to\n      // wait for a GC to do so.\n      if (!success) {\n        doClose();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["c5e023aa3e1228b8ccacdc30d852eb88e996d1b2"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"0f4b0a12e9aa5d9fd24932c99a893fb6a04c74c1":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"385e2d9c17fe505794d82f2776a4151e9a47c7ca":["d7a119dd14350e2cfe6ba6b5e71d48ee6c356482"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c4ff8864209d2e972cb4393600c26082f9a6533d":["066b6ff5a08e35c3b6880e7c3ddda79526acdab1"],"066b6ff5a08e35c3b6880e7c3ddda79526acdab1":["de9999a287e59e079489cd445c376fa0dab501ba"],"902ba79f4590a41c663c447756d2e5041cbbdda9":["385e2d9c17fe505794d82f2776a4151e9a47c7ca"],"de9999a287e59e079489cd445c376fa0dab501ba":["902ba79f4590a41c663c447756d2e5041cbbdda9"],"d7a119dd14350e2cfe6ba6b5e71d48ee6c356482":["3b9d7142a399ac70a71ce5b40ee66695eda5b7e8"],"3b9d7142a399ac70a71ce5b40ee66695eda5b7e8":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f4b0a12e9aa5d9fd24932c99a893fb6a04c74c1"]},"commit2Childs":{"0f4b0a12e9aa5d9fd24932c99a893fb6a04c74c1":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"385e2d9c17fe505794d82f2776a4151e9a47c7ca":["902ba79f4590a41c663c447756d2e5041cbbdda9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"c4ff8864209d2e972cb4393600c26082f9a6533d":["0f4b0a12e9aa5d9fd24932c99a893fb6a04c74c1"],"066b6ff5a08e35c3b6880e7c3ddda79526acdab1":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"902ba79f4590a41c663c447756d2e5041cbbdda9":["de9999a287e59e079489cd445c376fa0dab501ba"],"de9999a287e59e079489cd445c376fa0dab501ba":["066b6ff5a08e35c3b6880e7c3ddda79526acdab1"],"d7a119dd14350e2cfe6ba6b5e71d48ee6c356482":["385e2d9c17fe505794d82f2776a4151e9a47c7ca"],"3b9d7142a399ac70a71ce5b40ee66695eda5b7e8":["d7a119dd14350e2cfe6ba6b5e71d48ee6c356482"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["3b9d7142a399ac70a71ce5b40ee66695eda5b7e8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}