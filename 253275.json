{"path":"modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter#testReset().mjava","commits":[{"id":"0f080986da691a3bba7b757f43ab72cdc82b57ce","date":1273069619,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter#testReset().mjava","pathOld":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter#testReset().mjava","sourceNew":"  public void testReset() throws Exception {\n    String[] dict = { \"Rind\", \"Fleisch\", \"Draht\", \"Schere\", \"Gesetz\",\n        \"Aufgabe\", \"Überwachung\" };\n\n    Tokenizer wsTokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\n        \"Rindfleischüberwachungsgesetz\"));\n    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT, \n        wsTokenizer, dict,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);\n    \n    TermAttribute termAtt = tf.getAttribute(TermAttribute.class);\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.term());\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rind\", termAtt.term());\n    wsTokenizer.reset(new StringReader(\"Rindfleischüberwachungsgesetz\"));\n    tf.reset();\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.term());\n  }\n\n","sourceOld":"  public void testReset() throws Exception {\n    String[] dict = { \"Rind\", \"Fleisch\", \"Draht\", \"Schere\", \"Gesetz\",\n        \"Aufgabe\", \"Überwachung\" };\n\n    Tokenizer wsTokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\n        \"Rindfleischüberwachungsgesetz\"));\n    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT, \n        wsTokenizer, dict,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);\n    \n    TermAttribute termAtt = tf.getAttribute(TermAttribute.class);\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.term());\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rind\", termAtt.term());\n    wsTokenizer.reset(new StringReader(\"Rindfleischüberwachungsgesetz\"));\n    tf.reset();\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.term());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a7347509fad0711ac30cb15a746e9a3830a38ebd","date":1275388513,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter#testReset().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter#testReset().mjava","sourceNew":"  public void testReset() throws Exception {\n    String[] dict = { \"Rind\", \"Fleisch\", \"Draht\", \"Schere\", \"Gesetz\",\n        \"Aufgabe\", \"Überwachung\" };\n\n    Tokenizer wsTokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\n        \"Rindfleischüberwachungsgesetz\"));\n    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT, \n        wsTokenizer, dict,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);\n    \n    CharTermAttribute termAtt = tf.getAttribute(CharTermAttribute.class);\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.toString());\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rind\", termAtt.toString());\n    wsTokenizer.reset(new StringReader(\"Rindfleischüberwachungsgesetz\"));\n    tf.reset();\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.toString());\n  }\n\n","sourceOld":"  public void testReset() throws Exception {\n    String[] dict = { \"Rind\", \"Fleisch\", \"Draht\", \"Schere\", \"Gesetz\",\n        \"Aufgabe\", \"Überwachung\" };\n\n    Tokenizer wsTokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\n        \"Rindfleischüberwachungsgesetz\"));\n    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT, \n        wsTokenizer, dict,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);\n    \n    TermAttribute termAtt = tf.getAttribute(TermAttribute.class);\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.term());\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rind\", termAtt.term());\n    wsTokenizer.reset(new StringReader(\"Rindfleischüberwachungsgesetz\"));\n    tf.reset();\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.term());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"906468dad7061882fe0b86de96423c16d5e9bc1e","date":1319542276,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter#testReset().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter#testReset().mjava","sourceNew":"  public void testReset() throws Exception {\n    CharArraySet dict = makeDictionary(\"Rind\", \"Fleisch\", \"Draht\", \"Schere\", \"Gesetz\",\n        \"Aufgabe\", \"Überwachung\");\n\n    Tokenizer wsTokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\n        \"Rindfleischüberwachungsgesetz\"));\n    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT, \n        wsTokenizer, dict,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);\n    \n    CharTermAttribute termAtt = tf.getAttribute(CharTermAttribute.class);\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.toString());\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rind\", termAtt.toString());\n    wsTokenizer.reset(new StringReader(\"Rindfleischüberwachungsgesetz\"));\n    tf.reset();\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.toString());\n  }\n\n","sourceOld":"  public void testReset() throws Exception {\n    String[] dict = { \"Rind\", \"Fleisch\", \"Draht\", \"Schere\", \"Gesetz\",\n        \"Aufgabe\", \"Überwachung\" };\n\n    Tokenizer wsTokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\n        \"Rindfleischüberwachungsgesetz\"));\n    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT, \n        wsTokenizer, dict,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);\n    \n    CharTermAttribute termAtt = tf.getAttribute(CharTermAttribute.class);\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.toString());\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rind\", termAtt.toString());\n    wsTokenizer.reset(new StringReader(\"Rindfleischüberwachungsgesetz\"));\n    tf.reset();\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter#testReset().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter#testReset().mjava","sourceNew":"  public void testReset() throws Exception {\n    CharArraySet dict = makeDictionary(\"Rind\", \"Fleisch\", \"Draht\", \"Schere\", \"Gesetz\",\n        \"Aufgabe\", \"Überwachung\");\n\n    Tokenizer wsTokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\n        \"Rindfleischüberwachungsgesetz\"));\n    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT, \n        wsTokenizer, dict,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);\n    \n    CharTermAttribute termAtt = tf.getAttribute(CharTermAttribute.class);\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.toString());\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rind\", termAtt.toString());\n    wsTokenizer.reset(new StringReader(\"Rindfleischüberwachungsgesetz\"));\n    tf.reset();\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.toString());\n  }\n\n","sourceOld":"  public void testReset() throws Exception {\n    CharArraySet dict = makeDictionary(\"Rind\", \"Fleisch\", \"Draht\", \"Schere\", \"Gesetz\",\n        \"Aufgabe\", \"Überwachung\");\n\n    Tokenizer wsTokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(\n        \"Rindfleischüberwachungsgesetz\"));\n    DictionaryCompoundWordTokenFilter tf = new DictionaryCompoundWordTokenFilter(TEST_VERSION_CURRENT, \n        wsTokenizer, dict,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_WORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MIN_SUBWORD_SIZE,\n        CompoundWordTokenFilterBase.DEFAULT_MAX_SUBWORD_SIZE, false);\n    \n    CharTermAttribute termAtt = tf.getAttribute(CharTermAttribute.class);\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.toString());\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rind\", termAtt.toString());\n    wsTokenizer.reset(new StringReader(\"Rindfleischüberwachungsgesetz\"));\n    tf.reset();\n    assertTrue(tf.incrementToken());\n    assertEquals(\"Rindfleischüberwachungsgesetz\", termAtt.toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["906468dad7061882fe0b86de96423c16d5e9bc1e"],"906468dad7061882fe0b86de96423c16d5e9bc1e":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"906468dad7061882fe0b86de96423c16d5e9bc1e":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["906468dad7061882fe0b86de96423c16d5e9bc1e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}