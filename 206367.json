{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase#doTestNormsVersusDocValues(double,LongProducer).mjava","commits":[{"id":"7f3090f7e0cab5b1f5acf12d21f31f00fe74a262","date":1475755647,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase#doTestNormsVersusDocValues(double,LongProducer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase#doTestNormsVersusDocValues(LongProducer).mjava","sourceNew":"  private void doTestNormsVersusDocValues(double density, LongProducer longs) throws Exception {\n    int numDocs = atLeast(500);\n    final FixedBitSet docsWithField = new FixedBitSet(numDocs);\n    final int numDocsWithField = Math.max(1, (int) (density * numDocs));\n    if (numDocsWithField == numDocs) {\n      docsWithField.set(0, numDocs);\n    } else {\n      int i = 0;\n      while (i < numDocsWithField) {\n        int doc = random().nextInt(numDocs);\n        if (docsWithField.get(doc) == false) {\n          docsWithField.set(doc);\n          ++i;\n        }\n      }\n    }\n    long norms[] = new long[numDocsWithField];\n    for (int i = 0; i < numDocsWithField; i++) {\n      norms[i] = longs.next();\n    }\n    \n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);conf.setMergePolicy(NoMergePolicy.INSTANCE);\n    conf.setSimilarity(new CannedNormSimilarity(norms));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field idField = new StringField(\"id\", \"\", Field.Store.NO);\n    Field indexedField = new TextField(\"indexed\", \"\", Field.Store.NO);\n    Field dvField = new NumericDocValuesField(\"dv\", 0);\n    doc.add(idField);\n    doc.add(indexedField);\n    doc.add(dvField);\n    \n    for (int i = 0, j = 0; i < numDocs; i++) {\n      idField.setStringValue(Integer.toString(i));\n      if (docsWithField.get(i) == false) {\n        Document doc2 = new Document();\n        doc2.add(idField);\n        writer.addDocument(doc2);\n      } else {\n        long value = norms[j++];\n        dvField.setLongValue(value);\n        indexedField.setStringValue(Long.toString(value));\n        writer.addDocument(doc);\n      }\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/20);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n    \n    writer.commit();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      NumericDocValues expected = r.getNumericDocValues(\"dv\");\n      NumericDocValues actual = r.getNormValues(\"indexed\");\n      assertEquals(expected == null, actual == null);\n      if (expected != null) {\n        for (int d = expected.nextDoc(); d != DocIdSetIterator.NO_MORE_DOCS; d = expected.nextDoc()) {\n          assertEquals(d, actual.nextDoc());\n          assertEquals(\"doc \" + d, expected.longValue(), actual.longValue());\n        }\n        assertEquals(NO_MORE_DOCS, actual.nextDoc());\n      }\n    }\n    ir.close();\n    \n    writer.forceMerge(1);\n    \n    // compare again\n    ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      NumericDocValues expected = r.getNumericDocValues(\"dv\");\n      NumericDocValues actual = r.getNormValues(\"indexed\");\n      assertEquals(expected == null, actual == null);\n      if (expected != null) {\n        for (int d = expected.nextDoc(); d != DocIdSetIterator.NO_MORE_DOCS; d = expected.nextDoc()) {\n          assertEquals(d, actual.nextDoc());\n          assertEquals(\"doc \" + d, expected.longValue(), actual.longValue());\n        }\n        assertEquals(NO_MORE_DOCS, actual.nextDoc());\n      }\n    }\n    \n    writer.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestNormsVersusDocValues(LongProducer longs) throws Exception {\n    int numDocs = atLeast(500);\n    long norms[] = new long[numDocs];\n    for (int i = 0; i < numDocs; i++) {\n      norms[i] = longs.next();\n    }\n    \n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);\n    conf.setSimilarity(new CannedNormSimilarity(norms));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field idField = new StringField(\"id\", \"\", Field.Store.NO);\n    Field indexedField = new TextField(\"indexed\", \"\", Field.Store.NO);\n    Field dvField = new NumericDocValuesField(\"dv\", 0);\n    doc.add(idField);\n    doc.add(indexedField);\n    doc.add(dvField);\n    \n    for (int i = 0; i < numDocs; i++) {\n      idField.setStringValue(Integer.toString(i));\n      long value = norms[i];\n      dvField.setLongValue(value);\n      indexedField.setStringValue(Long.toString(value));\n      writer.addDocument(doc);\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/20);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n    \n    writer.commit();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      NumericDocValues expected = r.getNumericDocValues(\"dv\");\n      NumericDocValues actual = r.getNormValues(\"indexed\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        assertEquals(i, expected.nextDoc());\n        assertEquals(i, actual.nextDoc());\n        assertEquals(\"doc \" + i, expected.longValue(), actual.longValue());\n      }\n      assertEquals(NO_MORE_DOCS, expected.nextDoc());\n    }\n    ir.close();\n    \n    writer.forceMerge(1);\n    \n    // compare again\n    ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      NumericDocValues expected = r.getNumericDocValues(\"dv\");\n      NumericDocValues actual = r.getNormValues(\"indexed\");\n      for (int i = 0; i < r.maxDoc(); i++) {\n        assertEquals(i, expected.nextDoc());\n        assertEquals(i, actual.nextDoc());\n        assertEquals(\"doc \" + i, expected.longValue(), actual.longValue());\n      }\n    }\n    \n    writer.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7ca1fe3f1f5edea2339f7e7a31f0754878a72b0e","date":1476167489,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase#doTestNormsVersusDocValues(double,LongProducer).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase#doTestNormsVersusDocValues(double,LongProducer).mjava","sourceNew":"  private void doTestNormsVersusDocValues(double density, LongProducer longs) throws Exception {\n    int numDocs = atLeast(500);\n    final FixedBitSet docsWithField = new FixedBitSet(numDocs);\n    final int numDocsWithField = Math.max(1, (int) (density * numDocs));\n    if (numDocsWithField == numDocs) {\n      docsWithField.set(0, numDocs);\n    } else {\n      int i = 0;\n      while (i < numDocsWithField) {\n        int doc = random().nextInt(numDocs);\n        if (docsWithField.get(doc) == false) {\n          docsWithField.set(doc);\n          ++i;\n        }\n      }\n    }\n    long norms[] = new long[numDocsWithField];\n    for (int i = 0; i < numDocsWithField; i++) {\n      norms[i] = longs.next();\n    }\n    \n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);conf.setMergePolicy(NoMergePolicy.INSTANCE);\n    conf.setSimilarity(new CannedNormSimilarity(norms));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field idField = new StringField(\"id\", \"\", Field.Store.NO);\n    Field indexedField = new TextField(\"indexed\", \"\", Field.Store.NO);\n    Field dvField = new NumericDocValuesField(\"dv\", 0);\n    doc.add(idField);\n    doc.add(indexedField);\n    doc.add(dvField);\n    \n    for (int i = 0, j = 0; i < numDocs; i++) {\n      idField.setStringValue(Integer.toString(i));\n      if (docsWithField.get(i) == false) {\n        Document doc2 = new Document();\n        doc2.add(idField);\n        writer.addDocument(doc2);\n      } else {\n        long value = norms[j++];\n        dvField.setLongValue(value);\n        indexedField.setStringValue(Long.toString(value));\n        writer.addDocument(doc);\n      }\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/20);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n    \n    writer.commit();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    ir.close();\n    \n    writer.forceMerge(1);\n    \n    // compare again\n    ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    \n    writer.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestNormsVersusDocValues(double density, LongProducer longs) throws Exception {\n    int numDocs = atLeast(500);\n    final FixedBitSet docsWithField = new FixedBitSet(numDocs);\n    final int numDocsWithField = Math.max(1, (int) (density * numDocs));\n    if (numDocsWithField == numDocs) {\n      docsWithField.set(0, numDocs);\n    } else {\n      int i = 0;\n      while (i < numDocsWithField) {\n        int doc = random().nextInt(numDocs);\n        if (docsWithField.get(doc) == false) {\n          docsWithField.set(doc);\n          ++i;\n        }\n      }\n    }\n    long norms[] = new long[numDocsWithField];\n    for (int i = 0; i < numDocsWithField; i++) {\n      norms[i] = longs.next();\n    }\n    \n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);conf.setMergePolicy(NoMergePolicy.INSTANCE);\n    conf.setSimilarity(new CannedNormSimilarity(norms));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field idField = new StringField(\"id\", \"\", Field.Store.NO);\n    Field indexedField = new TextField(\"indexed\", \"\", Field.Store.NO);\n    Field dvField = new NumericDocValuesField(\"dv\", 0);\n    doc.add(idField);\n    doc.add(indexedField);\n    doc.add(dvField);\n    \n    for (int i = 0, j = 0; i < numDocs; i++) {\n      idField.setStringValue(Integer.toString(i));\n      if (docsWithField.get(i) == false) {\n        Document doc2 = new Document();\n        doc2.add(idField);\n        writer.addDocument(doc2);\n      } else {\n        long value = norms[j++];\n        dvField.setLongValue(value);\n        indexedField.setStringValue(Long.toString(value));\n        writer.addDocument(doc);\n      }\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/20);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n    \n    writer.commit();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      NumericDocValues expected = r.getNumericDocValues(\"dv\");\n      NumericDocValues actual = r.getNormValues(\"indexed\");\n      assertEquals(expected == null, actual == null);\n      if (expected != null) {\n        for (int d = expected.nextDoc(); d != DocIdSetIterator.NO_MORE_DOCS; d = expected.nextDoc()) {\n          assertEquals(d, actual.nextDoc());\n          assertEquals(\"doc \" + d, expected.longValue(), actual.longValue());\n        }\n        assertEquals(NO_MORE_DOCS, actual.nextDoc());\n      }\n    }\n    ir.close();\n    \n    writer.forceMerge(1);\n    \n    // compare again\n    ir = DirectoryReader.open(dir);\n    for (LeafReaderContext context : ir.leaves()) {\n      LeafReader r = context.reader();\n      NumericDocValues expected = r.getNumericDocValues(\"dv\");\n      NumericDocValues actual = r.getNormValues(\"indexed\");\n      assertEquals(expected == null, actual == null);\n      if (expected != null) {\n        for (int d = expected.nextDoc(); d != DocIdSetIterator.NO_MORE_DOCS; d = expected.nextDoc()) {\n          assertEquals(d, actual.nextDoc());\n          assertEquals(\"doc \" + d, expected.longValue(), actual.longValue());\n        }\n        assertEquals(NO_MORE_DOCS, actual.nextDoc());\n      }\n    }\n    \n    writer.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase#doTestNormsVersusDocValues(double,LongProducer).mjava","pathOld":"/dev/null","sourceNew":"  private void doTestNormsVersusDocValues(double density, LongProducer longs) throws Exception {\n    int numDocs = atLeast(500);\n    final FixedBitSet docsWithField = new FixedBitSet(numDocs);\n    final int numDocsWithField = Math.max(1, (int) (density * numDocs));\n    if (numDocsWithField == numDocs) {\n      docsWithField.set(0, numDocs);\n    } else {\n      int i = 0;\n      while (i < numDocsWithField) {\n        int doc = random().nextInt(numDocs);\n        if (docsWithField.get(doc) == false) {\n          docsWithField.set(doc);\n          ++i;\n        }\n      }\n    }\n    long norms[] = new long[numDocsWithField];\n    for (int i = 0; i < numDocsWithField; i++) {\n      norms[i] = longs.next();\n    }\n    \n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);conf.setMergePolicy(NoMergePolicy.INSTANCE);\n    conf.setSimilarity(new CannedNormSimilarity(norms));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field idField = new StringField(\"id\", \"\", Field.Store.NO);\n    Field indexedField = new TextField(\"indexed\", \"\", Field.Store.NO);\n    Field dvField = new NumericDocValuesField(\"dv\", 0);\n    doc.add(idField);\n    doc.add(indexedField);\n    doc.add(dvField);\n    \n    for (int i = 0, j = 0; i < numDocs; i++) {\n      idField.setStringValue(Integer.toString(i));\n      if (docsWithField.get(i) == false) {\n        Document doc2 = new Document();\n        doc2.add(idField);\n        writer.addDocument(doc2);\n      } else {\n        long value = norms[j++];\n        dvField.setLongValue(value);\n        indexedField.setStringValue(Long.toString(value));\n        writer.addDocument(doc);\n      }\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/20);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n    \n    writer.commit();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    ir.close();\n    \n    writer.forceMerge(1);\n    \n    // compare again\n    ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    \n    writer.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"11134e449dabe11d6d0ff6a564d84b82cbe93722","date":1477299083,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase#doTestNormsVersusDocValues(double,LongSupplier).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase#doTestNormsVersusDocValues(double,LongProducer).mjava","sourceNew":"  private void doTestNormsVersusDocValues(double density, LongSupplier longs) throws Exception {\n    int numDocs = atLeast(500);\n    final FixedBitSet docsWithField = new FixedBitSet(numDocs);\n    final int numDocsWithField = Math.max(1, (int) (density * numDocs));\n    if (numDocsWithField == numDocs) {\n      docsWithField.set(0, numDocs);\n    } else {\n      int i = 0;\n      while (i < numDocsWithField) {\n        int doc = random().nextInt(numDocs);\n        if (docsWithField.get(doc) == false) {\n          docsWithField.set(doc);\n          ++i;\n        }\n      }\n    }\n    long norms[] = new long[numDocsWithField];\n    for (int i = 0; i < numDocsWithField; i++) {\n      norms[i] = longs.getAsLong();\n    }\n    \n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);conf.setMergePolicy(NoMergePolicy.INSTANCE);\n    conf.setSimilarity(new CannedNormSimilarity(norms));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field idField = new StringField(\"id\", \"\", Field.Store.NO);\n    Field indexedField = new TextField(\"indexed\", \"\", Field.Store.NO);\n    Field dvField = new NumericDocValuesField(\"dv\", 0);\n    doc.add(idField);\n    doc.add(indexedField);\n    doc.add(dvField);\n    \n    for (int i = 0, j = 0; i < numDocs; i++) {\n      idField.setStringValue(Integer.toString(i));\n      if (docsWithField.get(i) == false) {\n        Document doc2 = new Document();\n        doc2.add(idField);\n        writer.addDocument(doc2);\n      } else {\n        long value = norms[j++];\n        dvField.setLongValue(value);\n        indexedField.setStringValue(Long.toString(value));\n        writer.addDocument(doc);\n      }\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/20);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n    \n    writer.commit();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    ir.close();\n    \n    writer.forceMerge(1);\n    \n    // compare again\n    ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    \n    writer.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestNormsVersusDocValues(double density, LongProducer longs) throws Exception {\n    int numDocs = atLeast(500);\n    final FixedBitSet docsWithField = new FixedBitSet(numDocs);\n    final int numDocsWithField = Math.max(1, (int) (density * numDocs));\n    if (numDocsWithField == numDocs) {\n      docsWithField.set(0, numDocs);\n    } else {\n      int i = 0;\n      while (i < numDocsWithField) {\n        int doc = random().nextInt(numDocs);\n        if (docsWithField.get(doc) == false) {\n          docsWithField.set(doc);\n          ++i;\n        }\n      }\n    }\n    long norms[] = new long[numDocsWithField];\n    for (int i = 0; i < numDocsWithField; i++) {\n      norms[i] = longs.next();\n    }\n    \n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);conf.setMergePolicy(NoMergePolicy.INSTANCE);\n    conf.setSimilarity(new CannedNormSimilarity(norms));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field idField = new StringField(\"id\", \"\", Field.Store.NO);\n    Field indexedField = new TextField(\"indexed\", \"\", Field.Store.NO);\n    Field dvField = new NumericDocValuesField(\"dv\", 0);\n    doc.add(idField);\n    doc.add(indexedField);\n    doc.add(dvField);\n    \n    for (int i = 0, j = 0; i < numDocs; i++) {\n      idField.setStringValue(Integer.toString(i));\n      if (docsWithField.get(i) == false) {\n        Document doc2 = new Document();\n        doc2.add(idField);\n        writer.addDocument(doc2);\n      } else {\n        long value = norms[j++];\n        dvField.setLongValue(value);\n        indexedField.setStringValue(Long.toString(value));\n        writer.addDocument(doc);\n      }\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/20);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n    \n    writer.commit();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    ir.close();\n    \n    writer.forceMerge(1);\n    \n    // compare again\n    ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    \n    writer.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2714c85633b642b29871cf5ff8d17d3ba7bfd76","date":1477307753,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase#doTestNormsVersusDocValues(double,LongSupplier).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase#doTestNormsVersusDocValues(double,LongProducer).mjava","sourceNew":"  private void doTestNormsVersusDocValues(double density, LongSupplier longs) throws Exception {\n    int numDocs = atLeast(500);\n    final FixedBitSet docsWithField = new FixedBitSet(numDocs);\n    final int numDocsWithField = Math.max(1, (int) (density * numDocs));\n    if (numDocsWithField == numDocs) {\n      docsWithField.set(0, numDocs);\n    } else {\n      int i = 0;\n      while (i < numDocsWithField) {\n        int doc = random().nextInt(numDocs);\n        if (docsWithField.get(doc) == false) {\n          docsWithField.set(doc);\n          ++i;\n        }\n      }\n    }\n    long norms[] = new long[numDocsWithField];\n    for (int i = 0; i < numDocsWithField; i++) {\n      norms[i] = longs.getAsLong();\n    }\n    \n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);conf.setMergePolicy(NoMergePolicy.INSTANCE);\n    conf.setSimilarity(new CannedNormSimilarity(norms));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field idField = new StringField(\"id\", \"\", Field.Store.NO);\n    Field indexedField = new TextField(\"indexed\", \"\", Field.Store.NO);\n    Field dvField = new NumericDocValuesField(\"dv\", 0);\n    doc.add(idField);\n    doc.add(indexedField);\n    doc.add(dvField);\n    \n    for (int i = 0, j = 0; i < numDocs; i++) {\n      idField.setStringValue(Integer.toString(i));\n      if (docsWithField.get(i) == false) {\n        Document doc2 = new Document();\n        doc2.add(idField);\n        writer.addDocument(doc2);\n      } else {\n        long value = norms[j++];\n        dvField.setLongValue(value);\n        indexedField.setStringValue(Long.toString(value));\n        writer.addDocument(doc);\n      }\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/20);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n    \n    writer.commit();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    ir.close();\n    \n    writer.forceMerge(1);\n    \n    // compare again\n    ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    \n    writer.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestNormsVersusDocValues(double density, LongProducer longs) throws Exception {\n    int numDocs = atLeast(500);\n    final FixedBitSet docsWithField = new FixedBitSet(numDocs);\n    final int numDocsWithField = Math.max(1, (int) (density * numDocs));\n    if (numDocsWithField == numDocs) {\n      docsWithField.set(0, numDocs);\n    } else {\n      int i = 0;\n      while (i < numDocsWithField) {\n        int doc = random().nextInt(numDocs);\n        if (docsWithField.get(doc) == false) {\n          docsWithField.set(doc);\n          ++i;\n        }\n      }\n    }\n    long norms[] = new long[numDocsWithField];\n    for (int i = 0; i < numDocsWithField; i++) {\n      norms[i] = longs.next();\n    }\n    \n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);conf.setMergePolicy(NoMergePolicy.INSTANCE);\n    conf.setSimilarity(new CannedNormSimilarity(norms));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field idField = new StringField(\"id\", \"\", Field.Store.NO);\n    Field indexedField = new TextField(\"indexed\", \"\", Field.Store.NO);\n    Field dvField = new NumericDocValuesField(\"dv\", 0);\n    doc.add(idField);\n    doc.add(indexedField);\n    doc.add(dvField);\n    \n    for (int i = 0, j = 0; i < numDocs; i++) {\n      idField.setStringValue(Integer.toString(i));\n      if (docsWithField.get(i) == false) {\n        Document doc2 = new Document();\n        doc2.add(idField);\n        writer.addDocument(doc2);\n      } else {\n        long value = norms[j++];\n        dvField.setLongValue(value);\n        indexedField.setStringValue(Long.toString(value));\n        writer.addDocument(doc);\n      }\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/20);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n    \n    writer.commit();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    ir.close();\n    \n    writer.forceMerge(1);\n    \n    // compare again\n    ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    \n    writer.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"80d0e6d59ae23f4a6f30eaf40bfb40742300287f","date":1477598926,"type":5,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase#doTestNormsVersusDocValues(double,LongSupplier).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseNormsFormatTestCase#doTestNormsVersusDocValues(double,LongProducer).mjava","sourceNew":"  private void doTestNormsVersusDocValues(double density, LongSupplier longs) throws Exception {\n    int numDocs = atLeast(500);\n    final FixedBitSet docsWithField = new FixedBitSet(numDocs);\n    final int numDocsWithField = Math.max(1, (int) (density * numDocs));\n    if (numDocsWithField == numDocs) {\n      docsWithField.set(0, numDocs);\n    } else {\n      int i = 0;\n      while (i < numDocsWithField) {\n        int doc = random().nextInt(numDocs);\n        if (docsWithField.get(doc) == false) {\n          docsWithField.set(doc);\n          ++i;\n        }\n      }\n    }\n    long norms[] = new long[numDocsWithField];\n    for (int i = 0; i < numDocsWithField; i++) {\n      norms[i] = longs.getAsLong();\n    }\n    \n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);conf.setMergePolicy(NoMergePolicy.INSTANCE);\n    conf.setSimilarity(new CannedNormSimilarity(norms));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field idField = new StringField(\"id\", \"\", Field.Store.NO);\n    Field indexedField = new TextField(\"indexed\", \"\", Field.Store.NO);\n    Field dvField = new NumericDocValuesField(\"dv\", 0);\n    doc.add(idField);\n    doc.add(indexedField);\n    doc.add(dvField);\n    \n    for (int i = 0, j = 0; i < numDocs; i++) {\n      idField.setStringValue(Integer.toString(i));\n      if (docsWithField.get(i) == false) {\n        Document doc2 = new Document();\n        doc2.add(idField);\n        writer.addDocument(doc2);\n      } else {\n        long value = norms[j++];\n        dvField.setLongValue(value);\n        indexedField.setStringValue(Long.toString(value));\n        writer.addDocument(doc);\n      }\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/20);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n    \n    writer.commit();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    ir.close();\n    \n    writer.forceMerge(1);\n    \n    // compare again\n    ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    \n    writer.close();\n    ir.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTestNormsVersusDocValues(double density, LongProducer longs) throws Exception {\n    int numDocs = atLeast(500);\n    final FixedBitSet docsWithField = new FixedBitSet(numDocs);\n    final int numDocsWithField = Math.max(1, (int) (density * numDocs));\n    if (numDocsWithField == numDocs) {\n      docsWithField.set(0, numDocs);\n    } else {\n      int i = 0;\n      while (i < numDocsWithField) {\n        int doc = random().nextInt(numDocs);\n        if (docsWithField.get(doc) == false) {\n          docsWithField.set(doc);\n          ++i;\n        }\n      }\n    }\n    long norms[] = new long[numDocsWithField];\n    for (int i = 0; i < numDocsWithField; i++) {\n      norms[i] = longs.next();\n    }\n    \n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);conf.setMergePolicy(NoMergePolicy.INSTANCE);\n    conf.setSimilarity(new CannedNormSimilarity(norms));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field idField = new StringField(\"id\", \"\", Field.Store.NO);\n    Field indexedField = new TextField(\"indexed\", \"\", Field.Store.NO);\n    Field dvField = new NumericDocValuesField(\"dv\", 0);\n    doc.add(idField);\n    doc.add(indexedField);\n    doc.add(dvField);\n    \n    for (int i = 0, j = 0; i < numDocs; i++) {\n      idField.setStringValue(Integer.toString(i));\n      if (docsWithField.get(i) == false) {\n        Document doc2 = new Document();\n        doc2.add(idField);\n        writer.addDocument(doc2);\n      } else {\n        long value = norms[j++];\n        dvField.setLongValue(value);\n        indexedField.setStringValue(Long.toString(value));\n        writer.addDocument(doc);\n      }\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    \n    // delete some docs\n    int numDeletions = random().nextInt(numDocs/20);\n    for (int i = 0; i < numDeletions; i++) {\n      int id = random().nextInt(numDocs);\n      writer.deleteDocuments(new Term(\"id\", Integer.toString(id)));\n    }\n    \n    writer.commit();\n    \n    // compare\n    DirectoryReader ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    ir.close();\n    \n    writer.forceMerge(1);\n    \n    // compare again\n    ir = DirectoryReader.open(dir);\n    checkNormsVsDocValues(ir);\n    \n    writer.close();\n    ir.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7f3090f7e0cab5b1f5acf12d21f31f00fe74a262":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7ca1fe3f1f5edea2339f7e7a31f0754878a72b0e":["7f3090f7e0cab5b1f5acf12d21f31f00fe74a262"],"11134e449dabe11d6d0ff6a564d84b82cbe93722":["7ca1fe3f1f5edea2339f7e7a31f0754878a72b0e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d2714c85633b642b29871cf5ff8d17d3ba7bfd76"],"d2714c85633b642b29871cf5ff8d17d3ba7bfd76":["7ca1fe3f1f5edea2339f7e7a31f0754878a72b0e","11134e449dabe11d6d0ff6a564d84b82cbe93722"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7ca1fe3f1f5edea2339f7e7a31f0754878a72b0e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d2714c85633b642b29871cf5ff8d17d3ba7bfd76"]},"commit2Childs":{"7f3090f7e0cab5b1f5acf12d21f31f00fe74a262":["7ca1fe3f1f5edea2339f7e7a31f0754878a72b0e"],"7ca1fe3f1f5edea2339f7e7a31f0754878a72b0e":["11134e449dabe11d6d0ff6a564d84b82cbe93722","d2714c85633b642b29871cf5ff8d17d3ba7bfd76","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"11134e449dabe11d6d0ff6a564d84b82cbe93722":["d2714c85633b642b29871cf5ff8d17d3ba7bfd76"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7f3090f7e0cab5b1f5acf12d21f31f00fe74a262","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":[],"d2714c85633b642b29871cf5ff8d17d3ba7bfd76":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}