{"path":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","commits":[{"id":"893269407f5c988a4b2ee13c7ffc01ec43136c1d","date":1268598046,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"/dev/null","sourceNew":"  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","date":1268599006,"type":4,"author":"Mark Robert Miller","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":null,"sourceOld":"  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n  }\n\n","sourceOld":"  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1da8d55113b689b06716246649de6f62430f15c0","date":1453508340,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"/dev/null","sourceNew":"  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b61c4d4553ce38d24ed06a43de07d3c3ca420b62","date":1269037941,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n  }\n\n","sourceOld":"  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9c8b12bda3f5864b27e3e04df1be4f6736ec067a","date":1270088127,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(DEFAULT_VERSION, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f9058535b2d760062e15c60434989564a2b8302b","date":1270214348,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(DEFAULT_VERSION, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(DEFAULT_VERSION,\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(DEFAULT_VERSION, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n  }\n\n","bugFix":null,"bugIntro":["c85fa43e6918808743daa7847ba0264373af687f","ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8598a11db0eb9efa116ba7656c437f5bed7de0f7","date":1272964265,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(DEFAULT_VERSION, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(DEFAULT_VERSION,\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":["fcbc12aa8147f5203ca283e7252ba4280d6ffd16"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"9c8b12bda3f5864b27e3e04df1be4f6736ec067a":["b61c4d4553ce38d24ed06a43de07d3c3ca420b62"],"1da8d55113b689b06716246649de6f62430f15c0":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","ad94625fb8d088209f46650c8097196fec67f00c"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8598a11db0eb9efa116ba7656c437f5bed7de0f7":["f9058535b2d760062e15c60434989564a2b8302b"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ad94625fb8d088209f46650c8097196fec67f00c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"b61c4d4553ce38d24ed06a43de07d3c3ca420b62":["1da8d55113b689b06716246649de6f62430f15c0"],"f9058535b2d760062e15c60434989564a2b8302b":["9c8b12bda3f5864b27e3e04df1be4f6736ec067a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8598a11db0eb9efa116ba7656c437f5bed7de0f7"]},"commit2Childs":{"9c8b12bda3f5864b27e3e04df1be4f6736ec067a":["f9058535b2d760062e15c60434989564a2b8302b"],"1da8d55113b689b06716246649de6f62430f15c0":["b61c4d4553ce38d24ed06a43de07d3c3ca420b62"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8"],"8598a11db0eb9efa116ba7656c437f5bed7de0f7":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["1da8d55113b689b06716246649de6f62430f15c0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ad94625fb8d088209f46650c8097196fec67f00c"],"ad94625fb8d088209f46650c8097196fec67f00c":["1da8d55113b689b06716246649de6f62430f15c0"],"b61c4d4553ce38d24ed06a43de07d3c3ca420b62":["9c8b12bda3f5864b27e3e04df1be4f6736ec067a"],"f9058535b2d760062e15c60434989564a2b8302b":["8598a11db0eb9efa116ba7656c437f5bed7de0f7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}