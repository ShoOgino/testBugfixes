{"path":"lucene/backwards/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","pathOld":"backwards/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6","date":1272983566,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backwards/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":null,"sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  final private int mergeMiddle(MergePolicy.OneMerge merge) \n    throws CorruptIndexException, IOException {\n    \n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.name;\n    \n    SegmentMerger merger = null;\n\n    int mergedDocCount = 0;\n\n    SegmentInfos sourceSegments = merge.segments;\n    final int numSegments = sourceSegments.size();\n\n    if (infoStream != null)\n      message(\"merging \" + merge.segString(directory));\n\n    merger = new SegmentMerger(this, mergedName, merge);\n\n    merge.readers = new SegmentReader[numSegments];\n    merge.readersClone = new SegmentReader[numSegments];\n\n    boolean mergeDocStores = false;\n\n    final Set<String> dss = new HashSet<String>();\n    \n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int totDocCount = 0;\n\n      for (int i = 0; i < numSegments; i++) {\n\n        final SegmentInfo info = sourceSegments.info(i);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        SegmentReader reader = merge.readers[i] = readerPool.get(info, merge.mergeDocStores,\n                                                                 MERGE_READ_BUFFER_SIZE,\n                                                                 -1);\n\n        // We clone the segment readers because other\n        // deletes may come in while we're merging so we\n        // need readers that will not change\n        SegmentReader clone = merge.readersClone[i] = (SegmentReader) reader.clone(true);\n        merger.add(clone);\n\n        if (clone.hasDeletions()) {\n          mergeDocStores = true;\n        }\n        \n        if (info.getDocStoreOffset() != -1) {\n          dss.add(info.getDocStoreSegment());\n        }\n\n        totDocCount += clone.numDocs();\n      }\n\n      if (infoStream != null) {\n        message(\"merge: total \"+totDocCount+\" docs\");\n      }\n\n      merge.checkAborted(directory);\n\n      // If deletions have arrived and it has now become\n      // necessary to merge doc stores, go and open them:\n      if (mergeDocStores && !merge.mergeDocStores) {\n        merge.mergeDocStores = true;\n        synchronized(this) {\n          if (dss.contains(docWriter.getDocStoreSegment())) {\n            if (infoStream != null)\n              message(\"now flush at mergeMiddle\");\n            doFlush(true, false);\n          }\n        }\n\n        for(int i=0;i<numSegments;i++) {\n          merge.readersClone[i].openDocStores();\n        }\n\n        // Clear DSS\n        synchronized(this) {\n          merge.info.setDocStore(-1, null, false);\n        }\n      }\n\n      // This is where all the work happens:\n      mergedDocCount = merge.info.docCount = merger.merge(merge.mergeDocStores);\n\n      assert mergedDocCount == totDocCount;\n\n      // TODO: in the non-realtime case, we may want to only\n      // keep deletes (it's costly to open entire reader\n      // when we just need deletes)\n\n      final SegmentReader mergedReader = readerPool.get(merge.info, false, BufferedIndexInput.BUFFER_SIZE, -1);\n      try {\n        if (poolReaders && mergedSegmentWarmer != null) {\n          mergedSegmentWarmer.warm(mergedReader);\n        }\n        if (!commitMerge(merge, merger, mergedDocCount, mergedReader))\n          // commitMerge will return false if this merge was aborted\n          return 0;\n      } finally {\n        synchronized(this) {\n          readerPool.release(mergedReader);\n        }\n      }\n\n      success = true;\n    } finally {\n      synchronized(this) {\n        if (!success) {\n          // Suppress any new exceptions so we throw the\n          // original cause\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              try {\n                readerPool.release(merge.readers[i], true);\n              } catch (Throwable t) {\n              }\n            }\n\n            if (merge.readersClone[i] != null) {\n              try {\n                merge.readersClone[i].close();\n              } catch (Throwable t) {\n              }\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        } else {\n          for (int i=0;i<numSegments;i++) {\n            if (merge.readers[i] != null) {\n              readerPool.release(merge.readers[i], true);\n            }\n\n            if (merge.readersClone[i] != null) {\n              merge.readersClone[i].close();\n              // This was a private clone and we had the only reference\n              assert merge.readersClone[i].getRefCount() == 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Must checkpoint before decrefing so any newly\n    // referenced files in the new merge.info are incref'd\n    // first:\n    synchronized(this) {\n      deleter.checkpoint(segmentInfos, false);\n    }\n    decrefMergeSegments(merge);\n\n    if (merge.useCompoundFile) {\n\n      success = false;\n      final String compoundFileName = mergedName + \".\" + IndexFileNames.COMPOUND_FILE_EXTENSION;\n\n      try {\n        merger.createCompoundFile(compoundFileName);\n        success = true;\n      } catch (IOException ioe) {\n        synchronized(this) {\n          if (merge.isAborted()) {\n            // This can happen if rollback or close(false)\n            // is called -- fall through to logic below to\n            // remove the partially created CFS:\n            success = true;\n          } else\n            handleMergeException(ioe, merge);\n        }\n      } catch (Throwable t) {\n        handleMergeException(t, merge);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception creating compound file during merge\");\n          synchronized(this) {\n            deleter.deleteFile(compoundFileName);\n          }\n        }\n      }\n\n      if (merge.isAborted()) {\n        if (infoStream != null)\n          message(\"abort merge after building CFS\");\n        deleter.deleteFile(compoundFileName);\n        return 0;\n      }\n\n      synchronized(this) {\n        if (segmentInfos.indexOf(merge.info) == -1 || merge.isAborted()) {\n          // Our segment (committed in non-compound\n          // format) got merged away while we were\n          // building the compound format.\n          deleter.deleteFile(compoundFileName);\n        } else {\n          merge.info.setUseCompoundFile(true);\n          checkpoint();\n        }\n      }\n    }\n\n    return mergedDocCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}