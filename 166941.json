{"path":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getSentences(String,int).mjava","commits":[{"id":"d68e5c46e6a5ebdf4dafec4a123344092b915cc0","date":1256752193,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getSentences(String,int).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil#getSentences(String,int).mjava","sourceNew":"  /**\n   * Returns at most the first N sentences of the given text. Delimiting\n   * characters are excluded from the results. Each returned sentence is\n   * whitespace-trimmed via String.trim(), potentially an empty string.\n   * \n   * @param text\n   *            the text to tokenize into sentences\n   * @param limit\n   *            the maximum number of sentences to return; zero indicates \"as\n   *            many as possible\".\n   * @return the first N sentences\n   */\n  public static String[] getSentences(String text, int limit) {\n//    return tokenize(SENTENCES, text, limit); // equivalent but slower\n    int len = text.length();\n    if (len == 0) return new String[] { text };\n    if (limit <= 0) limit = Integer.MAX_VALUE;\n    \n    // average sentence length heuristic\n    String[] tokens = new String[Math.min(limit, 1 + len/40)];\n    int size = 0;\n    int i = 0;\n    \n    while (i < len && size < limit) {\n      \n      // scan to end of current sentence\n      int start = i;\n      while (i < len && !isSentenceSeparator(text.charAt(i))) i++;\n      \n      if (size == tokens.length) { // grow array\n        String[] tmp = new String[tokens.length << 1];\n        System.arraycopy(tokens, 0, tmp, 0, size);\n        tokens = tmp;\n      }\n      // add sentence (potentially empty)\n      tokens[size++] = text.substring(start, i).trim();\n\n      // scan to beginning of next sentence\n      while (i < len && isSentenceSeparator(text.charAt(i))) i++;\n    }\n    \n    if (size == tokens.length) return tokens;\n    String[] results = new String[size];\n    System.arraycopy(tokens, 0, results, 0, size);\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Returns at most the first N sentences of the given text. Delimiting\n   * characters are excluded from the results. Each returned sentence is\n   * whitespace-trimmed via String.trim(), potentially an empty string.\n   * \n   * @param text\n   *            the text to tokenize into sentences\n   * @param limit\n   *            the maximum number of sentences to return; zero indicates \"as\n   *            many as possible\".\n   * @return the first N sentences\n   */\n  public static String[] getSentences(String text, int limit) {\n//    return tokenize(SENTENCES, text, limit); // equivalent but slower\n    int len = text.length();\n    if (len == 0) return new String[] { text };\n    if (limit <= 0) limit = Integer.MAX_VALUE;\n    \n    // average sentence length heuristic\n    String[] tokens = new String[Math.min(limit, 1 + len/40)];\n    int size = 0;\n    int i = 0;\n    \n    while (i < len && size < limit) {\n      \n      // scan to end of current sentence\n      int start = i;\n      while (i < len && !isSentenceSeparator(text.charAt(i))) i++;\n      \n      if (size == tokens.length) { // grow array\n        String[] tmp = new String[tokens.length << 1];\n        System.arraycopy(tokens, 0, tmp, 0, size);\n        tokens = tmp;\n      }\n      // add sentence (potentially empty)\n      tokens[size++] = text.substring(start, i).trim();\n\n      // scan to beginning of next sentence\n      while (i < len && isSentenceSeparator(text.charAt(i))) i++;\n    }\n    \n    if (size == tokens.length) return tokens;\n    String[] results = new String[size];\n    System.arraycopy(tokens, 0, results, 0, size);\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getSentences(String,int).mjava","pathOld":"contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil#getSentences(String,int).mjava","sourceNew":"  /**\n   * Returns at most the first N sentences of the given text. Delimiting\n   * characters are excluded from the results. Each returned sentence is\n   * whitespace-trimmed via String.trim(), potentially an empty string.\n   * \n   * @param text\n   *            the text to tokenize into sentences\n   * @param limit\n   *            the maximum number of sentences to return; zero indicates \"as\n   *            many as possible\".\n   * @return the first N sentences\n   */\n  public static String[] getSentences(String text, int limit) {\n//    return tokenize(SENTENCES, text, limit); // equivalent but slower\n    int len = text.length();\n    if (len == 0) return new String[] { text };\n    if (limit <= 0) limit = Integer.MAX_VALUE;\n    \n    // average sentence length heuristic\n    String[] tokens = new String[Math.min(limit, 1 + len/40)];\n    int size = 0;\n    int i = 0;\n    \n    while (i < len && size < limit) {\n      \n      // scan to end of current sentence\n      int start = i;\n      while (i < len && !isSentenceSeparator(text.charAt(i))) i++;\n      \n      if (size == tokens.length) { // grow array\n        String[] tmp = new String[tokens.length << 1];\n        System.arraycopy(tokens, 0, tmp, 0, size);\n        tokens = tmp;\n      }\n      // add sentence (potentially empty)\n      tokens[size++] = text.substring(start, i).trim();\n\n      // scan to beginning of next sentence\n      while (i < len && isSentenceSeparator(text.charAt(i))) i++;\n    }\n    \n    if (size == tokens.length) return tokens;\n    String[] results = new String[size];\n    System.arraycopy(tokens, 0, results, 0, size);\n    return results;\n  }\n\n","sourceOld":"  /**\n   * Returns at most the first N sentences of the given text. Delimiting\n   * characters are excluded from the results. Each returned sentence is\n   * whitespace-trimmed via String.trim(), potentially an empty string.\n   * \n   * @param text\n   *            the text to tokenize into sentences\n   * @param limit\n   *            the maximum number of sentences to return; zero indicates \"as\n   *            many as possible\".\n   * @return the first N sentences\n   */\n  public static String[] getSentences(String text, int limit) {\n//    return tokenize(SENTENCES, text, limit); // equivalent but slower\n    int len = text.length();\n    if (len == 0) return new String[] { text };\n    if (limit <= 0) limit = Integer.MAX_VALUE;\n    \n    // average sentence length heuristic\n    String[] tokens = new String[Math.min(limit, 1 + len/40)];\n    int size = 0;\n    int i = 0;\n    \n    while (i < len && size < limit) {\n      \n      // scan to end of current sentence\n      int start = i;\n      while (i < len && !isSentenceSeparator(text.charAt(i))) i++;\n      \n      if (size == tokens.length) { // grow array\n        String[] tmp = new String[tokens.length << 1];\n        System.arraycopy(tokens, 0, tmp, 0, size);\n        tokens = tmp;\n      }\n      // add sentence (potentially empty)\n      tokens[size++] = text.substring(start, i).trim();\n\n      // scan to beginning of next sentence\n      while (i < len && isSentenceSeparator(text.charAt(i))) i++;\n    }\n    \n    if (size == tokens.length) return tokens;\n    String[] results = new String[size];\n    System.arraycopy(tokens, 0, results, 0, size);\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["d68e5c46e6a5ebdf4dafec4a123344092b915cc0"],"d68e5c46e6a5ebdf4dafec4a123344092b915cc0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d68e5c46e6a5ebdf4dafec4a123344092b915cc0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d68e5c46e6a5ebdf4dafec4a123344092b915cc0":["9454a6510e2db155fb01faa5c049b06ece95fab9"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}