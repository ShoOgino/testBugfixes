{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilterFactory#testMaxPosition1WithShingles().mjava","commits":[{"id":"fdd61b10b980a6d0b8a8d63baf7e8f5e19e8437a","date":1363558184,"type":0,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilterFactory#testMaxPosition1WithShingles().mjava","pathOld":"/dev/null","sourceNew":"  public void testMaxPosition1WithShingles() throws IOException {\n    LimitTokenPositionFilterFactory factory = new LimitTokenPositionFilterFactory();\n    Map<String, String> args = new HashMap<String, String>();\n    args.put(LimitTokenPositionFilterFactory.MAX_TOKEN_POSITION_KEY, \"1\");\n    factory.init(args);\n    String input = \"one two three four five\";\n    MockTokenizer tok = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);\n    // LimitTokenPositionFilter doesn't consume the entire stream that it wraps\n    tok.setEnableChecks(false);\n    ShingleFilter shingleFilter = new ShingleFilter(tok, 2, 3);\n    shingleFilter.setOutputUnigrams(true);\n    TokenStream stream = factory.create(shingleFilter);\n    assertTokenStreamContents(stream, new String[] { \"one\", \"one two\", \"one two three\" });\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57da959ec15bb701bd1d1bf3c613b69009ff4bfd","date":1364833800,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilterFactory#testMaxPosition1WithShingles().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilterFactory#testMaxPosition1WithShingles().mjava","sourceNew":"  public void testMaxPosition1WithShingles() throws Exception {\n    Reader reader = new StringReader(\"one two three four five\");\n    MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n    // LimitTokenPositionFilter doesn't consume the entire stream that it wraps\n    tokenizer.setEnableChecks(false);\n    TokenStream stream = tokenizer;\n    stream = tokenFilterFactory(\"Shingle\",\n        \"minShingleSize\", \"2\",\n        \"maxShingleSize\", \"3\",\n        \"outputUnigrams\", \"true\").create(stream);\n    stream = tokenFilterFactory(\"LimitTokenPosition\",\n        \"maxTokenPosition\", \"1\").create(stream);\n    assertTokenStreamContents(stream, new String[] { \"one\", \"one two\", \"one two three\" });\n  }\n\n","sourceOld":"  public void testMaxPosition1WithShingles() throws IOException {\n    LimitTokenPositionFilterFactory factory = new LimitTokenPositionFilterFactory();\n    Map<String, String> args = new HashMap<String, String>();\n    args.put(LimitTokenPositionFilterFactory.MAX_TOKEN_POSITION_KEY, \"1\");\n    factory.init(args);\n    String input = \"one two three four five\";\n    MockTokenizer tok = new MockTokenizer(new StringReader(input), MockTokenizer.WHITESPACE, false);\n    // LimitTokenPositionFilter doesn't consume the entire stream that it wraps\n    tok.setEnableChecks(false);\n    ShingleFilter shingleFilter = new ShingleFilter(tok, 2, 3);\n    shingleFilter.setOutputUnigrams(true);\n    TokenStream stream = factory.create(shingleFilter);\n    assertTokenStreamContents(stream, new String[] { \"one\", \"one two\", \"one two three\" });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilterFactory#testMaxPosition1WithShingles().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilterFactory#testMaxPosition1WithShingles().mjava","sourceNew":"  public void testMaxPosition1WithShingles() throws Exception {\n    Reader reader = new StringReader(\"one two three four five\");\n    MockTokenizer tokenizer = whitespaceMockTokenizer(reader);\n    // LimitTokenPositionFilter doesn't consume the entire stream that it wraps\n    tokenizer.setEnableChecks(false);\n    TokenStream stream = tokenizer;\n    stream = tokenFilterFactory(\"Shingle\",\n        \"minShingleSize\", \"2\",\n        \"maxShingleSize\", \"3\",\n        \"outputUnigrams\", \"true\").create(stream);\n    stream = tokenFilterFactory(\"LimitTokenPosition\",\n        \"maxTokenPosition\", \"1\").create(stream);\n    assertTokenStreamContents(stream, new String[] { \"one\", \"one two\", \"one two three\" });\n  }\n\n","sourceOld":"  public void testMaxPosition1WithShingles() throws Exception {\n    Reader reader = new StringReader(\"one two three four five\");\n    MockTokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n    // LimitTokenPositionFilter doesn't consume the entire stream that it wraps\n    tokenizer.setEnableChecks(false);\n    TokenStream stream = tokenizer;\n    stream = tokenFilterFactory(\"Shingle\",\n        \"minShingleSize\", \"2\",\n        \"maxShingleSize\", \"3\",\n        \"outputUnigrams\", \"true\").create(stream);\n    stream = tokenFilterFactory(\"LimitTokenPosition\",\n        \"maxTokenPosition\", \"1\").create(stream);\n    assertTokenStreamContents(stream, new String[] { \"one\", \"one two\", \"one two three\" });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43acd3a99a12a5bec9c72097de0e294c80cb6daa","date":1396327381,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilterFactory#testMaxPosition1WithShingles().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilterFactory#testMaxPosition1WithShingles().mjava","sourceNew":"  public void testMaxPosition1WithShingles() throws Exception {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      Reader reader = new StringReader(\"one two three four five\");\n      MockTokenizer tokenizer = whitespaceMockTokenizer(reader);\n      // if we are consuming all tokens, we can use the checks, otherwise we can't\n      tokenizer.setEnableChecks(consumeAll);\n      TokenStream stream = tokenizer;\n      stream = tokenFilterFactory(\"Shingle\",\n          \"minShingleSize\", \"2\",\n          \"maxShingleSize\", \"3\",\n          \"outputUnigrams\", \"true\").create(stream);\n      stream = tokenFilterFactory(\"LimitTokenPosition\",\n          LimitTokenPositionFilterFactory.MAX_TOKEN_POSITION_KEY, \"1\",\n          LimitTokenPositionFilterFactory.CONSUME_ALL_TOKENS_KEY, Boolean.toString(consumeAll)\n      ).create(stream);\n      assertTokenStreamContents(stream, new String[]{\"one\", \"one two\", \"one two three\"});\n    }\n  }\n\n","sourceOld":"  public void testMaxPosition1WithShingles() throws Exception {\n    Reader reader = new StringReader(\"one two three four five\");\n    MockTokenizer tokenizer = whitespaceMockTokenizer(reader);\n    // LimitTokenPositionFilter doesn't consume the entire stream that it wraps\n    tokenizer.setEnableChecks(false);\n    TokenStream stream = tokenizer;\n    stream = tokenFilterFactory(\"Shingle\",\n        \"minShingleSize\", \"2\",\n        \"maxShingleSize\", \"3\",\n        \"outputUnigrams\", \"true\").create(stream);\n    stream = tokenFilterFactory(\"LimitTokenPosition\",\n        \"maxTokenPosition\", \"1\").create(stream);\n    assertTokenStreamContents(stream, new String[] { \"one\", \"one two\", \"one two three\" });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilterFactory#testMaxPosition1WithShingles().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilterFactory#testMaxPosition1WithShingles().mjava","sourceNew":"  public void testMaxPosition1WithShingles() throws Exception {\n    for (final boolean consumeAll : new boolean[]{true, false}) {\n      Reader reader = new StringReader(\"one two three four five\");\n      MockTokenizer tokenizer = whitespaceMockTokenizer(reader);\n      // if we are consuming all tokens, we can use the checks, otherwise we can't\n      tokenizer.setEnableChecks(consumeAll);\n      TokenStream stream = tokenizer;\n      stream = tokenFilterFactory(\"Shingle\",\n          \"minShingleSize\", \"2\",\n          \"maxShingleSize\", \"3\",\n          \"outputUnigrams\", \"true\").create(stream);\n      stream = tokenFilterFactory(\"LimitTokenPosition\",\n          LimitTokenPositionFilterFactory.MAX_TOKEN_POSITION_KEY, \"1\",\n          LimitTokenPositionFilterFactory.CONSUME_ALL_TOKENS_KEY, Boolean.toString(consumeAll)\n      ).create(stream);\n      assertTokenStreamContents(stream, new String[]{\"one\", \"one two\", \"one two three\"});\n    }\n  }\n\n","sourceOld":"  public void testMaxPosition1WithShingles() throws Exception {\n    Reader reader = new StringReader(\"one two three four five\");\n    MockTokenizer tokenizer = whitespaceMockTokenizer(reader);\n    // LimitTokenPositionFilter doesn't consume the entire stream that it wraps\n    tokenizer.setEnableChecks(false);\n    TokenStream stream = tokenizer;\n    stream = tokenFilterFactory(\"Shingle\",\n        \"minShingleSize\", \"2\",\n        \"maxShingleSize\", \"3\",\n        \"outputUnigrams\", \"true\").create(stream);\n    stream = tokenFilterFactory(\"LimitTokenPosition\",\n        \"maxTokenPosition\", \"1\").create(stream);\n    assertTokenStreamContents(stream, new String[] { \"one\", \"one two\", \"one two three\" });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5eb2511ababf862ea11e10761c70ee560cd84510":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","43acd3a99a12a5bec9c72097de0e294c80cb6daa"],"fdd61b10b980a6d0b8a8d63baf7e8f5e19e8437a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["57da959ec15bb701bd1d1bf3c613b69009ff4bfd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"57da959ec15bb701bd1d1bf3c613b69009ff4bfd":["fdd61b10b980a6d0b8a8d63baf7e8f5e19e8437a"],"43acd3a99a12a5bec9c72097de0e294c80cb6daa":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["43acd3a99a12a5bec9c72097de0e294c80cb6daa"]},"commit2Childs":{"5eb2511ababf862ea11e10761c70ee560cd84510":[],"fdd61b10b980a6d0b8a8d63baf7e8f5e19e8437a":["57da959ec15bb701bd1d1bf3c613b69009ff4bfd"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["5eb2511ababf862ea11e10761c70ee560cd84510","43acd3a99a12a5bec9c72097de0e294c80cb6daa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["fdd61b10b980a6d0b8a8d63baf7e8f5e19e8437a"],"57da959ec15bb701bd1d1bf3c613b69009ff4bfd":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"43acd3a99a12a5bec9c72097de0e294c80cb6daa":["5eb2511ababf862ea11e10761c70ee560cd84510","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5eb2511ababf862ea11e10761c70ee560cd84510","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}