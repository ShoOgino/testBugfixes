{"path":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","commits":[{"id":"8598a11db0eb9efa116ba7656c437f5bed7de0f7","date":1272964265,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)),\n    1,1,0,0,1,1,0);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)),\n    1,1,0,0,1,1,0);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)),\n    1,1,0,0,1,1,0);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)),\n    1,1,0,0,1,1,0);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f080986da691a3bba7b757f43ab72cdc82b57ce","date":1273069619,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","pathOld":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testOffsets().mjava","sourceNew":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)),\n    1,1,0,0,1,1,0);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)),\n    1,1,0,0,1,1,0);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","sourceOld":"  /***\n  public void testPerformance() throws IOException {\n    String s = \"now is the time-for all good men to come to-the aid of their country.\";\n    Token tok = new Token();\n    long start = System.currentTimeMillis();\n    int ret=0;\n    for (int i=0; i<1000000; i++) {\n      StringReader r = new StringReader(s);\n      TokenStream ts = new WhitespaceTokenizer(r);\n      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);\n\n      while (ts.next(tok) != null) ret++;\n    }\n\n    System.out.println(\"ret=\"+ret+\" time=\"+(System.currentTimeMillis()-start));\n  }\n  ***/\n\n  @Test\n  public void testOffsets() throws IOException {\n\n    // test that subwords and catenated subwords have\n    // the correct offsets.\n    WordDelimiterFilter wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 12)),\n    1,1,0,0,1,1,0);\n\n    assertTokenStreamContents(wdf, \n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 9, 5 }, \n        new int[] { 8, 12, 12 });\n\n    wdf = new WordDelimiterFilter(\n            new SingleTokenTokenStream(new Token(\"foo-bar\", 5, 6)),\n    1,1,0,0,1,1,0);\n    \n    assertTokenStreamContents(wdf,\n        new String[] { \"foo\", \"bar\", \"foobar\" },\n        new int[] { 5, 5, 5 },\n        new int[] { 6, 6, 6 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"8598a11db0eb9efa116ba7656c437f5bed7de0f7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["8598a11db0eb9efa116ba7656c437f5bed7de0f7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f080986da691a3bba7b757f43ab72cdc82b57ce"]},"commit2Childs":{"8598a11db0eb9efa116ba7656c437f5bed7de0f7":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8598a11db0eb9efa116ba7656c437f5bed7de0f7"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}