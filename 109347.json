{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.Parser#analyze(String,CharsRefBuilder).mjava","commits":[{"id":"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","date":1407854805,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.Parser#analyze(String,CharsRefBuilder).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.Parser#analyze(String,CharsRef).mjava","sourceNew":"    /** Sugar: analyzes the text with the analyzer and\n     *  separates by {@link SynonymMap#WORD_SEPARATOR}.\n     *  reuse and its chars must not be null. */\n    public CharsRef analyze(String text, CharsRefBuilder reuse) throws IOException {\n      try (TokenStream ts = analyzer.tokenStream(\"\", text)) {\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n        ts.reset();\n        reuse.clear();\n        while (ts.incrementToken()) {\n          int length = termAtt.length();\n          if (length == 0) {\n            throw new IllegalArgumentException(\"term: \" + text + \" analyzed to a zero-length token\");\n          }\n          if (posIncAtt.getPositionIncrement() != 1) {\n            throw new IllegalArgumentException(\"term: \" + text + \" analyzed to a token with posinc != 1\");\n          }\n          reuse.grow(reuse.length() + length + 1); /* current + word + separator */\n          int end = reuse.length();\n          if (reuse.length() > 0) {\n            reuse.setCharAt(end++, SynonymMap.WORD_SEPARATOR);\n            reuse.setLength(reuse.length() + 1);\n          }\n          System.arraycopy(termAtt.buffer(), 0, reuse.chars(), end, length);\n          reuse.setLength(reuse.length() + length);\n        }\n        ts.end();\n      }\n      if (reuse.length() == 0) {\n        throw new IllegalArgumentException(\"term: \" + text + \" was completely eliminated by analyzer\");\n      }\n      return reuse.get();\n    }\n\n","sourceOld":"    /** Sugar: analyzes the text with the analyzer and\n     *  separates by {@link SynonymMap#WORD_SEPARATOR}.\n     *  reuse and its chars must not be null. */\n    public CharsRef analyze(String text, CharsRef reuse) throws IOException {\n      try (TokenStream ts = analyzer.tokenStream(\"\", text)) {\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n        ts.reset();\n        reuse.length = 0;\n        while (ts.incrementToken()) {\n          int length = termAtt.length();\n          if (length == 0) {\n            throw new IllegalArgumentException(\"term: \" + text + \" analyzed to a zero-length token\");\n          }\n          if (posIncAtt.getPositionIncrement() != 1) {\n            throw new IllegalArgumentException(\"term: \" + text + \" analyzed to a token with posinc != 1\");\n          }\n          reuse.grow(reuse.length + length + 1); /* current + word + separator */\n          int end = reuse.offset + reuse.length;\n          if (reuse.length > 0) {\n            reuse.chars[end++] = SynonymMap.WORD_SEPARATOR;\n            reuse.length++;\n          }\n          System.arraycopy(termAtt.buffer(), 0, reuse.chars, end, length);\n          reuse.length += length;\n        }\n        ts.end();\n      }\n      if (reuse.length == 0) {\n        throw new IllegalArgumentException(\"term: \" + text + \" was completely eliminated by analyzer\");\n      }\n      return reuse;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2911e6f8907a96ca3942993f8541b66feaa51ff1","date":1505845388,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.Parser#analyze(String,CharsRefBuilder).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.Parser#analyze(String,CharsRefBuilder).mjava","sourceNew":"    /** Sugar: analyzes the text with the analyzer and\n     *  separates by {@link SynonymMap#WORD_SEPARATOR}.\n     *  reuse and its chars must not be null. */\n    public CharsRef analyze(String text, CharsRefBuilder reuse) throws IOException {\n      try (TokenStream ts = analyzer.tokenStream(\"\", text)) {\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n        ts.reset();\n        reuse.clear();\n        while (ts.incrementToken()) {\n          int length = termAtt.length();\n          if (length == 0) {\n            throw new IllegalArgumentException(\"term: \" + text + \" analyzed to a zero-length token\");\n          }\n          if (posIncAtt.getPositionIncrement() != 1) {\n            throw new IllegalArgumentException(\"term: \" + text + \" analyzed to a token (\" + termAtt +\n                                               \") with position increment != 1 (got: \" + posIncAtt.getPositionIncrement() + \")\");\n          }\n          reuse.grow(reuse.length() + length + 1); /* current + word + separator */\n          int end = reuse.length();\n          if (reuse.length() > 0) {\n            reuse.setCharAt(end++, SynonymMap.WORD_SEPARATOR);\n            reuse.setLength(reuse.length() + 1);\n          }\n          System.arraycopy(termAtt.buffer(), 0, reuse.chars(), end, length);\n          reuse.setLength(reuse.length() + length);\n        }\n        ts.end();\n      }\n      if (reuse.length() == 0) {\n        throw new IllegalArgumentException(\"term: \" + text + \" was completely eliminated by analyzer\");\n      }\n      return reuse.get();\n    }\n\n","sourceOld":"    /** Sugar: analyzes the text with the analyzer and\n     *  separates by {@link SynonymMap#WORD_SEPARATOR}.\n     *  reuse and its chars must not be null. */\n    public CharsRef analyze(String text, CharsRefBuilder reuse) throws IOException {\n      try (TokenStream ts = analyzer.tokenStream(\"\", text)) {\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n        ts.reset();\n        reuse.clear();\n        while (ts.incrementToken()) {\n          int length = termAtt.length();\n          if (length == 0) {\n            throw new IllegalArgumentException(\"term: \" + text + \" analyzed to a zero-length token\");\n          }\n          if (posIncAtt.getPositionIncrement() != 1) {\n            throw new IllegalArgumentException(\"term: \" + text + \" analyzed to a token with posinc != 1\");\n          }\n          reuse.grow(reuse.length() + length + 1); /* current + word + separator */\n          int end = reuse.length();\n          if (reuse.length() > 0) {\n            reuse.setCharAt(end++, SynonymMap.WORD_SEPARATOR);\n            reuse.setLength(reuse.length() + 1);\n          }\n          System.arraycopy(termAtt.buffer(), 0, reuse.chars(), end, length);\n          reuse.setLength(reuse.length() + length);\n        }\n        ts.end();\n      }\n      if (reuse.length() == 0) {\n        throw new IllegalArgumentException(\"term: \" + text + \" was completely eliminated by analyzer\");\n      }\n      return reuse.get();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04af4a840c8f501ba34e09e382b8f8ace82aa51e","date":1505847466,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.Parser#analyze(String,CharsRefBuilder).mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymMap.Parser#analyze(String,CharsRefBuilder).mjava","sourceNew":"    /** Sugar: analyzes the text with the analyzer and\n     *  separates by {@link SynonymMap#WORD_SEPARATOR}.\n     *  reuse and its chars must not be null. */\n    public CharsRef analyze(String text, CharsRefBuilder reuse) throws IOException {\n      try (TokenStream ts = analyzer.tokenStream(\"\", text)) {\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n        ts.reset();\n        reuse.clear();\n        while (ts.incrementToken()) {\n          int length = termAtt.length();\n          if (length == 0) {\n            throw new IllegalArgumentException(\"term: \" + text + \" analyzed to a zero-length token\");\n          }\n          if (posIncAtt.getPositionIncrement() != 1) {\n            throw new IllegalArgumentException(\"term: \" + text + \" analyzed to a token (\" + termAtt +\n                                               \") with position increment != 1 (got: \" + posIncAtt.getPositionIncrement() + \")\");\n          }\n          reuse.grow(reuse.length() + length + 1); /* current + word + separator */\n          int end = reuse.length();\n          if (reuse.length() > 0) {\n            reuse.setCharAt(end++, SynonymMap.WORD_SEPARATOR);\n            reuse.setLength(reuse.length() + 1);\n          }\n          System.arraycopy(termAtt.buffer(), 0, reuse.chars(), end, length);\n          reuse.setLength(reuse.length() + length);\n        }\n        ts.end();\n      }\n      if (reuse.length() == 0) {\n        throw new IllegalArgumentException(\"term: \" + text + \" was completely eliminated by analyzer\");\n      }\n      return reuse.get();\n    }\n\n","sourceOld":"    /** Sugar: analyzes the text with the analyzer and\n     *  separates by {@link SynonymMap#WORD_SEPARATOR}.\n     *  reuse and its chars must not be null. */\n    public CharsRef analyze(String text, CharsRefBuilder reuse) throws IOException {\n      try (TokenStream ts = analyzer.tokenStream(\"\", text)) {\n        CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n        PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n        ts.reset();\n        reuse.clear();\n        while (ts.incrementToken()) {\n          int length = termAtt.length();\n          if (length == 0) {\n            throw new IllegalArgumentException(\"term: \" + text + \" analyzed to a zero-length token\");\n          }\n          if (posIncAtt.getPositionIncrement() != 1) {\n            throw new IllegalArgumentException(\"term: \" + text + \" analyzed to a token with posinc != 1\");\n          }\n          reuse.grow(reuse.length() + length + 1); /* current + word + separator */\n          int end = reuse.length();\n          if (reuse.length() > 0) {\n            reuse.setCharAt(end++, SynonymMap.WORD_SEPARATOR);\n            reuse.setLength(reuse.length() + 1);\n          }\n          System.arraycopy(termAtt.buffer(), 0, reuse.chars(), end, length);\n          reuse.setLength(reuse.length() + length);\n        }\n        ts.end();\n      }\n      if (reuse.length() == 0) {\n        throw new IllegalArgumentException(\"term: \" + text + \" was completely eliminated by analyzer\");\n      }\n      return reuse.get();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"04af4a840c8f501ba34e09e382b8f8ace82aa51e":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","2911e6f8907a96ca3942993f8541b66feaa51ff1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2911e6f8907a96ca3942993f8541b66feaa51ff1":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["2911e6f8907a96ca3942993f8541b66feaa51ff1"]},"commit2Childs":{"04af4a840c8f501ba34e09e382b8f8ace82aa51e":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["04af4a840c8f501ba34e09e382b8f8ace82aa51e","2911e6f8907a96ca3942993f8541b66feaa51ff1"],"2911e6f8907a96ca3942993f8541b66feaa51ff1":["04af4a840c8f501ba34e09e382b8f8ace82aa51e","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["04af4a840c8f501ba34e09e382b8f8ace82aa51e","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}