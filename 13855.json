{"path":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","commits":[{"id":"79759974460bc59933cd169acc94f5c6b16368d5","date":1471318443,"type":1,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":1,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorStream[FacetField]#setup().mjava","sourceNew":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  public void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e07c409cff8701e4dc3d45934b021a949a5a8822","date":1475694629,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","sourceNew":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getSlowAtomicReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getLeafReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","pathOld":"/dev/null","sourceNew":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getSlowAtomicReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d81baa64023bbb9b43f6d929ee168b105940d30","date":1486492702,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","sourceNew":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumberType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getSlowAtomicReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumericType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getSlowAtomicReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","date":1497408244,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","sourceNew":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumberType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Terms terms = fcontext.searcher.getSlowAtomicReader().terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumberType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getSlowAtomicReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","sourceNew":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumberType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Terms terms = fcontext.searcher.getSlowAtomicReader().terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumberType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getSlowAtomicReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","pathOld":"solr/core/src/java/org/apache/solr/search/facet/FacetFieldProcessorByEnumTermsStream#setup().mjava","sourceNew":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumberType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Terms terms = fcontext.searcher.getSlowAtomicReader().terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","sourceOld":"  private void setup() throws IOException {\n\n    countOnly = freq.facetStats.size() == 0 || freq.facetStats.values().iterator().next() instanceof CountAgg;\n    hasSubFacets = freq.subFacets.size() > 0;\n    bucketsToSkip = freq.offset;\n\n    createAccs(-1, 1);\n\n    // Minimum term docFreq in order to use the filterCache for that term.\n    if (freq.cacheDf == -1) { // -1 means never cache\n      minDfFilterCache = Integer.MAX_VALUE;\n    } else if (freq.cacheDf == 0) { // default; compute as fraction of maxDoc\n      minDfFilterCache = Math.max(fcontext.searcher.maxDoc() >> 4, 3);  // (minimum of 3 is for test coverage purposes)\n    } else {\n      minDfFilterCache = freq.cacheDf;\n    }\n\n    docs = fcontext.base;\n    fastForRandomSet = null;\n\n    if (freq.prefix != null) {\n      String indexedPrefix = sf.getType().toInternal(freq.prefix);\n      startTermBytes = new BytesRef(indexedPrefix);\n    } else if (sf.getType().getNumberType() != null) {\n      String triePrefix = TrieField.getMainValuePrefix(sf.getType());\n      if (triePrefix != null) {\n        startTermBytes = new BytesRef(triePrefix);\n      }\n    }\n\n    Fields fields = fcontext.searcher.getSlowAtomicReader().fields();\n    Terms terms = fields == null ? null : fields.terms(sf.getName());\n\n    termsEnum = null;\n    deState = null;\n    term = null;\n\n\n    if (terms != null) {\n\n      termsEnum = terms.iterator();\n\n      // TODO: OPT: if seek(ord) is supported for this termsEnum, then we could use it for\n      // facet.offset when sorting by index order.\n\n      if (startTermBytes != null) {\n        if (termsEnum.seekCeil(startTermBytes) == TermsEnum.SeekStatus.END) {\n          termsEnum = null;\n        } else {\n          term = termsEnum.term();\n        }\n      } else {\n        // position termsEnum on first term\n        term = termsEnum.next();\n      }\n    }\n\n    List<LeafReaderContext> leafList = fcontext.searcher.getTopReaderContext().leaves();\n    leaves = leafList.toArray( new LeafReaderContext[ leafList.size() ]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"403d05f7f8d69b65659157eff1bc1d2717f04c66":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["3d81baa64023bbb9b43f6d929ee168b105940d30"],"3d81baa64023bbb9b43f6d929ee168b105940d30":["e07c409cff8701e4dc3d45934b021a949a5a8822"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","79759974460bc59933cd169acc94f5c6b16368d5"],"79759974460bc59933cd169acc94f5c6b16368d5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e07c409cff8701e4dc3d45934b021a949a5a8822":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"28288370235ed02234a64753cdbf0c6ec096304a":["3d81baa64023bbb9b43f6d929ee168b105940d30","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["3d81baa64023bbb9b43f6d929ee168b105940d30","e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e07c409cff8701e4dc3d45934b021a949a5a8822"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28288370235ed02234a64753cdbf0c6ec096304a"]},"commit2Childs":{"403d05f7f8d69b65659157eff1bc1d2717f04c66":["e07c409cff8701e4dc3d45934b021a949a5a8822"],"e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"3d81baa64023bbb9b43f6d929ee168b105940d30":["e588e8c82e32e29ef3837c0b06a2ad34f3c51a2b","28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["403d05f7f8d69b65659157eff1bc1d2717f04c66","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","79759974460bc59933cd169acc94f5c6b16368d5","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"79759974460bc59933cd169acc94f5c6b16368d5":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"e07c409cff8701e4dc3d45934b021a949a5a8822":["3d81baa64023bbb9b43f6d929ee168b105940d30","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"28288370235ed02234a64753cdbf0c6ec096304a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}