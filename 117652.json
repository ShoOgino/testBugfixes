{"path":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","commits":[{"id":"915201312eef398482b3dffe8a297e1809f3b29a","date":1468722990,"type":0,"author":"jbernste","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n        .commit(cluster.getSolrClient(), COLLECTION);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTION);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTION);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\",\"13\");\n    } finally {\n      cache.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bcbcca681ad951d22b9bb07a88683ce966986ae7","date":1468722990,"type":3,"author":"jbernste","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","sourceNew":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\",\"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\",\"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTION);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTION);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTION);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\",\"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n        .commit(cluster.getSolrClient(), COLLECTION);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTION);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTION);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\",\"13\");\n    } finally {\n      cache.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\",\"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\",\"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTION);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTION);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTION);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\",\"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8c969f15cd04d31e520319c619a445ae21f02d72","date":1479263638,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","sourceNew":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    Assume.assumeTrue(!useAlias);\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\",\"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\",\"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\",\"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\",\"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\",\"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTION);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTION);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTION);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\",\"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1ef55e1fff7ff44354432770ad8bc19be1fcc75","date":1479266056,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","sourceNew":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    Assume.assumeTrue(!useAlias);\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\",\"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\",\"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\",\"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\",\"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\",\"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTION);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTION);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTION);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\",\"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c405288c4553ffb50ab8ca5adbdde9881bcec4e4","date":1491938682,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","sourceNew":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    Assume.assumeTrue(!useAlias);\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\", \"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\", \"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\", \"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    Assume.assumeTrue(!useAlias);\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\",\"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\",\"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\",\"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54ca69905c5d9d1529286f06ab1d12c68f6c13cb","date":1492683554,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","sourceNew":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    Assume.assumeTrue(!useAlias);\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\", \"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\", \"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\", \"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    Assume.assumeTrue(!useAlias);\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\",\"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\",\"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\",\"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108","date":1533256859,"type":3,"author":"Erick","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","sourceNew":"  @Test\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testParallelTopicStream() throws Exception {\n\n    Assume.assumeTrue(!useAlias);\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\", \"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\", \"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\", \"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","sourceOld":"  @Test\n  public void testParallelTopicStream() throws Exception {\n\n    Assume.assumeTrue(!useAlias);\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\", \"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\", \"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\", \"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f7ba5de6e78dd9b7518e92274a8a4cf8823a9c4a","date":1536291831,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testParallelTopicStream().mjava","sourceNew":"  @Test\n  // commented 4-Sep-2018 @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testParallelTopicStream() throws Exception {\n\n    Assume.assumeTrue(!useAlias);\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\", \"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\", \"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\", \"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","sourceOld":"  @Test\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testParallelTopicStream() throws Exception {\n\n    Assume.assumeTrue(!useAlias);\n\n    new UpdateRequest()\n        .add(id, \"0\", \"a_s\", \"hello\", \"a_i\", \"0\", \"a_f\", \"1\", \"subject\", \"ha ha bla blah0\")\n        .add(id, \"2\", \"a_s\", \"hello\", \"a_i\", \"2\", \"a_f\", \"2\", \"subject\", \"ha ha bla blah2\")\n        .add(id, \"3\", \"a_s\", \"hello\", \"a_i\", \"3\", \"a_f\", \"3\", \"subject\", \"ha ha bla blah3\")\n        .add(id, \"4\", \"a_s\", \"hello\", \"a_i\", \"4\", \"a_f\", \"4\", \"subject\", \"ha ha bla blah4\")\n        .add(id, \"1\", \"a_s\", \"hello\", \"a_i\", \"1\", \"a_f\", \"5\", \"subject\", \"ha ha bla blah5\")\n        .add(id, \"5\", \"a_s\", \"hello\", \"a_i\", \"10\", \"a_f\", \"6\", \"subject\", \"ha ha bla blah6\")\n        .add(id, \"6\", \"a_s\", \"hello\", \"a_i\", \"11\", \"a_f\", \"7\", \"subject\", \"ha ha bla blah7\")\n        .add(id, \"7\", \"a_s\", \"hello\", \"a_i\", \"12\", \"a_f\", \"8\", \"subject\", \"ha ha bla blah8\")\n        .add(id, \"8\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\", \"subject\", \"ha ha bla blah9\")\n        .add(id, \"9\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\", \"subject\", \"ha ha bla blah10\")\n        .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"topic\", TopicStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"parallel\", ParallelStream.class)\n        .withFunctionName(\"daemon\", DaemonStream.class);\n\n    StreamExpression expression;\n    TupleStream stream;\n    List<Tuple> tuples;\n\n    SolrClientCache cache = new SolrClientCache();\n\n    try {\n      //Store checkpoints in the same index as the main documents. This is perfectly valid\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n                                                         \"workers=\\\"2\\\", \" +\n                                                         \"sort=\\\"_version_ asc\\\",\" +\n                                                         \"topic(collection1, \" +\n                                                               \"collection1, \" +\n                                                               \"q=\\\"a_s:hello\\\", \" +\n                                                               \"fl=\\\"id\\\", \" +\n                                                               \"id=\\\"1000000\\\", \" +\n                                                               \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      StreamContext context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n\n      //Should be zero because the checkpoints will be set to the highest version on the shards.\n      assertEquals(tuples.size(), 0);\n\n      cluster.getSolrClient().commit(\"collection1\");\n      //Now check to see if the checkpoints are present\n\n      expression = StreamExpressionParser.parse(\"search(collection1, q=\\\"id:1000000*\\\", fl=\\\"id, checkpoint_ss, _version_\\\", sort=\\\"id asc\\\")\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      tuples = getTuples(stream);\n      assertEquals(tuples.size(), 2);\n      List<String> checkpoints = tuples.get(0).getStrings(\"checkpoint_ss\");\n      assertEquals(checkpoints.size(), 2);\n      String id1 = tuples.get(0).getString(\"id\");\n      String id2 = tuples.get(1).getString(\"id\");\n      assertTrue(id1.equals(\"1000000_0\"));\n      assertTrue(id2.equals(\"1000000_1\"));\n\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"10\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"11\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"1000000\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicRun(stream, \"10\", \"11\");\n\n      //Test will initial checkpoint. This should pull all\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"a_s:hello\\\", \" +\n          \"fl=\\\"id\\\", \" +\n          \"id=\\\"2000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\");\n\n      //Add more documents\n      //Index a few more documents\n      new UpdateRequest()\n          .add(id, \"12\", \"a_s\", \"hello\", \"a_i\", \"13\", \"a_f\", \"9\")\n          .add(id, \"13\", \"a_s\", \"hello\", \"a_i\", \"14\", \"a_f\", \"10\")\n          .commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n      //Run the same topic again including the initialCheckpoint. It should start where it left off.\n      //initialCheckpoint should be ignored for all but the first run.\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n      assertTopicRun(stream, \"12\", \"13\");\n\n      //Test text extraction\n\n      expression = StreamExpressionParser.parse(\"parallel(collection1, \" +\n          \"workers=\\\"2\\\", \" +\n          \"sort=\\\"_version_ asc\\\",\" +\n          \"topic(collection1, \" +\n          \"collection1, \" +\n          \"q=\\\"subject:bla\\\", \" +\n          \"fl=\\\"subject\\\", \" +\n          \"id=\\\"3000000\\\", \" +\n          \"initialCheckpoint=\\\"0\\\", \" +\n          \"partitionKeys=\\\"id\\\"))\");\n\n      stream = factory.constructStream(expression);\n      context = new StreamContext();\n      context.setSolrClientCache(cache);\n      stream.setStreamContext(context);\n\n      assertTopicSubject(stream, \"ha ha bla blah0\",\n          \"ha ha bla blah1\",\n          \"ha ha bla blah2\",\n          \"ha ha bla blah3\",\n          \"ha ha bla blah4\",\n          \"ha ha bla blah5\",\n          \"ha ha bla blah6\",\n          \"ha ha bla blah7\",\n          \"ha ha bla blah8\",\n          \"ha ha bla blah9\",\n          \"ha ha bla blah10\");\n\n    } finally {\n      cache.close();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":["8c969f15cd04d31e520319c619a445ae21f02d72"],"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108":["c405288c4553ffb50ab8ca5adbdde9881bcec4e4"],"bcbcca681ad951d22b9bb07a88683ce966986ae7":["915201312eef398482b3dffe8a297e1809f3b29a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","8c969f15cd04d31e520319c619a445ae21f02d72"],"c405288c4553ffb50ab8ca5adbdde9881bcec4e4":["8c969f15cd04d31e520319c619a445ae21f02d72"],"915201312eef398482b3dffe8a297e1809f3b29a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f7ba5de6e78dd9b7518e92274a8a4cf8823a9c4a":["05a3c9b5f1dfb39879069eb1dac3ca104d3e4108"],"8c969f15cd04d31e520319c619a445ae21f02d72":["bcbcca681ad951d22b9bb07a88683ce966986ae7"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","bcbcca681ad951d22b9bb07a88683ce966986ae7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f7ba5de6e78dd9b7518e92274a8a4cf8823a9c4a"]},"commit2Childs":{"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":[],"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108":["f7ba5de6e78dd9b7518e92274a8a4cf8823a9c4a"],"bcbcca681ad951d22b9bb07a88683ce966986ae7":["8c969f15cd04d31e520319c619a445ae21f02d72","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["915201312eef398482b3dffe8a297e1809f3b29a","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":[],"c405288c4553ffb50ab8ca5adbdde9881bcec4e4":["05a3c9b5f1dfb39879069eb1dac3ca104d3e4108"],"915201312eef398482b3dffe8a297e1809f3b29a":["bcbcca681ad951d22b9bb07a88683ce966986ae7"],"f7ba5de6e78dd9b7518e92274a8a4cf8823a9c4a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"8c969f15cd04d31e520319c619a445ae21f02d72":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb","a1ef55e1fff7ff44354432770ad8bc19be1fcc75","c405288c4553ffb50ab8ca5adbdde9881bcec4e4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb","a1ef55e1fff7ff44354432770ad8bc19be1fcc75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}