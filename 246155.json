{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","commits":[{"id":"70b55953b6a72596cb534ead735a8b849a473cac","date":1363634568,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingStoredFieldsFormat#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene42Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == NON_COMPRESSING_CODEC) {\n          iwConf.setCodec(CompressingCodec.randomInstance(random()));\n        } else {\n          iwConf.setCodec(NON_COMPRESSING_CODEC);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ce24aa081e44190692bbebc8aead342ad7060e8","date":1374951664,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene42Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene42Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":["70b55953b6a72596cb534ead735a8b849a473cac","7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene42Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene42Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"11a746437bc5c0a0b3df0337ed249c387c812871","date":1376687959,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene45Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene42Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff","date":1377034255,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene45Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene42Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene45Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene42Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8435160e9702b19398118ddf76b61c846612b6a4","date":1380349140,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene46Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene45Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["8106bc60c7452250f84c65cdb43ab6b1d8eb1534"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene46Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.shutdown();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene46Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cfc45818441587d8004ff1a119fb60ac9ecb9a14","date":1401437797,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene46Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.shutdown();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene46Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.shutdown();\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConf.setCodec(Codec.getDefault());\n        } else {\n          iwConf.setCodec(otherCodec);\n        }\n        iw = new RandomIndexWriter(random(), dir, iwConf.clone());\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8106bc60c7452250f84c65cdb43ab6b1d8eb1534","date":1401906364,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene49Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.shutdown();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene46Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.shutdown();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.shutdown();\n    dir.close();\n  }\n\n","bugFix":["8435160e9702b19398118ddf76b61c846612b6a4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene49Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.shutdown();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene49Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.shutdown();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene49Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene49Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.shutdown();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793","date":1408030244,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene410Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene49Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"941b7027a51547b0a38d711bc08ec354f9e2e4a7","date":1411394279,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = new Lucene410Codec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3184874f7f3aca850248483485b4995343066875","date":1413876758,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(null);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a22eafe3f72a4c2945eaad9547e6c78816978f4","date":1413956657,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(null);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexed(false);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2bb2842e561df4e8e9ad89010605fc86ac265465","date":1414768208,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NO);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(null);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f95ce1375367b92d411a06175eab3915fe93c6bc","date":1414788502,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NO);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"770342641f7b505eaa8dccdc666158bff2419109","date":1449868421,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    LegacyIntField id = new LegacyIntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(LegacyNumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntField id = new IntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(NumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","date":1453060490,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    LegacyIntField id = new LegacyIntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(LegacyNumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    LegacyIntField id = new LegacyIntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(LegacyNumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final StoredDocument doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"906cb8d88e3e0d9b0dc30dafb99c128a6b7ad004","date":1457440594,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    LegacyIntField id = new LegacyIntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(LegacyNumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6c3ddb1d93e1f94eb5a00f9bc3e64439807ecf7e","date":1457443460,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    LegacyIntField id = new LegacyIntField(\"id\", 0, Store.YES);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      id.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(LegacyNumericRangeQuery.newIntRange(\"id\", min, max, true, false));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6448f67be45147de82a85cd903fec34e8930da75","date":1477041277,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomNumbers.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomNumbers.randomIntBetween(random(), 1, 500)\n          : RandomNumbers.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"80d0e6d59ae23f4a6f30eaf40bfb40742300287f","date":1477598926,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomNumbers.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomNumbers.randomIntBetween(random(), 1, 500)\n          : RandomNumbers.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n    \n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomInts.randomIntBetween(random(), 1, 500)\n          : RandomInts.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9798d0818e7a880546802b509792d3f3d57babd2","date":1528358901,"type":3,"author":"Nhat Nguyen","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomNumbers.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomNumbers.randomIntBetween(random(), 1, 500)\n          : RandomNumbers.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = BytesRef.deepCopyOf(arr2Ref).bytes;\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomNumbers.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomNumbers.randomIntBetween(random(), 1, 500)\n          : RandomNumbers.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomNumbers.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomNumbers.randomIntBetween(random(), 1, 500)\n          : RandomNumbers.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = BytesRef.deepCopyOf(arr2Ref).bytes;\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomNumbers.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomNumbers.randomIntBetween(random(), 1, 500)\n          : RandomNumbers.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomNumbers.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomNumbers.randomIntBetween(random(), 1, 500)\n          : RandomNumbers.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = BytesRef.deepCopyOf(arr2Ref).bytes;\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomNumbers.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomNumbers.randomIntBetween(random(), 1, 500)\n          : RandomNumbers.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = Arrays.copyOfRange(arr2Ref.bytes, arr2Ref.offset, arr2Ref.offset + arr2Ref.length);\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dbc046116d49cd3d0c50f7169cabaa295bc23a4a","date":1552989114,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase#testWriteReadMerge().mjava","sourceNew":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomNumbers.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomNumbers.randomIntBetween(random(), 1, 500)\n          : RandomNumbers.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = maybeWrapWithMergingReader(DirectoryReader.open(dir));\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = BytesRef.deepCopyOf(arr2Ref).bytes;\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testWriteReadMerge() throws IOException {\n    // get another codec, other than the default: so we are merging segments across different codecs\n    final Codec otherCodec;\n    if (\"SimpleText\".equals(Codec.getDefault().getName())) {\n      otherCodec = TestUtil.getDefaultCodec();\n    } else {\n      otherCodec = new SimpleTextCodec();\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig iwConf = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwConf.setMaxBufferedDocs(RandomNumbers.randomIntBetween(random(), 2, 30));\n    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);\n\n    final int docCount = atLeast(200);\n    final byte[][][] data = new byte [docCount][][];\n    for (int i = 0; i < docCount; ++i) {\n      final int fieldCount = rarely()\n          ? RandomNumbers.randomIntBetween(random(), 1, 500)\n          : RandomNumbers.randomIntBetween(random(), 1, 5);\n      data[i] = new byte[fieldCount][];\n      for (int j = 0; j < fieldCount; ++j) {\n        final int length = rarely()\n            ? random().nextInt(1000)\n            : random().nextInt(10);\n        final int max = rarely() ? 256 : 2;\n        data[i][j] = randomByteArray(length, max);\n      }\n    }\n\n    final FieldType type = new FieldType(StringField.TYPE_STORED);\n    type.setIndexOptions(IndexOptions.NONE);\n    type.freeze();\n    IntPoint id = new IntPoint(\"id\", 0);\n    StoredField idStored = new StoredField(\"id\", 0);\n    for (int i = 0; i < data.length; ++i) {\n      Document doc = new Document();\n      doc.add(id);\n      doc.add(idStored);\n      id.setIntValue(i);\n      idStored.setIntValue(i);\n      for (int j = 0; j < data[i].length; ++j) {\n        Field f = new Field(\"bytes\" + j, data[i][j], type);\n        doc.add(f);\n      }\n      iw.w.addDocument(doc);\n      if (random().nextBoolean() && (i % (data.length / 10) == 0)) {\n        iw.w.close();\n        IndexWriterConfig iwConfNew = newIndexWriterConfig(new MockAnalyzer(random()));\n        // test merging against a non-compressing codec\n        if (iwConf.getCodec() == otherCodec) {\n          iwConfNew.setCodec(Codec.getDefault());\n        } else {\n          iwConfNew.setCodec(otherCodec);\n        }\n        iwConf = iwConfNew;\n        iw = new RandomIndexWriter(random(), dir, iwConf);\n      }\n    }\n\n    for (int i = 0; i < 10; ++i) {\n      final int min = random().nextInt(data.length);\n      final int max = min + random().nextInt(20);\n      iw.deleteDocuments(IntPoint.newRangeQuery(\"id\", min, max-1));\n    }\n\n    iw.forceMerge(2); // force merges with deletions\n\n    iw.commit();\n\n    final DirectoryReader ir = DirectoryReader.open(dir);\n    assertTrue(ir.numDocs() > 0);\n    int numDocs = 0;\n    for (int i = 0; i < ir.maxDoc(); ++i) {\n      final Document doc = ir.document(i);\n      if (doc == null) {\n        continue;\n      }\n      ++ numDocs;\n      final int docId = doc.getField(\"id\").numericValue().intValue();\n      assertEquals(data[docId].length + 1, doc.getFields().size());\n      for (int j = 0; j < data[docId].length; ++j) {\n        final byte[] arr = data[docId][j];\n        final BytesRef arr2Ref = doc.getBinaryValue(\"bytes\" + j);\n        final byte[] arr2 = BytesRef.deepCopyOf(arr2Ref).bytes;\n        assertArrayEquals(arr, arr2);\n      }\n    }\n    assertTrue(ir.numDocs() <= numDocs);\n    ir.close();\n\n    iw.deleteAll();\n    iw.commit();\n    iw.forceMerge(1);\n    \n    iw.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"2bb2842e561df4e8e9ad89010605fc86ac265465":["3184874f7f3aca850248483485b4995343066875"],"906cb8d88e3e0d9b0dc30dafb99c128a6b7ad004":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"4ce24aa081e44190692bbebc8aead342ad7060e8":["70b55953b6a72596cb534ead735a8b849a473cac"],"11a746437bc5c0a0b3df0337ed249c387c812871":["4ce24aa081e44190692bbebc8aead342ad7060e8"],"6448f67be45147de82a85cd903fec34e8930da75":["6c3ddb1d93e1f94eb5a00f9bc3e64439807ecf7e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["6448f67be45147de82a85cd903fec34e8930da75","9798d0818e7a880546802b509792d3f3d57babd2"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["70b55953b6a72596cb534ead735a8b849a473cac"],"70b55953b6a72596cb534ead735a8b849a473cac":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["8435160e9702b19398118ddf76b61c846612b6a4"],"0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["770342641f7b505eaa8dccdc666158bff2419109"],"941b7027a51547b0a38d711bc08ec354f9e2e4a7":["0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["6448f67be45147de82a85cd903fec34e8930da75","9798d0818e7a880546802b509792d3f3d57babd2"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["8106bc60c7452250f84c65cdb43ab6b1d8eb1534"],"cfc45818441587d8004ff1a119fb60ac9ecb9a14":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"dbc046116d49cd3d0c50f7169cabaa295bc23a4a":["9798d0818e7a880546802b509792d3f3d57babd2"],"8435160e9702b19398118ddf76b61c846612b6a4":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"6c3ddb1d93e1f94eb5a00f9bc3e64439807ecf7e":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c","906cb8d88e3e0d9b0dc30dafb99c128a6b7ad004"],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["4ce24aa081e44190692bbebc8aead342ad7060e8","11a746437bc5c0a0b3df0337ed249c387c812871"],"770342641f7b505eaa8dccdc666158bff2419109":["f95ce1375367b92d411a06175eab3915fe93c6bc"],"f95ce1375367b92d411a06175eab3915fe93c6bc":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"3184874f7f3aca850248483485b4995343066875":["941b7027a51547b0a38d711bc08ec354f9e2e4a7"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":["941b7027a51547b0a38d711bc08ec354f9e2e4a7","3184874f7f3aca850248483485b4995343066875"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["6c3ddb1d93e1f94eb5a00f9bc3e64439807ecf7e","6448f67be45147de82a85cd903fec34e8930da75"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"8106bc60c7452250f84c65cdb43ab6b1d8eb1534":["cfc45818441587d8004ff1a119fb60ac9ecb9a14"],"9798d0818e7a880546802b509792d3f3d57babd2":["6448f67be45147de82a85cd903fec34e8930da75"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["dbc046116d49cd3d0c50f7169cabaa295bc23a4a"]},"commit2Childs":{"2bb2842e561df4e8e9ad89010605fc86ac265465":["f95ce1375367b92d411a06175eab3915fe93c6bc"],"906cb8d88e3e0d9b0dc30dafb99c128a6b7ad004":["6c3ddb1d93e1f94eb5a00f9bc3e64439807ecf7e"],"4ce24aa081e44190692bbebc8aead342ad7060e8":["11a746437bc5c0a0b3df0337ed249c387c812871","e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"11a746437bc5c0a0b3df0337ed249c387c812871":["e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff"],"6448f67be45147de82a85cd903fec34e8930da75":["b70042a8a492f7054d480ccdd2be9796510d4327","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","80d0e6d59ae23f4a6f30eaf40bfb40742300287f","9798d0818e7a880546802b509792d3f3d57babd2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["70b55953b6a72596cb534ead735a8b849a473cac"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"70b55953b6a72596cb534ead735a8b849a473cac":["4ce24aa081e44190692bbebc8aead342ad7060e8","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["cfc45818441587d8004ff1a119fb60ac9ecb9a14"],"0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793":["941b7027a51547b0a38d711bc08ec354f9e2e4a7"],"6654c5f3ec2e4a84ef867c82d4eec872c2372c8c":["906cb8d88e3e0d9b0dc30dafb99c128a6b7ad004","6c3ddb1d93e1f94eb5a00f9bc3e64439807ecf7e"],"941b7027a51547b0a38d711bc08ec354f9e2e4a7":["3184874f7f3aca850248483485b4995343066875","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"cfc45818441587d8004ff1a119fb60ac9ecb9a14":["8106bc60c7452250f84c65cdb43ab6b1d8eb1534"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"6c3ddb1d93e1f94eb5a00f9bc3e64439807ecf7e":["6448f67be45147de82a85cd903fec34e8930da75","80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"8435160e9702b19398118ddf76b61c846612b6a4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"dbc046116d49cd3d0c50f7169cabaa295bc23a4a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e70ec9cf78e14cbbf13fd0e1a9aefa8081c325ff":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","8435160e9702b19398118ddf76b61c846612b6a4"],"3184874f7f3aca850248483485b4995343066875":["2bb2842e561df4e8e9ad89010605fc86ac265465","0a22eafe3f72a4c2945eaad9547e6c78816978f4"],"770342641f7b505eaa8dccdc666158bff2419109":["6654c5f3ec2e4a84ef867c82d4eec872c2372c8c"],"f95ce1375367b92d411a06175eab3915fe93c6bc":["770342641f7b505eaa8dccdc666158bff2419109"],"0a22eafe3f72a4c2945eaad9547e6c78816978f4":[],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["0ce5e7f280a7b3f0f96f2623d9f0ce70f742b793"],"8106bc60c7452250f84c65cdb43ab6b1d8eb1534":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"9798d0818e7a880546802b509792d3f3d57babd2":["b70042a8a492f7054d480ccdd2be9796510d4327","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","dbc046116d49cd3d0c50f7169cabaa295bc23a4a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b70042a8a492f7054d480ccdd2be9796510d4327","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","0a22eafe3f72a4c2945eaad9547e6c78816978f4","80d0e6d59ae23f4a6f30eaf40bfb40742300287f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}