{"path":"solr/contrib/analysis-extras/src/java/org/apache/solr/update/processor/OpenNLPExtractNamedEntitiesUpdateProcessorFactory#getInstance(SolrQueryRequest,SolrQueryResponse,UpdateRequestProcessor).mjava","commits":[{"id":"afc5b4b2446e392448f36ae4f5a164540f2ccb65","date":1513355058,"type":0,"author":"Steve Rowe","isMerge":false,"pathNew":"solr/contrib/analysis-extras/src/java/org/apache/solr/update/processor/OpenNLPExtractNamedEntitiesUpdateProcessorFactory#getInstance(SolrQueryRequest,SolrQueryResponse,UpdateRequestProcessor).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public final UpdateRequestProcessor getInstance\n      (SolrQueryRequest req, SolrQueryResponse rsp, UpdateRequestProcessor next) {\n    final FieldNameSelector srcSelector = getSourceSelector();\n    return new UpdateRequestProcessor(next) {\n      private final NLPNERTaggerOp nerTaggerOp;\n      private Analyzer analyzer = null;\n      {\n        try {\n          nerTaggerOp = OpenNLPOpsFactory.getNERTagger(modelFile);\n          FieldType fieldType = req.getSchema().getFieldTypeByName(analyzerFieldType);\n          if (fieldType == null) {\n            throw new SolrException\n                (SERVER_ERROR, ANALYZER_FIELD_TYPE_PARAM + \" '\" + analyzerFieldType + \"' not found in the schema.\");\n          }\n          analyzer = fieldType.getIndexAnalyzer();\n        } catch (IOException e) {\n          throw new IllegalArgumentException(e);\n        }\n      }\n\n      @Override\n      public void processAdd(AddUpdateCommand cmd) throws IOException {\n\n        final SolrInputDocument doc = cmd.getSolrInputDocument();\n\n        // Destination may be regex replace string, or \"{EntityType}\" replaced by\n        // each entity's type, both of which can cause multiple output fields.\n        Map<String,SolrInputField> destMap = new HashMap<>();\n\n        // preserve initial values\n        for (final String fname : doc.getFieldNames()) {\n          if ( ! srcSelector.shouldMutate(fname)) continue;\n\n          Collection<Object> srcFieldValues = doc.getFieldValues(fname);\n          if (srcFieldValues == null || srcFieldValues.isEmpty()) continue;\n\n          String resolvedDest = dest;\n\n          if (pattern != null) {\n            Matcher matcher = pattern.matcher(fname);\n            if (matcher.find()) {\n              resolvedDest = matcher.replaceAll(dest);\n            } else {\n              log.debug(\"srcSelector.shouldMutate(\\\"{}\\\") returned true, \" +\n                  \"but replacement pattern did not match, field skipped.\", fname);\n              continue;\n            }\n          }\n\n          for (Object val : srcFieldValues) {\n            for (Pair<String,String> entity : extractTypedNamedEntities(val)) {\n              SolrInputField destField = null;\n              String entityName = entity.first();\n              String entityType = entity.second();\n              resolvedDest = resolvedDest.replace(ENTITY_TYPE, entityType);\n              if (doc.containsKey(resolvedDest)) {\n                destField = doc.getField(resolvedDest);\n              } else {\n                SolrInputField targetField = destMap.get(resolvedDest);\n                if (targetField == null) {\n                  destField = new SolrInputField(resolvedDest);\n                } else {\n                  destField = targetField;\n                }\n              }\n              destField.addValue(entityName);\n\n              // put it in map to avoid concurrent modification...\n              destMap.put(resolvedDest, destField);\n            }\n          }\n        }\n\n        for (Map.Entry<String,SolrInputField> entry : destMap.entrySet()) {\n          doc.put(entry.getKey(), entry.getValue());\n        }\n        super.processAdd(cmd);\n      }\n\n      /** Using configured NER model, extracts (name, type) pairs from the given source field value */\n      private List<Pair<String,String>> extractTypedNamedEntities(Object srcFieldValue) throws IOException {\n        List<Pair<String,String>> entitiesWithType = new ArrayList<>();\n        List<String> terms = new ArrayList<>();\n        List<Integer> startOffsets = new ArrayList<>();\n        List<Integer> endOffsets = new ArrayList<>();\n        String fullText = srcFieldValue.toString();\n        TokenStream tokenStream = analyzer.tokenStream(\"\", fullText);\n        CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n        FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n        tokenStream.reset();\n        synchronized (nerTaggerOp) {\n          while (tokenStream.incrementToken()) {\n            terms.add(termAtt.toString());\n            startOffsets.add(offsetAtt.startOffset());\n            endOffsets.add(offsetAtt.endOffset());\n            boolean endOfSentence = 0 != (flagsAtt.getFlags() & OpenNLPTokenizer.EOS_FLAG_BIT);\n            if (endOfSentence) {    // extract named entities one sentence at a time\n              extractEntitiesFromSentence(fullText, terms, startOffsets, endOffsets, entitiesWithType);\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n          if (!terms.isEmpty()) { // In case last token of last sentence isn't properly flagged with EOS_FLAG_BIT\n            extractEntitiesFromSentence(fullText, terms, startOffsets, endOffsets, entitiesWithType);\n          }\n          nerTaggerOp.reset();      // Forget all adaptive data collected during previous calls\n        }\n        return entitiesWithType;\n      }\n\n      private void extractEntitiesFromSentence(String fullText, List<String> terms, List<Integer> startOffsets,\n                                               List<Integer> endOffsets, List<Pair<String,String>> entitiesWithType) {\n        for (Span span : nerTaggerOp.getNames(terms.toArray(new String[terms.size()]))) {\n          String text = fullText.substring(startOffsets.get(span.getStart()), endOffsets.get(span.getEnd() - 1));\n          entitiesWithType.add(new Pair<>(text, span.getType()));\n        }\n        terms.clear();\n        startOffsets.clear();\n        endOffsets.clear();\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["5dd459dd6133a495a55874c7c2fda19048ac3207"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5dd459dd6133a495a55874c7c2fda19048ac3207","date":1532332726,"type":3,"author":"koji","isMerge":false,"pathNew":"solr/contrib/analysis-extras/src/java/org/apache/solr/update/processor/OpenNLPExtractNamedEntitiesUpdateProcessorFactory#getInstance(SolrQueryRequest,SolrQueryResponse,UpdateRequestProcessor).mjava","pathOld":"solr/contrib/analysis-extras/src/java/org/apache/solr/update/processor/OpenNLPExtractNamedEntitiesUpdateProcessorFactory#getInstance(SolrQueryRequest,SolrQueryResponse,UpdateRequestProcessor).mjava","sourceNew":"  @Override\n  public final UpdateRequestProcessor getInstance\n      (SolrQueryRequest req, SolrQueryResponse rsp, UpdateRequestProcessor next) {\n    final FieldNameSelector srcSelector = getSourceSelector();\n    return new UpdateRequestProcessor(next) {\n      private final NLPNERTaggerOp nerTaggerOp;\n      private Analyzer analyzer = null;\n      {\n        try {\n          nerTaggerOp = OpenNLPOpsFactory.getNERTagger(modelFile);\n          FieldType fieldType = req.getSchema().getFieldTypeByName(analyzerFieldType);\n          if (fieldType == null) {\n            throw new SolrException\n                (SERVER_ERROR, ANALYZER_FIELD_TYPE_PARAM + \" '\" + analyzerFieldType + \"' not found in the schema.\");\n          }\n          analyzer = fieldType.getIndexAnalyzer();\n        } catch (IOException e) {\n          throw new IllegalArgumentException(e);\n        }\n      }\n\n      @Override\n      public void processAdd(AddUpdateCommand cmd) throws IOException {\n\n        final SolrInputDocument doc = cmd.getSolrInputDocument();\n\n        // Destination may be regex replace string, or \"{EntityType}\" replaced by\n        // each entity's type, both of which can cause multiple output fields.\n        Map<String,SolrInputField> destMap = new HashMap<>();\n\n        // preserve initial values\n        for (final String fname : doc.getFieldNames()) {\n          if ( ! srcSelector.shouldMutate(fname)) continue;\n\n          Collection<Object> srcFieldValues = doc.getFieldValues(fname);\n          if (srcFieldValues == null || srcFieldValues.isEmpty()) continue;\n\n          String resolvedDest = dest;\n\n          if (pattern != null) {\n            Matcher matcher = pattern.matcher(fname);\n            if (matcher.find()) {\n              resolvedDest = matcher.replaceAll(dest);\n            } else {\n              log.debug(\"srcSelector.shouldMutate(\\\"{}\\\") returned true, \" +\n                  \"but replacement pattern did not match, field skipped.\", fname);\n              continue;\n            }\n          }\n\n          for (Object val : srcFieldValues) {\n            for (Pair<String,String> entity : extractTypedNamedEntities(val)) {\n              SolrInputField destField = null;\n              String entityName = entity.first();\n              String entityType = entity.second();\n              final String resolved = resolvedDest.replace(ENTITY_TYPE, entityType);\n              if (doc.containsKey(resolved)) {\n                destField = doc.getField(resolved);\n              } else {\n                SolrInputField targetField = destMap.get(resolved);\n                if (targetField == null) {\n                  destField = new SolrInputField(resolved);\n                } else {\n                  destField = targetField;\n                }\n              }\n              destField.addValue(entityName);\n\n              // put it in map to avoid concurrent modification...\n              destMap.put(resolved, destField);\n            }\n          }\n        }\n\n        for (Map.Entry<String,SolrInputField> entry : destMap.entrySet()) {\n          doc.put(entry.getKey(), entry.getValue());\n        }\n        super.processAdd(cmd);\n      }\n\n      /** Using configured NER model, extracts (name, type) pairs from the given source field value */\n      private List<Pair<String,String>> extractTypedNamedEntities(Object srcFieldValue) throws IOException {\n        List<Pair<String,String>> entitiesWithType = new ArrayList<>();\n        List<String> terms = new ArrayList<>();\n        List<Integer> startOffsets = new ArrayList<>();\n        List<Integer> endOffsets = new ArrayList<>();\n        String fullText = srcFieldValue.toString();\n        TokenStream tokenStream = analyzer.tokenStream(\"\", fullText);\n        CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n        FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n        tokenStream.reset();\n        synchronized (nerTaggerOp) {\n          while (tokenStream.incrementToken()) {\n            terms.add(termAtt.toString());\n            startOffsets.add(offsetAtt.startOffset());\n            endOffsets.add(offsetAtt.endOffset());\n            boolean endOfSentence = 0 != (flagsAtt.getFlags() & OpenNLPTokenizer.EOS_FLAG_BIT);\n            if (endOfSentence) {    // extract named entities one sentence at a time\n              extractEntitiesFromSentence(fullText, terms, startOffsets, endOffsets, entitiesWithType);\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n          if (!terms.isEmpty()) { // In case last token of last sentence isn't properly flagged with EOS_FLAG_BIT\n            extractEntitiesFromSentence(fullText, terms, startOffsets, endOffsets, entitiesWithType);\n          }\n          nerTaggerOp.reset();      // Forget all adaptive data collected during previous calls\n        }\n        return entitiesWithType;\n      }\n\n      private void extractEntitiesFromSentence(String fullText, List<String> terms, List<Integer> startOffsets,\n                                               List<Integer> endOffsets, List<Pair<String,String>> entitiesWithType) {\n        for (Span span : nerTaggerOp.getNames(terms.toArray(new String[terms.size()]))) {\n          String text = fullText.substring(startOffsets.get(span.getStart()), endOffsets.get(span.getEnd() - 1));\n          entitiesWithType.add(new Pair<>(text, span.getType()));\n        }\n        terms.clear();\n        startOffsets.clear();\n        endOffsets.clear();\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public final UpdateRequestProcessor getInstance\n      (SolrQueryRequest req, SolrQueryResponse rsp, UpdateRequestProcessor next) {\n    final FieldNameSelector srcSelector = getSourceSelector();\n    return new UpdateRequestProcessor(next) {\n      private final NLPNERTaggerOp nerTaggerOp;\n      private Analyzer analyzer = null;\n      {\n        try {\n          nerTaggerOp = OpenNLPOpsFactory.getNERTagger(modelFile);\n          FieldType fieldType = req.getSchema().getFieldTypeByName(analyzerFieldType);\n          if (fieldType == null) {\n            throw new SolrException\n                (SERVER_ERROR, ANALYZER_FIELD_TYPE_PARAM + \" '\" + analyzerFieldType + \"' not found in the schema.\");\n          }\n          analyzer = fieldType.getIndexAnalyzer();\n        } catch (IOException e) {\n          throw new IllegalArgumentException(e);\n        }\n      }\n\n      @Override\n      public void processAdd(AddUpdateCommand cmd) throws IOException {\n\n        final SolrInputDocument doc = cmd.getSolrInputDocument();\n\n        // Destination may be regex replace string, or \"{EntityType}\" replaced by\n        // each entity's type, both of which can cause multiple output fields.\n        Map<String,SolrInputField> destMap = new HashMap<>();\n\n        // preserve initial values\n        for (final String fname : doc.getFieldNames()) {\n          if ( ! srcSelector.shouldMutate(fname)) continue;\n\n          Collection<Object> srcFieldValues = doc.getFieldValues(fname);\n          if (srcFieldValues == null || srcFieldValues.isEmpty()) continue;\n\n          String resolvedDest = dest;\n\n          if (pattern != null) {\n            Matcher matcher = pattern.matcher(fname);\n            if (matcher.find()) {\n              resolvedDest = matcher.replaceAll(dest);\n            } else {\n              log.debug(\"srcSelector.shouldMutate(\\\"{}\\\") returned true, \" +\n                  \"but replacement pattern did not match, field skipped.\", fname);\n              continue;\n            }\n          }\n\n          for (Object val : srcFieldValues) {\n            for (Pair<String,String> entity : extractTypedNamedEntities(val)) {\n              SolrInputField destField = null;\n              String entityName = entity.first();\n              String entityType = entity.second();\n              resolvedDest = resolvedDest.replace(ENTITY_TYPE, entityType);\n              if (doc.containsKey(resolvedDest)) {\n                destField = doc.getField(resolvedDest);\n              } else {\n                SolrInputField targetField = destMap.get(resolvedDest);\n                if (targetField == null) {\n                  destField = new SolrInputField(resolvedDest);\n                } else {\n                  destField = targetField;\n                }\n              }\n              destField.addValue(entityName);\n\n              // put it in map to avoid concurrent modification...\n              destMap.put(resolvedDest, destField);\n            }\n          }\n        }\n\n        for (Map.Entry<String,SolrInputField> entry : destMap.entrySet()) {\n          doc.put(entry.getKey(), entry.getValue());\n        }\n        super.processAdd(cmd);\n      }\n\n      /** Using configured NER model, extracts (name, type) pairs from the given source field value */\n      private List<Pair<String,String>> extractTypedNamedEntities(Object srcFieldValue) throws IOException {\n        List<Pair<String,String>> entitiesWithType = new ArrayList<>();\n        List<String> terms = new ArrayList<>();\n        List<Integer> startOffsets = new ArrayList<>();\n        List<Integer> endOffsets = new ArrayList<>();\n        String fullText = srcFieldValue.toString();\n        TokenStream tokenStream = analyzer.tokenStream(\"\", fullText);\n        CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n        FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n        tokenStream.reset();\n        synchronized (nerTaggerOp) {\n          while (tokenStream.incrementToken()) {\n            terms.add(termAtt.toString());\n            startOffsets.add(offsetAtt.startOffset());\n            endOffsets.add(offsetAtt.endOffset());\n            boolean endOfSentence = 0 != (flagsAtt.getFlags() & OpenNLPTokenizer.EOS_FLAG_BIT);\n            if (endOfSentence) {    // extract named entities one sentence at a time\n              extractEntitiesFromSentence(fullText, terms, startOffsets, endOffsets, entitiesWithType);\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n          if (!terms.isEmpty()) { // In case last token of last sentence isn't properly flagged with EOS_FLAG_BIT\n            extractEntitiesFromSentence(fullText, terms, startOffsets, endOffsets, entitiesWithType);\n          }\n          nerTaggerOp.reset();      // Forget all adaptive data collected during previous calls\n        }\n        return entitiesWithType;\n      }\n\n      private void extractEntitiesFromSentence(String fullText, List<String> terms, List<Integer> startOffsets,\n                                               List<Integer> endOffsets, List<Pair<String,String>> entitiesWithType) {\n        for (Span span : nerTaggerOp.getNames(terms.toArray(new String[terms.size()]))) {\n          String text = fullText.substring(startOffsets.get(span.getStart()), endOffsets.get(span.getEnd() - 1));\n          entitiesWithType.add(new Pair<>(text, span.getType()));\n        }\n        terms.clear();\n        startOffsets.clear();\n        endOffsets.clear();\n      }\n    };\n  }\n\n","bugFix":["afc5b4b2446e392448f36ae4f5a164540f2ccb65"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55b3f58cbc7d70156d61fad1f2c0ef2dd2d97854","date":1588352817,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/contrib/analysis-extras/src/java/org/apache/solr/update/processor/OpenNLPExtractNamedEntitiesUpdateProcessorFactory#getInstance(SolrQueryRequest,SolrQueryResponse,UpdateRequestProcessor).mjava","pathOld":"solr/contrib/analysis-extras/src/java/org/apache/solr/update/processor/OpenNLPExtractNamedEntitiesUpdateProcessorFactory#getInstance(SolrQueryRequest,SolrQueryResponse,UpdateRequestProcessor).mjava","sourceNew":"  @Override\n  public final UpdateRequestProcessor getInstance\n      (SolrQueryRequest req, SolrQueryResponse rsp, UpdateRequestProcessor next) {\n    final FieldNameSelector srcSelector = getSourceSelector();\n    return new UpdateRequestProcessor(next) {\n      private final NLPNERTaggerOp nerTaggerOp;\n      private Analyzer analyzer = null;\n      {\n        try {\n          nerTaggerOp = OpenNLPOpsFactory.getNERTagger(modelFile);\n          FieldType fieldType = req.getSchema().getFieldTypeByName(analyzerFieldType);\n          if (fieldType == null) {\n            throw new SolrException\n                (SERVER_ERROR, ANALYZER_FIELD_TYPE_PARAM + \" '\" + analyzerFieldType + \"' not found in the schema.\");\n          }\n          analyzer = fieldType.getIndexAnalyzer();\n        } catch (IOException e) {\n          throw new IllegalArgumentException(e);\n        }\n      }\n\n      @Override\n      public void processAdd(AddUpdateCommand cmd) throws IOException {\n\n        final SolrInputDocument doc = cmd.getSolrInputDocument();\n\n        // Destination may be regex replace string, or \"{EntityType}\" replaced by\n        // each entity's type, both of which can cause multiple output fields.\n        Map<String,SolrInputField> destMap = new HashMap<>();\n\n        // preserve initial values\n        for (final String fname : doc.getFieldNames()) {\n          if ( ! srcSelector.shouldMutate(fname)) continue;\n\n          Collection<Object> srcFieldValues = doc.getFieldValues(fname);\n          if (srcFieldValues == null || srcFieldValues.isEmpty()) continue;\n\n          String resolvedDest = dest;\n\n          if (pattern != null) {\n            Matcher matcher = pattern.matcher(fname);\n            if (matcher.find()) {\n              resolvedDest = matcher.replaceAll(dest);\n            } else {\n              log.debug(\"srcSelector.shouldMutate('{}') returned true, \" +\n                  \"but replacement pattern did not match, field skipped.\", fname);\n              continue;\n            }\n          }\n\n          for (Object val : srcFieldValues) {\n            for (Pair<String,String> entity : extractTypedNamedEntities(val)) {\n              SolrInputField destField = null;\n              String entityName = entity.first();\n              String entityType = entity.second();\n              final String resolved = resolvedDest.replace(ENTITY_TYPE, entityType);\n              if (doc.containsKey(resolved)) {\n                destField = doc.getField(resolved);\n              } else {\n                SolrInputField targetField = destMap.get(resolved);\n                if (targetField == null) {\n                  destField = new SolrInputField(resolved);\n                } else {\n                  destField = targetField;\n                }\n              }\n              destField.addValue(entityName);\n\n              // put it in map to avoid concurrent modification...\n              destMap.put(resolved, destField);\n            }\n          }\n        }\n\n        for (Map.Entry<String,SolrInputField> entry : destMap.entrySet()) {\n          doc.put(entry.getKey(), entry.getValue());\n        }\n        super.processAdd(cmd);\n      }\n\n      /** Using configured NER model, extracts (name, type) pairs from the given source field value */\n      private List<Pair<String,String>> extractTypedNamedEntities(Object srcFieldValue) throws IOException {\n        List<Pair<String,String>> entitiesWithType = new ArrayList<>();\n        List<String> terms = new ArrayList<>();\n        List<Integer> startOffsets = new ArrayList<>();\n        List<Integer> endOffsets = new ArrayList<>();\n        String fullText = srcFieldValue.toString();\n        TokenStream tokenStream = analyzer.tokenStream(\"\", fullText);\n        CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n        FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n        tokenStream.reset();\n        synchronized (nerTaggerOp) {\n          while (tokenStream.incrementToken()) {\n            terms.add(termAtt.toString());\n            startOffsets.add(offsetAtt.startOffset());\n            endOffsets.add(offsetAtt.endOffset());\n            boolean endOfSentence = 0 != (flagsAtt.getFlags() & OpenNLPTokenizer.EOS_FLAG_BIT);\n            if (endOfSentence) {    // extract named entities one sentence at a time\n              extractEntitiesFromSentence(fullText, terms, startOffsets, endOffsets, entitiesWithType);\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n          if (!terms.isEmpty()) { // In case last token of last sentence isn't properly flagged with EOS_FLAG_BIT\n            extractEntitiesFromSentence(fullText, terms, startOffsets, endOffsets, entitiesWithType);\n          }\n          nerTaggerOp.reset();      // Forget all adaptive data collected during previous calls\n        }\n        return entitiesWithType;\n      }\n\n      private void extractEntitiesFromSentence(String fullText, List<String> terms, List<Integer> startOffsets,\n                                               List<Integer> endOffsets, List<Pair<String,String>> entitiesWithType) {\n        for (Span span : nerTaggerOp.getNames(terms.toArray(new String[terms.size()]))) {\n          String text = fullText.substring(startOffsets.get(span.getStart()), endOffsets.get(span.getEnd() - 1));\n          entitiesWithType.add(new Pair<>(text, span.getType()));\n        }\n        terms.clear();\n        startOffsets.clear();\n        endOffsets.clear();\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public final UpdateRequestProcessor getInstance\n      (SolrQueryRequest req, SolrQueryResponse rsp, UpdateRequestProcessor next) {\n    final FieldNameSelector srcSelector = getSourceSelector();\n    return new UpdateRequestProcessor(next) {\n      private final NLPNERTaggerOp nerTaggerOp;\n      private Analyzer analyzer = null;\n      {\n        try {\n          nerTaggerOp = OpenNLPOpsFactory.getNERTagger(modelFile);\n          FieldType fieldType = req.getSchema().getFieldTypeByName(analyzerFieldType);\n          if (fieldType == null) {\n            throw new SolrException\n                (SERVER_ERROR, ANALYZER_FIELD_TYPE_PARAM + \" '\" + analyzerFieldType + \"' not found in the schema.\");\n          }\n          analyzer = fieldType.getIndexAnalyzer();\n        } catch (IOException e) {\n          throw new IllegalArgumentException(e);\n        }\n      }\n\n      @Override\n      public void processAdd(AddUpdateCommand cmd) throws IOException {\n\n        final SolrInputDocument doc = cmd.getSolrInputDocument();\n\n        // Destination may be regex replace string, or \"{EntityType}\" replaced by\n        // each entity's type, both of which can cause multiple output fields.\n        Map<String,SolrInputField> destMap = new HashMap<>();\n\n        // preserve initial values\n        for (final String fname : doc.getFieldNames()) {\n          if ( ! srcSelector.shouldMutate(fname)) continue;\n\n          Collection<Object> srcFieldValues = doc.getFieldValues(fname);\n          if (srcFieldValues == null || srcFieldValues.isEmpty()) continue;\n\n          String resolvedDest = dest;\n\n          if (pattern != null) {\n            Matcher matcher = pattern.matcher(fname);\n            if (matcher.find()) {\n              resolvedDest = matcher.replaceAll(dest);\n            } else {\n              log.debug(\"srcSelector.shouldMutate(\\\"{}\\\") returned true, \" +\n                  \"but replacement pattern did not match, field skipped.\", fname);\n              continue;\n            }\n          }\n\n          for (Object val : srcFieldValues) {\n            for (Pair<String,String> entity : extractTypedNamedEntities(val)) {\n              SolrInputField destField = null;\n              String entityName = entity.first();\n              String entityType = entity.second();\n              final String resolved = resolvedDest.replace(ENTITY_TYPE, entityType);\n              if (doc.containsKey(resolved)) {\n                destField = doc.getField(resolved);\n              } else {\n                SolrInputField targetField = destMap.get(resolved);\n                if (targetField == null) {\n                  destField = new SolrInputField(resolved);\n                } else {\n                  destField = targetField;\n                }\n              }\n              destField.addValue(entityName);\n\n              // put it in map to avoid concurrent modification...\n              destMap.put(resolved, destField);\n            }\n          }\n        }\n\n        for (Map.Entry<String,SolrInputField> entry : destMap.entrySet()) {\n          doc.put(entry.getKey(), entry.getValue());\n        }\n        super.processAdd(cmd);\n      }\n\n      /** Using configured NER model, extracts (name, type) pairs from the given source field value */\n      private List<Pair<String,String>> extractTypedNamedEntities(Object srcFieldValue) throws IOException {\n        List<Pair<String,String>> entitiesWithType = new ArrayList<>();\n        List<String> terms = new ArrayList<>();\n        List<Integer> startOffsets = new ArrayList<>();\n        List<Integer> endOffsets = new ArrayList<>();\n        String fullText = srcFieldValue.toString();\n        TokenStream tokenStream = analyzer.tokenStream(\"\", fullText);\n        CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n        OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n        FlagsAttribute flagsAtt = tokenStream.addAttribute(FlagsAttribute.class);\n        tokenStream.reset();\n        synchronized (nerTaggerOp) {\n          while (tokenStream.incrementToken()) {\n            terms.add(termAtt.toString());\n            startOffsets.add(offsetAtt.startOffset());\n            endOffsets.add(offsetAtt.endOffset());\n            boolean endOfSentence = 0 != (flagsAtt.getFlags() & OpenNLPTokenizer.EOS_FLAG_BIT);\n            if (endOfSentence) {    // extract named entities one sentence at a time\n              extractEntitiesFromSentence(fullText, terms, startOffsets, endOffsets, entitiesWithType);\n            }\n          }\n          tokenStream.end();\n          tokenStream.close();\n          if (!terms.isEmpty()) { // In case last token of last sentence isn't properly flagged with EOS_FLAG_BIT\n            extractEntitiesFromSentence(fullText, terms, startOffsets, endOffsets, entitiesWithType);\n          }\n          nerTaggerOp.reset();      // Forget all adaptive data collected during previous calls\n        }\n        return entitiesWithType;\n      }\n\n      private void extractEntitiesFromSentence(String fullText, List<String> terms, List<Integer> startOffsets,\n                                               List<Integer> endOffsets, List<Pair<String,String>> entitiesWithType) {\n        for (Span span : nerTaggerOp.getNames(terms.toArray(new String[terms.size()]))) {\n          String text = fullText.substring(startOffsets.get(span.getStart()), endOffsets.get(span.getEnd() - 1));\n          entitiesWithType.add(new Pair<>(text, span.getType()));\n        }\n        terms.clear();\n        startOffsets.clear();\n        endOffsets.clear();\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5dd459dd6133a495a55874c7c2fda19048ac3207":["afc5b4b2446e392448f36ae4f5a164540f2ccb65"],"55b3f58cbc7d70156d61fad1f2c0ef2dd2d97854":["5dd459dd6133a495a55874c7c2fda19048ac3207"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["55b3f58cbc7d70156d61fad1f2c0ef2dd2d97854"],"afc5b4b2446e392448f36ae4f5a164540f2ccb65":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["afc5b4b2446e392448f36ae4f5a164540f2ccb65"],"5dd459dd6133a495a55874c7c2fda19048ac3207":["55b3f58cbc7d70156d61fad1f2c0ef2dd2d97854"],"55b3f58cbc7d70156d61fad1f2c0ef2dd2d97854":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"afc5b4b2446e392448f36ae4f5a164540f2ccb65":["5dd459dd6133a495a55874c7c2fda19048ac3207"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}