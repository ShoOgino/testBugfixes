{"path":"src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory#testIndexingAnalysis().mjava","commits":[{"id":"f66fbc07ecf3707c1ec81e35ca9e4df7ff22101a","date":1252677016,"type":0,"author":"Grant Ingersoll","isMerge":false,"pathNew":"src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory#testIndexingAnalysis().mjava","pathOld":"/dev/null","sourceNew":"  public void testIndexingAnalysis() throws Exception {\n    Analyzer a = schema.getAnalyzer();\n    String text = \"one two three\";\n    String expected1 = \"one \\u0001eno two \\u0001owt three \\u0001eerht\";\n    List<Token> expectedTokens1 = getTokens(\n            new WhitespaceTokenizer(new StringReader(expected1)));\n    // set positionIncrements and offsets in expected tokens\n    for (int i = 1; i < expectedTokens1.size(); i += 2) {\n      Token t = expectedTokens1.get(i);\n      t.setPositionIncrement(0);\n    }\n    String expected2 = \"\\u0001eno \\u0001owt \\u0001eerht\";\n    List<Token> expectedTokens2 = getTokens(\n            new WhitespaceTokenizer(new StringReader(expected2)));\n    String expected3 = \"one two three\";\n    List<Token> expectedTokens3 = getTokens(\n            new WhitespaceTokenizer(new StringReader(expected3)));\n    // field one\n    TokenStream input = a.tokenStream(\"one\", new StringReader(text));\n    List<Token> realTokens = getTokens(input);\n    assertTokEqual(realTokens, expectedTokens1);\n    // field two\n    input = a.tokenStream(\"two\", new StringReader(text));\n    realTokens = getTokens(input);\n    assertTokEqual(realTokens, expectedTokens2);\n    // field three\n    input = a.tokenStream(\"three\", new StringReader(text));\n    realTokens = getTokens(input);\n    assertTokEqual(realTokens, expectedTokens3);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["4ea4e0df42b1e8d32da66955b10083ed12ba7cc4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4ea4e0df42b1e8d32da66955b10083ed12ba7cc4","date":1258992545,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory#testIndexingAnalysis().mjava","pathOld":"src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory#testIndexingAnalysis().mjava","sourceNew":"  public void testIndexingAnalysis() throws Exception {\n    Analyzer a = schema.getAnalyzer();\n    String text = \"one two three si\\uD834\\uDD1Ex\";\n    String expected1 = \"one \\u0001eno two \\u0001owt three \\u0001eerht si\\uD834\\uDD1Ex \\u0001x\\uD834\\uDD1Eis\";\n    List<Token> expectedTokens1 = getTokens(\n            new WhitespaceTokenizer(new StringReader(expected1)));\n    // set positionIncrements and offsets in expected tokens\n    for (int i = 1; i < expectedTokens1.size(); i += 2) {\n      Token t = expectedTokens1.get(i);\n      t.setPositionIncrement(0);\n    }\n    String expected2 = \"\\u0001eno \\u0001owt \\u0001eerht \\u0001x\\uD834\\uDD1Eis\";\n    List<Token> expectedTokens2 = getTokens(\n            new WhitespaceTokenizer(new StringReader(expected2)));\n    String expected3 = \"one two three si\\uD834\\uDD1Ex\";\n    List<Token> expectedTokens3 = getTokens(\n            new WhitespaceTokenizer(new StringReader(expected3)));\n    // field one\n    TokenStream input = a.tokenStream(\"one\", new StringReader(text));\n    List<Token> realTokens = getTokens(input);\n    assertTokEqual(realTokens, expectedTokens1);\n    // field two\n    input = a.tokenStream(\"two\", new StringReader(text));\n    realTokens = getTokens(input);\n    assertTokEqual(realTokens, expectedTokens2);\n    // field three\n    input = a.tokenStream(\"three\", new StringReader(text));\n    realTokens = getTokens(input);\n    assertTokEqual(realTokens, expectedTokens3);\n  }\n\n","sourceOld":"  public void testIndexingAnalysis() throws Exception {\n    Analyzer a = schema.getAnalyzer();\n    String text = \"one two three\";\n    String expected1 = \"one \\u0001eno two \\u0001owt three \\u0001eerht\";\n    List<Token> expectedTokens1 = getTokens(\n            new WhitespaceTokenizer(new StringReader(expected1)));\n    // set positionIncrements and offsets in expected tokens\n    for (int i = 1; i < expectedTokens1.size(); i += 2) {\n      Token t = expectedTokens1.get(i);\n      t.setPositionIncrement(0);\n    }\n    String expected2 = \"\\u0001eno \\u0001owt \\u0001eerht\";\n    List<Token> expectedTokens2 = getTokens(\n            new WhitespaceTokenizer(new StringReader(expected2)));\n    String expected3 = \"one two three\";\n    List<Token> expectedTokens3 = getTokens(\n            new WhitespaceTokenizer(new StringReader(expected3)));\n    // field one\n    TokenStream input = a.tokenStream(\"one\", new StringReader(text));\n    List<Token> realTokens = getTokens(input);\n    assertTokEqual(realTokens, expectedTokens1);\n    // field two\n    input = a.tokenStream(\"two\", new StringReader(text));\n    realTokens = getTokens(input);\n    assertTokEqual(realTokens, expectedTokens2);\n    // field three\n    input = a.tokenStream(\"three\", new StringReader(text));\n    realTokens = getTokens(input);\n    assertTokEqual(realTokens, expectedTokens3);\n  }\n\n","bugFix":["f66fbc07ecf3707c1ec81e35ca9e4df7ff22101a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2fd023a662cc25ae7e0ad0f33d71c476a16d0579","date":1261403630,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory#testIndexingAnalysis().mjava","pathOld":"src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory#testIndexingAnalysis().mjava","sourceNew":"  public void testIndexingAnalysis() throws Exception {\n    Analyzer a = schema.getAnalyzer();\n    String text = \"one two three si\\uD834\\uDD1Ex\";\n\n    // field one\n    TokenStream input = a.tokenStream(\"one\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"\\u0001eno\", \"one\", \"\\u0001owt\", \"two\", \n          \"\\u0001eerht\", \"three\", \"\\u0001x\\uD834\\uDD1Eis\", \"si\\uD834\\uDD1Ex\" },\n        new int[] { 0, 0, 4, 4, 8, 8, 14, 14 },\n        new int[] { 3, 3, 7, 7, 13, 13, 19, 19 },\n        new int[] { 1, 0, 1, 0, 1, 0, 1, 0 }\n    );\n    // field two\n    input = a.tokenStream(\"two\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"\\u0001eno\", \"\\u0001owt\", \n          \"\\u0001eerht\", \"\\u0001x\\uD834\\uDD1Eis\" },\n        new int[] { 0, 4, 8, 14 },\n        new int[] { 3, 7, 13, 19 },\n        new int[] { 1, 1, 1, 1 }\n    );\n    // field three\n    input = a.tokenStream(\"three\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"one\", \"two\", \"three\", \"si\\uD834\\uDD1Ex\" },\n        new int[] { 0, 4, 8, 14 },\n        new int[] { 3, 7, 13, 19 },\n        new int[] { 1, 1, 1, 1 }\n    );\n  }\n\n","sourceOld":"  public void testIndexingAnalysis() throws Exception {\n    Analyzer a = schema.getAnalyzer();\n    String text = \"one two three si\\uD834\\uDD1Ex\";\n    String expected1 = \"one \\u0001eno two \\u0001owt three \\u0001eerht si\\uD834\\uDD1Ex \\u0001x\\uD834\\uDD1Eis\";\n    List<Token> expectedTokens1 = getTokens(\n            new WhitespaceTokenizer(new StringReader(expected1)));\n    // set positionIncrements and offsets in expected tokens\n    for (int i = 1; i < expectedTokens1.size(); i += 2) {\n      Token t = expectedTokens1.get(i);\n      t.setPositionIncrement(0);\n    }\n    String expected2 = \"\\u0001eno \\u0001owt \\u0001eerht \\u0001x\\uD834\\uDD1Eis\";\n    List<Token> expectedTokens2 = getTokens(\n            new WhitespaceTokenizer(new StringReader(expected2)));\n    String expected3 = \"one two three si\\uD834\\uDD1Ex\";\n    List<Token> expectedTokens3 = getTokens(\n            new WhitespaceTokenizer(new StringReader(expected3)));\n    // field one\n    TokenStream input = a.tokenStream(\"one\", new StringReader(text));\n    List<Token> realTokens = getTokens(input);\n    assertTokEqual(realTokens, expectedTokens1);\n    // field two\n    input = a.tokenStream(\"two\", new StringReader(text));\n    realTokens = getTokens(input);\n    assertTokEqual(realTokens, expectedTokens2);\n    // field three\n    input = a.tokenStream(\"three\", new StringReader(text));\n    realTokens = getTokens(input);\n    assertTokEqual(realTokens, expectedTokens3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"434d3dd2a84fe27b3101b21528019c74c8534e03","date":1268610420,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory#testIndexingAnalysis().mjava","pathOld":"src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory#testIndexingAnalysis().mjava","sourceNew":"  public void testIndexingAnalysis() throws Exception {\n    Analyzer a = schema.getAnalyzer();\n    String text = \"one two three si\\uD834\\uDD1Ex\";\n\n    // field one\n    TokenStream input = a.tokenStream(\"one\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"\\u0001eno\", \"one\", \"\\u0001owt\", \"two\", \n          \"\\u0001eerht\", \"three\", \"\\u0001x\\uD834\\uDD1Eis\", \"si\\uD834\\uDD1Ex\" },\n        new int[] { 0, 0, 4, 4, 8, 8, 14, 14 },\n        new int[] { 3, 3, 7, 7, 13, 13, 19, 19 },\n        new int[] { 1, 0, 1, 0, 1, 0, 1, 0 }\n    );\n    // field two\n    input = a.tokenStream(\"two\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"\\u0001eno\", \"\\u0001owt\", \n          \"\\u0001eerht\", \"\\u0001x\\uD834\\uDD1Eis\" },\n        new int[] { 0, 4, 8, 14 },\n        new int[] { 3, 7, 13, 19 },\n        new int[] { 1, 1, 1, 1 }\n    );\n    // field three\n    input = a.tokenStream(\"three\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"one\", \"two\", \"three\", \"si\\uD834\\uDD1Ex\" },\n        new int[] { 0, 4, 8, 14 },\n        new int[] { 3, 7, 13, 19 }\n    );\n  }\n\n","sourceOld":"  public void testIndexingAnalysis() throws Exception {\n    Analyzer a = schema.getAnalyzer();\n    String text = \"one two three si\\uD834\\uDD1Ex\";\n\n    // field one\n    TokenStream input = a.tokenStream(\"one\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"\\u0001eno\", \"one\", \"\\u0001owt\", \"two\", \n          \"\\u0001eerht\", \"three\", \"\\u0001x\\uD834\\uDD1Eis\", \"si\\uD834\\uDD1Ex\" },\n        new int[] { 0, 0, 4, 4, 8, 8, 14, 14 },\n        new int[] { 3, 3, 7, 7, 13, 13, 19, 19 },\n        new int[] { 1, 0, 1, 0, 1, 0, 1, 0 }\n    );\n    // field two\n    input = a.tokenStream(\"two\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"\\u0001eno\", \"\\u0001owt\", \n          \"\\u0001eerht\", \"\\u0001x\\uD834\\uDD1Eis\" },\n        new int[] { 0, 4, 8, 14 },\n        new int[] { 3, 7, 13, 19 },\n        new int[] { 1, 1, 1, 1 }\n    );\n    // field three\n    input = a.tokenStream(\"three\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"one\", \"two\", \"three\", \"si\\uD834\\uDD1Ex\" },\n        new int[] { 0, 4, 8, 14 },\n        new int[] { 3, 7, 13, 19 },\n        new int[] { 1, 1, 1, 1 }\n    );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory#testIndexingAnalysis().mjava","pathOld":"src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory#testIndexingAnalysis().mjava","sourceNew":"  public void testIndexingAnalysis() throws Exception {\n    Analyzer a = schema.getAnalyzer();\n    String text = \"one two three si\\uD834\\uDD1Ex\";\n\n    // field one\n    TokenStream input = a.tokenStream(\"one\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"\\u0001eno\", \"one\", \"\\u0001owt\", \"two\", \n          \"\\u0001eerht\", \"three\", \"\\u0001x\\uD834\\uDD1Eis\", \"si\\uD834\\uDD1Ex\" },\n        new int[] { 0, 0, 4, 4, 8, 8, 14, 14 },\n        new int[] { 3, 3, 7, 7, 13, 13, 19, 19 },\n        new int[] { 1, 0, 1, 0, 1, 0, 1, 0 }\n    );\n    // field two\n    input = a.tokenStream(\"two\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"\\u0001eno\", \"\\u0001owt\", \n          \"\\u0001eerht\", \"\\u0001x\\uD834\\uDD1Eis\" },\n        new int[] { 0, 4, 8, 14 },\n        new int[] { 3, 7, 13, 19 },\n        new int[] { 1, 1, 1, 1 }\n    );\n    // field three\n    input = a.tokenStream(\"three\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"one\", \"two\", \"three\", \"si\\uD834\\uDD1Ex\" },\n        new int[] { 0, 4, 8, 14 },\n        new int[] { 3, 7, 13, 19 }\n    );\n  }\n\n","sourceOld":"  public void testIndexingAnalysis() throws Exception {\n    Analyzer a = schema.getAnalyzer();\n    String text = \"one two three si\\uD834\\uDD1Ex\";\n\n    // field one\n    TokenStream input = a.tokenStream(\"one\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"\\u0001eno\", \"one\", \"\\u0001owt\", \"two\", \n          \"\\u0001eerht\", \"three\", \"\\u0001x\\uD834\\uDD1Eis\", \"si\\uD834\\uDD1Ex\" },\n        new int[] { 0, 0, 4, 4, 8, 8, 14, 14 },\n        new int[] { 3, 3, 7, 7, 13, 13, 19, 19 },\n        new int[] { 1, 0, 1, 0, 1, 0, 1, 0 }\n    );\n    // field two\n    input = a.tokenStream(\"two\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"\\u0001eno\", \"\\u0001owt\", \n          \"\\u0001eerht\", \"\\u0001x\\uD834\\uDD1Eis\" },\n        new int[] { 0, 4, 8, 14 },\n        new int[] { 3, 7, 13, 19 },\n        new int[] { 1, 1, 1, 1 }\n    );\n    // field three\n    input = a.tokenStream(\"three\", new StringReader(text));\n    assertTokenStreamContents(input,\n        new String[] { \"one\", \"two\", \"three\", \"si\\uD834\\uDD1Ex\" },\n        new int[] { 0, 4, 8, 14 },\n        new int[] { 3, 7, 13, 19 }\n    );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4ea4e0df42b1e8d32da66955b10083ed12ba7cc4":["f66fbc07ecf3707c1ec81e35ca9e4df7ff22101a"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"f66fbc07ecf3707c1ec81e35ca9e4df7ff22101a":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"ad94625fb8d088209f46650c8097196fec67f00c":["434d3dd2a84fe27b3101b21528019c74c8534e03"],"434d3dd2a84fe27b3101b21528019c74c8534e03":["2fd023a662cc25ae7e0ad0f33d71c476a16d0579"],"2fd023a662cc25ae7e0ad0f33d71c476a16d0579":["4ea4e0df42b1e8d32da66955b10083ed12ba7cc4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4ea4e0df42b1e8d32da66955b10083ed12ba7cc4":["2fd023a662cc25ae7e0ad0f33d71c476a16d0579"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["f66fbc07ecf3707c1ec81e35ca9e4df7ff22101a"],"f66fbc07ecf3707c1ec81e35ca9e4df7ff22101a":["4ea4e0df42b1e8d32da66955b10083ed12ba7cc4"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"434d3dd2a84fe27b3101b21528019c74c8534e03":["ad94625fb8d088209f46650c8097196fec67f00c"],"2fd023a662cc25ae7e0ad0f33d71c476a16d0579":["434d3dd2a84fe27b3101b21528019c74c8534e03"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}