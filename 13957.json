{"path":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<String,ArrayIntList> terms = new HashMap<String,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      \n      stream.reset();\n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<String,ArrayIntList> terms = new HashMap<String,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      \n      stream.reset();\n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a7347509fad0711ac30cb15a746e9a3830a38ebd","date":1275388513,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<String,ArrayIntList> terms = new HashMap<String,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.addAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = new BytesRef(10);\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.toBytesRef(ref);\n        // TODO: support non-UTF8 strings (like numerics) here\n        String term = ref.utf8ToString();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<String,ArrayIntList> terms = new HashMap<String,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermAttribute termAtt = stream.addAttribute(TermAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      \n      stream.reset();\n      while (stream.incrementToken()) {\n        String term = termAtt.term();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = new BytesRef(10);\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.toBytesRef(ref);\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(new BytesRef(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<String,ArrayIntList> terms = new HashMap<String,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.addAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = new BytesRef(10);\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.toBytesRef(ref);\n        // TODO: support non-UTF8 strings (like numerics) here\n        String term = ref.utf8ToString();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = new BytesRef(10);\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.toBytesRef(ref);\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(new BytesRef(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<String,ArrayIntList> terms = new HashMap<String,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.addAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = new BytesRef(10);\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.toBytesRef(ref);\n        // TODO: support non-UTF8 strings (like numerics) here\n        String term = ref.utf8ToString();\n        if (term.length() == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(term);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(term, positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3d07f1ae3b58102f36f3393c397d78ba4e547a4","date":1300715535,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(new BytesRef(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = new BytesRef(10);\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.toBytesRef(ref);\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(new BytesRef(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(new BytesRef(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = new BytesRef(10);\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.toBytesRef(ref);\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(new BytesRef(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(new BytesRef(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = new BytesRef(10);\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.toBytesRef(ref);\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(new BytesRef(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e6e919043fa85ee891123768dd655a98edbbf63c","date":1322225413,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(new BytesRef(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3321cfbf7f8aba27e37e7a4d6901531a97ac2b06","date":1326148180,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      fieldInfos.addOrUpdate(fieldName, true);\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","pathOld":"lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex#addField(String,TokenStream,float).mjava","sourceNew":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      fieldInfos.addOrUpdate(fieldName, true);\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","sourceOld":"  /**\n   * Iterates over the given token stream and adds the resulting terms to the index;\n   * Equivalent to adding a tokenized, indexed, termVectorStored, unstored,\n   * Lucene {@link org.apache.lucene.document.Field}.\n   * Finally closes the token stream. Note that untokenized keywords can be added with this method via \n   * {@link #keywordTokenStream(Collection)}, the Lucene contrib <code>KeywordTokenizer</code> or similar utilities.\n   * \n   * @param fieldName\n   *            a name to be associated with the text\n   * @param stream\n   *            the token stream to retrieve tokens from.\n   * @param boost\n   *            the boost factor for hits for this field\n   * @see org.apache.lucene.document.Field#setBoost(float)\n   */\n  public void addField(String fieldName, TokenStream stream, float boost) {\n    try {\n      if (fieldName == null)\n        throw new IllegalArgumentException(\"fieldName must not be null\");\n      if (stream == null)\n          throw new IllegalArgumentException(\"token stream must not be null\");\n      if (boost <= 0.0f)\n          throw new IllegalArgumentException(\"boost factor must be greater than 0.0\");\n      if (fields.get(fieldName) != null)\n        throw new IllegalArgumentException(\"field must not be added more than once\");\n      \n      HashMap<BytesRef,ArrayIntList> terms = new HashMap<BytesRef,ArrayIntList>();\n      int numTokens = 0;\n      int numOverlapTokens = 0;\n      int pos = -1;\n\n      fieldInfos.addOrUpdate(fieldName, true);\n      \n      TermToBytesRefAttribute termAtt = stream.getAttribute(TermToBytesRefAttribute.class);\n      PositionIncrementAttribute posIncrAttribute = stream.addAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = stream.addAttribute(OffsetAttribute.class);\n      BytesRef ref = termAtt.getBytesRef();\n      stream.reset();\n      while (stream.incrementToken()) {\n        termAtt.fillBytesRef();\n        if (ref.length == 0) continue; // nothing to do\n//        if (DEBUG) System.err.println(\"token='\" + term + \"'\");\n        numTokens++;\n        final int posIncr = posIncrAttribute.getPositionIncrement();\n        if (posIncr == 0)\n          numOverlapTokens++;\n        pos += posIncr;\n        \n        ArrayIntList positions = terms.get(ref);\n        if (positions == null) { // term not seen before\n          positions = new ArrayIntList(stride);\n          terms.put(BytesRef.deepCopyOf(ref), positions);\n        }\n        if (stride == 1) {\n          positions.add(pos);\n        } else {\n          positions.add(pos, offsetAtt.startOffset(), offsetAtt.endOffset());\n        }\n      }\n      stream.end();\n\n      // ensure infos.numTokens > 0 invariant; needed for correct operation of terms()\n      if (numTokens > 0) {\n        boost = boost * docBoost; // see DocumentWriter.addDocument(...)\n        fields.put(fieldName, new Info(terms, numTokens, numOverlapTokens, boost));\n        sortedFields = null;    // invalidate sorted view, if any\n      }\n    } catch (IOException e) { // can never happen\n      throw new RuntimeException(e);\n    } finally {\n      try {\n        if (stream != null) stream.close();\n      } catch (IOException e2) {\n        throw new RuntimeException(e2);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["3321cfbf7f8aba27e37e7a4d6901531a97ac2b06"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["5f4e87790277826a2aea119328600dfb07761f32","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3321cfbf7f8aba27e37e7a4d6901531a97ac2b06":["e6e919043fa85ee891123768dd655a98edbbf63c"],"5f4e87790277826a2aea119328600dfb07761f32":["a7347509fad0711ac30cb15a746e9a3830a38ebd","4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"b3d07f1ae3b58102f36f3393c397d78ba4e547a4":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"e6e919043fa85ee891123768dd655a98edbbf63c":["b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["d619839baa8ce5503e496b94a9e42ad6f079293f","5f4e87790277826a2aea119328600dfb07761f32","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":[],"d619839baa8ce5503e496b94a9e42ad6f079293f":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"3321cfbf7f8aba27e37e7a4d6901531a97ac2b06":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"5f4e87790277826a2aea119328600dfb07761f32":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df"],"b3d07f1ae3b58102f36f3393c397d78ba4e547a4":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","e6e919043fa85ee891123768dd655a98edbbf63c"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","5f4e87790277826a2aea119328600dfb07761f32"],"e6e919043fa85ee891123768dd655a98edbbf63c":["3321cfbf7f8aba27e37e7a4d6901531a97ac2b06"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}