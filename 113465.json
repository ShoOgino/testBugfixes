{"path":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"877cc2c5a8035f48e78b21f0814a79e6607df85d","date":1331901478,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d08eba3d52b63561ebf936481ce73e6b6a14aa03","date":1333879759,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,InvertedFieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, InvertedFieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","date":1333892281,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,InvertedFieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, InvertedFieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"76923f6a33f2c4bec7f584e3f251261afe7ea276","date":1337149711,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"16cbef32b882ec68df422af3f08845ec82620335","date":1337802266,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.indexOptions;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.numDocs);\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.numDocs: \"doc=\" + docID + \" maxDoc=\" + state.numDocs;\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.numDocs);\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"83ede60c0b5bb96ad193414bbd663193b56689b3","date":1338331478,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  if (writeOffsets) {\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626","date":1339522233,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                offset = startOffset;\n                if (writePositions) {\n                  if (writeOffsets) {\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4d3e8520fd031bab31fd0e4d480e55958bc45efe","date":1340901565,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws CorruptIndexException, IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29e23e367cc757f42cdfce2bcbf21e68cd209cda","date":1343071560,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":["bde51b089eb7f86171eb3406e38a274743f9b7ac","25433c5cacacb7a2055d62d4d36b0daf210e0a10"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55c178897422fc01a257353a67f2ee23f1c82403","date":1343076368,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termDocFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57635ff388fa1bee703f3b892a86a3e48975576a","date":1343077051,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termDocFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termDocFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = 0;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = 0;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, termDocFreq);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, totTF));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e2aa5e951cc02d4c8152098ebec9c4bac57b3a65","date":1344352872,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int docFreq = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termFreq = postings.termFreqs[termID];\n            } else {\n              termFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        docFreq++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(docFreq, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += docFreq;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termDocFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int docFreq = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termFreq = postings.termFreqs[termID];\n            } else {\n              termFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        docFreq++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(docFreq, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += docFreq;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termDocFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int docFreq = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termFreq = postings.termFreqs[termID];\n            } else {\n              termFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        docFreq++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(docFreq, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += docFreq;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int numDocs = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termDocFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termDocFreq = postings.docFreqs[termID];\n            } else {\n              termDocFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termDocFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termDocFreq = 1;\n            } else {\n              termDocFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        numDocs++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termDocFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termDocFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termDocFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(numDocs, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += numDocs;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8f0a05a7e6cf7545945fdbef14567a1c852b25","date":1351978200,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    Term protoTerm = new Term(fieldName);\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        protoTerm.bytes = text;\n        final Integer docIDUpto = segDeletes.get(protoTerm);\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int docFreq = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termFreq = postings.termFreqs[termID];\n            } else {\n              termFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        docFreq++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(docFreq, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += docFreq;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        final Integer docIDUpto = segDeletes.get(new Term(fieldName, text));\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int docFreq = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termFreq = postings.termFreqs[termID];\n            } else {\n              termFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        docFreq++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(docFreq, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += docFreq;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b50a71ceb03a6ec444729f51a5a5bf9d385dfe5f","date":1373071459,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    Term protoTerm = new Term(fieldName);\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        protoTerm.bytes = text;\n        final Integer docIDUpto = segDeletes.get(protoTerm);\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int docFreq = 0;\n      long totalTermFreq = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termFreq = postings.termFreqs[termID];\n            } else {\n              termFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        docFreq++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totalTermFreq += termFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(docFreq, writeTermFreq ? totalTermFreq : -1));\n      sumTotalTermFreq += totalTermFreq;\n      sumDocFreq += docFreq;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    Term protoTerm = new Term(fieldName);\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        protoTerm.bytes = text;\n        final Integer docIDUpto = segDeletes.get(protoTerm);\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int docFreq = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termFreq = postings.termFreqs[termID];\n            } else {\n              termFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        docFreq++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(docFreq, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += docFreq;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    Term protoTerm = new Term(fieldName);\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        protoTerm.bytes = text;\n        final Integer docIDUpto = segDeletes.get(protoTerm);\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int docFreq = 0;\n      long totalTermFreq = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termFreq = postings.termFreqs[termID];\n            } else {\n              termFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        docFreq++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totalTermFreq += termFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(docFreq, writeTermFreq ? totalTermFreq : -1));\n      sumTotalTermFreq += totalTermFreq;\n      sumDocFreq += docFreq;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    Term protoTerm = new Term(fieldName);\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        protoTerm.bytes = text;\n        final Integer docIDUpto = segDeletes.get(protoTerm);\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int docFreq = 0;\n      long totTF = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termFreq = postings.termFreqs[termID];\n            } else {\n              termFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        docFreq++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totTF += termFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(docFreq, writeTermFreq ? totTF : -1));\n      sumTotalTermFreq += totTF;\n      sumDocFreq += docFreq;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","date":1379624229,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField#flush(String,FieldsConsumer,SegmentWriteState).mjava","sourceNew":null,"sourceOld":"  /* Walk through all unique text tokens (Posting\n   * instances) found in this field and serialize them\n   * into a single RAM segment. */\n  void flush(String fieldName, FieldsConsumer consumer,  final SegmentWriteState state)\n    throws IOException {\n\n    if (!fieldInfo.isIndexed()) {\n      return; // nothing to flush, don't bother the codec with the unindexed field\n    }\n    \n    final TermsConsumer termsConsumer = consumer.addField(fieldInfo);\n    final Comparator<BytesRef> termComp = termsConsumer.getComparator();\n\n    // CONFUSING: this.indexOptions holds the index options\n    // that were current when we first saw this field.  But\n    // it's possible this has changed, eg when other\n    // documents are indexed that cause a \"downgrade\" of the\n    // IndexOptions.  So we must decode the in-RAM buffer\n    // according to this.indexOptions, but then write the\n    // new segment to the directory according to\n    // currentFieldIndexOptions:\n    final IndexOptions currentFieldIndexOptions = fieldInfo.getIndexOptions();\n    assert currentFieldIndexOptions != null;\n\n    final boolean writeTermFreq = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;\n    final boolean writePositions = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;\n    final boolean writeOffsets = currentFieldIndexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;\n\n    final boolean readTermFreq = this.hasFreq;\n    final boolean readPositions = this.hasProx;\n    final boolean readOffsets = this.hasOffsets;\n\n    //System.out.println(\"flush readTF=\" + readTermFreq + \" readPos=\" + readPositions + \" readOffs=\" + readOffsets);\n\n    // Make sure FieldInfo.update is working correctly!:\n    assert !writeTermFreq || readTermFreq;\n    assert !writePositions || readPositions;\n    assert !writeOffsets || readOffsets;\n\n    assert !writeOffsets || writePositions;\n\n    final Map<Term,Integer> segDeletes;\n    if (state.segDeletes != null && state.segDeletes.terms.size() > 0) {\n      segDeletes = state.segDeletes.terms;\n    } else {\n      segDeletes = null;\n    }\n\n    final int[] termIDs = termsHashPerField.sortPostings(termComp);\n    final int numTerms = termsHashPerField.bytesHash.size();\n    final BytesRef text = new BytesRef();\n    final FreqProxPostingsArray postings = (FreqProxPostingsArray) termsHashPerField.postingsArray;\n    final ByteSliceReader freq = new ByteSliceReader();\n    final ByteSliceReader prox = new ByteSliceReader();\n\n    FixedBitSet visitedDocs = new FixedBitSet(state.segmentInfo.getDocCount());\n    long sumTotalTermFreq = 0;\n    long sumDocFreq = 0;\n\n    Term protoTerm = new Term(fieldName);\n    for (int i = 0; i < numTerms; i++) {\n      final int termID = termIDs[i];\n      //System.out.println(\"term=\" + termID);\n      // Get BytesRef\n      final int textStart = postings.textStarts[termID];\n      termsHashPerField.bytePool.setBytesRef(text, textStart);\n\n      termsHashPerField.initReader(freq, termID, 0);\n      if (readPositions || readOffsets) {\n        termsHashPerField.initReader(prox, termID, 1);\n      }\n\n      // TODO: really TermsHashPerField should take over most\n      // of this loop, including merge sort of terms from\n      // multiple threads and interacting with the\n      // TermsConsumer, only calling out to us (passing us the\n      // DocsConsumer) to handle delivery of docs/positions\n\n      final PostingsConsumer postingsConsumer = termsConsumer.startTerm(text);\n\n      final int delDocLimit;\n      if (segDeletes != null) {\n        protoTerm.bytes = text;\n        final Integer docIDUpto = segDeletes.get(protoTerm);\n        if (docIDUpto != null) {\n          delDocLimit = docIDUpto;\n        } else {\n          delDocLimit = 0;\n        }\n      } else {\n        delDocLimit = 0;\n      }\n\n      // Now termStates has numToMerge FieldMergeStates\n      // which all share the same term.  Now we must\n      // interleave the docID streams.\n      int docFreq = 0;\n      long totalTermFreq = 0;\n      int docID = 0;\n\n      while(true) {\n        //System.out.println(\"  cycle\");\n        final int termFreq;\n        if (freq.eof()) {\n          if (postings.lastDocCodes[termID] != -1) {\n            // Return last doc\n            docID = postings.lastDocIDs[termID];\n            if (readTermFreq) {\n              termFreq = postings.termFreqs[termID];\n            } else {\n              termFreq = -1;\n            }\n            postings.lastDocCodes[termID] = -1;\n          } else {\n            // EOF\n            break;\n          }\n        } else {\n          final int code = freq.readVInt();\n          if (!readTermFreq) {\n            docID += code;\n            termFreq = -1;\n          } else {\n            docID += code >>> 1;\n            if ((code & 1) != 0) {\n              termFreq = 1;\n            } else {\n              termFreq = freq.readVInt();\n            }\n          }\n\n          assert docID != postings.lastDocIDs[termID];\n        }\n\n        docFreq++;\n        assert docID < state.segmentInfo.getDocCount(): \"doc=\" + docID + \" maxDoc=\" + state.segmentInfo.getDocCount();\n\n        // NOTE: we could check here if the docID was\n        // deleted, and skip it.  However, this is somewhat\n        // dangerous because it can yield non-deterministic\n        // behavior since we may see the docID before we see\n        // the term that caused it to be deleted.  This\n        // would mean some (but not all) of its postings may\n        // make it into the index, which'd alter the docFreq\n        // for those terms.  We could fix this by doing two\n        // passes, ie first sweep marks all del docs, and\n        // 2nd sweep does the real flush, but I suspect\n        // that'd add too much time to flush.\n        visitedDocs.set(docID);\n        postingsConsumer.startDoc(docID, writeTermFreq ? termFreq : -1);\n        if (docID < delDocLimit) {\n          // Mark it deleted.  TODO: we could also skip\n          // writing its postings; this would be\n          // deterministic (just for this Term's docs).\n          \n          // TODO: can we do this reach-around in a cleaner way????\n          if (state.liveDocs == null) {\n            state.liveDocs = docState.docWriter.codec.liveDocsFormat().newLiveDocs(state.segmentInfo.getDocCount());\n          }\n          if (state.liveDocs.get(docID)) {\n            state.delCountOnFlush++;\n            state.liveDocs.clear(docID);\n          }\n        }\n\n        totalTermFreq += termFreq;\n        \n        // Carefully copy over the prox + payload info,\n        // changing the format to match Lucene's segment\n        // format.\n\n        if (readPositions || readOffsets) {\n          // we did record positions (& maybe payload) and/or offsets\n          int position = 0;\n          int offset = 0;\n          for(int j=0;j<termFreq;j++) {\n            final BytesRef thisPayload;\n\n            if (readPositions) {\n              final int code = prox.readVInt();\n              position += code >>> 1;\n\n              if ((code & 1) != 0) {\n\n                // This position has a payload\n                final int payloadLength = prox.readVInt();\n\n                if (payload == null) {\n                  payload = new BytesRef();\n                  payload.bytes = new byte[payloadLength];\n                } else if (payload.bytes.length < payloadLength) {\n                  payload.grow(payloadLength);\n                }\n\n                prox.readBytes(payload.bytes, 0, payloadLength);\n                payload.length = payloadLength;\n                thisPayload = payload;\n\n              } else {\n                thisPayload = null;\n              }\n\n              if (readOffsets) {\n                final int startOffset = offset + prox.readVInt();\n                final int endOffset = startOffset + prox.readVInt();\n                if (writePositions) {\n                  if (writeOffsets) {\n                    assert startOffset >=0 && endOffset >= startOffset : \"startOffset=\" + startOffset + \",endOffset=\" + endOffset + \",offset=\" + offset;\n                    postingsConsumer.addPosition(position, thisPayload, startOffset, endOffset);\n                  } else {\n                    postingsConsumer.addPosition(position, thisPayload, -1, -1);\n                  }\n                }\n                offset = startOffset;\n              } else if (writePositions) {\n                postingsConsumer.addPosition(position, thisPayload, -1, -1);\n              }\n            }\n          }\n        }\n        postingsConsumer.finishDoc();\n      }\n      termsConsumer.finishTerm(text, new TermStats(docFreq, writeTermFreq ? totalTermFreq : -1));\n      sumTotalTermFreq += totalTermFreq;\n      sumDocFreq += docFreq;\n    }\n\n    termsConsumer.finish(writeTermFreq ? sumTotalTermFreq : -1, sumDocFreq, visitedDocs.cardinality());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"55c178897422fc01a257353a67f2ee23f1c82403":["29e23e367cc757f42cdfce2bcbf21e68cd209cda"],"877cc2c5a8035f48e78b21f0814a79e6607df85d":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["2c8f0a05a7e6cf7545945fdbef14567a1c852b25","b50a71ceb03a6ec444729f51a5a5bf9d385dfe5f"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b50a71ceb03a6ec444729f51a5a5bf9d385dfe5f":["2c8f0a05a7e6cf7545945fdbef14567a1c852b25"],"16cbef32b882ec68df422af3f08845ec82620335":["76923f6a33f2c4bec7f584e3f251261afe7ea276"],"83ede60c0b5bb96ad193414bbd663193b56689b3":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["877cc2c5a8035f48e78b21f0814a79e6607df85d"],"29e23e367cc757f42cdfce2bcbf21e68cd209cda":["4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"57635ff388fa1bee703f3b892a86a3e48975576a":["fe33227f6805edab2036cbb80645cc4e2d1fa424","55c178897422fc01a257353a67f2ee23f1c82403"],"76923f6a33f2c4bec7f584e3f251261afe7ea276":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"],"2c8f0a05a7e6cf7545945fdbef14567a1c852b25":["e2aa5e951cc02d4c8152098ebec9c4bac57b3a65"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf","16cbef32b882ec68df422af3f08845ec82620335"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["57635ff388fa1bee703f3b892a86a3e48975576a","e2aa5e951cc02d4c8152098ebec9c4bac57b3a65"],"aba371508186796cc6151d8223a5b4e16d02e26e":["4d3e8520fd031bab31fd0e4d480e55958bc45efe","55c178897422fc01a257353a67f2ee23f1c82403"],"e2aa5e951cc02d4c8152098ebec9c4bac57b3a65":["55c178897422fc01a257353a67f2ee23f1c82403"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["b50a71ceb03a6ec444729f51a5a5bf9d385dfe5f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["aba371508186796cc6151d8223a5b4e16d02e26e","e2aa5e951cc02d4c8152098ebec9c4bac57b3a65"],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626":["83ede60c0b5bb96ad193414bbd663193b56689b3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"]},"commit2Childs":{"55c178897422fc01a257353a67f2ee23f1c82403":["57635ff388fa1bee703f3b892a86a3e48975576a","aba371508186796cc6151d8223a5b4e16d02e26e","e2aa5e951cc02d4c8152098ebec9c4bac57b3a65"],"877cc2c5a8035f48e78b21f0814a79e6607df85d":["d08eba3d52b63561ebf936481ce73e6b6a14aa03"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["877cc2c5a8035f48e78b21f0814a79e6607df85d"],"b50a71ceb03a6ec444729f51a5a5bf9d385dfe5f":["37a0f60745e53927c4c876cfe5b5a58170f0646c","519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"16cbef32b882ec68df422af3f08845ec82620335":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"83ede60c0b5bb96ad193414bbd663193b56689b3":["d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626"],"d08eba3d52b63561ebf936481ce73e6b6a14aa03":["e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf"],"29e23e367cc757f42cdfce2bcbf21e68cd209cda":["55c178897422fc01a257353a67f2ee23f1c82403"],"57635ff388fa1bee703f3b892a86a3e48975576a":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"76923f6a33f2c4bec7f584e3f251261afe7ea276":["16cbef32b882ec68df422af3f08845ec82620335"],"2c8f0a05a7e6cf7545945fdbef14567a1c852b25":["37a0f60745e53927c4c876cfe5b5a58170f0646c","b50a71ceb03a6ec444729f51a5a5bf9d385dfe5f"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["83ede60c0b5bb96ad193414bbd663193b56689b3"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":[],"aba371508186796cc6151d8223a5b4e16d02e26e":["d6f074e73200c07d54f242d3880a8da5a35ff97b"],"e2aa5e951cc02d4c8152098ebec9c4bac57b3a65":["2c8f0a05a7e6cf7545945fdbef14567a1c852b25","c7869f64c874ebf7f317d22c00baf2b6857797a6","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["57635ff388fa1bee703f3b892a86a3e48975576a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e3f8ac3877ad6d160de0fd3a6f7155b243dfbddf":["76923f6a33f2c4bec7f584e3f251261afe7ea276","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"4d3e8520fd031bab31fd0e4d480e55958bc45efe":["29e23e367cc757f42cdfce2bcbf21e68cd209cda","aba371508186796cc6151d8223a5b4e16d02e26e","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"d2d5b1f6ad16c5f1ce7e0a00225e2c9ffd0bc626":["fe33227f6805edab2036cbb80645cc4e2d1fa424","4d3e8520fd031bab31fd0e4d480e55958bc45efe"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c7869f64c874ebf7f317d22c00baf2b6857797a6","d6f074e73200c07d54f242d3880a8da5a35ff97b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}