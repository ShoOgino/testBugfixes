{"path":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#testNonIdeographicNonLetter().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#testNonIdeographicNonLetter().mjava","pathOld":"contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#testNonIdeographicNonLetter().mjava","sourceNew":"  /*\n   * Non-english text with nonletters (non-spacing marks,etc) is treated as C1C2 C2C3,\n   * except for words are split around non-letters.\n   */\n  public void testNonIdeographicNonLetter() throws Exception {\n    String str = \"\\u4e00 رُوبرت موير\";\n    TestToken[] out_tokens = {\n        newToken(\"\\u4e00\", 0, 1, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ر\", 2, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وب\", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"بر\", 5, 7, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"رت\", 6, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"مو\", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وي\", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ير\", 11, 13, CJKTokenizer.DOUBLE_TOKEN_TYPE)\n    };\n    checkCJKToken(str, out_tokens);\n  }\n\n","sourceOld":"  /*\n   * Non-english text with nonletters (non-spacing marks,etc) is treated as C1C2 C2C3,\n   * except for words are split around non-letters.\n   */\n  public void testNonIdeographicNonLetter() throws Exception {\n    String str = \"\\u4e00 رُوبرت موير\";\n    TestToken[] out_tokens = {\n        newToken(\"\\u4e00\", 0, 1, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ر\", 2, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وب\", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"بر\", 5, 7, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"رت\", 6, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"مو\", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وي\", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ير\", 11, 13, CJKTokenizer.DOUBLE_TOKEN_TYPE)\n    };\n    checkCJKToken(str, out_tokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f080986da691a3bba7b757f43ab72cdc82b57ce","date":1273069619,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#testNonIdeographicNonLetter().mjava","pathOld":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/cjk/TestCJKTokenizer#testNonIdeographicNonLetter().mjava","sourceNew":"  /*\n   * Non-english text with nonletters (non-spacing marks,etc) is treated as C1C2 C2C3,\n   * except for words are split around non-letters.\n   */\n  public void testNonIdeographicNonLetter() throws Exception {\n    String str = \"\\u4e00 رُوبرت موير\";\n    TestToken[] out_tokens = {\n        newToken(\"\\u4e00\", 0, 1, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ر\", 2, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وب\", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"بر\", 5, 7, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"رت\", 6, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"مو\", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وي\", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ير\", 11, 13, CJKTokenizer.DOUBLE_TOKEN_TYPE)\n    };\n    checkCJKToken(str, out_tokens);\n  }\n\n","sourceOld":"  /*\n   * Non-english text with nonletters (non-spacing marks,etc) is treated as C1C2 C2C3,\n   * except for words are split around non-letters.\n   */\n  public void testNonIdeographicNonLetter() throws Exception {\n    String str = \"\\u4e00 رُوبرت موير\";\n    TestToken[] out_tokens = {\n        newToken(\"\\u4e00\", 0, 1, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ر\", 2, 3, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وب\", 4, 6, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"بر\", 5, 7, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"رت\", 6, 8, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"مو\", 9, 11, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"وي\", 10, 12, CJKTokenizer.DOUBLE_TOKEN_TYPE),\n        newToken(\"ير\", 11, 13, CJKTokenizer.DOUBLE_TOKEN_TYPE)\n    };\n    checkCJKToken(str, out_tokens);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}