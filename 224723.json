{"path":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setTermBuffer(new char[]{'b'}, 0, 1);\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.termBuffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.termBuffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          TermAttribute termAtt = addAttribute(TermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    TermPositions termPositions = reader.termPositions(new Term(\"f1\", \"a\"));\n    assertTrue(termPositions.next());\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.isPayloadAvailable());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setTermBuffer(new char[]{'b'}, 0, 1);\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.termBuffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.termBuffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          TermAttribute termAtt = addAttribute(TermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    TermPositions termPositions = reader.termPositions(new Term(\"f1\", \"a\"));\n    assertTrue(termPositions.next());\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.isPayloadAvailable());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a775c547c3519b47efd41c09cb47100ddb9604c7","date":1270914087,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    TermPositions termPositions = reader.termPositions(new Term(\"f1\", \"a\"));\n    assertTrue(termPositions.next());\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.isPayloadAvailable());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setTermBuffer(new char[]{'b'}, 0, 1);\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.termBuffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.termBuffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          TermAttribute termAtt = addAttribute(TermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    TermPositions termPositions = reader.termPositions(new Term(\"f1\", \"a\"));\n    assertTrue(termPositions.next());\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.isPayloadAvailable());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"03276b2744036b1b19a7a2dd4b74ba7bc484f107","date":1274048508,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    TermPositions termPositions = reader.termPositions(new Term(\"f1\", \"a\"));\n    assertTrue(termPositions.next());\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.isPayloadAvailable());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    TermPositions termPositions = reader.termPositions(new Term(\"f1\", \"a\"));\n    assertTrue(termPositions.next());\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.isPayloadAvailable());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28427ef110c4c5bf5b4057731b83110bd1e13724","date":1276701452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    TermPositions termPositions = reader.termPositions(new Term(\"f1\", \"a\"));\n    assertTrue(termPositions.next());\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.isPayloadAvailable());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    TermPositions termPositions = reader.termPositions(new Term(\"f1\", \"a\"));\n    assertTrue(termPositions.next());\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.isPayloadAvailable());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.isPayloadAvailable());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(newRandom(), TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(newRandom(), TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(new Field(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, IOContext.READ);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6f9be74ca7baaef11857ad002cad40419979516","date":1309449808,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, IOContext.READ);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getDeletedDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR);\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4196a8f99ce1dc39a13b325cd9fc21616fa54164","date":1314268949,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first=true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);          \n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", Field.Store.YES, Field.Index.ANALYZED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"53ae89cd75b0acbdfb8890710c6742f3fb80e65d","date":1315806626,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new MockTokenizer(reader, MockTokenizer.WHITESPACE, false)) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        };\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2f49143da0a5d278a72f741432047fcfa6da996e","date":1316927425,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e2297162a22c55456e200caef2cbcb00fe381120","date":1321551342,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = reader.fields().terms(\"f1\").docsAndPositions(reader.getLiveDocs(), new BytesRef(\"a\"), null);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f","date":1323210518,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(true, info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ce667c6d3400b22523701c549c0d35e26da8b46","date":1324405053,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = SegmentReader.get(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"868186558eb3a854ce7e720a52bb445795d54910","date":1327853682,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, IndexReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["e2297162a22c55456e200caef2cbcb00fe381120","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","132903c28af3aa6f67284b78de91c0f0a99488c2"],"a775c547c3519b47efd41c09cb47100ddb9604c7":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b6f9be74ca7baaef11857ad002cad40419979516":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["03276b2744036b1b19a7a2dd4b74ba7bc484f107"],"2f49143da0a5d278a72f741432047fcfa6da996e":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["817d8435e9135b756f08ce6710ab0baac51bdf88","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"9ce667c6d3400b22523701c549c0d35e26da8b46":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["4196a8f99ce1dc39a13b325cd9fc21616fa54164"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["28427ef110c4c5bf5b4057731b83110bd1e13724"],"cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f":["e2297162a22c55456e200caef2cbcb00fe381120"],"868186558eb3a854ce7e720a52bb445795d54910":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["9ce667c6d3400b22523701c549c0d35e26da8b46"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"03276b2744036b1b19a7a2dd4b74ba7bc484f107":["a775c547c3519b47efd41c09cb47100ddb9604c7"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"4196a8f99ce1dc39a13b325cd9fc21616fa54164":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"e2297162a22c55456e200caef2cbcb00fe381120":["2f49143da0a5d278a72f741432047fcfa6da996e"],"5f4e87790277826a2aea119328600dfb07761f32":["03276b2744036b1b19a7a2dd4b74ba7bc484f107","28427ef110c4c5bf5b4057731b83110bd1e13724"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["b6f9be74ca7baaef11857ad002cad40419979516","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["31f025ae60076ae95274433f3fe8e6ace2857a87","868186558eb3a854ce7e720a52bb445795d54910"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["132903c28af3aa6f67284b78de91c0f0a99488c2","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"53ae89cd75b0acbdfb8890710c6742f3fb80e65d":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["e2297162a22c55456e200caef2cbcb00fe381120","cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["ddc4c914be86e34b54f70023f45a60fa7f04e929","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["b6f9be74ca7baaef11857ad002cad40419979516"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","639c36565ce03aed5b0fce7c9e4448e53a1f7efd","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","817d8435e9135b756f08ce6710ab0baac51bdf88"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"a775c547c3519b47efd41c09cb47100ddb9604c7":["03276b2744036b1b19a7a2dd4b74ba7bc484f107"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"b6f9be74ca7baaef11857ad002cad40419979516":["d083e83f225b11e5fdd900e83d26ddb385b6955c"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["b21422ff1d1d56499dec481f193b402e5e8def5b","5f4e87790277826a2aea119328600dfb07761f32"],"2f49143da0a5d278a72f741432047fcfa6da996e":["e2297162a22c55456e200caef2cbcb00fe381120"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["5d004d0e0b3f65bb40da76d476d659d7888270e8","4196a8f99ce1dc39a13b325cd9fc21616fa54164"],"9ce667c6d3400b22523701c549c0d35e26da8b46":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"868186558eb3a854ce7e720a52bb445795d54910":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a775c547c3519b47efd41c09cb47100ddb9604c7"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["868186558eb3a854ce7e720a52bb445795d54910","5cab9a86bd67202d20b6adc463008c8e982b070a"],"03276b2744036b1b19a7a2dd4b74ba7bc484f107":["28427ef110c4c5bf5b4057731b83110bd1e13724","5f4e87790277826a2aea119328600dfb07761f32"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"4196a8f99ce1dc39a13b325cd9fc21616fa54164":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"e2297162a22c55456e200caef2cbcb00fe381120":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","cb4972c6aaf6c714c8f5957b5aeb14dcce34b75f","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"53ae89cd75b0acbdfb8890710c6742f3fb80e65d":["2f49143da0a5d278a72f741432047fcfa6da996e"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","9ce667c6d3400b22523701c549c0d35e26da8b46"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","5d004d0e0b3f65bb40da76d476d659d7888270e8","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}