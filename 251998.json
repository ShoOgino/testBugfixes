{"path":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","commits":[{"id":"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5","date":1292695408,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"/dev/null","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      long startNumBytesUsed = bytesUsed();\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + flushState.flushedFiles);\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : flushState.flushedFiles) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(flushState.flushedFiles);\n\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final long newSegmentSize = newSegment.sizeInBytes();\n        message(\"  ramUsed=\" + nf.format(startNumBytesUsed / 1024. / 1024.) + \" MB\" +\n            \" newFlushedSize=\" + nf.format(newSegmentSize / 1024 / 1024) + \" MB\" +\n            \" docs/MB=\" + nf.format(numDocs / (newSegmentSize / 1024. / 1024.)) +\n            \" new/old=\" + nf.format(100.0 * newSegmentSize / startNumBytesUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5","date":1292711882,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      long startNumBytesUsed = bytesUsed();\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + flushState.flushedFiles);\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : flushState.flushedFiles) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(flushState.flushedFiles);\n\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final long newSegmentSize = newSegment.sizeInBytes();\n        message(\"  ramUsed=\" + nf.format(startNumBytesUsed / 1024. / 1024.) + \" MB\" +\n            \" newFlushedSize=\" + nf.format(newSegmentSize / 1024 / 1024) + \" MB\" +\n            \" docs/MB=\" + nf.format(numDocs / (newSegmentSize / 1024. / 1024.)) +\n            \" new/old=\" + nf.format(100.0 * newSegmentSize / startNumBytesUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      long startNumBytesUsed = bytesUsed();\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + flushState.flushedFiles);\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : flushState.flushedFiles) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(flushState.flushedFiles);\n\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final long newSegmentSize = newSegment.sizeInBytes();\n        message(\"  ramUsed=\" + nf.format(startNumBytesUsed / 1024. / 1024.) + \" MB\" +\n            \" newFlushedSize=\" + nf.format(newSegmentSize / 1024 / 1024) + \" MB\" +\n            \" docs/MB=\" + nf.format(numDocs / (newSegmentSize / 1024. / 1024.)) +\n            \" new/old=\" + nf.format(100.0 * newSegmentSize / startNumBytesUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"/dev/null","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = segWriteState();\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      long startNumBytesUsed = bytesUsed();\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + flushState.flushedFiles);\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : flushState.flushedFiles) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(flushState.flushedFiles);\n\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final long newSegmentSize = newSegment.sizeInBytes();\n        message(\"  ramUsed=\" + nf.format(startNumBytesUsed / 1024. / 1024.) + \" MB\" +\n            \" newFlushedSize=\" + nf.format(newSegmentSize / 1024 / 1024) + \" MB\" +\n            \" docs/MB=\" + nf.format(numDocs / (newSegmentSize / 1024. / 1024.)) +\n            \" new/old=\" + nf.format(100.0 * newSegmentSize / startNumBytesUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d5efdc434c21e20adcb20d316e227be9eaf377d2","date":1292842437,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      long startNumBytesUsed = bytesUsed();\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + flushState.flushedFiles);\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : flushState.flushedFiles) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(flushState.flushedFiles);\n\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final long newSegmentSizeNoStore = newSegment.sizeInBytes(false);\n        final long newSegmentSize = newSegment.sizeInBytes(true);\n        message(\"  ramUsed=\" + nf.format(startNumBytesUsed / 1024. / 1024.) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize / 1024 / 1024) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore / 1024 / 1024) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / (newSegmentSize / 1024. / 1024.)) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startNumBytesUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      long startNumBytesUsed = bytesUsed();\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + flushState.flushedFiles);\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : flushState.flushedFiles) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(flushState.flushedFiles);\n\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final long newSegmentSize = newSegment.sizeInBytes();\n        message(\"  ramUsed=\" + nf.format(startNumBytesUsed / 1024. / 1024.) + \" MB\" +\n            \" newFlushedSize=\" + nf.format(newSegmentSize / 1024 / 1024) + \" MB\" +\n            \" docs/MB=\" + nf.format(numDocs / (newSegmentSize / 1024. / 1024.)) +\n            \" new/old=\" + nf.format(100.0 * newSegmentSize / startNumBytesUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9644142c68aea2bf99294da013f8c944f3b6c7a4","date":1292843970,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + flushState.flushedFiles);\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : flushState.flushedFiles) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(flushState.flushedFiles);\n\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      long startNumBytesUsed = bytesUsed();\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + flushState.flushedFiles);\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : flushState.flushedFiles) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(flushState.flushedFiles);\n\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final long newSegmentSizeNoStore = newSegment.sizeInBytes(false);\n        final long newSegmentSize = newSegment.sizeInBytes(true);\n        message(\"  ramUsed=\" + nf.format(startNumBytesUsed / 1024. / 1024.) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize / 1024 / 1024) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore / 1024 / 1024) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / (newSegmentSize / 1024. / 1024.)) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startNumBytesUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe","date":1294227869,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + flushState.flushedFiles);\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : flushState.flushedFiles) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(flushState.flushedFiles);\n\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = segWriteState();\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = segWriteState();\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      long startNumBytesUsed = bytesUsed();\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + flushState.flushedFiles);\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : flushState.flushedFiles) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(flushState.flushedFiles);\n\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final long newSegmentSize = newSegment.sizeInBytes();\n        message(\"  ramUsed=\" + nf.format(startNumBytesUsed / 1024. / 1024.) + \" MB\" +\n            \" newFlushedSize=\" + nf.format(newSegmentSize / 1024 / 1024) + \" MB\" +\n            \" docs/MB=\" + nf.format(numDocs / (newSegmentSize / 1024. / 1024.)) +\n            \" new/old=\" + nf.format(100.0 * newSegmentSize / startNumBytesUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7340c507e2e266c5dd97b9e8a05744140a0ef10f","date":1295350295,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c19f985e36a65cc969e8e564fe337a0d41512075","date":1296330536,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"69a6d2d525aeab53c867ed26934185e5bb627d0e","date":1296516902,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        boolean success2 = false;\n        try {\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs));\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = segWriteState();\n\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        boolean success2 = false;\n        try {\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = segWriteState();\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","date":1297940445,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        boolean success2 = false;\n        try {\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f1bdbf92da222965b46c0a942c3857ba56e5c638","date":1298297608,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = segWriteState();\n\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = segWriteState();\n\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        boolean success2 = false;\n        try {\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0c7a8f7304b75b1528814c5820fa23a96816c27","date":1298314239,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, flushState.segmentCodecs, fieldInfos);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.clearFilesCache();\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14ec33385f6fbb6ce172882d14605790418a5d31","date":1298910796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, flushState.segmentCodecs, fieldInfos);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.clearFilesCache();\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false, fieldInfos);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e92442af786151ee55bc283eb472f629e3c7b52b","date":1301070252,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n      \n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 fieldInfos.buildSegmentCodecs(true),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false, fieldInfos);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 SegmentCodecs.build(fieldInfos, writer.codecs),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false, fieldInfos);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n      \n      final SegmentWriteState flushState = segWriteState(true);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false, fieldInfos);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n\n      final SegmentWriteState flushState = segWriteState();\n\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":4,"author":"Uwe Schindler","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":null,"sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n      \n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 fieldInfos.buildSegmentCodecs(true),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false, fieldInfos);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":null,"sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n      \n      final SegmentWriteState flushState = segWriteState(true);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false, fieldInfos);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":4,"author":"Steven Rowe","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#flush(IndexWriter,IndexFileDeleter,MergePolicy,SegmentInfos).mjava","sourceNew":null,"sourceOld":"  /** Flush all pending docs to a new segment */\n  // Lock order: IW -> DW\n  synchronized SegmentInfo flush(IndexWriter writer, IndexFileDeleter deleter, MergePolicy mergePolicy, SegmentInfos segmentInfos) throws IOException {\n\n    final long startTime = System.currentTimeMillis();\n\n    // We change writer's segmentInfos:\n    assert Thread.holdsLock(writer);\n\n    waitIdle();\n\n    if (numDocs == 0) {\n      // nothing to do!\n      if (infoStream != null) {\n        message(\"flush: no docs; skipping\");\n      }\n      // Lock order: IW -> DW -> BD\n      pushDeletes(null, segmentInfos);\n      return null;\n    }\n\n    if (aborting) {\n      if (infoStream != null) {\n        message(\"flush: skip because aborting is set\");\n      }\n      return null;\n    }\n\n    boolean success = false;\n\n    SegmentInfo newSegment;\n\n    try {\n      assert nextDocID == numDocs;\n      assert waitQueue.numWaiting == 0;\n      assert waitQueue.waitingBytes == 0;\n\n      if (infoStream != null) {\n        message(\"flush postings as segment \" + segment + \" numDocs=\" + numDocs);\n      }\n      \n      final SegmentWriteState flushState = new SegmentWriteState(infoStream, directory, segment, fieldInfos,\n                                                                 numDocs, writer.getConfig().getTermIndexInterval(),\n                                                                 fieldInfos.buildSegmentCodecs(true),\n                                                                 pendingDeletes);\n      // Apply delete-by-docID now (delete-byDocID only\n      // happens when an exception is hit processing that\n      // doc, eg if analyzer has some problem w/ the text):\n      if (pendingDeletes.docIDs.size() > 0) {\n        flushState.deletedDocs = new BitVector(numDocs);\n        for(int delDocID : pendingDeletes.docIDs) {\n          flushState.deletedDocs.set(delDocID);\n        }\n        pendingDeletes.bytesUsed.addAndGet(-pendingDeletes.docIDs.size() * BufferedDeletes.BYTES_PER_DEL_DOCID);\n        pendingDeletes.docIDs.clear();\n      }\n\n      newSegment = new SegmentInfo(segment, numDocs, directory, false, fieldInfos.hasProx(), flushState.segmentCodecs, false, fieldInfos);\n\n      Collection<DocConsumerPerThread> threads = new HashSet<DocConsumerPerThread>();\n      for (DocumentsWriterThreadState threadState : threadStates) {\n        threads.add(threadState.consumer);\n      }\n\n      double startMBUsed = bytesUsed()/1024./1024.;\n\n      consumer.flush(threads, flushState);\n\n      newSegment.setHasVectors(flushState.hasVectors);\n\n      if (infoStream != null) {\n        message(\"new segment has \" + (flushState.hasVectors ? \"vectors\" : \"no vectors\"));\n        if (flushState.deletedDocs != null) {\n          message(\"new segment has \" + flushState.deletedDocs.count() + \" deleted docs\");\n        }\n        message(\"flushedFiles=\" + newSegment.files());\n        message(\"flushed codecs=\" + newSegment.getSegmentCodecs());\n      }\n\n      if (mergePolicy.useCompoundFile(segmentInfos, newSegment)) {\n        final String cfsFileName = IndexFileNames.segmentFileName(segment, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION);\n\n        if (infoStream != null) {\n          message(\"flush: create compound file \\\"\" + cfsFileName + \"\\\"\");\n        }\n\n        CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, cfsFileName);\n        for(String fileName : newSegment.files()) {\n          cfsWriter.addFile(fileName);\n        }\n        cfsWriter.close();\n        deleter.deleteNewFiles(newSegment.files());\n        newSegment.setUseCompoundFile(true);\n      }\n\n      // Must write deleted docs after the CFS so we don't\n      // slurp the del file into CFS:\n      if (flushState.deletedDocs != null) {\n        final int delCount = flushState.deletedDocs.count();\n        assert delCount > 0;\n        newSegment.setDelCount(delCount);\n        newSegment.advanceDelGen();\n        final String delFileName = newSegment.getDelFileName();\n        if (infoStream != null) {\n          message(\"flush: write \" + delCount + \" deletes to \" + delFileName);\n        }\n        boolean success2 = false;\n        try {\n          // TODO: in the NRT case it'd be better to hand\n          // this del vector over to the\n          // shortly-to-be-opened SegmentReader and let it\n          // carry the changes; there's no reason to use\n          // filesystem as intermediary here.\n          flushState.deletedDocs.write(directory, delFileName);\n          success2 = true;\n        } finally {\n          if (!success2) {\n            try {\n              directory.deleteFile(delFileName);\n            } catch (Throwable t) {\n              // suppress this so we keep throwing the\n              // original exception\n            }\n          }\n        }\n      }\n\n      if (infoStream != null) {\n        message(\"flush: segment=\" + newSegment);\n        final double newSegmentSizeNoStore = newSegment.sizeInBytes(false)/1024./1024.;\n        final double newSegmentSize = newSegment.sizeInBytes(true)/1024./1024.;\n        message(\"  ramUsed=\" + nf.format(startMBUsed) + \" MB\" +\n                \" newFlushedSize=\" + nf.format(newSegmentSize) + \" MB\" +\n                \" (\" + nf.format(newSegmentSizeNoStore) + \" MB w/o doc stores)\" +\n                \" docs/MB=\" + nf.format(numDocs / newSegmentSize) +\n                \" new/old=\" + nf.format(100.0 * newSegmentSizeNoStore / startMBUsed) + \"%\");\n      }\n\n      success = true;\n    } finally {\n      notifyAll();\n      if (!success) {\n        if (segment != null) {\n          deleter.refresh(segment);\n        }\n        abort();\n      }\n    }\n\n    doAfterFlush();\n\n    // Lock order: IW -> DW -> BD\n    pushDeletes(newSegment, segmentInfos);\n    if (infoStream != null) {\n      message(\"flush time \" + (System.currentTimeMillis()-startTime) + \" msec\");\n    }\n\n    return newSegment;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70ad682703b8585f5d0a637efec044d57ec05efb":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["29ef99d61cda9641b6250bf9567329a6e65f901d","e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"7340c507e2e266c5dd97b9e8a05744140a0ef10f":["fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["e92442af786151ee55bc283eb472f629e3c7b52b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"d5efdc434c21e20adcb20d316e227be9eaf377d2":["94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"c19f985e36a65cc969e8e564fe337a0d41512075":["7340c507e2e266c5dd97b9e8a05744140a0ef10f"],"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3":["69a6d2d525aeab53c867ed26934185e5bb627d0e"],"9644142c68aea2bf99294da013f8c944f3b6c7a4":["d5efdc434c21e20adcb20d316e227be9eaf377d2"],"e92442af786151ee55bc283eb472f629e3c7b52b":["1224a4027481acce15495b03bce9b48b93b42722"],"14ec33385f6fbb6ce172882d14605790418a5d31":["b0c7a8f7304b75b1528814c5820fa23a96816c27"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["f1bdbf92da222965b46c0a942c3857ba56e5c638","e92442af786151ee55bc283eb472f629e3c7b52b"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3"],"a3776dccca01c11e7046323cfad46a3b4a471233":["e92442af786151ee55bc283eb472f629e3c7b52b","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["70ad682703b8585f5d0a637efec044d57ec05efb","69a6d2d525aeab53c867ed26934185e5bb627d0e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5":["4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"69a6d2d525aeab53c867ed26934185e5bb627d0e":["c19f985e36a65cc969e8e564fe337a0d41512075"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe":["9644142c68aea2bf99294da013f8c944f3b6c7a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"1224a4027481acce15495b03bce9b48b93b42722":["14ec33385f6fbb6ce172882d14605790418a5d31"]},"commit2Childs":{"70ad682703b8585f5d0a637efec044d57ec05efb":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"7340c507e2e266c5dd97b9e8a05744140a0ef10f":["c19f985e36a65cc969e8e564fe337a0d41512075"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["70ad682703b8585f5d0a637efec044d57ec05efb"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d5efdc434c21e20adcb20d316e227be9eaf377d2":["9644142c68aea2bf99294da013f8c944f3b6c7a4"],"c19f985e36a65cc969e8e564fe337a0d41512075":["69a6d2d525aeab53c867ed26934185e5bb627d0e"],"e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3":["f1bdbf92da222965b46c0a942c3857ba56e5c638","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"9644142c68aea2bf99294da013f8c944f3b6c7a4":["fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe"],"e92442af786151ee55bc283eb472f629e3c7b52b":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233"],"14ec33385f6fbb6ce172882d14605790418a5d31":["1224a4027481acce15495b03bce9b48b93b42722"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["f1bdbf92da222965b46c0a942c3857ba56e5c638"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["14ec33385f6fbb6ce172882d14605790418a5d31"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","ab5cb6a74aefb78aa0569857970b9151dfe2e787","4948bc5d29211f0c9b5ccc31b2632cdd27066ea5"],"94cb8b3ec0439dfd8e179637ee4191cd9c6227e5":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","d5efdc434c21e20adcb20d316e227be9eaf377d2"],"69a6d2d525aeab53c867ed26934185e5bb627d0e":["e69553ac9cbe3b2693b93c2fb0c211529b8ee4c3","29ef99d61cda9641b6250bf9567329a6e65f901d"],"4948bc5d29211f0c9b5ccc31b2632cdd27066ea5":["94cb8b3ec0439dfd8e179637ee4191cd9c6227e5"],"fdc49cf4bbf2603a647b53ff5cfa6878743a3ffe":["70ad682703b8585f5d0a637efec044d57ec05efb","7340c507e2e266c5dd97b9e8a05744140a0ef10f"],"1224a4027481acce15495b03bce9b48b93b42722":["e92442af786151ee55bc283eb472f629e3c7b52b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}