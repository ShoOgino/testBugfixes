{"path":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#[T-extends-DocValuesUpdate]_freezeDVUpdates(Map[String,LinkedHashMap[Term,T]],IntConsumer).mjava","commits":[{"id":"7e129bd6cb34a236558a49edf108a49d5c15e0e1","date":1525081316,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#[T-extends-DocValuesUpdate]_freezeDVUpdates(Map[String,LinkedHashMap[Term,T]],IntConsumer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeBinaryDVUpdates(Map[String,LinkedHashMap[Term,BinaryDocValuesUpdate]]).mjava","sourceNew":"  private static <T extends DocValuesUpdate> byte[] freezeDVUpdates(Map<String,LinkedHashMap<Term, T>> dvUpdates,\n                                                                    IntConsumer updateSizeConsumer)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, T> updates : dvUpdates.values()) {\n        updateSizeConsumer.accept(updates.size());\n        for (T update : updates.values()) {\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if (termField.equals(lastTermField) == false) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if (updateField.equals(lastUpdateField) == false) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n          update.writeTo(out);\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","sourceOld":"  private byte[] freezeBinaryDVUpdates(Map<String,LinkedHashMap<Term,BinaryDocValuesUpdate>> binaryDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, BinaryDocValuesUpdate> binaryUpdates : binaryDVUpdates.values()) {\n        binaryDVUpdateCount += binaryUpdates.size();\n        for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if (termField.equals(lastTermField) == false) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if (updateField.equals(lastUpdateField) == false) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n\n          BytesRef value = (BytesRef) update.value;\n          out.writeVInt(value.length);\n          out.writeBytes(value.bytes, value.offset, value.length);\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f42883db49d143abc1a0f176ba47e3388dafb608","date":1525083166,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#[T-extends-DocValuesUpdate]_freezeDVUpdates(Map[String,LinkedHashMap[Term,T]],IntConsumer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#freezeBinaryDVUpdates(Map[String,LinkedHashMap[Term,BinaryDocValuesUpdate]]).mjava","sourceNew":"  private static <T extends DocValuesUpdate> byte[] freezeDVUpdates(Map<String,LinkedHashMap<Term, T>> dvUpdates,\n                                                                    IntConsumer updateSizeConsumer)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, T> updates : dvUpdates.values()) {\n        updateSizeConsumer.accept(updates.size());\n        for (T update : updates.values()) {\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if (termField.equals(lastTermField) == false) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if (updateField.equals(lastUpdateField) == false) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n          update.writeTo(out);\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","sourceOld":"  private byte[] freezeBinaryDVUpdates(Map<String,LinkedHashMap<Term,BinaryDocValuesUpdate>> binaryDVUpdates)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, BinaryDocValuesUpdate> binaryUpdates : binaryDVUpdates.values()) {\n        binaryDVUpdateCount += binaryUpdates.size();\n        for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if (termField.equals(lastTermField) == false) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if (updateField.equals(lastUpdateField) == false) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n\n          BytesRef value = (BytesRef) update.value;\n          out.writeVInt(value.length);\n          out.writeBytes(value.bytes, value.offset, value.length);\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"33adea025f43af3243278587a46b8d9fd2e8ccf9","date":1525885077,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#[T-extends-DocValuesUpdate]_freezeDVUpdates(Map[String,LinkedHashMap[Term,T]],IntConsumer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#[T-extends-DocValuesUpdate]_freezeDVUpdates(Map[String,LinkedHashMap[Term,T]],IntConsumer).mjava","sourceNew":"  private static <T extends DocValuesUpdate> byte[] freezeDVUpdates(Map<String,LinkedHashMap<Term, T>> dvUpdates,\n                                                                    IntConsumer updateSizeConsumer)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, T> updates : dvUpdates.values()) {\n        updateSizeConsumer.accept(updates.size());\n        for (T update : updates.values()) {\n          int code = update.term.bytes().length << 3;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          if (update.hasValue()) {\n            code |= 4;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if (termField.equals(lastTermField) == false) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if (updateField.equals(lastUpdateField) == false) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n          if (update.hasValue()) {\n            update.writeTo(out);\n          }\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","sourceOld":"  private static <T extends DocValuesUpdate> byte[] freezeDVUpdates(Map<String,LinkedHashMap<Term, T>> dvUpdates,\n                                                                    IntConsumer updateSizeConsumer)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, T> updates : dvUpdates.values()) {\n        updateSizeConsumer.accept(updates.size());\n        for (T update : updates.values()) {\n          int code = update.term.bytes().length << 2;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if (termField.equals(lastTermField) == false) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if (updateField.equals(lastUpdateField) == false) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n          update.writeTo(out);\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28211671436f185419b3f7e53ccfc3911441ab65","date":1544026960,"type":4,"author":"Simon Willnauer","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#[T-extends-DocValuesUpdate]_freezeDVUpdates(Map[String,LinkedHashMap[Term,T]],IntConsumer).mjava","sourceNew":null,"sourceOld":"  private static <T extends DocValuesUpdate> byte[] freezeDVUpdates(Map<String,LinkedHashMap<Term, T>> dvUpdates,\n                                                                    IntConsumer updateSizeConsumer)\n    throws IOException {\n    // TODO: we could do better here, e.g. collate the updates by field\n    // so if you are updating 2 fields interleaved we don't keep writing the field strings\n    try (RAMOutputStream out = new RAMOutputStream()) {\n      String lastTermField = null;\n      String lastUpdateField = null;\n      for (LinkedHashMap<Term, T> updates : dvUpdates.values()) {\n        updateSizeConsumer.accept(updates.size());\n        for (T update : updates.values()) {\n          int code = update.term.bytes().length << 3;\n\n          String termField = update.term.field();\n          if (termField.equals(lastTermField) == false) {\n            code |= 1;\n          }\n          String updateField = update.field;\n          if (updateField.equals(lastUpdateField) == false) {\n            code |= 2;\n          }\n          if (update.hasValue()) {\n            code |= 4;\n          }\n          out.writeVInt(code);\n          out.writeVInt(update.docIDUpto);\n          if (termField.equals(lastTermField) == false) {\n            out.writeString(termField);\n            lastTermField = termField;\n          }\n          if (updateField.equals(lastUpdateField) == false) {\n            out.writeString(updateField);\n            lastUpdateField = updateField;\n          }\n          out.writeBytes(update.term.bytes().bytes, update.term.bytes().offset, update.term.bytes().length);\n          if (update.hasValue()) {\n            update.writeTo(out);\n          }\n        }\n      }\n      byte[] bytes = new byte[(int) out.getFilePointer()];\n      out.writeTo(bytes, 0);\n      return bytes;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"28211671436f185419b3f7e53ccfc3911441ab65":["33adea025f43af3243278587a46b8d9fd2e8ccf9"],"7e129bd6cb34a236558a49edf108a49d5c15e0e1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"33adea025f43af3243278587a46b8d9fd2e8ccf9":["f42883db49d143abc1a0f176ba47e3388dafb608"],"f42883db49d143abc1a0f176ba47e3388dafb608":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7e129bd6cb34a236558a49edf108a49d5c15e0e1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28211671436f185419b3f7e53ccfc3911441ab65"]},"commit2Childs":{"28211671436f185419b3f7e53ccfc3911441ab65":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7e129bd6cb34a236558a49edf108a49d5c15e0e1":["f42883db49d143abc1a0f176ba47e3388dafb608"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7e129bd6cb34a236558a49edf108a49d5c15e0e1","f42883db49d143abc1a0f176ba47e3388dafb608"],"33adea025f43af3243278587a46b8d9fd2e8ccf9":["28211671436f185419b3f7e53ccfc3911441ab65"],"f42883db49d143abc1a0f176ba47e3388dafb608":["33adea025f43af3243278587a46b8d9fd2e8ccf9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}