{"path":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","commits":[{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","pathOld":"/dev/null","sourceNew":"      /* Invert one occurrence of one field in the document */\n      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException {\n\n        if (length>0)\n          position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          token = localToken;\n          String stringValue = field.stringValue();\n          token.setTermText(stringValue);\n          token.setStartOffset(offset);\n          token.setEndOffset(offset + stringValue.length());\n          addPosition();\n          offset += stringValue.length();\n          length++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              stringReader.init(stringValue);\n              reader = stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = analyzer.tokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            offsetEnd = offset-1;\n            for (token = stream.next(); token != null; token = stream.next()) {\n              position += (token.getPositionIncrement() - 1);\n              addPosition();\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        boost *= field.getBoost();\n      }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6864413dbc0c12104c978c05456f3da1d45adb03","date":1186770873,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","sourceNew":"      /* Invert one occurrence of one field in the document */\n      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException {\n\n        if (length>0)\n          position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          Token token = localToken;\n          token.setTermText(stringValue);\n          token.setStartOffset(offset);\n          token.setEndOffset(offset + stringValue.length());\n          addPosition(token);\n          offset += stringValue.length();\n          length++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              stringReader.init(stringValue);\n              reader = stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            offsetEnd = offset-1;\n            Token token;\n            while((token = stream.next(localToken)) != null) {\n              position += (token.getPositionIncrement() - 1);\n              addPosition(token);\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        boost *= field.getBoost();\n      }\n\n","sourceOld":"      /* Invert one occurrence of one field in the document */\n      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException {\n\n        if (length>0)\n          position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          token = localToken;\n          String stringValue = field.stringValue();\n          token.setTermText(stringValue);\n          token.setStartOffset(offset);\n          token.setEndOffset(offset + stringValue.length());\n          addPosition();\n          offset += stringValue.length();\n          length++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              stringReader.init(stringValue);\n              reader = stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = analyzer.tokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            offsetEnd = offset-1;\n            for (token = stream.next(); token != null; token = stream.next()) {\n              position += (token.getPositionIncrement() - 1);\n              addPosition();\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        boost *= field.getBoost();\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"30f43b40223d2630f5d720fe68cadf04b354a1c5","date":1195499437,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","sourceNew":"      /* Invert one occurrence of one field in the document */\n      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException {\n\n        if (length>0)\n          position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          Token token = localToken;\n          token.clear();\n          token.setTermText(stringValue);\n          token.setStartOffset(offset);\n          token.setEndOffset(offset + stringValue.length());\n          addPosition(token);\n          offset += stringValue.length();\n          length++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              stringReader.init(stringValue);\n              reader = stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            offsetEnd = offset-1;\n            Token token;\n            for(;;) {\n              localToken.clear();\n              token = stream.next(localToken);\n              if (token == null) break;\n              position += (token.getPositionIncrement() - 1);\n              addPosition(token);\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        boost *= field.getBoost();\n      }\n\n","sourceOld":"      /* Invert one occurrence of one field in the document */\n      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException {\n\n        if (length>0)\n          position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          Token token = localToken;\n          token.setTermText(stringValue);\n          token.setStartOffset(offset);\n          token.setEndOffset(offset + stringValue.length());\n          addPosition(token);\n          offset += stringValue.length();\n          length++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              stringReader.init(stringValue);\n              reader = stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            offsetEnd = offset-1;\n            Token token;\n            while((token = stream.next(localToken)) != null) {\n              position += (token.getPositionIncrement() - 1);\n              addPosition(token);\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        boost *= field.getBoost();\n      }\n\n","bugFix":null,"bugIntro":["fee44d0bd0b9443ff6068d0ba8458fd103dff4aa"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fee44d0bd0b9443ff6068d0ba8458fd103dff4aa","date":1199000070,"type":3,"author":"Doron Cohen","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","sourceNew":"      /* Invert one occurrence of one field in the document */\n      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException {\n\n        if (length>0)\n          position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          Token token = localToken;\n          token.clear();\n          token.setTermText(stringValue);\n          token.setStartOffset(offset);\n          token.setEndOffset(offset + stringValue.length());\n          addPosition(token);\n          offset += stringValue.length();\n          length++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              stringReader.init(stringValue);\n              reader = stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            offsetEnd = offset-1;\n            Token token;\n            for(;;) {\n              token = stream.next(localToken);\n              if (token == null) break;\n              position += (token.getPositionIncrement() - 1);\n              addPosition(token);\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        boost *= field.getBoost();\n      }\n\n","sourceOld":"      /* Invert one occurrence of one field in the document */\n      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException {\n\n        if (length>0)\n          position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          Token token = localToken;\n          token.clear();\n          token.setTermText(stringValue);\n          token.setStartOffset(offset);\n          token.setEndOffset(offset + stringValue.length());\n          addPosition(token);\n          offset += stringValue.length();\n          length++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              stringReader.init(stringValue);\n              reader = stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            offsetEnd = offset-1;\n            Token token;\n            for(;;) {\n              localToken.clear();\n              token = stream.next(localToken);\n              if (token == null) break;\n              position += (token.getPositionIncrement() - 1);\n              addPosition(token);\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        boost *= field.getBoost();\n      }\n\n","bugFix":["30f43b40223d2630f5d720fe68cadf04b354a1c5"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cb32d170321240dece082a7706be297fbb09adf9","date":1199699286,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","sourceNew":"      /* Invert one occurrence of one field in the document */\n      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException {\n\n        if (length>0)\n          position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          Token token = localToken;\n          token.clear();\n          char[] termBuffer = token.termBuffer();\n          if (termBuffer.length < valueLength)\n            termBuffer = token.resizeTermBuffer(valueLength);\n          stringValue.getChars(0, valueLength, termBuffer, 0);\n          token.setTermLength(valueLength);\n          token.setStartOffset(offset);\n          token.setEndOffset(offset + stringValue.length());\n          addPosition(token);\n          offset += stringValue.length();\n          length++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              stringReader.init(stringValue);\n              reader = stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            offsetEnd = offset-1;\n            Token token;\n            for(;;) {\n              token = stream.next(localToken);\n              if (token == null) break;\n              position += (token.getPositionIncrement() - 1);\n              addPosition(token);\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        boost *= field.getBoost();\n      }\n\n","sourceOld":"      /* Invert one occurrence of one field in the document */\n      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException {\n\n        if (length>0)\n          position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          Token token = localToken;\n          token.clear();\n          token.setTermText(stringValue);\n          token.setStartOffset(offset);\n          token.setEndOffset(offset + stringValue.length());\n          addPosition(token);\n          offset += stringValue.length();\n          length++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              stringReader.init(stringValue);\n              reader = stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            offsetEnd = offset-1;\n            Token token;\n            for(;;) {\n              token = stream.next(localToken);\n              if (token == null) break;\n              position += (token.getPositionIncrement() - 1);\n              addPosition(token);\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        boost *= field.getBoost();\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"83bbb041887bbef07b8a98d08a0e1713ce137039","date":1200330381,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","sourceNew":"      /* Invert one occurrence of one field in the document */\n      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException, AbortException {\n\n        if (length>0)\n          position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          Token token = localToken;\n          token.clear();\n          char[] termBuffer = token.termBuffer();\n          if (termBuffer.length < valueLength)\n            termBuffer = token.resizeTermBuffer(valueLength);\n          stringValue.getChars(0, valueLength, termBuffer, 0);\n          token.setTermLength(valueLength);\n          token.setStartOffset(offset);\n          token.setEndOffset(offset + stringValue.length());\n          addPosition(token);\n          offset += stringValue.length();\n          length++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              stringReader.init(stringValue);\n              reader = stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            offsetEnd = offset-1;\n            Token token;\n            for(;;) {\n              token = stream.next(localToken);\n              if (token == null) break;\n              position += (token.getPositionIncrement() - 1);\n              addPosition(token);\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        boost *= field.getBoost();\n      }\n\n","sourceOld":"      /* Invert one occurrence of one field in the document */\n      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException {\n\n        if (length>0)\n          position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          Token token = localToken;\n          token.clear();\n          char[] termBuffer = token.termBuffer();\n          if (termBuffer.length < valueLength)\n            termBuffer = token.resizeTermBuffer(valueLength);\n          stringValue.getChars(0, valueLength, termBuffer, 0);\n          token.setTermLength(valueLength);\n          token.setStartOffset(offset);\n          token.setEndOffset(offset + stringValue.length());\n          addPosition(token);\n          offset += stringValue.length();\n          length++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              stringReader.init(stringValue);\n              reader = stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            offsetEnd = offset-1;\n            Token token;\n            for(;;) {\n              token = stream.next(localToken);\n              if (token == null) break;\n              position += (token.getPositionIncrement() - 1);\n              addPosition(token);\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        boost *= field.getBoost();\n      }\n\n","bugFix":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a0af3a442be522899177e5e11384a45a6784a3f","date":1205348952,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentsWriter.ThreadState.FieldData#invertField(Fieldable,Analyzer,int).mjava","sourceNew":null,"sourceOld":"      /* Invert one occurrence of one field in the document */\n      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException, AbortException {\n\n        if (length>0)\n          position += analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          Token token = localToken;\n          token.clear();\n          char[] termBuffer = token.termBuffer();\n          if (termBuffer.length < valueLength)\n            termBuffer = token.resizeTermBuffer(valueLength);\n          stringValue.getChars(0, valueLength, termBuffer, 0);\n          token.setTermLength(valueLength);\n          token.setStartOffset(offset);\n          token.setEndOffset(offset + stringValue.length());\n          addPosition(token);\n          offset += stringValue.length();\n          length++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              stringReader.init(stringValue);\n              reader = stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            offsetEnd = offset-1;\n            Token token;\n            for(;;) {\n              token = stream.next(localToken);\n              if (token == null) break;\n              position += (token.getPositionIncrement() - 1);\n              addPosition(token);\n              if (++length >= maxFieldLength) {\n                if (infoStream != null)\n                  infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        boost *= field.getBoost();\n      }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"30f43b40223d2630f5d720fe68cadf04b354a1c5":["6864413dbc0c12104c978c05456f3da1d45adb03"],"6864413dbc0c12104c978c05456f3da1d45adb03":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"83bbb041887bbef07b8a98d08a0e1713ce137039":["cb32d170321240dece082a7706be297fbb09adf9"],"fee44d0bd0b9443ff6068d0ba8458fd103dff4aa":["30f43b40223d2630f5d720fe68cadf04b354a1c5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5a0af3a442be522899177e5e11384a45a6784a3f":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"cb32d170321240dece082a7706be297fbb09adf9":["fee44d0bd0b9443ff6068d0ba8458fd103dff4aa"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5a0af3a442be522899177e5e11384a45a6784a3f"]},"commit2Childs":{"30f43b40223d2630f5d720fe68cadf04b354a1c5":["fee44d0bd0b9443ff6068d0ba8458fd103dff4aa"],"6864413dbc0c12104c978c05456f3da1d45adb03":["30f43b40223d2630f5d720fe68cadf04b354a1c5"],"83bbb041887bbef07b8a98d08a0e1713ce137039":["5a0af3a442be522899177e5e11384a45a6784a3f"],"fee44d0bd0b9443ff6068d0ba8458fd103dff4aa":["cb32d170321240dece082a7706be297fbb09adf9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"cb32d170321240dece082a7706be297fbb09adf9":["83bbb041887bbef07b8a98d08a0e1713ce137039"],"5a0af3a442be522899177e5e11384a45a6784a3f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["6864413dbc0c12104c978c05456f3da1d45adb03"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}