{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter#testPositionIncrements().mjava","commits":[{"id":"c85fa43e6918808743daa7847ba0264373af687f","date":1395166336,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter#testPositionIncrements().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 1 },\n        null,\n        false);\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 1, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 1, 1 },\n        null,\n        false);\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        null,\n        new int[] { 1, 10, 1 },\n        null,\n        false);\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 11 },\n        null,\n        false);\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 11, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 11, 1 },\n        null,\n        false);\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        null,\n        new int[] { 1, 1, 0 },\n        null,\n        false);\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        null,\n        new int[] { 2, 1, 0 },\n        null,\n        false);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff4227bb146f97aabae888091c19e48c88dbb0db","date":1406758576,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter#testPositionIncrements().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 1 },\n        null,\n        false);\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 1, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 1, 1 },\n        null,\n        false);\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        null,\n        new int[] { 1, 10, 1 },\n        null,\n        false);\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 11 },\n        null,\n        false);\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 11, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 11, 1 },\n        null,\n        false);\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        null,\n        new int[] { 1, 1, 0 },\n        null,\n        false);\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        null,\n        new int[] { 2, 1, 0 },\n        null,\n        false);\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 1 },\n        null,\n        false);\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 1, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 1, 1 },\n        null,\n        false);\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        null,\n        new int[] { 1, 10, 1 },\n        null,\n        false);\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 11 },\n        null,\n        false);\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 11, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 11, 1 },\n        null,\n        false);\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        null,\n        new int[] { 1, 1, 0 },\n        null,\n        false);\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        null,\n        new int[] { 2, 1, 0 },\n        null,\n        false);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5cdab62f058ea765dd33deb05b4f19b7d626c801","date":1406803479,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter#testPositionIncrements().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 1 },\n        null,\n        false);\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 1, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 1, 1 },\n        null,\n        false);\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        null,\n        new int[] { 1, 10, 1 },\n        null,\n        false);\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 11 },\n        null,\n        false);\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 11, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 11, 1 },\n        null,\n        false);\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        null,\n        new int[] { 1, 1, 0 },\n        null,\n        false);\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        null,\n        new int[] { 2, 1, 0 },\n        null,\n        false);\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 1 },\n        null,\n        false);\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 1, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 1, 1 },\n        null,\n        false);\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        null,\n        new int[] { 1, 10, 1 },\n        null,\n        false);\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 11 },\n        null,\n        false);\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 11, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 11, 1 },\n        null,\n        false);\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        null,\n        new int[] { 1, 1, 0 },\n        null,\n        false);\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        null,\n        new int[] { 2, 1, 0 },\n        null,\n        false);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"379db3ad24c4f0214f30a122265a6d6be003a99d","date":1407537768,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter#testPositionIncrements().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 1 },\n        null,\n        false);\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 1, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 1, 1 },\n        null,\n        false);\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        null,\n        new int[] { 1, 10, 1 },\n        null,\n        false);\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 11 },\n        null,\n        false);\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 11, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 11, 1 },\n        null,\n        false);\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        null,\n        new int[] { 1, 1, 0 },\n        null,\n        false);\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        null,\n        new int[] { 2, 1, 0 },\n        null,\n        false);\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 1 },\n        null,\n        false);\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 1, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 1, 1 },\n        null,\n        false);\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        null,\n        new int[] { 1, 10, 1 },\n        null,\n        false);\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 11 },\n        null,\n        false);\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 11, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 11, 1 },\n        null,\n        false);\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        null,\n        new int[] { 1, 1, 0 },\n        null,\n        false);\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        null,\n        new int[] { 2, 1, 0 },\n        null,\n        false);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71387d8cb6923eb831b17a8b734608ba2e21c653","date":1414126093,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLucene47WordDelimiterFilter#testPositionIncrements().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final int flags = GENERATE_WORD_PARTS | GENERATE_NUMBER_PARTS | CATENATE_ALL | SPLIT_ON_CASE_CHANGE | SPLIT_ON_NUMERICS | STEM_ENGLISH_POSSESSIVE;\n    final CharArraySet protWords = new CharArraySet(new HashSet<>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            tokenizer,\n            flags, protWords));\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 1 },\n        null,\n        false);\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 1, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 1, 1 },\n        null,\n        false);\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(\n            new LargePosIncTokenFilter(tokenizer),\n            flags, protWords));\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        null,\n        new int[] { 1, 10, 1 },\n        null,\n        false);\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        null,\n        new int[] { 1, 11 },\n        null,\n        false);\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        null,\n        new int[] { 1, 11, 1, 0 },\n        null,\n        false);\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        null,\n        new int[] { 1, 11, 1 },\n        null,\n        false);\n\n    Analyzer a3 = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String field) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        StopFilter filter = new StopFilter(tokenizer, StandardAnalyzer.STOP_WORDS_SET);\n        return new TokenStreamComponents(tokenizer, new Lucene47WordDelimiterFilter(filter, flags, protWords));\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        null,\n        new int[] { 1, 1, 0 },\n        null,\n        false);\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        null,\n        new int[] { 2, 1, 0 },\n        null,\n        false);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ff4227bb146f97aabae888091c19e48c88dbb0db":["c85fa43e6918808743daa7847ba0264373af687f"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"c85fa43e6918808743daa7847ba0264373af687f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71387d8cb6923eb831b17a8b734608ba2e21c653"]},"commit2Childs":{"ff4227bb146f97aabae888091c19e48c88dbb0db":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c85fa43e6918808743daa7847ba0264373af687f"],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"c85fa43e6918808743daa7847ba0264373af687f":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}