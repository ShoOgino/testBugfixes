{"path":"contrib/miscellaneous/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","commits":[{"id":"a56158a9862832b67c76de543de1da36596a1133","date":1219676540,"type":0,"author":"Karl-Johan Wettin","isMerge":false,"pathNew":"contrib/miscellaneous/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"/dev/null","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a","date":1221082732,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"contrib/miscellaneous/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"contrib/miscellaneous/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"48bedd31c61edafb8baaff4bcbcac19449fb7c3a","date":1251468037,"type":5,"author":"Mark Robert Miller","isMerge":false,"pathNew":"contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","pathOld":"contrib/miscellaneous/src/test/org/apache/lucene/index/TestTermVectorAccessor#test().mjava","sourceNew":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","sourceOld":"  public void test() throws Exception {\n\n    Directory dir = new RAMDirectory();\n    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);\n\n    Document doc;\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    iw.addDocument(doc);\n\n    doc = new Document();\n    doc.add(new Field(\"a\", \"a b a c a d a e a f a g a h a\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    doc.add(new Field(\"b\", \"a b c b d b e b f b g b h b\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.NO));\n    doc.add(new Field(\"c\", \"a c b c d c e c f c g c h c\", Field.Store.NO, Field.Index.ANALYZED, Field.TermVector.YES));\n    iw.addDocument(doc);\n\n    iw.close();\n\n    IndexReader ir = IndexReader.open(dir);\n\n    TermVectorAccessor accessor = new TermVectorAccessor();\n\n    ParallelArrayTermVectorMapper mapper;\n    TermFreqVector tfv;\n\n    for (int i = 0; i < ir.maxDoc(); i++) {\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"a\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, \"a\", tfv.getTerms()[0]);\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies()[0]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"b\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"b\", tfv.getTerms()[1]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[1]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"c\", mapper);\n      tfv = mapper.materializeVector();\n      assertEquals(\"doc \" + i, 8, tfv.getTermFrequencies().length);\n      assertEquals(\"doc \" + i, \"c\", tfv.getTerms()[2]);\n      assertEquals(\"doc \" + i, 7, tfv.getTermFrequencies()[2]);\n\n      mapper = new ParallelArrayTermVectorMapper();\n      accessor.accept(ir, i, \"q\", mapper);\n      tfv = mapper.materializeVector();\n      assertNull(\"doc \" + i, tfv);\n\n    }\n\n    ir.close();\n\n    dir.close();\n\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"48bedd31c61edafb8baaff4bcbcac19449fb7c3a":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["a56158a9862832b67c76de543de1da36596a1133"],"a56158a9862832b67c76de543de1da36596a1133":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["48bedd31c61edafb8baaff4bcbcac19449fb7c3a"]},"commit2Childs":{"48bedd31c61edafb8baaff4bcbcac19449fb7c3a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a56158a9862832b67c76de543de1da36596a1133"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["48bedd31c61edafb8baaff4bcbcac19449fb7c3a"],"a56158a9862832b67c76de543de1da36596a1133":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}