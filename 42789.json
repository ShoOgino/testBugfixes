{"path":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#doTest().mjava","commits":[{"id":"ca70fffb953aae4d27efbfc28758033a317f39b1","date":1396371342,"type":0,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    waitForThingsToLevelOut(30);\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["072f211dfa8387028bb978d128c35bf9a450bbbf"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#doTest().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    waitForThingsToLevelOut(30);\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"072f211dfa8387028bb978d128c35bf9a450bbbf","date":1406041363,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#doTest().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#doTest().mjava","sourceNew":"  @Override\n  public void doTest() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    waitForThingsToLevelOut(30);\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    waitForThingsToLevelOut(30);\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","bugFix":["ca70fffb953aae4d27efbfc28758033a317f39b1"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":5,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/DistribDocExpirationUpdateProcessorTest#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    waitForThingsToLevelOut(30);\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    assertTrue(\"only one shard?!?!?!\", 1 < shardToJetty.keySet().size());\n    log.info(\"number of shards: {}\", shardToJetty.keySet().size());\n\n    handle.clear();\n    handle.put(\"maxScore\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    // some docs with no expiration\n    for (int i = 1; i <= 100; i++) {\n      indexDoc(sdoc(\"id\", i));\n    }\n    commit();\n    waitForThingsToLevelOut(30);\n\n    // this doc better not already exist\n    waitForNoResults(0, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"sanity_check\"));\n    \n    // record the indexversion for each server so we can check later\n    // that it only changes for one shard\n    final Map<String,Long> initIndexVersions = getIndexVersionOfAllReplicas();\n    assertTrue(\"WTF? no versions?\", 0 < initIndexVersions.size());\n\n\n    // add a doc with a short TTL \n    indexDoc(sdoc(\"id\", \"999\", \"tTl_s\",\"+30SECONDS\"));\n    commit();\n\n    // wait for one doc to be deleted\n    waitForNoResults(180, params(\"q\",\"id:999\",\"rows\",\"0\",\"_trace\",\"did_it_expire_yet\"));\n\n    // verify only one shard changed\n    waitForThingsToLevelOut(30);\n    final Map<String,Long> finalIndexVersions = getIndexVersionOfAllReplicas();\n    assertEquals(\"WTF? not same num versions?\", \n                 initIndexVersions.size(),\n                 finalIndexVersions.size());\n    \n    final Set<String> nodesThatChange = new HashSet<String>();\n    final Set<String> shardsThatChange = new HashSet<String>();\n    \n    int coresCompared = 0;\n    for (String shard : shardToJetty.keySet()) {\n      for (CloudJettyRunner replicaRunner : shardToJetty.get(shard)) {\n        coresCompared++;\n\n        String core = replicaRunner.coreNodeName;\n        Long initVersion = initIndexVersions.get(core);\n        Long finalVersion = finalIndexVersions.get(core);\n        assertNotNull(shard + \": no init version for core: \" + core, initVersion);\n        assertNotNull(shard + \": no final version for core: \" + core, finalVersion);\n\n        if (!initVersion.equals(finalVersion)) {\n          nodesThatChange.add(core + \"(\"+shard+\")\");\n          shardsThatChange.add(shard);\n        }\n      }\n    }\n\n    assertEquals(\"Exactly one shard should have changed, instead: \" + shardsThatChange\n                 + \" nodes=(\" + nodesThatChange + \")\",\n                 1, shardsThatChange.size());\n    assertEquals(\"somehow we missed some cores?\", \n                 initIndexVersions.size(), coresCompared);\n\n    // TODO: above logic verifies that deleteByQuery happens on all nodes, and ...\n    // doesn't affect searcher re-open on shards w/o expired docs ... can we also verify \n    // that *only* one node is sending the deletes ?\n    // (ie: no flood of redundent deletes?)\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5eb2511ababf862ea11e10761c70ee560cd84510":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ca70fffb953aae4d27efbfc28758033a317f39b1"],"072f211dfa8387028bb978d128c35bf9a450bbbf":["ca70fffb953aae4d27efbfc28758033a317f39b1"],"abb23fcc2461782ab204e61213240feb77d355aa":["072f211dfa8387028bb978d128c35bf9a450bbbf"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ca70fffb953aae4d27efbfc28758033a317f39b1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["abb23fcc2461782ab204e61213240feb77d355aa"]},"commit2Childs":{"5eb2511ababf862ea11e10761c70ee560cd84510":[],"072f211dfa8387028bb978d128c35bf9a450bbbf":["abb23fcc2461782ab204e61213240feb77d355aa"],"abb23fcc2461782ab204e61213240feb77d355aa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5eb2511ababf862ea11e10761c70ee560cd84510","ca70fffb953aae4d27efbfc28758033a317f39b1"],"ca70fffb953aae4d27efbfc28758033a317f39b1":["5eb2511ababf862ea11e10761c70ee560cd84510","072f211dfa8387028bb978d128c35bf9a450bbbf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5eb2511ababf862ea11e10761c70ee560cd84510","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}