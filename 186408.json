{"path":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimUtils#calculateStats(SolrCloudManager,AutoScalingConfig,boolean).mjava","commits":[{"id":"edf5b262a72d10530eb2f01dc8f19060355b213e","date":1557765866,"type":0,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimUtils#calculateStats(SolrCloudManager,AutoScalingConfig,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Calculate statistics of node / collection and replica layouts for the provided {@link SolrCloudManager}.\n   * @param cloudManager manager\n   * @param config autoscaling config, or null if the one from the provided manager should be used\n   * @param verbose if true then add more details about replicas.\n   * @return a map containing detailed statistics\n   */\n  public static Map<String, Object> calculateStats(SolrCloudManager cloudManager, AutoScalingConfig config, boolean verbose) throws Exception {\n    ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n    Map<String, Map<String, Number>> collStats = new TreeMap<>();\n    Policy.Session session = config.getPolicy().createSession(cloudManager);\n    clusterState.forEachCollection(coll -> {\n      Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n      AtomicInteger numCores = new AtomicInteger();\n      HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n      coll.getSlices().forEach(s -> {\n        numCores.addAndGet(s.getReplicas().size());\n        s.getReplicas().forEach(r -> {\n          nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n              .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n        });\n      });\n      int maxCoresPerNode = 0;\n      int minCoresPerNode = 0;\n      int maxActualShardsPerNode = 0;\n      int minActualShardsPerNode = 0;\n      int maxShardReplicasPerNode = 0;\n      int minShardReplicasPerNode = 0;\n      if (!nodes.isEmpty()) {\n        minCoresPerNode = Integer.MAX_VALUE;\n        minActualShardsPerNode = Integer.MAX_VALUE;\n        minShardReplicasPerNode = Integer.MAX_VALUE;\n        for (Map<String, AtomicInteger> counts : nodes.values()) {\n          int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n          for (AtomicInteger count : counts.values()) {\n            if (count.get() > maxShardReplicasPerNode) {\n              maxShardReplicasPerNode = count.get();\n            }\n            if (count.get() < minShardReplicasPerNode) {\n              minShardReplicasPerNode = count.get();\n            }\n          }\n          if (total > maxCoresPerNode) {\n            maxCoresPerNode = total;\n          }\n          if (total < minCoresPerNode) {\n            minCoresPerNode = total;\n          }\n          if (counts.size() > maxActualShardsPerNode) {\n            maxActualShardsPerNode = counts.size();\n          }\n          if (counts.size() < minActualShardsPerNode) {\n            minActualShardsPerNode = counts.size();\n          }\n        }\n      }\n      perColl.put(\"activeShards\", coll.getActiveSlices().size());\n      perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n      perColl.put(\"rf\", coll.getReplicationFactor());\n      perColl.put(\"maxShardsPerNode\", coll.getMaxShardsPerNode());\n      perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n      perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n      perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n      perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n      perColl.put(\"numCores\", numCores.get());\n      perColl.put(\"numNodes\", nodes.size());\n      perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n      perColl.put(\"minCoresPerNode\", minCoresPerNode);\n    });\n    Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n    Map<Integer, AtomicInteger> coreStats = new TreeMap<>();\n    List<Row> rows = session.getSortedNodes();\n    // check consistency\n    if (rows.size() != clusterState.getLiveNodes().size()) {\n      throw new Exception(\"Mismatch between autoscaling matrix size (\" + rows.size() + \") and liveNodes size (\" + clusterState.getLiveNodes().size() + \")\");\n    }\n    for (Row row : rows) {\n      Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n      nodeStat.put(\"isLive\", row.isLive());\n      nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n      nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n      int cores = ((Number)row.getVal(\"cores\", 0)).intValue();\n      nodeStat.put(\"cores\", cores);\n      coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();\n      Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n      // check consistency\n      AtomicInteger rowCores = new AtomicInteger();\n      row.forEachReplica(ri -> rowCores.incrementAndGet());\n      if (cores != rowCores.get()) {\n        throw new Exception(\"Mismatch between autoscaling matrix row replicas (\" + rowCores.get() + \") and number of cores (\" + cores + \")\");\n      }\n      row.forEachReplica(ri -> {\n        Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n            .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n//            if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n//              perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n//            }\n        if (ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute) != null) {\n          perReplica.put(Variable.Type.CORE_IDX.metricsAttribute, ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute));\n          if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n            perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n          } else {\n            perReplica.put(Variable.Type.CORE_IDX.tagName,\n                Variable.Type.CORE_IDX.convertVal(ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute)));\n          }\n        }\n        perReplica.put(\"coreNode\", ri.getName());\n        if (ri.isLeader || ri.getBool(\"leader\", false)) {\n          perReplica.put(\"leader\", true);\n          Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n              .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n          Number riSize = (Number)ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute);\n          if (riSize != null) {\n            totalSize += riSize.doubleValue();\n            collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n            Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n            if (max == null) max = 0.0;\n            if (riSize.doubleValue() > max) {\n              collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n            }\n            Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n            if (min == null) min = Double.MAX_VALUE;\n            if (riSize.doubleValue() < min) {\n              collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n            }\n          } else {\n            throw new RuntimeException(\"ReplicaInfo without size information: \" + ri);\n          }\n        }\n        if (verbose) {\n          nodeStat.put(\"replicas\", collReplicas);\n        }\n      });\n    }\n\n    // calculate average per shard and convert the units\n    for (Map<String, Number> perColl : collStats.values()) {\n      Number avg = perColl.get(\"avgShardSize\");\n      if (avg != null) {\n        avg = avg.doubleValue() / perColl.get(\"activeShards\").doubleValue();\n        perColl.put(\"avgShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(avg));\n      }\n      Number num = perColl.get(\"maxShardSize\");\n      if (num != null) {\n        perColl.put(\"maxShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n      num = perColl.get(\"minShardSize\");\n      if (num != null) {\n        perColl.put(\"minShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n    }\n    Map<String, Object> stats = new LinkedHashMap<>();\n    stats.put(\"coresPerNodes\", coreStats);\n    stats.put(\"sortedNodeStats\", nodeStats);\n    stats.put(\"collectionStats\", collStats);\n    return stats;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3d2a34ea3732f91149b31bcad82026ad85fda69","date":1567850949,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimUtils#calculateStats(SolrCloudManager,AutoScalingConfig,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimUtils#calculateStats(SolrCloudManager,AutoScalingConfig,boolean).mjava","sourceNew":"  /**\n   * Calculate statistics of node / collection and replica layouts for the provided {@link SolrCloudManager}.\n   * @param cloudManager manager\n   * @param config autoscaling config, or null if the one from the provided manager should be used\n   * @param verbose if true then add more details about replicas.\n   * @return a map containing detailed statistics\n   */\n  public static Map<String, Object> calculateStats(SolrCloudManager cloudManager, AutoScalingConfig config, boolean verbose) throws Exception {\n    ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n    Map<String, Map<String, Number>> collStats = new TreeMap<>();\n    Policy.Session session = config.getPolicy().createSession(cloudManager);\n    clusterState.forEachCollection(coll -> {\n      Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n      AtomicInteger numCores = new AtomicInteger();\n      HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n      coll.getSlices().forEach(s -> {\n        numCores.addAndGet(s.getReplicas().size());\n        s.getReplicas().forEach(r -> {\n          nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n              .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n        });\n      });\n      int maxCoresPerNode = 0;\n      int minCoresPerNode = 0;\n      int maxActualShardsPerNode = 0;\n      int minActualShardsPerNode = 0;\n      int maxShardReplicasPerNode = 0;\n      int minShardReplicasPerNode = 0;\n      if (!nodes.isEmpty()) {\n        minCoresPerNode = Integer.MAX_VALUE;\n        minActualShardsPerNode = Integer.MAX_VALUE;\n        minShardReplicasPerNode = Integer.MAX_VALUE;\n        for (Map<String, AtomicInteger> counts : nodes.values()) {\n          int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n          for (AtomicInteger count : counts.values()) {\n            if (count.get() > maxShardReplicasPerNode) {\n              maxShardReplicasPerNode = count.get();\n            }\n            if (count.get() < minShardReplicasPerNode) {\n              minShardReplicasPerNode = count.get();\n            }\n          }\n          if (total > maxCoresPerNode) {\n            maxCoresPerNode = total;\n          }\n          if (total < minCoresPerNode) {\n            minCoresPerNode = total;\n          }\n          if (counts.size() > maxActualShardsPerNode) {\n            maxActualShardsPerNode = counts.size();\n          }\n          if (counts.size() < minActualShardsPerNode) {\n            minActualShardsPerNode = counts.size();\n          }\n        }\n      }\n      perColl.put(\"activeShards\", coll.getActiveSlices().size());\n      perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n      perColl.put(\"rf\", coll.getReplicationFactor());\n      perColl.put(\"maxShardsPerNode\", coll.getMaxShardsPerNode());\n      perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n      perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n      perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n      perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n      perColl.put(\"numCores\", numCores.get());\n      perColl.put(\"numNodes\", nodes.size());\n      perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n      perColl.put(\"minCoresPerNode\", minCoresPerNode);\n    });\n    Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n    Map<Integer, AtomicInteger> coreStats = new TreeMap<>();\n    List<Row> rows = session.getSortedNodes();\n    // check consistency\n    if (rows.size() != clusterState.getLiveNodes().size()) {\n      throw new Exception(\"Mismatch between autoscaling matrix size (\" + rows.size() + \") and liveNodes size (\" + clusterState.getLiveNodes().size() + \")\");\n    }\n    for (Row row : rows) {\n      Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n      nodeStat.put(\"isLive\", row.isLive());\n      for (Cell cell : row.getCells()) {\n        nodeStat.put(cell.getName(), cell.getValue());\n      }\n//      nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n//      nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n      int cores = ((Number)row.getVal(\"cores\", 0)).intValue();\n//      nodeStat.put(\"cores\", cores);\n      coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();\n      Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n      // check consistency\n      AtomicInteger rowCores = new AtomicInteger();\n      row.forEachReplica(ri -> rowCores.incrementAndGet());\n      if (cores != rowCores.get()) {\n        throw new Exception(\"Mismatch between autoscaling matrix row replicas (\" + rowCores.get() + \") and number of cores (\" + cores + \")\");\n      }\n      row.forEachReplica(ri -> {\n        Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n            .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n//            if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n//              perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n//            }\n        if (ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute) != null) {\n          perReplica.put(Variable.Type.CORE_IDX.metricsAttribute, ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute));\n          if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n            perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n          } else {\n            perReplica.put(Variable.Type.CORE_IDX.tagName,\n                Variable.Type.CORE_IDX.convertVal(ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute)));\n          }\n        }\n        perReplica.put(\"coreNode\", ri.getName());\n        if (ri.isLeader || ri.getBool(\"leader\", false)) {\n          perReplica.put(\"leader\", true);\n          Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n              .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n          Number riSize = (Number)ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute);\n          if (riSize != null) {\n            totalSize += riSize.doubleValue();\n            collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n            Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n            if (max == null) max = 0.0;\n            if (riSize.doubleValue() > max) {\n              collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n            }\n            Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n            if (min == null) min = Double.MAX_VALUE;\n            if (riSize.doubleValue() < min) {\n              collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n            }\n          } else {\n            throw new RuntimeException(\"ReplicaInfo without size information: \" + ri);\n          }\n        }\n        if (verbose) {\n          nodeStat.put(\"replicas\", collReplicas);\n        }\n      });\n    }\n\n    // calculate average per shard and convert the units\n    for (Map<String, Number> perColl : collStats.values()) {\n      Number avg = perColl.get(\"avgShardSize\");\n      if (avg != null) {\n        avg = avg.doubleValue() / perColl.get(\"activeShards\").doubleValue();\n        perColl.put(\"avgShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(avg));\n      }\n      Number num = perColl.get(\"maxShardSize\");\n      if (num != null) {\n        perColl.put(\"maxShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n      num = perColl.get(\"minShardSize\");\n      if (num != null) {\n        perColl.put(\"minShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n    }\n    Map<String, Object> stats = new LinkedHashMap<>();\n    stats.put(\"coresPerNodes\", coreStats);\n    stats.put(\"sortedNodeStats\", nodeStats);\n    stats.put(\"collectionStats\", collStats);\n    return stats;\n  }\n\n","sourceOld":"  /**\n   * Calculate statistics of node / collection and replica layouts for the provided {@link SolrCloudManager}.\n   * @param cloudManager manager\n   * @param config autoscaling config, or null if the one from the provided manager should be used\n   * @param verbose if true then add more details about replicas.\n   * @return a map containing detailed statistics\n   */\n  public static Map<String, Object> calculateStats(SolrCloudManager cloudManager, AutoScalingConfig config, boolean verbose) throws Exception {\n    ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n    Map<String, Map<String, Number>> collStats = new TreeMap<>();\n    Policy.Session session = config.getPolicy().createSession(cloudManager);\n    clusterState.forEachCollection(coll -> {\n      Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n      AtomicInteger numCores = new AtomicInteger();\n      HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n      coll.getSlices().forEach(s -> {\n        numCores.addAndGet(s.getReplicas().size());\n        s.getReplicas().forEach(r -> {\n          nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n              .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n        });\n      });\n      int maxCoresPerNode = 0;\n      int minCoresPerNode = 0;\n      int maxActualShardsPerNode = 0;\n      int minActualShardsPerNode = 0;\n      int maxShardReplicasPerNode = 0;\n      int minShardReplicasPerNode = 0;\n      if (!nodes.isEmpty()) {\n        minCoresPerNode = Integer.MAX_VALUE;\n        minActualShardsPerNode = Integer.MAX_VALUE;\n        minShardReplicasPerNode = Integer.MAX_VALUE;\n        for (Map<String, AtomicInteger> counts : nodes.values()) {\n          int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n          for (AtomicInteger count : counts.values()) {\n            if (count.get() > maxShardReplicasPerNode) {\n              maxShardReplicasPerNode = count.get();\n            }\n            if (count.get() < minShardReplicasPerNode) {\n              minShardReplicasPerNode = count.get();\n            }\n          }\n          if (total > maxCoresPerNode) {\n            maxCoresPerNode = total;\n          }\n          if (total < minCoresPerNode) {\n            minCoresPerNode = total;\n          }\n          if (counts.size() > maxActualShardsPerNode) {\n            maxActualShardsPerNode = counts.size();\n          }\n          if (counts.size() < minActualShardsPerNode) {\n            minActualShardsPerNode = counts.size();\n          }\n        }\n      }\n      perColl.put(\"activeShards\", coll.getActiveSlices().size());\n      perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n      perColl.put(\"rf\", coll.getReplicationFactor());\n      perColl.put(\"maxShardsPerNode\", coll.getMaxShardsPerNode());\n      perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n      perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n      perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n      perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n      perColl.put(\"numCores\", numCores.get());\n      perColl.put(\"numNodes\", nodes.size());\n      perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n      perColl.put(\"minCoresPerNode\", minCoresPerNode);\n    });\n    Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n    Map<Integer, AtomicInteger> coreStats = new TreeMap<>();\n    List<Row> rows = session.getSortedNodes();\n    // check consistency\n    if (rows.size() != clusterState.getLiveNodes().size()) {\n      throw new Exception(\"Mismatch between autoscaling matrix size (\" + rows.size() + \") and liveNodes size (\" + clusterState.getLiveNodes().size() + \")\");\n    }\n    for (Row row : rows) {\n      Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n      nodeStat.put(\"isLive\", row.isLive());\n      nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n      nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n      int cores = ((Number)row.getVal(\"cores\", 0)).intValue();\n      nodeStat.put(\"cores\", cores);\n      coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();\n      Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n      // check consistency\n      AtomicInteger rowCores = new AtomicInteger();\n      row.forEachReplica(ri -> rowCores.incrementAndGet());\n      if (cores != rowCores.get()) {\n        throw new Exception(\"Mismatch between autoscaling matrix row replicas (\" + rowCores.get() + \") and number of cores (\" + cores + \")\");\n      }\n      row.forEachReplica(ri -> {\n        Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n            .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n//            if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n//              perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n//            }\n        if (ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute) != null) {\n          perReplica.put(Variable.Type.CORE_IDX.metricsAttribute, ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute));\n          if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n            perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n          } else {\n            perReplica.put(Variable.Type.CORE_IDX.tagName,\n                Variable.Type.CORE_IDX.convertVal(ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute)));\n          }\n        }\n        perReplica.put(\"coreNode\", ri.getName());\n        if (ri.isLeader || ri.getBool(\"leader\", false)) {\n          perReplica.put(\"leader\", true);\n          Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n              .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n          Number riSize = (Number)ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute);\n          if (riSize != null) {\n            totalSize += riSize.doubleValue();\n            collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n            Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n            if (max == null) max = 0.0;\n            if (riSize.doubleValue() > max) {\n              collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n            }\n            Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n            if (min == null) min = Double.MAX_VALUE;\n            if (riSize.doubleValue() < min) {\n              collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n            }\n          } else {\n            throw new RuntimeException(\"ReplicaInfo without size information: \" + ri);\n          }\n        }\n        if (verbose) {\n          nodeStat.put(\"replicas\", collReplicas);\n        }\n      });\n    }\n\n    // calculate average per shard and convert the units\n    for (Map<String, Number> perColl : collStats.values()) {\n      Number avg = perColl.get(\"avgShardSize\");\n      if (avg != null) {\n        avg = avg.doubleValue() / perColl.get(\"activeShards\").doubleValue();\n        perColl.put(\"avgShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(avg));\n      }\n      Number num = perColl.get(\"maxShardSize\");\n      if (num != null) {\n        perColl.put(\"maxShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n      num = perColl.get(\"minShardSize\");\n      if (num != null) {\n        perColl.put(\"minShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n    }\n    Map<String, Object> stats = new LinkedHashMap<>();\n    stats.put(\"coresPerNodes\", coreStats);\n    stats.put(\"sortedNodeStats\", nodeStats);\n    stats.put(\"collectionStats\", collStats);\n    return stats;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimUtils#calculateStats(SolrCloudManager,AutoScalingConfig,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimUtils#calculateStats(SolrCloudManager,AutoScalingConfig,boolean).mjava","sourceNew":"  /**\n   * Calculate statistics of node / collection and replica layouts for the provided {@link SolrCloudManager}.\n   * @param cloudManager manager\n   * @param config autoscaling config, or null if the one from the provided manager should be used\n   * @param verbose if true then add more details about replicas.\n   * @return a map containing detailed statistics\n   */\n  public static Map<String, Object> calculateStats(SolrCloudManager cloudManager, AutoScalingConfig config, boolean verbose) throws Exception {\n    ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n    Map<String, Map<String, Number>> collStats = new TreeMap<>();\n    Policy.Session session = config.getPolicy().createSession(cloudManager);\n    clusterState.forEachCollection(coll -> {\n      Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n      AtomicInteger numCores = new AtomicInteger();\n      HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n      coll.getSlices().forEach(s -> {\n        numCores.addAndGet(s.getReplicas().size());\n        s.getReplicas().forEach(r -> {\n          nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n              .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n        });\n      });\n      int maxCoresPerNode = 0;\n      int minCoresPerNode = 0;\n      int maxActualShardsPerNode = 0;\n      int minActualShardsPerNode = 0;\n      int maxShardReplicasPerNode = 0;\n      int minShardReplicasPerNode = 0;\n      if (!nodes.isEmpty()) {\n        minCoresPerNode = Integer.MAX_VALUE;\n        minActualShardsPerNode = Integer.MAX_VALUE;\n        minShardReplicasPerNode = Integer.MAX_VALUE;\n        for (Map<String, AtomicInteger> counts : nodes.values()) {\n          int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n          for (AtomicInteger count : counts.values()) {\n            if (count.get() > maxShardReplicasPerNode) {\n              maxShardReplicasPerNode = count.get();\n            }\n            if (count.get() < minShardReplicasPerNode) {\n              minShardReplicasPerNode = count.get();\n            }\n          }\n          if (total > maxCoresPerNode) {\n            maxCoresPerNode = total;\n          }\n          if (total < minCoresPerNode) {\n            minCoresPerNode = total;\n          }\n          if (counts.size() > maxActualShardsPerNode) {\n            maxActualShardsPerNode = counts.size();\n          }\n          if (counts.size() < minActualShardsPerNode) {\n            minActualShardsPerNode = counts.size();\n          }\n        }\n      }\n      perColl.put(\"activeShards\", coll.getActiveSlices().size());\n      perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n      perColl.put(\"rf\", coll.getReplicationFactor());\n      perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n      perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n      perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n      perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n      perColl.put(\"numCores\", numCores.get());\n      perColl.put(\"numNodes\", nodes.size());\n      perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n      perColl.put(\"minCoresPerNode\", minCoresPerNode);\n    });\n    Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n    Map<Integer, AtomicInteger> coreStats = new TreeMap<>();\n    List<Row> rows = session.getSortedNodes();\n    // check consistency\n    if (rows.size() != clusterState.getLiveNodes().size()) {\n      throw new Exception(\"Mismatch between autoscaling matrix size (\" + rows.size() + \") and liveNodes size (\" + clusterState.getLiveNodes().size() + \")\");\n    }\n    for (Row row : rows) {\n      Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n      nodeStat.put(\"isLive\", row.isLive());\n      for (Cell cell : row.getCells()) {\n        nodeStat.put(cell.getName(), cell.getValue());\n      }\n//      nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n//      nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n      int cores = ((Number)row.getVal(\"cores\", 0)).intValue();\n//      nodeStat.put(\"cores\", cores);\n      coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();\n      Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n      // check consistency\n      AtomicInteger rowCores = new AtomicInteger();\n      row.forEachReplica(ri -> rowCores.incrementAndGet());\n      if (cores != rowCores.get()) {\n        throw new Exception(\"Mismatch between autoscaling matrix row replicas (\" + rowCores.get() + \") and number of cores (\" + cores + \")\");\n      }\n      row.forEachReplica(ri -> {\n        Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n            .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n//            if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n//              perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n//            }\n        if (ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute) != null) {\n          perReplica.put(Variable.Type.CORE_IDX.metricsAttribute, ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute));\n          if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n            perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n          } else {\n            perReplica.put(Variable.Type.CORE_IDX.tagName,\n                Variable.Type.CORE_IDX.convertVal(ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute)));\n          }\n        }\n        perReplica.put(\"coreNode\", ri.getName());\n        if (ri.isLeader || ri.getBool(\"leader\", false)) {\n          perReplica.put(\"leader\", true);\n          Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n              .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n          Number riSize = (Number)ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute);\n          if (riSize != null) {\n            totalSize += riSize.doubleValue();\n            collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n            Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n            if (max == null) max = 0.0;\n            if (riSize.doubleValue() > max) {\n              collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n            }\n            Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n            if (min == null) min = Double.MAX_VALUE;\n            if (riSize.doubleValue() < min) {\n              collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n            }\n          } else {\n            throw new RuntimeException(\"ReplicaInfo without size information: \" + ri);\n          }\n        }\n        if (verbose) {\n          nodeStat.put(\"replicas\", collReplicas);\n        }\n      });\n    }\n\n    // calculate average per shard and convert the units\n    for (Map<String, Number> perColl : collStats.values()) {\n      Number avg = perColl.get(\"avgShardSize\");\n      if (avg != null) {\n        avg = avg.doubleValue() / perColl.get(\"activeShards\").doubleValue();\n        perColl.put(\"avgShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(avg));\n      }\n      Number num = perColl.get(\"maxShardSize\");\n      if (num != null) {\n        perColl.put(\"maxShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n      num = perColl.get(\"minShardSize\");\n      if (num != null) {\n        perColl.put(\"minShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n    }\n    Map<String, Object> stats = new LinkedHashMap<>();\n    stats.put(\"coresPerNodes\", coreStats);\n    stats.put(\"sortedNodeStats\", nodeStats);\n    stats.put(\"collectionStats\", collStats);\n    return stats;\n  }\n\n","sourceOld":"  /**\n   * Calculate statistics of node / collection and replica layouts for the provided {@link SolrCloudManager}.\n   * @param cloudManager manager\n   * @param config autoscaling config, or null if the one from the provided manager should be used\n   * @param verbose if true then add more details about replicas.\n   * @return a map containing detailed statistics\n   */\n  public static Map<String, Object> calculateStats(SolrCloudManager cloudManager, AutoScalingConfig config, boolean verbose) throws Exception {\n    ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n    Map<String, Map<String, Number>> collStats = new TreeMap<>();\n    Policy.Session session = config.getPolicy().createSession(cloudManager);\n    clusterState.forEachCollection(coll -> {\n      Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n      AtomicInteger numCores = new AtomicInteger();\n      HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n      coll.getSlices().forEach(s -> {\n        numCores.addAndGet(s.getReplicas().size());\n        s.getReplicas().forEach(r -> {\n          nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n              .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n        });\n      });\n      int maxCoresPerNode = 0;\n      int minCoresPerNode = 0;\n      int maxActualShardsPerNode = 0;\n      int minActualShardsPerNode = 0;\n      int maxShardReplicasPerNode = 0;\n      int minShardReplicasPerNode = 0;\n      if (!nodes.isEmpty()) {\n        minCoresPerNode = Integer.MAX_VALUE;\n        minActualShardsPerNode = Integer.MAX_VALUE;\n        minShardReplicasPerNode = Integer.MAX_VALUE;\n        for (Map<String, AtomicInteger> counts : nodes.values()) {\n          int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n          for (AtomicInteger count : counts.values()) {\n            if (count.get() > maxShardReplicasPerNode) {\n              maxShardReplicasPerNode = count.get();\n            }\n            if (count.get() < minShardReplicasPerNode) {\n              minShardReplicasPerNode = count.get();\n            }\n          }\n          if (total > maxCoresPerNode) {\n            maxCoresPerNode = total;\n          }\n          if (total < minCoresPerNode) {\n            minCoresPerNode = total;\n          }\n          if (counts.size() > maxActualShardsPerNode) {\n            maxActualShardsPerNode = counts.size();\n          }\n          if (counts.size() < minActualShardsPerNode) {\n            minActualShardsPerNode = counts.size();\n          }\n        }\n      }\n      perColl.put(\"activeShards\", coll.getActiveSlices().size());\n      perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n      perColl.put(\"rf\", coll.getReplicationFactor());\n      perColl.put(\"maxShardsPerNode\", coll.getMaxShardsPerNode());\n      perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n      perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n      perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n      perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n      perColl.put(\"numCores\", numCores.get());\n      perColl.put(\"numNodes\", nodes.size());\n      perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n      perColl.put(\"minCoresPerNode\", minCoresPerNode);\n    });\n    Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n    Map<Integer, AtomicInteger> coreStats = new TreeMap<>();\n    List<Row> rows = session.getSortedNodes();\n    // check consistency\n    if (rows.size() != clusterState.getLiveNodes().size()) {\n      throw new Exception(\"Mismatch between autoscaling matrix size (\" + rows.size() + \") and liveNodes size (\" + clusterState.getLiveNodes().size() + \")\");\n    }\n    for (Row row : rows) {\n      Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n      nodeStat.put(\"isLive\", row.isLive());\n      for (Cell cell : row.getCells()) {\n        nodeStat.put(cell.getName(), cell.getValue());\n      }\n//      nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n//      nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n      int cores = ((Number)row.getVal(\"cores\", 0)).intValue();\n//      nodeStat.put(\"cores\", cores);\n      coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();\n      Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n      // check consistency\n      AtomicInteger rowCores = new AtomicInteger();\n      row.forEachReplica(ri -> rowCores.incrementAndGet());\n      if (cores != rowCores.get()) {\n        throw new Exception(\"Mismatch between autoscaling matrix row replicas (\" + rowCores.get() + \") and number of cores (\" + cores + \")\");\n      }\n      row.forEachReplica(ri -> {\n        Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n            .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n//            if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n//              perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n//            }\n        if (ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute) != null) {\n          perReplica.put(Variable.Type.CORE_IDX.metricsAttribute, ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute));\n          if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n            perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n          } else {\n            perReplica.put(Variable.Type.CORE_IDX.tagName,\n                Variable.Type.CORE_IDX.convertVal(ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute)));\n          }\n        }\n        perReplica.put(\"coreNode\", ri.getName());\n        if (ri.isLeader || ri.getBool(\"leader\", false)) {\n          perReplica.put(\"leader\", true);\n          Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n              .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n          Number riSize = (Number)ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute);\n          if (riSize != null) {\n            totalSize += riSize.doubleValue();\n            collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n            Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n            if (max == null) max = 0.0;\n            if (riSize.doubleValue() > max) {\n              collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n            }\n            Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n            if (min == null) min = Double.MAX_VALUE;\n            if (riSize.doubleValue() < min) {\n              collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n            }\n          } else {\n            throw new RuntimeException(\"ReplicaInfo without size information: \" + ri);\n          }\n        }\n        if (verbose) {\n          nodeStat.put(\"replicas\", collReplicas);\n        }\n      });\n    }\n\n    // calculate average per shard and convert the units\n    for (Map<String, Number> perColl : collStats.values()) {\n      Number avg = perColl.get(\"avgShardSize\");\n      if (avg != null) {\n        avg = avg.doubleValue() / perColl.get(\"activeShards\").doubleValue();\n        perColl.put(\"avgShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(avg));\n      }\n      Number num = perColl.get(\"maxShardSize\");\n      if (num != null) {\n        perColl.put(\"maxShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n      num = perColl.get(\"minShardSize\");\n      if (num != null) {\n        perColl.put(\"minShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n    }\n    Map<String, Object> stats = new LinkedHashMap<>();\n    stats.put(\"coresPerNodes\", coreStats);\n    stats.put(\"sortedNodeStats\", nodeStats);\n    stats.put(\"collectionStats\", collStats);\n    return stats;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd","date":1594731683,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimUtils#calculateStats(SolrCloudManager,AutoScalingConfig,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimUtils#calculateStats(SolrCloudManager,AutoScalingConfig,boolean).mjava","sourceNew":"  /**\n   * Calculate statistics of node / collection and replica layouts for the provided {@link SolrCloudManager}.\n   * @param cloudManager manager\n   * @param config autoscaling config, or null if the one from the provided manager should be used\n   * @param verbose if true then add more details about replicas.\n   * @return a map containing detailed statistics\n   */\n  public static Map<String, Object> calculateStats(SolrCloudManager cloudManager, AutoScalingConfig config, boolean verbose) throws Exception {\n    ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n    Map<String, Map<String, Number>> collStats = new TreeMap<>();\n    Policy.Session session = config.getPolicy().createSession(cloudManager);\n    clusterState.forEachCollection(coll -> {\n      Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n      AtomicInteger numCores = new AtomicInteger();\n      HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n      coll.getSlices().forEach(s -> {\n        numCores.addAndGet(s.getReplicas().size());\n        s.getReplicas().forEach(r -> {\n          nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n              .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n        });\n      });\n      int maxCoresPerNode = 0;\n      int minCoresPerNode = 0;\n      int maxActualShardsPerNode = 0;\n      int minActualShardsPerNode = 0;\n      int maxShardReplicasPerNode = 0;\n      int minShardReplicasPerNode = 0;\n      if (!nodes.isEmpty()) {\n        minCoresPerNode = Integer.MAX_VALUE;\n        minActualShardsPerNode = Integer.MAX_VALUE;\n        minShardReplicasPerNode = Integer.MAX_VALUE;\n        for (Map<String, AtomicInteger> counts : nodes.values()) {\n          int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n          for (AtomicInteger count : counts.values()) {\n            if (count.get() > maxShardReplicasPerNode) {\n              maxShardReplicasPerNode = count.get();\n            }\n            if (count.get() < minShardReplicasPerNode) {\n              minShardReplicasPerNode = count.get();\n            }\n          }\n          if (total > maxCoresPerNode) {\n            maxCoresPerNode = total;\n          }\n          if (total < minCoresPerNode) {\n            minCoresPerNode = total;\n          }\n          if (counts.size() > maxActualShardsPerNode) {\n            maxActualShardsPerNode = counts.size();\n          }\n          if (counts.size() < minActualShardsPerNode) {\n            minActualShardsPerNode = counts.size();\n          }\n        }\n      }\n      perColl.put(\"activeShards\", coll.getActiveSlices().size());\n      perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n      perColl.put(\"rf\", coll.getReplicationFactor());\n      perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n      perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n      perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n      perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n      perColl.put(\"numCores\", numCores.get());\n      perColl.put(\"numNodes\", nodes.size());\n      perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n      perColl.put(\"minCoresPerNode\", minCoresPerNode);\n    });\n    Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n    Map<Integer, AtomicInteger> coreStats = new TreeMap<>();\n    List<Row> rows = session.getSortedNodes();\n    // check consistency\n    if (rows.size() != clusterState.getLiveNodes().size()) {\n      throw new Exception(\"Mismatch between autoscaling matrix size (\" + rows.size() + \") and liveNodes size (\" + clusterState.getLiveNodes().size() + \")\");\n    }\n    for (Row row : rows) {\n      Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n      nodeStat.put(\"isLive\", row.isLive());\n      for (Cell cell : row.getCells()) {\n        nodeStat.put(cell.getName(), cell.getValue());\n      }\n//      nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n//      nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n      int cores = ((Number)row.getVal(\"cores\", 0)).intValue();\n//      nodeStat.put(\"cores\", cores);\n      coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();\n      Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n      // check consistency\n      AtomicInteger rowCores = new AtomicInteger();\n      row.forEachReplica(ri -> rowCores.incrementAndGet());\n      if (cores != rowCores.get()) {\n        throw new Exception(\"Mismatch between autoscaling matrix row replicas (\" + rowCores.get() + \") and number of cores (\" + cores + \")\");\n      }\n      row.forEachReplica(ri -> {\n        Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n            .computeIfAbsent(ri.getCoreName().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n//            if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n//              perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n//            }\n        if (ri.get(Variable.Type.CORE_IDX.metricsAttribute) != null) {\n          perReplica.put(Variable.Type.CORE_IDX.metricsAttribute, ri.get(Variable.Type.CORE_IDX.metricsAttribute));\n          if (ri.get(Variable.Type.CORE_IDX.tagName) != null) {\n            perReplica.put(Variable.Type.CORE_IDX.tagName, ri.get(Variable.Type.CORE_IDX.tagName));\n          } else {\n            perReplica.put(Variable.Type.CORE_IDX.tagName,\n                Variable.Type.CORE_IDX.convertVal(ri.get(Variable.Type.CORE_IDX.metricsAttribute)));\n          }\n        }\n        perReplica.put(\"coreNode\", ri.getName());\n        if (ri.isLeader() || ri.getBool(\"leader\", false)) {\n          perReplica.put(\"leader\", true);\n          Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n              .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n          Number riSize = (Number)ri.get(Variable.Type.CORE_IDX.metricsAttribute);\n          if (riSize != null) {\n            totalSize += riSize.doubleValue();\n            collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n            Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n            if (max == null) max = 0.0;\n            if (riSize.doubleValue() > max) {\n              collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n            }\n            Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n            if (min == null) min = Double.MAX_VALUE;\n            if (riSize.doubleValue() < min) {\n              collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n            }\n          } else {\n            throw new RuntimeException(\"ReplicaInfo without size information: \" + ri);\n          }\n        }\n        if (verbose) {\n          nodeStat.put(\"replicas\", collReplicas);\n        }\n      });\n    }\n\n    // calculate average per shard and convert the units\n    for (Map<String, Number> perColl : collStats.values()) {\n      Number avg = perColl.get(\"avgShardSize\");\n      if (avg != null) {\n        avg = avg.doubleValue() / perColl.get(\"activeShards\").doubleValue();\n        perColl.put(\"avgShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(avg));\n      }\n      Number num = perColl.get(\"maxShardSize\");\n      if (num != null) {\n        perColl.put(\"maxShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n      num = perColl.get(\"minShardSize\");\n      if (num != null) {\n        perColl.put(\"minShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n    }\n    Map<String, Object> stats = new LinkedHashMap<>();\n    stats.put(\"coresPerNodes\", coreStats);\n    stats.put(\"sortedNodeStats\", nodeStats);\n    stats.put(\"collectionStats\", collStats);\n    return stats;\n  }\n\n","sourceOld":"  /**\n   * Calculate statistics of node / collection and replica layouts for the provided {@link SolrCloudManager}.\n   * @param cloudManager manager\n   * @param config autoscaling config, or null if the one from the provided manager should be used\n   * @param verbose if true then add more details about replicas.\n   * @return a map containing detailed statistics\n   */\n  public static Map<String, Object> calculateStats(SolrCloudManager cloudManager, AutoScalingConfig config, boolean verbose) throws Exception {\n    ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n    Map<String, Map<String, Number>> collStats = new TreeMap<>();\n    Policy.Session session = config.getPolicy().createSession(cloudManager);\n    clusterState.forEachCollection(coll -> {\n      Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n      AtomicInteger numCores = new AtomicInteger();\n      HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n      coll.getSlices().forEach(s -> {\n        numCores.addAndGet(s.getReplicas().size());\n        s.getReplicas().forEach(r -> {\n          nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n              .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n        });\n      });\n      int maxCoresPerNode = 0;\n      int minCoresPerNode = 0;\n      int maxActualShardsPerNode = 0;\n      int minActualShardsPerNode = 0;\n      int maxShardReplicasPerNode = 0;\n      int minShardReplicasPerNode = 0;\n      if (!nodes.isEmpty()) {\n        minCoresPerNode = Integer.MAX_VALUE;\n        minActualShardsPerNode = Integer.MAX_VALUE;\n        minShardReplicasPerNode = Integer.MAX_VALUE;\n        for (Map<String, AtomicInteger> counts : nodes.values()) {\n          int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n          for (AtomicInteger count : counts.values()) {\n            if (count.get() > maxShardReplicasPerNode) {\n              maxShardReplicasPerNode = count.get();\n            }\n            if (count.get() < minShardReplicasPerNode) {\n              minShardReplicasPerNode = count.get();\n            }\n          }\n          if (total > maxCoresPerNode) {\n            maxCoresPerNode = total;\n          }\n          if (total < minCoresPerNode) {\n            minCoresPerNode = total;\n          }\n          if (counts.size() > maxActualShardsPerNode) {\n            maxActualShardsPerNode = counts.size();\n          }\n          if (counts.size() < minActualShardsPerNode) {\n            minActualShardsPerNode = counts.size();\n          }\n        }\n      }\n      perColl.put(\"activeShards\", coll.getActiveSlices().size());\n      perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n      perColl.put(\"rf\", coll.getReplicationFactor());\n      perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n      perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n      perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n      perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n      perColl.put(\"numCores\", numCores.get());\n      perColl.put(\"numNodes\", nodes.size());\n      perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n      perColl.put(\"minCoresPerNode\", minCoresPerNode);\n    });\n    Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n    Map<Integer, AtomicInteger> coreStats = new TreeMap<>();\n    List<Row> rows = session.getSortedNodes();\n    // check consistency\n    if (rows.size() != clusterState.getLiveNodes().size()) {\n      throw new Exception(\"Mismatch between autoscaling matrix size (\" + rows.size() + \") and liveNodes size (\" + clusterState.getLiveNodes().size() + \")\");\n    }\n    for (Row row : rows) {\n      Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n      nodeStat.put(\"isLive\", row.isLive());\n      for (Cell cell : row.getCells()) {\n        nodeStat.put(cell.getName(), cell.getValue());\n      }\n//      nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n//      nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n      int cores = ((Number)row.getVal(\"cores\", 0)).intValue();\n//      nodeStat.put(\"cores\", cores);\n      coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();\n      Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n      // check consistency\n      AtomicInteger rowCores = new AtomicInteger();\n      row.forEachReplica(ri -> rowCores.incrementAndGet());\n      if (cores != rowCores.get()) {\n        throw new Exception(\"Mismatch between autoscaling matrix row replicas (\" + rowCores.get() + \") and number of cores (\" + cores + \")\");\n      }\n      row.forEachReplica(ri -> {\n        Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n            .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n//            if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n//              perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n//            }\n        if (ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute) != null) {\n          perReplica.put(Variable.Type.CORE_IDX.metricsAttribute, ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute));\n          if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n            perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n          } else {\n            perReplica.put(Variable.Type.CORE_IDX.tagName,\n                Variable.Type.CORE_IDX.convertVal(ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute)));\n          }\n        }\n        perReplica.put(\"coreNode\", ri.getName());\n        if (ri.isLeader || ri.getBool(\"leader\", false)) {\n          perReplica.put(\"leader\", true);\n          Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n              .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n          Number riSize = (Number)ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute);\n          if (riSize != null) {\n            totalSize += riSize.doubleValue();\n            collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n            Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n            if (max == null) max = 0.0;\n            if (riSize.doubleValue() > max) {\n              collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n            }\n            Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n            if (min == null) min = Double.MAX_VALUE;\n            if (riSize.doubleValue() < min) {\n              collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n            }\n          } else {\n            throw new RuntimeException(\"ReplicaInfo without size information: \" + ri);\n          }\n        }\n        if (verbose) {\n          nodeStat.put(\"replicas\", collReplicas);\n        }\n      });\n    }\n\n    // calculate average per shard and convert the units\n    for (Map<String, Number> perColl : collStats.values()) {\n      Number avg = perColl.get(\"avgShardSize\");\n      if (avg != null) {\n        avg = avg.doubleValue() / perColl.get(\"activeShards\").doubleValue();\n        perColl.put(\"avgShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(avg));\n      }\n      Number num = perColl.get(\"maxShardSize\");\n      if (num != null) {\n        perColl.put(\"maxShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n      num = perColl.get(\"minShardSize\");\n      if (num != null) {\n        perColl.put(\"minShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n    }\n    Map<String, Object> stats = new LinkedHashMap<>();\n    stats.put(\"coresPerNodes\", coreStats);\n    stats.put(\"sortedNodeStats\", nodeStats);\n    stats.put(\"collectionStats\", collStats);\n    return stats;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f504512a03d978990cbff30db0522b354e846db","date":1595247421,"type":4,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/autoscaling/sim/SimUtils#calculateStats(SolrCloudManager,AutoScalingConfig,boolean).mjava","sourceNew":null,"sourceOld":"  /**\n   * Calculate statistics of node / collection and replica layouts for the provided {@link SolrCloudManager}.\n   * @param cloudManager manager\n   * @param config autoscaling config, or null if the one from the provided manager should be used\n   * @param verbose if true then add more details about replicas.\n   * @return a map containing detailed statistics\n   */\n  public static Map<String, Object> calculateStats(SolrCloudManager cloudManager, AutoScalingConfig config, boolean verbose) throws Exception {\n    ClusterState clusterState = cloudManager.getClusterStateProvider().getClusterState();\n    Map<String, Map<String, Number>> collStats = new TreeMap<>();\n    Policy.Session session = config.getPolicy().createSession(cloudManager);\n    clusterState.forEachCollection(coll -> {\n      Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n      AtomicInteger numCores = new AtomicInteger();\n      HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n      coll.getSlices().forEach(s -> {\n        numCores.addAndGet(s.getReplicas().size());\n        s.getReplicas().forEach(r -> {\n          nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n              .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n        });\n      });\n      int maxCoresPerNode = 0;\n      int minCoresPerNode = 0;\n      int maxActualShardsPerNode = 0;\n      int minActualShardsPerNode = 0;\n      int maxShardReplicasPerNode = 0;\n      int minShardReplicasPerNode = 0;\n      if (!nodes.isEmpty()) {\n        minCoresPerNode = Integer.MAX_VALUE;\n        minActualShardsPerNode = Integer.MAX_VALUE;\n        minShardReplicasPerNode = Integer.MAX_VALUE;\n        for (Map<String, AtomicInteger> counts : nodes.values()) {\n          int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n          for (AtomicInteger count : counts.values()) {\n            if (count.get() > maxShardReplicasPerNode) {\n              maxShardReplicasPerNode = count.get();\n            }\n            if (count.get() < minShardReplicasPerNode) {\n              minShardReplicasPerNode = count.get();\n            }\n          }\n          if (total > maxCoresPerNode) {\n            maxCoresPerNode = total;\n          }\n          if (total < minCoresPerNode) {\n            minCoresPerNode = total;\n          }\n          if (counts.size() > maxActualShardsPerNode) {\n            maxActualShardsPerNode = counts.size();\n          }\n          if (counts.size() < minActualShardsPerNode) {\n            minActualShardsPerNode = counts.size();\n          }\n        }\n      }\n      perColl.put(\"activeShards\", coll.getActiveSlices().size());\n      perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n      perColl.put(\"rf\", coll.getReplicationFactor());\n      perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n      perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n      perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n      perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n      perColl.put(\"numCores\", numCores.get());\n      perColl.put(\"numNodes\", nodes.size());\n      perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n      perColl.put(\"minCoresPerNode\", minCoresPerNode);\n    });\n    Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n    Map<Integer, AtomicInteger> coreStats = new TreeMap<>();\n    List<Row> rows = session.getSortedNodes();\n    // check consistency\n    if (rows.size() != clusterState.getLiveNodes().size()) {\n      throw new Exception(\"Mismatch between autoscaling matrix size (\" + rows.size() + \") and liveNodes size (\" + clusterState.getLiveNodes().size() + \")\");\n    }\n    for (Row row : rows) {\n      Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n      nodeStat.put(\"isLive\", row.isLive());\n      for (Cell cell : row.getCells()) {\n        nodeStat.put(cell.getName(), cell.getValue());\n      }\n//      nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n//      nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n      int cores = ((Number)row.getVal(\"cores\", 0)).intValue();\n//      nodeStat.put(\"cores\", cores);\n      coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();\n      Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n      // check consistency\n      AtomicInteger rowCores = new AtomicInteger();\n      row.forEachReplica(ri -> rowCores.incrementAndGet());\n      if (cores != rowCores.get()) {\n        throw new Exception(\"Mismatch between autoscaling matrix row replicas (\" + rowCores.get() + \") and number of cores (\" + cores + \")\");\n      }\n      row.forEachReplica(ri -> {\n        Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n            .computeIfAbsent(ri.getCoreName().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n//            if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n//              perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n//            }\n        if (ri.get(Variable.Type.CORE_IDX.metricsAttribute) != null) {\n          perReplica.put(Variable.Type.CORE_IDX.metricsAttribute, ri.get(Variable.Type.CORE_IDX.metricsAttribute));\n          if (ri.get(Variable.Type.CORE_IDX.tagName) != null) {\n            perReplica.put(Variable.Type.CORE_IDX.tagName, ri.get(Variable.Type.CORE_IDX.tagName));\n          } else {\n            perReplica.put(Variable.Type.CORE_IDX.tagName,\n                Variable.Type.CORE_IDX.convertVal(ri.get(Variable.Type.CORE_IDX.metricsAttribute)));\n          }\n        }\n        perReplica.put(\"coreNode\", ri.getName());\n        if (ri.isLeader() || ri.getBool(\"leader\", false)) {\n          perReplica.put(\"leader\", true);\n          Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n              .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n          Number riSize = (Number)ri.get(Variable.Type.CORE_IDX.metricsAttribute);\n          if (riSize != null) {\n            totalSize += riSize.doubleValue();\n            collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n            Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n            if (max == null) max = 0.0;\n            if (riSize.doubleValue() > max) {\n              collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n            }\n            Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n            if (min == null) min = Double.MAX_VALUE;\n            if (riSize.doubleValue() < min) {\n              collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n            }\n          } else {\n            throw new RuntimeException(\"ReplicaInfo without size information: \" + ri);\n          }\n        }\n        if (verbose) {\n          nodeStat.put(\"replicas\", collReplicas);\n        }\n      });\n    }\n\n    // calculate average per shard and convert the units\n    for (Map<String, Number> perColl : collStats.values()) {\n      Number avg = perColl.get(\"avgShardSize\");\n      if (avg != null) {\n        avg = avg.doubleValue() / perColl.get(\"activeShards\").doubleValue();\n        perColl.put(\"avgShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(avg));\n      }\n      Number num = perColl.get(\"maxShardSize\");\n      if (num != null) {\n        perColl.put(\"maxShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n      num = perColl.get(\"minShardSize\");\n      if (num != null) {\n        perColl.put(\"minShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n      }\n    }\n    Map<String, Object> stats = new LinkedHashMap<>();\n    stats.put(\"coresPerNodes\", coreStats);\n    stats.put(\"sortedNodeStats\", nodeStats);\n    stats.put(\"collectionStats\", collStats);\n    return stats;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3d2a34ea3732f91149b31bcad82026ad85fda69":["edf5b262a72d10530eb2f01dc8f19060355b213e"],"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"3f504512a03d978990cbff30db0522b354e846db":["7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"edf5b262a72d10530eb2f01dc8f19060355b213e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["d3d2a34ea3732f91149b31bcad82026ad85fda69"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3f504512a03d978990cbff30db0522b354e846db"]},"commit2Childs":{"d3d2a34ea3732f91149b31bcad82026ad85fda69":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd":["3f504512a03d978990cbff30db0522b354e846db"],"3f504512a03d978990cbff30db0522b354e846db":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["edf5b262a72d10530eb2f01dc8f19060355b213e"],"edf5b262a72d10530eb2f01dc8f19060355b213e":["d3d2a34ea3732f91149b31bcad82026ad85fda69"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}