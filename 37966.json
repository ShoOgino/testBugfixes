{"path":"lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","sourceNew":"  /**\n   * Loads the segment information at segment load time.\n   * \n   * @param indexEnum\n   *          the term enum.\n   * @param indexDivisor\n   *          the index divisor.\n   * @param tiiFileLength\n   *          the size of the tii file, used to approximate the size of the\n   *          buffer.\n   * @param totalIndexInterval\n   *          the total index interval.\n   */\n  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {\n    this.totalIndexInterval = totalIndexInterval;\n    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;\n    skipInterval = indexEnum.skipInterval;\n    // this is only an inital size, it will be GCed once the build is complete\n    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;\n    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));\n    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();\n\n    GrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false);\n    String currentField = null;\n    List<String> fieldStrs = new ArrayList<String>();\n    int fieldCounter = -1;\n    for (int i = 0; indexEnum.next(); i++) {\n      Term term = indexEnum.term();\n      if (currentField == null || !currentField.equals(term.field())) {\n        currentField = term.field();\n        fieldStrs.add(currentField);\n        fieldCounter++;\n      }\n      TermInfo termInfo = indexEnum.termInfo();\n      indexToTerms.set(i, dataOutput.getPosition());\n      dataOutput.writeVInt(fieldCounter);\n      dataOutput.writeString(term.text());\n      dataOutput.writeVInt(termInfo.docFreq);\n      if (termInfo.docFreq >= skipInterval) {\n        dataOutput.writeVInt(termInfo.skipOffset);\n      }\n      dataOutput.writeVLong(termInfo.freqPointer);\n      dataOutput.writeVLong(termInfo.proxPointer);\n      dataOutput.writeVLong(indexEnum.indexPointer);\n      for (int j = 1; j < indexDivisor; j++) {\n        if (!indexEnum.next()) {\n          break;\n        }\n      }\n    }\n\n    fields = new Term[fieldStrs.size()];\n    for (int i = 0; i < fields.length; i++) {\n      fields[i] = new Term(fieldStrs.get(i));\n    }\n    \n    dataPagedBytes.freeze(true);\n    dataInput = dataPagedBytes.getDataInput();\n    indexToDataOffset = indexToTerms.getMutable();\n  }\n\n","sourceOld":"  /**\n   * Loads the segment information at segment load time.\n   * \n   * @param indexEnum\n   *          the term enum.\n   * @param indexDivisor\n   *          the index divisor.\n   * @param tiiFileLength\n   *          the size of the tii file, used to approximate the size of the\n   *          buffer.\n   * @param totalIndexInterval\n   *          the total index interval.\n   */\n  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {\n    this.totalIndexInterval = totalIndexInterval;\n    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;\n    skipInterval = indexEnum.skipInterval;\n    // this is only an inital size, it will be GCed once the build is complete\n    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;\n    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));\n    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();\n\n    GrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false);\n    String currentField = null;\n    List<String> fieldStrs = new ArrayList<String>();\n    int fieldCounter = -1;\n    for (int i = 0; indexEnum.next(); i++) {\n      Term term = indexEnum.term();\n      if (currentField == null || !currentField.equals(term.field())) {\n        currentField = term.field();\n        fieldStrs.add(currentField);\n        fieldCounter++;\n      }\n      TermInfo termInfo = indexEnum.termInfo();\n      indexToTerms.set(i, dataOutput.getPosition());\n      dataOutput.writeVInt(fieldCounter);\n      dataOutput.writeString(term.text());\n      dataOutput.writeVInt(termInfo.docFreq);\n      if (termInfo.docFreq >= skipInterval) {\n        dataOutput.writeVInt(termInfo.skipOffset);\n      }\n      dataOutput.writeVLong(termInfo.freqPointer);\n      dataOutput.writeVLong(termInfo.proxPointer);\n      dataOutput.writeVLong(indexEnum.indexPointer);\n      for (int j = 1; j < indexDivisor; j++) {\n        if (!indexEnum.next()) {\n          break;\n        }\n      }\n    }\n\n    fields = new Term[fieldStrs.size()];\n    for (int i = 0; i < fields.length; i++) {\n      fields[i] = new Term(fieldStrs.get(i));\n    }\n    \n    dataPagedBytes.freeze(true);\n    dataInput = dataPagedBytes.getDataInput();\n    indexToDataOffset = indexToTerms.getMutable();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2f72b850f14bb4291a8c2728f0073d07da0462d8","date":1333634910,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","sourceNew":"  /**\n   * Loads the segment information at segment load time.\n   * \n   * @param indexEnum\n   *          the term enum.\n   * @param indexDivisor\n   *          the index divisor.\n   * @param tiiFileLength\n   *          the size of the tii file, used to approximate the size of the\n   *          buffer.\n   * @param totalIndexInterval\n   *          the total index interval.\n   */\n  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {\n    this.totalIndexInterval = totalIndexInterval;\n    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;\n    skipInterval = indexEnum.skipInterval;\n    // this is only an inital size, it will be GCed once the build is complete\n    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;\n    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));\n    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();\n\n    final int bitEstimate = 1+MathUtil.log(tiiFileLength, 2);\n    GrowableWriter indexToTerms = new GrowableWriter(bitEstimate, indexSize, false);\n\n    String currentField = null;\n    List<String> fieldStrs = new ArrayList<String>();\n    int fieldCounter = -1;\n    for (int i = 0; indexEnum.next(); i++) {\n      Term term = indexEnum.term();\n      if (currentField == null || !currentField.equals(term.field())) {\n        currentField = term.field();\n        fieldStrs.add(currentField);\n        fieldCounter++;\n      }\n      TermInfo termInfo = indexEnum.termInfo();\n      indexToTerms.set(i, dataOutput.getPosition());\n      dataOutput.writeVInt(fieldCounter);\n      dataOutput.writeString(term.text());\n      dataOutput.writeVInt(termInfo.docFreq);\n      if (termInfo.docFreq >= skipInterval) {\n        dataOutput.writeVInt(termInfo.skipOffset);\n      }\n      dataOutput.writeVLong(termInfo.freqPointer);\n      dataOutput.writeVLong(termInfo.proxPointer);\n      dataOutput.writeVLong(indexEnum.indexPointer);\n      for (int j = 1; j < indexDivisor; j++) {\n        if (!indexEnum.next()) {\n          break;\n        }\n      }\n    }\n\n    fields = new Term[fieldStrs.size()];\n    for (int i = 0; i < fields.length; i++) {\n      fields[i] = new Term(fieldStrs.get(i));\n    }\n    \n    dataPagedBytes.freeze(true);\n    dataInput = dataPagedBytes.getDataInput();\n    indexToDataOffset = indexToTerms.getMutable();\n  }\n\n","sourceOld":"  /**\n   * Loads the segment information at segment load time.\n   * \n   * @param indexEnum\n   *          the term enum.\n   * @param indexDivisor\n   *          the index divisor.\n   * @param tiiFileLength\n   *          the size of the tii file, used to approximate the size of the\n   *          buffer.\n   * @param totalIndexInterval\n   *          the total index interval.\n   */\n  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {\n    this.totalIndexInterval = totalIndexInterval;\n    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;\n    skipInterval = indexEnum.skipInterval;\n    // this is only an inital size, it will be GCed once the build is complete\n    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;\n    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));\n    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();\n\n    GrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false);\n    String currentField = null;\n    List<String> fieldStrs = new ArrayList<String>();\n    int fieldCounter = -1;\n    for (int i = 0; indexEnum.next(); i++) {\n      Term term = indexEnum.term();\n      if (currentField == null || !currentField.equals(term.field())) {\n        currentField = term.field();\n        fieldStrs.add(currentField);\n        fieldCounter++;\n      }\n      TermInfo termInfo = indexEnum.termInfo();\n      indexToTerms.set(i, dataOutput.getPosition());\n      dataOutput.writeVInt(fieldCounter);\n      dataOutput.writeString(term.text());\n      dataOutput.writeVInt(termInfo.docFreq);\n      if (termInfo.docFreq >= skipInterval) {\n        dataOutput.writeVInt(termInfo.skipOffset);\n      }\n      dataOutput.writeVLong(termInfo.freqPointer);\n      dataOutput.writeVLong(termInfo.proxPointer);\n      dataOutput.writeVLong(indexEnum.indexPointer);\n      for (int j = 1; j < indexDivisor; j++) {\n        if (!indexEnum.next()) {\n          break;\n        }\n      }\n    }\n\n    fields = new Term[fieldStrs.size()];\n    for (int i = 0; i < fields.length; i++) {\n      fields[i] = new Term(fieldStrs.get(i));\n    }\n    \n    dataPagedBytes.freeze(true);\n    dataInput = dataPagedBytes.getDataInput();\n    indexToDataOffset = indexToTerms.getMutable();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a470c93b2b0f8f51241f52705fc110a01f27ad2","date":1337969379,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","sourceNew":"  /**\n   * Loads the segment information at segment load time.\n   * \n   * @param indexEnum\n   *          the term enum.\n   * @param indexDivisor\n   *          the index divisor.\n   * @param tiiFileLength\n   *          the size of the tii file, used to approximate the size of the\n   *          buffer.\n   * @param totalIndexInterval\n   *          the total index interval.\n   */\n  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {\n    this.totalIndexInterval = totalIndexInterval;\n    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;\n    skipInterval = indexEnum.skipInterval;\n    // this is only an inital size, it will be GCed once the build is complete\n    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;\n    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));\n    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();\n\n    final int bitEstimate = 1+MathUtil.log(tiiFileLength, 2);\n    GrowableWriter indexToTerms = new GrowableWriter(bitEstimate, indexSize, PackedInts.DEFAULT);\n\n    String currentField = null;\n    List<String> fieldStrs = new ArrayList<String>();\n    int fieldCounter = -1;\n    for (int i = 0; indexEnum.next(); i++) {\n      Term term = indexEnum.term();\n      if (currentField == null || !currentField.equals(term.field())) {\n        currentField = term.field();\n        fieldStrs.add(currentField);\n        fieldCounter++;\n      }\n      TermInfo termInfo = indexEnum.termInfo();\n      indexToTerms.set(i, dataOutput.getPosition());\n      dataOutput.writeVInt(fieldCounter);\n      dataOutput.writeString(term.text());\n      dataOutput.writeVInt(termInfo.docFreq);\n      if (termInfo.docFreq >= skipInterval) {\n        dataOutput.writeVInt(termInfo.skipOffset);\n      }\n      dataOutput.writeVLong(termInfo.freqPointer);\n      dataOutput.writeVLong(termInfo.proxPointer);\n      dataOutput.writeVLong(indexEnum.indexPointer);\n      for (int j = 1; j < indexDivisor; j++) {\n        if (!indexEnum.next()) {\n          break;\n        }\n      }\n    }\n\n    fields = new Term[fieldStrs.size()];\n    for (int i = 0; i < fields.length; i++) {\n      fields[i] = new Term(fieldStrs.get(i));\n    }\n    \n    dataPagedBytes.freeze(true);\n    dataInput = dataPagedBytes.getDataInput();\n    indexToDataOffset = indexToTerms.getMutable();\n  }\n\n","sourceOld":"  /**\n   * Loads the segment information at segment load time.\n   * \n   * @param indexEnum\n   *          the term enum.\n   * @param indexDivisor\n   *          the index divisor.\n   * @param tiiFileLength\n   *          the size of the tii file, used to approximate the size of the\n   *          buffer.\n   * @param totalIndexInterval\n   *          the total index interval.\n   */\n  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {\n    this.totalIndexInterval = totalIndexInterval;\n    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;\n    skipInterval = indexEnum.skipInterval;\n    // this is only an inital size, it will be GCed once the build is complete\n    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;\n    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));\n    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();\n\n    final int bitEstimate = 1+MathUtil.log(tiiFileLength, 2);\n    GrowableWriter indexToTerms = new GrowableWriter(bitEstimate, indexSize, false);\n\n    String currentField = null;\n    List<String> fieldStrs = new ArrayList<String>();\n    int fieldCounter = -1;\n    for (int i = 0; indexEnum.next(); i++) {\n      Term term = indexEnum.term();\n      if (currentField == null || !currentField.equals(term.field())) {\n        currentField = term.field();\n        fieldStrs.add(currentField);\n        fieldCounter++;\n      }\n      TermInfo termInfo = indexEnum.termInfo();\n      indexToTerms.set(i, dataOutput.getPosition());\n      dataOutput.writeVInt(fieldCounter);\n      dataOutput.writeString(term.text());\n      dataOutput.writeVInt(termInfo.docFreq);\n      if (termInfo.docFreq >= skipInterval) {\n        dataOutput.writeVInt(termInfo.skipOffset);\n      }\n      dataOutput.writeVLong(termInfo.freqPointer);\n      dataOutput.writeVLong(termInfo.proxPointer);\n      dataOutput.writeVLong(indexEnum.indexPointer);\n      for (int j = 1; j < indexDivisor; j++) {\n        if (!indexEnum.next()) {\n          break;\n        }\n      }\n    }\n\n    fields = new Term[fieldStrs.size()];\n    for (int i = 0; i < fields.length; i++) {\n      fields[i] = new Term(fieldStrs.get(i));\n    }\n    \n    dataPagedBytes.freeze(true);\n    dataInput = dataPagedBytes.getDataInput();\n    indexToDataOffset = indexToTerms.getMutable();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57ae3024996ccdb3c36c42cb890e1efb37df4ce8","date":1338343651,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","sourceNew":null,"sourceOld":"  /**\n   * Loads the segment information at segment load time.\n   * \n   * @param indexEnum\n   *          the term enum.\n   * @param indexDivisor\n   *          the index divisor.\n   * @param tiiFileLength\n   *          the size of the tii file, used to approximate the size of the\n   *          buffer.\n   * @param totalIndexInterval\n   *          the total index interval.\n   */\n  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {\n    this.totalIndexInterval = totalIndexInterval;\n    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;\n    skipInterval = indexEnum.skipInterval;\n    // this is only an inital size, it will be GCed once the build is complete\n    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;\n    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));\n    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();\n\n    final int bitEstimate = 1+MathUtil.log(tiiFileLength, 2);\n    GrowableWriter indexToTerms = new GrowableWriter(bitEstimate, indexSize, PackedInts.DEFAULT);\n\n    String currentField = null;\n    List<String> fieldStrs = new ArrayList<String>();\n    int fieldCounter = -1;\n    for (int i = 0; indexEnum.next(); i++) {\n      Term term = indexEnum.term();\n      if (currentField == null || !currentField.equals(term.field())) {\n        currentField = term.field();\n        fieldStrs.add(currentField);\n        fieldCounter++;\n      }\n      TermInfo termInfo = indexEnum.termInfo();\n      indexToTerms.set(i, dataOutput.getPosition());\n      dataOutput.writeVInt(fieldCounter);\n      dataOutput.writeString(term.text());\n      dataOutput.writeVInt(termInfo.docFreq);\n      if (termInfo.docFreq >= skipInterval) {\n        dataOutput.writeVInt(termInfo.skipOffset);\n      }\n      dataOutput.writeVLong(termInfo.freqPointer);\n      dataOutput.writeVLong(termInfo.proxPointer);\n      dataOutput.writeVLong(indexEnum.indexPointer);\n      for (int j = 1; j < indexDivisor; j++) {\n        if (!indexEnum.next()) {\n          break;\n        }\n      }\n    }\n\n    fields = new Term[fieldStrs.size()];\n    for (int i = 0; i < fields.length; i++) {\n      fields[i] = new Term(fieldStrs.get(i));\n    }\n    \n    dataPagedBytes.freeze(true);\n    dataInput = dataPagedBytes.getDataInput();\n    indexToDataOffset = indexToTerms.getMutable();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["4a470c93b2b0f8f51241f52705fc110a01f27ad2"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"2f72b850f14bb4291a8c2728f0073d07da0462d8":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4a470c93b2b0f8f51241f52705fc110a01f27ad2":["2f72b850f14bb4291a8c2728f0073d07da0462d8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"]},"commit2Childs":{"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["2f72b850f14bb4291a8c2728f0073d07da0462d8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"2f72b850f14bb4291a8c2728f0073d07da0462d8":["4a470c93b2b0f8f51241f52705fc110a01f27ad2"],"4a470c93b2b0f8f51241f52705fc110a01f27ad2":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}