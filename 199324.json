{"path":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#addPosition(Token).mjava","commits":[{"id":"5a0af3a442be522899177e5e11384a45a6784a3f","date":1205348952,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#addPosition(Token).mjava","pathOld":"/dev/null","sourceNew":"  /** This is the hotspot of indexing: it's called once\n   *  for every term of every document.  Its job is to *\n   *  update the postings byte stream (Postings hash) *\n   *  based on the occurence of a single term. */\n  private void addPosition(Token token) throws AbortException {\n\n    final Payload payload = token.getPayload();\n\n    // Get the text of this term.  Term can either\n    // provide a String token or offset into a char[]\n    // array\n    final char[] tokenText = token.termBuffer();\n    final int tokenTextLen = token.termLength();\n\n    int code = 0;\n\n    // Compute hashcode\n    int downto = tokenTextLen;\n    while (downto > 0)\n      code = (code*31) + tokenText[--downto];\n\n    // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate Posting in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && !postingEquals(tokenText, tokenTextLen));\n    }\n\n    final int proxCode;\n\n    // If we hit an exception below, it's possible the\n    // posting list or term vectors data will be\n    // partially written and thus inconsistent if\n    // flushed, so we have to abort all documents\n    // since the last flush:\n\n    try {\n\n      if (p != null) {       // term seen since last flush\n\n        if (threadState.docID != p.lastDocID) { // term not yet seen in this doc\n            \n          // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n          assert p.docFreq > 0;\n\n          // Now that we know doc freq for previous doc,\n          // write it & lastDocCode\n          freqUpto = p.freqUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n          freq = threadState.postingsPool.buffers[p.freqUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n          if (1 == p.docFreq)\n            writeFreqVInt(p.lastDocCode|1);\n          else {\n            writeFreqVInt(p.lastDocCode);\n            writeFreqVInt(p.docFreq);\n          }\n          p.freqUpto = freqUpto + (p.freqUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStartCode = offsetStart = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n\n          p.docFreq = 1;\n\n          // Store code so we can write this after we're\n          // done with this new doc\n          p.lastDocCode = (threadState.docID-p.lastDocID) << 1;\n          p.lastDocID = threadState.docID;\n\n        } else {                                // term already seen in this doc\n          // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n          p.docFreq++;\n\n          proxCode = position-p.lastPosition;\n\n          if (doVectors) {\n            vector = p.vector;\n            if (vector == null)\n              vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n              offsetStartCode = offsetStart-vector.lastOffset;\n            }\n          }\n        }\n      } else {\t\t\t\t\t  // term not seen before\n        // System.out.println(\"    never seen docID=\" + docID);\n\n        // Refill?\n        if (0 == threadState.postingsFreeCount) {\n          threadState.docWriter.getPostings(threadState.postingsFreeList);\n          threadState.postingsFreeCount = threadState.postingsFreeList.length;\n        }\n\n        final int textLen1 = 1+tokenTextLen;\n        if (textLen1 + threadState.charPool.byteUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n            // Just skip this term, to remain as robust as\n            // possible during indexing.  A TokenFilter\n            // can be inserted into the analyzer chain if\n            // other behavior is wanted (pruning the term\n            // to a prefix, throwing an exception, etc).\n            if (threadState.maxTermPrefix == null)\n              threadState.maxTermPrefix = new String(tokenText, 0, 30);\n\n            // Still increment position:\n            position++;\n            return;\n          }\n          threadState.charPool.nextBuffer();\n        }\n\n        final char[] text = threadState.charPool.buffer;\n        final int textUpto = threadState.charPool.byteUpto;\n\n        // Pull next free Posting from free list\n        p = threadState.postingsFreeList[--threadState.postingsFreeCount];\n\n        p.textStart = textUpto + threadState.charPool.byteOffset;\n        threadState.charPool.byteUpto += textLen1;\n\n        System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n        text[textUpto+tokenTextLen] = 0xffff;\n          \n        assert postingsHash[hashPos] == null;\n\n        postingsHash[hashPos] = p;\n        numPostings++;\n\n        if (numPostings == postingsHashHalfSize)\n          rehashPostings(2*postingsHashSize);\n\n        // Init first slice for freq & prox streams\n        final int upto1 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        p.freqStart = p.freqUpto = threadState.postingsPool.byteOffset + upto1;\n\n        final int upto2 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        p.proxStart = p.proxUpto = threadState.postingsPool.byteOffset + upto2;\n\n        p.lastDocCode = threadState.docID << 1;\n        p.lastDocID = threadState.docID;\n        p.docFreq = 1;\n\n        if (doVectors) {\n          vector = addNewVector();\n          if (doVectorOffsets) {\n            offsetStart = offsetStartCode = offset + token.startOffset();\n            offsetEnd = offset + token.endOffset();\n          }\n        }\n\n        proxCode = position;\n      }\n\n      proxUpto = p.proxUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n      prox = threadState.postingsPool.buffers[p.proxUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n      assert prox != null;\n\n      if (payload != null && payload.length > 0) {\n        writeProxVInt((proxCode<<1)|1);\n        writeProxVInt(payload.length);\n        writeProxBytes(payload.data, payload.offset, payload.length);\n        fieldInfo.storePayloads = true;\n      } else\n        writeProxVInt(proxCode<<1);\n\n      p.proxUpto = proxUpto + (p.proxUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n\n      p.lastPosition = position++;\n\n      if (doVectorPositions) {\n        posUpto = vector.posUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n        pos = threadState.vectorsPool.buffers[vector.posUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n        writePosVInt(proxCode);\n        vector.posUpto = posUpto + (vector.posUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n      }\n\n      if (doVectorOffsets) {\n        offsetUpto = vector.offsetUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n        offsets = threadState.vectorsPool.buffers[vector.offsetUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n        writeOffsetVInt(offsetStartCode);\n        writeOffsetVInt(offsetEnd-offsetStart);\n        vector.lastOffset = offsetEnd;\n        vector.offsetUpto = offsetUpto + (vector.offsetUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n      }\n    } catch (Throwable t) {\n      throw new AbortException(t, threadState.docWriter);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4","date":1206538765,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#addPosition(Token).mjava","sourceNew":"  /** This is the hotspot of indexing: it's called once\n   *  for every term of every document.  Its job is to *\n   *  update the postings byte stream (Postings hash) *\n   *  based on the occurence of a single term. */\n  private void addPosition(Token token) throws AbortException {\n\n    final Payload payload = token.getPayload();\n\n    // Get the text of this term.  Term can either\n    // provide a String token or offset into a char[]\n    // array\n    final char[] tokenText = token.termBuffer();\n    final int tokenTextLen = token.termLength();\n\n    int code = 0;\n\n    // Compute hashcode & replace any invalid UTF16 sequences\n    int downto = tokenTextLen;\n    while (downto > 0) {\n      char ch = tokenText[--downto];\n\n      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {\n        if (0 == downto) {\n          // Unpaired\n          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n        } else {\n          final char ch2 = tokenText[downto-1];\n          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {\n            // OK: high followed by low.  This is a valid\n            // surrogate pair.\n            code = ((code*31) + ch)*31+ch2;\n            downto--;\n            continue;\n          } else {\n            // Unpaired\n            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n          }            \n        }\n      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && ch <= UnicodeUtil.UNI_SUR_HIGH_END)\n        // Unpaired\n        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n\n      code = (code*31) + ch;\n    }\n\n    // System.out.println(\"  addPosition: field=\" + fieldInfo.name + \" buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate Posting in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && !postingEquals(tokenText, tokenTextLen));\n    }\n\n    final int proxCode;\n\n    // If we hit an exception below, it's possible the\n    // posting list or term vectors data will be\n    // partially written and thus inconsistent if\n    // flushed, so we have to abort all documents\n    // since the last flush:\n\n    try {\n\n      if (p != null) {       // term seen since last flush\n\n        if (threadState.docID != p.lastDocID) { // term not yet seen in this doc\n            \n          // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n          assert p.docFreq > 0;\n\n          // Now that we know doc freq for previous doc,\n          // write it & lastDocCode\n          freqUpto = p.freqUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n          freq = threadState.postingsPool.buffers[p.freqUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n          if (1 == p.docFreq)\n            writeFreqVInt(p.lastDocCode|1);\n          else {\n            writeFreqVInt(p.lastDocCode);\n            writeFreqVInt(p.docFreq);\n          }\n          p.freqUpto = freqUpto + (p.freqUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStartCode = offsetStart = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n\n          p.docFreq = 1;\n\n          // Store code so we can write this after we're\n          // done with this new doc\n          p.lastDocCode = (threadState.docID-p.lastDocID) << 1;\n          p.lastDocID = threadState.docID;\n\n        } else {                                // term already seen in this doc\n          // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n          p.docFreq++;\n\n          proxCode = position-p.lastPosition;\n\n          if (doVectors) {\n            vector = p.vector;\n            if (vector == null)\n              vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n              offsetStartCode = offsetStart-vector.lastOffset;\n            }\n          }\n        }\n      } else {\t\t\t\t\t  // term not seen before\n        // System.out.println(\"    never seen docID=\" + docID);\n\n        // Refill?\n        if (0 == threadState.postingsFreeCount) {\n          threadState.docWriter.getPostings(threadState.postingsFreeList);\n          threadState.postingsFreeCount = threadState.postingsFreeList.length;\n        }\n\n        final int textLen1 = 1+tokenTextLen;\n        if (textLen1 + threadState.charPool.byteUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n            // Just skip this term, to remain as robust as\n            // possible during indexing.  A TokenFilter\n            // can be inserted into the analyzer chain if\n            // other behavior is wanted (pruning the term\n            // to a prefix, throwing an exception, etc).\n            if (threadState.maxTermPrefix == null)\n              threadState.maxTermPrefix = new String(tokenText, 0, 30);\n\n            // Still increment position:\n            position++;\n            return;\n          }\n          threadState.charPool.nextBuffer();\n        }\n\n        final char[] text = threadState.charPool.buffer;\n        final int textUpto = threadState.charPool.byteUpto;\n\n        // Pull next free Posting from free list\n        p = threadState.postingsFreeList[--threadState.postingsFreeCount];\n\n        p.textStart = textUpto + threadState.charPool.byteOffset;\n        threadState.charPool.byteUpto += textLen1;\n\n        System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n        text[textUpto+tokenTextLen] = 0xffff;\n          \n        assert postingsHash[hashPos] == null;\n\n        postingsHash[hashPos] = p;\n        numPostings++;\n\n        if (numPostings == postingsHashHalfSize)\n          rehashPostings(2*postingsHashSize);\n\n        // Init first slice for freq & prox streams\n        final int upto1 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        p.freqStart = p.freqUpto = threadState.postingsPool.byteOffset + upto1;\n\n        final int upto2 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        p.proxStart = p.proxUpto = threadState.postingsPool.byteOffset + upto2;\n\n        p.lastDocCode = threadState.docID << 1;\n        p.lastDocID = threadState.docID;\n        p.docFreq = 1;\n\n        if (doVectors) {\n          vector = addNewVector();\n          if (doVectorOffsets) {\n            offsetStart = offsetStartCode = offset + token.startOffset();\n            offsetEnd = offset + token.endOffset();\n          }\n        }\n\n        proxCode = position;\n      }\n\n      proxUpto = p.proxUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n      prox = threadState.postingsPool.buffers[p.proxUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n      assert prox != null;\n\n      if (payload != null && payload.length > 0) {\n        writeProxVInt((proxCode<<1)|1);\n        writeProxVInt(payload.length);\n        writeProxBytes(payload.data, payload.offset, payload.length);\n        fieldInfo.storePayloads = true;\n      } else\n        writeProxVInt(proxCode<<1);\n\n      p.proxUpto = proxUpto + (p.proxUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n\n      p.lastPosition = position++;\n\n      if (doVectorPositions) {\n        posUpto = vector.posUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n        pos = threadState.vectorsPool.buffers[vector.posUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n        writePosVInt(proxCode);\n        vector.posUpto = posUpto + (vector.posUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n      }\n\n      if (doVectorOffsets) {\n        offsetUpto = vector.offsetUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n        offsets = threadState.vectorsPool.buffers[vector.offsetUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n        writeOffsetVInt(offsetStartCode);\n        writeOffsetVInt(offsetEnd-offsetStart);\n        vector.lastOffset = offsetEnd;\n        vector.offsetUpto = offsetUpto + (vector.offsetUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n      }\n    } catch (Throwable t) {\n      throw new AbortException(t, threadState.docWriter);\n    }\n  }\n\n","sourceOld":"  /** This is the hotspot of indexing: it's called once\n   *  for every term of every document.  Its job is to *\n   *  update the postings byte stream (Postings hash) *\n   *  based on the occurence of a single term. */\n  private void addPosition(Token token) throws AbortException {\n\n    final Payload payload = token.getPayload();\n\n    // Get the text of this term.  Term can either\n    // provide a String token or offset into a char[]\n    // array\n    final char[] tokenText = token.termBuffer();\n    final int tokenTextLen = token.termLength();\n\n    int code = 0;\n\n    // Compute hashcode\n    int downto = tokenTextLen;\n    while (downto > 0)\n      code = (code*31) + tokenText[--downto];\n\n    // System.out.println(\"  addPosition: buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate Posting in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && !postingEquals(tokenText, tokenTextLen));\n    }\n\n    final int proxCode;\n\n    // If we hit an exception below, it's possible the\n    // posting list or term vectors data will be\n    // partially written and thus inconsistent if\n    // flushed, so we have to abort all documents\n    // since the last flush:\n\n    try {\n\n      if (p != null) {       // term seen since last flush\n\n        if (threadState.docID != p.lastDocID) { // term not yet seen in this doc\n            \n          // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n          assert p.docFreq > 0;\n\n          // Now that we know doc freq for previous doc,\n          // write it & lastDocCode\n          freqUpto = p.freqUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n          freq = threadState.postingsPool.buffers[p.freqUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n          if (1 == p.docFreq)\n            writeFreqVInt(p.lastDocCode|1);\n          else {\n            writeFreqVInt(p.lastDocCode);\n            writeFreqVInt(p.docFreq);\n          }\n          p.freqUpto = freqUpto + (p.freqUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStartCode = offsetStart = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n\n          p.docFreq = 1;\n\n          // Store code so we can write this after we're\n          // done with this new doc\n          p.lastDocCode = (threadState.docID-p.lastDocID) << 1;\n          p.lastDocID = threadState.docID;\n\n        } else {                                // term already seen in this doc\n          // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n          p.docFreq++;\n\n          proxCode = position-p.lastPosition;\n\n          if (doVectors) {\n            vector = p.vector;\n            if (vector == null)\n              vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n              offsetStartCode = offsetStart-vector.lastOffset;\n            }\n          }\n        }\n      } else {\t\t\t\t\t  // term not seen before\n        // System.out.println(\"    never seen docID=\" + docID);\n\n        // Refill?\n        if (0 == threadState.postingsFreeCount) {\n          threadState.docWriter.getPostings(threadState.postingsFreeList);\n          threadState.postingsFreeCount = threadState.postingsFreeList.length;\n        }\n\n        final int textLen1 = 1+tokenTextLen;\n        if (textLen1 + threadState.charPool.byteUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n            // Just skip this term, to remain as robust as\n            // possible during indexing.  A TokenFilter\n            // can be inserted into the analyzer chain if\n            // other behavior is wanted (pruning the term\n            // to a prefix, throwing an exception, etc).\n            if (threadState.maxTermPrefix == null)\n              threadState.maxTermPrefix = new String(tokenText, 0, 30);\n\n            // Still increment position:\n            position++;\n            return;\n          }\n          threadState.charPool.nextBuffer();\n        }\n\n        final char[] text = threadState.charPool.buffer;\n        final int textUpto = threadState.charPool.byteUpto;\n\n        // Pull next free Posting from free list\n        p = threadState.postingsFreeList[--threadState.postingsFreeCount];\n\n        p.textStart = textUpto + threadState.charPool.byteOffset;\n        threadState.charPool.byteUpto += textLen1;\n\n        System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n        text[textUpto+tokenTextLen] = 0xffff;\n          \n        assert postingsHash[hashPos] == null;\n\n        postingsHash[hashPos] = p;\n        numPostings++;\n\n        if (numPostings == postingsHashHalfSize)\n          rehashPostings(2*postingsHashSize);\n\n        // Init first slice for freq & prox streams\n        final int upto1 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        p.freqStart = p.freqUpto = threadState.postingsPool.byteOffset + upto1;\n\n        final int upto2 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        p.proxStart = p.proxUpto = threadState.postingsPool.byteOffset + upto2;\n\n        p.lastDocCode = threadState.docID << 1;\n        p.lastDocID = threadState.docID;\n        p.docFreq = 1;\n\n        if (doVectors) {\n          vector = addNewVector();\n          if (doVectorOffsets) {\n            offsetStart = offsetStartCode = offset + token.startOffset();\n            offsetEnd = offset + token.endOffset();\n          }\n        }\n\n        proxCode = position;\n      }\n\n      proxUpto = p.proxUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n      prox = threadState.postingsPool.buffers[p.proxUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n      assert prox != null;\n\n      if (payload != null && payload.length > 0) {\n        writeProxVInt((proxCode<<1)|1);\n        writeProxVInt(payload.length);\n        writeProxBytes(payload.data, payload.offset, payload.length);\n        fieldInfo.storePayloads = true;\n      } else\n        writeProxVInt(proxCode<<1);\n\n      p.proxUpto = proxUpto + (p.proxUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n\n      p.lastPosition = position++;\n\n      if (doVectorPositions) {\n        posUpto = vector.posUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n        pos = threadState.vectorsPool.buffers[vector.posUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n        writePosVInt(proxCode);\n        vector.posUpto = posUpto + (vector.posUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n      }\n\n      if (doVectorOffsets) {\n        offsetUpto = vector.offsetUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n        offsets = threadState.vectorsPool.buffers[vector.offsetUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n        writeOffsetVInt(offsetStartCode);\n        writeOffsetVInt(offsetEnd-offsetStart);\n        vector.lastOffset = offsetEnd;\n        vector.offsetUpto = offsetUpto + (vector.offsetUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n      }\n    } catch (Throwable t) {\n      throw new AbortException(t, threadState.docWriter);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e8f450af7a7b034413833ed2a9508f99264ea49a","date":1211042958,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#addPosition(Token).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#addPosition(Token).mjava","sourceNew":"  /** This is the hotspot of indexing: it's called once\n   *  for every term of every document.  Its job is to *\n   *  update the postings byte stream (Postings hash) *\n   *  based on the occurence of a single term. */\n  private void addPosition(Token token) throws AbortException {\n\n    final Payload payload = token.getPayload();\n\n    // Get the text of this term.  Term can either\n    // provide a String token or offset into a char[]\n    // array\n    final char[] tokenText = token.termBuffer();\n    final int tokenTextLen = token.termLength();\n\n    int code = 0;\n\n    // Compute hashcode & replace any invalid UTF16 sequences\n    int downto = tokenTextLen;\n    while (downto > 0) {\n      char ch = tokenText[--downto];\n\n      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {\n        if (0 == downto) {\n          // Unpaired\n          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n        } else {\n          final char ch2 = tokenText[downto-1];\n          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {\n            // OK: high followed by low.  This is a valid\n            // surrogate pair.\n            code = ((code*31) + ch)*31+ch2;\n            downto--;\n            continue;\n          } else {\n            // Unpaired\n            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n          }            \n        }\n      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && ch <= UnicodeUtil.UNI_SUR_HIGH_END)\n        // Unpaired\n        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n\n      code = (code*31) + ch;\n    }\n\n    // System.out.println(\"  addPosition: field=\" + fieldInfo.name + \" buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate Posting in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && !postingEquals(tokenText, tokenTextLen));\n    }\n\n    final int proxCode;\n\n    // If we hit an exception below, it's possible the\n    // posting list or term vectors data will be\n    // partially written and thus inconsistent if\n    // flushed, so we have to abort all documents\n    // since the last flush:\n\n    try {\n\n      if (p != null) {       // term seen since last flush\n\n        if (threadState.docID != p.lastDocID) { // term not yet seen in this doc\n            \n          // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n          assert p.docFreq > 0;\n\n          // Now that we know doc freq for previous doc,\n          // write it & lastDocCode\n          sliceWriter.init(p.freqUpto);\n\n          if (1 == p.docFreq)\n            sliceWriter.writeVInt(p.lastDocCode|1);\n          else {\n            sliceWriter.writeVInt(p.lastDocCode);\n            sliceWriter.writeVInt(p.docFreq);\n          }\n          p.freqUpto = sliceWriter.getAddress();\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStartCode = offsetStart = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n\n          p.docFreq = 1;\n\n          // Store code so we can write this after we're\n          // done with this new doc\n          p.lastDocCode = (threadState.docID-p.lastDocID) << 1;\n          p.lastDocID = threadState.docID;\n\n        } else {                                // term already seen in this doc\n          // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n          p.docFreq++;\n\n          proxCode = position-p.lastPosition;\n\n          if (doVectors) {\n            vector = p.vector;\n            if (vector == null)\n              vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n              offsetStartCode = offsetStart-vector.lastOffset;\n            }\n          }\n        }\n      } else {\t\t\t\t\t  // term not seen before\n        // System.out.println(\"    never seen docID=\" + docID);\n\n        // Refill?\n        if (0 == threadState.postingsFreeCount) {\n          threadState.docWriter.getPostings(threadState.postingsFreeList);\n          threadState.postingsFreeCount = threadState.postingsFreeList.length;\n        }\n\n        final int textLen1 = 1+tokenTextLen;\n        if (textLen1 + threadState.charPool.byteUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n            // Just skip this term, to remain as robust as\n            // possible during indexing.  A TokenFilter\n            // can be inserted into the analyzer chain if\n            // other behavior is wanted (pruning the term\n            // to a prefix, throwing an exception, etc).\n            if (threadState.maxTermPrefix == null)\n              threadState.maxTermPrefix = new String(tokenText, 0, 30);\n\n            // Still increment position:\n            position++;\n            return;\n          }\n          threadState.charPool.nextBuffer();\n        }\n\n        final char[] text = threadState.charPool.buffer;\n        final int textUpto = threadState.charPool.byteUpto;\n\n        // Pull next free Posting from free list\n        p = threadState.postingsFreeList[--threadState.postingsFreeCount];\n\n        p.textStart = textUpto + threadState.charPool.byteOffset;\n        threadState.charPool.byteUpto += textLen1;\n\n        System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n        text[textUpto+tokenTextLen] = 0xffff;\n          \n        assert postingsHash[hashPos] == null;\n\n        postingsHash[hashPos] = p;\n        numPostings++;\n\n        if (numPostings == postingsHashHalfSize)\n          rehashPostings(2*postingsHashSize);\n\n        // Init first slice for freq & prox streams\n        final int upto1 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        p.freqStart = p.freqUpto = threadState.postingsPool.byteOffset + upto1;\n\n        final int upto2 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        p.proxStart = p.proxUpto = threadState.postingsPool.byteOffset + upto2;\n\n        p.lastDocCode = threadState.docID << 1;\n        p.lastDocID = threadState.docID;\n        p.docFreq = 1;\n\n        if (doVectors) {\n          vector = addNewVector();\n          if (doVectorOffsets) {\n            offsetStart = offsetStartCode = offset + token.startOffset();\n            offsetEnd = offset + token.endOffset();\n          }\n        }\n\n        proxCode = position;\n      }\n\n      sliceWriter.init(p.proxUpto);\n\n      if (payload != null && payload.length > 0) {\n        sliceWriter.writeVInt((proxCode<<1)|1);\n        sliceWriter.writeVInt(payload.length);\n        sliceWriter.writeBytes(payload.data, payload.offset, payload.length);\n        fieldInfo.storePayloads = true;\n      } else\n        sliceWriter.writeVInt(proxCode<<1);\n\n      p.proxUpto = sliceWriter.getAddress();\n      p.lastPosition = position++;\n\n      if (doVectorPositions) {\n        vectorsSliceWriter.init(vector.posUpto);\n        vectorsSliceWriter.writeVInt(proxCode);\n        vector.posUpto = vectorsSliceWriter.getAddress();\n      }\n\n      if (doVectorOffsets) {\n        vectorsSliceWriter.init(vector.offsetUpto);\n        vectorsSliceWriter.writeVInt(offsetStartCode);\n        vectorsSliceWriter.writeVInt(offsetEnd-offsetStart);\n        vector.lastOffset = offsetEnd;\n        vector.offsetUpto = vectorsSliceWriter.getAddress();\n      }\n    } catch (Throwable t) {\n      throw new AbortException(t, threadState.docWriter);\n    }\n  }\n\n","sourceOld":"  /** This is the hotspot of indexing: it's called once\n   *  for every term of every document.  Its job is to *\n   *  update the postings byte stream (Postings hash) *\n   *  based on the occurence of a single term. */\n  private void addPosition(Token token) throws AbortException {\n\n    final Payload payload = token.getPayload();\n\n    // Get the text of this term.  Term can either\n    // provide a String token or offset into a char[]\n    // array\n    final char[] tokenText = token.termBuffer();\n    final int tokenTextLen = token.termLength();\n\n    int code = 0;\n\n    // Compute hashcode & replace any invalid UTF16 sequences\n    int downto = tokenTextLen;\n    while (downto > 0) {\n      char ch = tokenText[--downto];\n\n      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {\n        if (0 == downto) {\n          // Unpaired\n          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n        } else {\n          final char ch2 = tokenText[downto-1];\n          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {\n            // OK: high followed by low.  This is a valid\n            // surrogate pair.\n            code = ((code*31) + ch)*31+ch2;\n            downto--;\n            continue;\n          } else {\n            // Unpaired\n            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n          }            \n        }\n      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && ch <= UnicodeUtil.UNI_SUR_HIGH_END)\n        // Unpaired\n        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n\n      code = (code*31) + ch;\n    }\n\n    // System.out.println(\"  addPosition: field=\" + fieldInfo.name + \" buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate Posting in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && !postingEquals(tokenText, tokenTextLen));\n    }\n\n    final int proxCode;\n\n    // If we hit an exception below, it's possible the\n    // posting list or term vectors data will be\n    // partially written and thus inconsistent if\n    // flushed, so we have to abort all documents\n    // since the last flush:\n\n    try {\n\n      if (p != null) {       // term seen since last flush\n\n        if (threadState.docID != p.lastDocID) { // term not yet seen in this doc\n            \n          // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n          assert p.docFreq > 0;\n\n          // Now that we know doc freq for previous doc,\n          // write it & lastDocCode\n          freqUpto = p.freqUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n          freq = threadState.postingsPool.buffers[p.freqUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n          if (1 == p.docFreq)\n            writeFreqVInt(p.lastDocCode|1);\n          else {\n            writeFreqVInt(p.lastDocCode);\n            writeFreqVInt(p.docFreq);\n          }\n          p.freqUpto = freqUpto + (p.freqUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStartCode = offsetStart = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n\n          p.docFreq = 1;\n\n          // Store code so we can write this after we're\n          // done with this new doc\n          p.lastDocCode = (threadState.docID-p.lastDocID) << 1;\n          p.lastDocID = threadState.docID;\n\n        } else {                                // term already seen in this doc\n          // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n          p.docFreq++;\n\n          proxCode = position-p.lastPosition;\n\n          if (doVectors) {\n            vector = p.vector;\n            if (vector == null)\n              vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n              offsetStartCode = offsetStart-vector.lastOffset;\n            }\n          }\n        }\n      } else {\t\t\t\t\t  // term not seen before\n        // System.out.println(\"    never seen docID=\" + docID);\n\n        // Refill?\n        if (0 == threadState.postingsFreeCount) {\n          threadState.docWriter.getPostings(threadState.postingsFreeList);\n          threadState.postingsFreeCount = threadState.postingsFreeList.length;\n        }\n\n        final int textLen1 = 1+tokenTextLen;\n        if (textLen1 + threadState.charPool.byteUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n            // Just skip this term, to remain as robust as\n            // possible during indexing.  A TokenFilter\n            // can be inserted into the analyzer chain if\n            // other behavior is wanted (pruning the term\n            // to a prefix, throwing an exception, etc).\n            if (threadState.maxTermPrefix == null)\n              threadState.maxTermPrefix = new String(tokenText, 0, 30);\n\n            // Still increment position:\n            position++;\n            return;\n          }\n          threadState.charPool.nextBuffer();\n        }\n\n        final char[] text = threadState.charPool.buffer;\n        final int textUpto = threadState.charPool.byteUpto;\n\n        // Pull next free Posting from free list\n        p = threadState.postingsFreeList[--threadState.postingsFreeCount];\n\n        p.textStart = textUpto + threadState.charPool.byteOffset;\n        threadState.charPool.byteUpto += textLen1;\n\n        System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n        text[textUpto+tokenTextLen] = 0xffff;\n          \n        assert postingsHash[hashPos] == null;\n\n        postingsHash[hashPos] = p;\n        numPostings++;\n\n        if (numPostings == postingsHashHalfSize)\n          rehashPostings(2*postingsHashSize);\n\n        // Init first slice for freq & prox streams\n        final int upto1 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        p.freqStart = p.freqUpto = threadState.postingsPool.byteOffset + upto1;\n\n        final int upto2 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        p.proxStart = p.proxUpto = threadState.postingsPool.byteOffset + upto2;\n\n        p.lastDocCode = threadState.docID << 1;\n        p.lastDocID = threadState.docID;\n        p.docFreq = 1;\n\n        if (doVectors) {\n          vector = addNewVector();\n          if (doVectorOffsets) {\n            offsetStart = offsetStartCode = offset + token.startOffset();\n            offsetEnd = offset + token.endOffset();\n          }\n        }\n\n        proxCode = position;\n      }\n\n      proxUpto = p.proxUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n      prox = threadState.postingsPool.buffers[p.proxUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n      assert prox != null;\n\n      if (payload != null && payload.length > 0) {\n        writeProxVInt((proxCode<<1)|1);\n        writeProxVInt(payload.length);\n        writeProxBytes(payload.data, payload.offset, payload.length);\n        fieldInfo.storePayloads = true;\n      } else\n        writeProxVInt(proxCode<<1);\n\n      p.proxUpto = proxUpto + (p.proxUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n\n      p.lastPosition = position++;\n\n      if (doVectorPositions) {\n        posUpto = vector.posUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n        pos = threadState.vectorsPool.buffers[vector.posUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n        writePosVInt(proxCode);\n        vector.posUpto = posUpto + (vector.posUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n      }\n\n      if (doVectorOffsets) {\n        offsetUpto = vector.offsetUpto & DocumentsWriter.BYTE_BLOCK_MASK;\n        offsets = threadState.vectorsPool.buffers[vector.offsetUpto >> DocumentsWriter.BYTE_BLOCK_SHIFT];\n        writeOffsetVInt(offsetStartCode);\n        writeOffsetVInt(offsetEnd-offsetStart);\n        vector.lastOffset = offsetEnd;\n        vector.offsetUpto = offsetUpto + (vector.offsetUpto & DocumentsWriter.BYTE_BLOCK_NOT_MASK);\n      }\n    } catch (Throwable t) {\n      throw new AbortException(t, threadState.docWriter);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentsWriterFieldData#addPosition(Token).mjava","sourceNew":null,"sourceOld":"  /** This is the hotspot of indexing: it's called once\n   *  for every term of every document.  Its job is to *\n   *  update the postings byte stream (Postings hash) *\n   *  based on the occurence of a single term. */\n  private void addPosition(Token token) throws AbortException {\n\n    final Payload payload = token.getPayload();\n\n    // Get the text of this term.  Term can either\n    // provide a String token or offset into a char[]\n    // array\n    final char[] tokenText = token.termBuffer();\n    final int tokenTextLen = token.termLength();\n\n    int code = 0;\n\n    // Compute hashcode & replace any invalid UTF16 sequences\n    int downto = tokenTextLen;\n    while (downto > 0) {\n      char ch = tokenText[--downto];\n\n      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {\n        if (0 == downto) {\n          // Unpaired\n          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n        } else {\n          final char ch2 = tokenText[downto-1];\n          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {\n            // OK: high followed by low.  This is a valid\n            // surrogate pair.\n            code = ((code*31) + ch)*31+ch2;\n            downto--;\n            continue;\n          } else {\n            // Unpaired\n            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n          }            \n        }\n      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && ch <= UnicodeUtil.UNI_SUR_HIGH_END)\n        // Unpaired\n        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;\n\n      code = (code*31) + ch;\n    }\n\n    // System.out.println(\"  addPosition: field=\" + fieldInfo.name + \" buffer=\" + new String(tokenText, 0, tokenTextLen) + \" pos=\" + position + \" offsetStart=\" + (offset+token.startOffset()) + \" offsetEnd=\" + (offset + token.endOffset()) + \" docID=\" + docID + \" doPos=\" + doVectorPositions + \" doOffset=\" + doVectorOffsets);\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate Posting in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && !postingEquals(tokenText, tokenTextLen)) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && !postingEquals(tokenText, tokenTextLen));\n    }\n\n    final int proxCode;\n\n    // If we hit an exception below, it's possible the\n    // posting list or term vectors data will be\n    // partially written and thus inconsistent if\n    // flushed, so we have to abort all documents\n    // since the last flush:\n\n    try {\n\n      if (p != null) {       // term seen since last flush\n\n        if (threadState.docID != p.lastDocID) { // term not yet seen in this doc\n            \n          // System.out.println(\"    seen before (new docID=\" + docID + \") freqUpto=\" + p.freqUpto +\" proxUpto=\" + p.proxUpto);\n\n          assert p.docFreq > 0;\n\n          // Now that we know doc freq for previous doc,\n          // write it & lastDocCode\n          sliceWriter.init(p.freqUpto);\n\n          if (1 == p.docFreq)\n            sliceWriter.writeVInt(p.lastDocCode|1);\n          else {\n            sliceWriter.writeVInt(p.lastDocCode);\n            sliceWriter.writeVInt(p.docFreq);\n          }\n          p.freqUpto = sliceWriter.getAddress();\n\n          if (doVectors) {\n            vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStartCode = offsetStart = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n            }\n          }\n\n          proxCode = position;\n\n          p.docFreq = 1;\n\n          // Store code so we can write this after we're\n          // done with this new doc\n          p.lastDocCode = (threadState.docID-p.lastDocID) << 1;\n          p.lastDocID = threadState.docID;\n\n        } else {                                // term already seen in this doc\n          // System.out.println(\"    seen before (same docID=\" + docID + \") proxUpto=\" + p.proxUpto);\n          p.docFreq++;\n\n          proxCode = position-p.lastPosition;\n\n          if (doVectors) {\n            vector = p.vector;\n            if (vector == null)\n              vector = addNewVector();\n            if (doVectorOffsets) {\n              offsetStart = offset + token.startOffset();\n              offsetEnd = offset + token.endOffset();\n              offsetStartCode = offsetStart-vector.lastOffset;\n            }\n          }\n        }\n      } else {\t\t\t\t\t  // term not seen before\n        // System.out.println(\"    never seen docID=\" + docID);\n\n        // Refill?\n        if (0 == threadState.postingsFreeCount) {\n          threadState.docWriter.getPostings(threadState.postingsFreeList);\n          threadState.postingsFreeCount = threadState.postingsFreeList.length;\n        }\n\n        final int textLen1 = 1+tokenTextLen;\n        if (textLen1 + threadState.charPool.byteUpto > DocumentsWriter.CHAR_BLOCK_SIZE) {\n          if (textLen1 > DocumentsWriter.CHAR_BLOCK_SIZE) {\n            // Just skip this term, to remain as robust as\n            // possible during indexing.  A TokenFilter\n            // can be inserted into the analyzer chain if\n            // other behavior is wanted (pruning the term\n            // to a prefix, throwing an exception, etc).\n            if (threadState.maxTermPrefix == null)\n              threadState.maxTermPrefix = new String(tokenText, 0, 30);\n\n            // Still increment position:\n            position++;\n            return;\n          }\n          threadState.charPool.nextBuffer();\n        }\n\n        final char[] text = threadState.charPool.buffer;\n        final int textUpto = threadState.charPool.byteUpto;\n\n        // Pull next free Posting from free list\n        p = threadState.postingsFreeList[--threadState.postingsFreeCount];\n\n        p.textStart = textUpto + threadState.charPool.byteOffset;\n        threadState.charPool.byteUpto += textLen1;\n\n        System.arraycopy(tokenText, 0, text, textUpto, tokenTextLen);\n\n        text[textUpto+tokenTextLen] = 0xffff;\n          \n        assert postingsHash[hashPos] == null;\n\n        postingsHash[hashPos] = p;\n        numPostings++;\n\n        if (numPostings == postingsHashHalfSize)\n          rehashPostings(2*postingsHashSize);\n\n        // Init first slice for freq & prox streams\n        final int upto1 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        p.freqStart = p.freqUpto = threadState.postingsPool.byteOffset + upto1;\n\n        final int upto2 = threadState.postingsPool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        p.proxStart = p.proxUpto = threadState.postingsPool.byteOffset + upto2;\n\n        p.lastDocCode = threadState.docID << 1;\n        p.lastDocID = threadState.docID;\n        p.docFreq = 1;\n\n        if (doVectors) {\n          vector = addNewVector();\n          if (doVectorOffsets) {\n            offsetStart = offsetStartCode = offset + token.startOffset();\n            offsetEnd = offset + token.endOffset();\n          }\n        }\n\n        proxCode = position;\n      }\n\n      sliceWriter.init(p.proxUpto);\n\n      if (payload != null && payload.length > 0) {\n        sliceWriter.writeVInt((proxCode<<1)|1);\n        sliceWriter.writeVInt(payload.length);\n        sliceWriter.writeBytes(payload.data, payload.offset, payload.length);\n        fieldInfo.storePayloads = true;\n      } else\n        sliceWriter.writeVInt(proxCode<<1);\n\n      p.proxUpto = sliceWriter.getAddress();\n      p.lastPosition = position++;\n\n      if (doVectorPositions) {\n        vectorsSliceWriter.init(vector.posUpto);\n        vectorsSliceWriter.writeVInt(proxCode);\n        vector.posUpto = vectorsSliceWriter.getAddress();\n      }\n\n      if (doVectorOffsets) {\n        vectorsSliceWriter.init(vector.offsetUpto);\n        vectorsSliceWriter.writeVInt(offsetStartCode);\n        vectorsSliceWriter.writeVInt(offsetEnd-offsetStart);\n        vector.lastOffset = offsetEnd;\n        vector.offsetUpto = vectorsSliceWriter.getAddress();\n      }\n    } catch (Throwable t) {\n      throw new AbortException(t, threadState.docWriter);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e8f450af7a7b034413833ed2a9508f99264ea49a":["dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4":["5a0af3a442be522899177e5e11384a45a6784a3f"],"5a0af3a442be522899177e5e11384a45a6784a3f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5350389bf83287111f7760b9e3db3af8e3648474":["e8f450af7a7b034413833ed2a9508f99264ea49a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5350389bf83287111f7760b9e3db3af8e3648474"]},"commit2Childs":{"e8f450af7a7b034413833ed2a9508f99264ea49a":["5350389bf83287111f7760b9e3db3af8e3648474"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5a0af3a442be522899177e5e11384a45a6784a3f"],"dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4":["e8f450af7a7b034413833ed2a9508f99264ea49a"],"5a0af3a442be522899177e5e11384a45a6784a3f":["dbb7b6f10bff9eedd5c9bc6cf9222ffbb2df74d4"],"5350389bf83287111f7760b9e3db3af8e3648474":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}