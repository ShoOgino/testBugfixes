{"path":"lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame#loadBlock().mjava","commits":[{"id":"3cf3e973ce145ef8b164e2e7c0d57cf807cf9a6e","date":1400786907,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame#loadBlock().mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame#loadBlock().mjava","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n    assert arc == null || (isLastInFloor || isFloor);\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n    assert arc == null || (isLastInFloor || isFloor);\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d637064d608752565d4f9f41b2497dfdfdde50e","date":1400798123,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame#loadBlock().mjava","pathOld":"/dev/null","sourceNew":"  /* Does initial decode of next block of terms; this\n     doesn't actually decode the docFreq, totalTermFreq,\n     postings details (frq/prx offset, etc.) metadata;\n     it just loads them as byte[] blobs which are then      \n     decoded on-demand if the metadata is ever requested\n     for any term in this block.  This enables terms-only\n     intensive consumes (eg certain MTQs, respelling) to\n     not pay the price of decoding metadata they won't\n     use. */\n  void loadBlock() throws IOException {\n\n    // Clone the IndexInput lazily, so that consumers\n    // that just pull a TermsEnum to\n    // seekExact(TermState) don't pay this cost:\n    ste.initIndexInput();\n\n    if (nextEnt != -1) {\n      // Already loaded\n      return;\n    }\n    //System.out.println(\"blc=\" + blockLoadCount);\n\n    ste.in.seek(fp);\n    int code = ste.in.readVInt();\n    entCount = code >>> 1;\n    assert entCount > 0;\n    isLastInFloor = (code & 1) != 0;\n    assert arc == null || (isLastInFloor || isFloor);\n\n    // TODO: if suffixes were stored in random-access\n    // array structure, then we could do binary search\n    // instead of linear scan to find target term; eg\n    // we could have simple array of offsets\n\n    // term suffixes:\n    code = ste.in.readVInt();\n    isLeafBlock = (code & 1) != 0;\n    int numBytes = code >>> 1;\n    if (suffixBytes.length < numBytes) {\n      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(suffixBytes, 0, numBytes);\n    suffixesReader.reset(suffixBytes, 0, numBytes);\n\n    /*if (DEBUG) {\n      if (arc == null) {\n      System.out.println(\"    loadBlock (next) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      } else {\n      System.out.println(\"    loadBlock (seek) fp=\" + fp + \" entCount=\" + entCount + \" prefixLen=\" + prefix + \" hasTerms?=\" + hasTerms + \" isFloor?=\" + isFloor + \" isLastInFloor=\" + isLastInFloor + \" leaf?=\" + isLeafBlock);\n      }\n      }*/\n\n    metaDataUpto = 0;\n\n    state.termBlockOrd = 0;\n    nextEnt = 0;\n    lastSubFP = -1;\n\n    // TODO: we could skip this if !hasTerms; but\n    // that's rare so won't help much\n    // metadata\n    numBytes = ste.in.readVInt();\n    if (bytes == null) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n      bytesReader = new ByteArrayDataInput();\n    } else if (bytes.length < numBytes) {\n      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];\n    }\n    ste.in.readBytes(bytes, 0, numBytes);\n    bytesReader.reset(bytes, 0, numBytes);\n\n    // Sub-blocks of a single floor block are always\n    // written one after another -- tail recurse:\n    fpEnd = ste.in.getFilePointer();\n    // if (DEBUG) {\n    //   System.out.println(\"      fpEnd=\" + fpEnd);\n    // }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3cf3e973ce145ef8b164e2e7c0d57cf807cf9a6e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4d637064d608752565d4f9f41b2497dfdfdde50e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cf3e973ce145ef8b164e2e7c0d57cf807cf9a6e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4d637064d608752565d4f9f41b2497dfdfdde50e"]},"commit2Childs":{"3cf3e973ce145ef8b164e2e7c0d57cf807cf9a6e":["4d637064d608752565d4f9f41b2497dfdfdde50e"],"4d637064d608752565d4f9f41b2497dfdfdde50e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3cf3e973ce145ef8b164e2e7c0d57cf807cf9a6e","4d637064d608752565d4f9f41b2497dfdfdde50e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}