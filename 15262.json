{"path":"solr/core/src/test/org/apache/solr/cloud/SplitShardTest#doLiveSplitShard(String,int).mjava","commits":[{"id":"88922bf68f0b509aba218f1b9e7ef5981b4d13bc","date":1570820823,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/SplitShardTest#doLiveSplitShard(String,int).mjava","pathOld":"/dev/null","sourceNew":"  void doLiveSplitShard(String collectionName, int repFactor) throws Exception {\n    final CloudSolrClient client = createCollection(collectionName, repFactor);\n\n    final AtomicBoolean doIndex = new AtomicBoolean(true);\n    final AtomicInteger docsIndexed = new AtomicInteger();\n    Thread indexThread = null;\n    try {\n      // start indexing client before we initiate a shard split\n      indexThread = new Thread(() -> {\n        while (doIndex.get()) {\n          try {\n            // Thread.sleep(10);  // uncomment this to cap indexing rate at 100 docs per second...\n            int currDoc = docsIndexed.get();\n\n            // Try all docs in the same update request\n            UpdateRequest updateReq = new UpdateRequest();\n            updateReq.add(sdoc(\"id\", \"doc_\" + currDoc));\n            UpdateResponse ursp = updateReq.commit(client, collectionName);\n            assertEquals(0, ursp.getStatus());  // for now, don't accept any failures\n            if (ursp.getStatus() == 0) {\n              docsIndexed.incrementAndGet();\n            }\n          } catch (Exception e) {\n            fail(e.getMessage());\n            break;\n          }\n        }\n      });\n      indexThread.start();\n\n      Thread.sleep(100);  // wait for a few docs to be indexed before invoking split\n      int docCount = docsIndexed.get();\n\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(collectionName)\n          .setShardName(\"shard1\");\n      splitShard.process(client);\n      waitForState(\"Timed out waiting for sub shards to be active.\",\n          collectionName, activeClusterShape(2, 3*repFactor));  // 2 repFactor for the new split shards, 1 repFactor for old replicas\n\n      // make sure that docs were able to be indexed during the split\n      assertTrue(docsIndexed.get() > docCount);\n\n      Thread.sleep(100);  // wait for a few more docs to be indexed after split\n\n    } finally {\n      // shut down the indexer\n      doIndex.set(false);\n      if (indexThread != null) {\n        indexThread.join();\n      }\n    }\n\n    assertTrue(docsIndexed.get() > 0);\n\n    long numDocs = getNumDocs(client);\n    if (numDocs != docsIndexed.get()) {\n      // Find out what docs are missing.\n      for (int i = 0; i < docsIndexed.get(); i++) {\n        String id = \"doc_\" + i;\n        long cloudClientDocs = client.query(new SolrQuery(\"id:\" + id)).getResults().getNumFound();\n        if (cloudClientDocs != 1) {\n          log.error(\"MISSING DOCUMENT \" + id);\n        }\n      }\n    }\n\n    assertEquals(\"Documents are missing!\", docsIndexed.get(), numDocs);\n    log.info(\"Number of documents indexed and queried : \" + numDocs);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f20e04b032e24aaaba9b775524b804f96b322e49","date":1571000830,"type":5,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/SplitShardTest#doLiveSplitShard(String,int,int).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/SplitShardTest#doLiveSplitShard(String,int).mjava","sourceNew":"  void doLiveSplitShard(String collectionName, int repFactor, int nThreads) throws Exception {\n    final CloudSolrClient client = createCollection(collectionName, repFactor);\n\n    final ConcurrentHashMap<String,Long> model = new ConcurrentHashMap<>();  // what the index should contain\n    final AtomicBoolean doIndex = new AtomicBoolean(true);\n    final AtomicInteger docsIndexed = new AtomicInteger();\n    Thread[] indexThreads = new Thread[nThreads];\n    try {\n\n      for (int i=0; i<nThreads; i++) {\n        indexThreads[i] = new Thread(() -> {\n          while (doIndex.get()) {\n            try {\n              // Thread.sleep(10);  // cap indexing rate at 100 docs per second per thread\n              int currDoc = docsIndexed.incrementAndGet();\n              String docId = \"doc_\" + currDoc;\n\n              // Try all docs in the same update request\n              UpdateRequest updateReq = new UpdateRequest();\n              updateReq.add(sdoc(\"id\", docId));\n              // UpdateResponse ursp = updateReq.commit(client, collectionName);  // uncomment this if you want a commit each time\n              UpdateResponse ursp = updateReq.process(client, collectionName);\n              assertEquals(0, ursp.getStatus());  // for now, don't accept any failures\n              if (ursp.getStatus() == 0) {\n                model.put(docId, 1L);  // in the future, keep track of a version per document and reuse ids to keep index from growing too large\n              }\n            } catch (Exception e) {\n              fail(e.getMessage());\n              break;\n            }\n          }\n        });\n      }\n\n      for (Thread thread : indexThreads) {\n        thread.start();\n      }\n\n      Thread.sleep(100);  // wait for a few docs to be indexed before invoking split\n      int docCount = model.size();\n\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(collectionName)\n          .setShardName(\"shard1\");\n      splitShard.process(client);\n      waitForState(\"Timed out waiting for sub shards to be active.\",\n          collectionName, activeClusterShape(2, 3*repFactor));  // 2 repFactor for the new split shards, 1 repFactor for old replicas\n\n      // make sure that docs were able to be indexed during the split\n      assertTrue(model.size() > docCount);\n\n      Thread.sleep(100);  // wait for a few more docs to be indexed after split\n\n    } finally {\n      // shut down the indexers\n      doIndex.set(false);\n      for (Thread thread : indexThreads) {\n        thread.join();\n      }\n    }\n\n    client.commit();  // final commit is needed for visibility\n\n    long numDocs = getNumDocs(client);\n    if (numDocs != model.size()) {\n      SolrDocumentList results = client.query(new SolrQuery(\"q\",\"*:*\", \"fl\",\"id\", \"rows\", Integer.toString(model.size()) )).getResults();\n      Map<String,Long> leftover = new HashMap<>(model);\n      for (SolrDocument doc : results) {\n        String id = (String) doc.get(\"id\");\n        leftover.remove(id);\n      }\n      log.error(\"MISSING DOCUMENTS: \" + leftover);\n    }\n\n    assertEquals(\"Documents are missing!\", docsIndexed.get(), numDocs);\n    log.info(\"Number of documents indexed and queried : \" + numDocs);\n  }\n\n","sourceOld":"  void doLiveSplitShard(String collectionName, int repFactor) throws Exception {\n    final CloudSolrClient client = createCollection(collectionName, repFactor);\n\n    final AtomicBoolean doIndex = new AtomicBoolean(true);\n    final AtomicInteger docsIndexed = new AtomicInteger();\n    Thread indexThread = null;\n    try {\n      // start indexing client before we initiate a shard split\n      indexThread = new Thread(() -> {\n        while (doIndex.get()) {\n          try {\n            // Thread.sleep(10);  // uncomment this to cap indexing rate at 100 docs per second...\n            int currDoc = docsIndexed.get();\n\n            // Try all docs in the same update request\n            UpdateRequest updateReq = new UpdateRequest();\n            updateReq.add(sdoc(\"id\", \"doc_\" + currDoc));\n            UpdateResponse ursp = updateReq.commit(client, collectionName);\n            assertEquals(0, ursp.getStatus());  // for now, don't accept any failures\n            if (ursp.getStatus() == 0) {\n              docsIndexed.incrementAndGet();\n            }\n          } catch (Exception e) {\n            fail(e.getMessage());\n            break;\n          }\n        }\n      });\n      indexThread.start();\n\n      Thread.sleep(100);  // wait for a few docs to be indexed before invoking split\n      int docCount = docsIndexed.get();\n\n      CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(collectionName)\n          .setShardName(\"shard1\");\n      splitShard.process(client);\n      waitForState(\"Timed out waiting for sub shards to be active.\",\n          collectionName, activeClusterShape(2, 3*repFactor));  // 2 repFactor for the new split shards, 1 repFactor for old replicas\n\n      // make sure that docs were able to be indexed during the split\n      assertTrue(docsIndexed.get() > docCount);\n\n      Thread.sleep(100);  // wait for a few more docs to be indexed after split\n\n    } finally {\n      // shut down the indexer\n      doIndex.set(false);\n      if (indexThread != null) {\n        indexThread.join();\n      }\n    }\n\n    assertTrue(docsIndexed.get() > 0);\n\n    long numDocs = getNumDocs(client);\n    if (numDocs != docsIndexed.get()) {\n      // Find out what docs are missing.\n      for (int i = 0; i < docsIndexed.get(); i++) {\n        String id = \"doc_\" + i;\n        long cloudClientDocs = client.query(new SolrQuery(\"id:\" + id)).getResults().getNumFound();\n        if (cloudClientDocs != 1) {\n          log.error(\"MISSING DOCUMENT \" + id);\n        }\n      }\n    }\n\n    assertEquals(\"Documents are missing!\", docsIndexed.get(), numDocs);\n    log.info(\"Number of documents indexed and queried : \" + numDocs);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"88922bf68f0b509aba218f1b9e7ef5981b4d13bc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f20e04b032e24aaaba9b775524b804f96b322e49":["88922bf68f0b509aba218f1b9e7ef5981b4d13bc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f20e04b032e24aaaba9b775524b804f96b322e49"]},"commit2Childs":{"88922bf68f0b509aba218f1b9e7ef5981b4d13bc":["f20e04b032e24aaaba9b775524b804f96b322e49"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["88922bf68f0b509aba218f1b9e7ef5981b4d13bc"],"f20e04b032e24aaaba9b775524b804f96b322e49":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}