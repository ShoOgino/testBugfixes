{"path":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","commits":[{"id":"91109046a59c58ee0ee5d0d2767b08d1f30d6702","date":1000830588,"type":0,"author":"Jason van Zyl","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","pathOld":"/dev/null","sourceNew":"  final void addDocument(String segment, Document doc)\n       throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n      \n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];\t  // init fieldLengths\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(doc, segment);\n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"14aec0a40da5a9c26f8752701a5aa10f78f5017d","date":1027969875,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","sourceNew":"  final void addDocument(String segment, Document doc)\n       throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n      \n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];\t  // init fieldLengths\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(doc, segment);\n    \n  }\n\n","sourceOld":"  final void addDocument(String segment, Document doc)\n       throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n      \n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];\t  // init fieldLengths\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(doc, segment);\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"03ae70812bc33b0270c1366378b2c2da95fe86a6","date":1036648540,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","sourceNew":"  final void addDocument(String segment, Document doc)\n       throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];\t  // init fieldLengths\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(doc, segment);\n\n  }\n\n","sourceOld":"  final void addDocument(String segment, Document doc)\n       throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n      \n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];\t  // init fieldLengths\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(doc, segment);\n    \n  }\n\n","bugFix":null,"bugIntro":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"66f8ac981e8707cfae011613a8168a2edeb0b6e3","date":1064079760,"type":3,"author":"Erik Hatcher","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","sourceNew":"  final void addDocument(String segment, Document doc)\n    throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];\t  // init fieldLengths\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(doc, segment);\n\n  }\n\n","sourceOld":"  final void addDocument(String segment, Document doc)\n       throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];\t  // init fieldLengths\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(doc, segment);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2ef9d8f446efc33e4fc919333c39cd20410ffa11","date":1072129218,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","sourceNew":"  final void addDocument(String segment, Document doc)\n    throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];    // init fieldLengths\n    fieldPositions = new int[fieldInfos.size()];  // init fieldPositions\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(doc, segment);\n\n  }\n\n","sourceOld":"  final void addDocument(String segment, Document doc)\n    throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];\t  // init fieldLengths\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(doc, segment);\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"770281b8a8459cafcdd2354b6a06078fea2d83c9","date":1077308096,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","sourceNew":"  final void addDocument(String segment, Document doc)\n          throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];    // init fieldLengths\n    fieldPositions = new int[fieldInfos.size()];  // init fieldPositions\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(doc, segment);\n\n  }\n\n","sourceOld":"  final void addDocument(String segment, Document doc)\n    throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n      new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];    // init fieldLengths\n    fieldPositions = new int[fieldInfos.size()];  // init fieldPositions\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(doc, segment);\n\n  }\n\n","bugFix":null,"bugIntro":["b443dda8f84ed86d67aeecb48ce98151c485dbe6","1b54a9bc667895a2095a886184bf69a3179e63df"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0","date":1096997448,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","sourceNew":"  final void addDocument(String segment, Document doc)\n          throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];    // init fieldLengths\n    fieldPositions = new int[fieldInfos.size()];  // init fieldPositions\n    fieldOffsets = new int[fieldInfos.size()];    // init fieldOffsets\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(segment);\n\n  }\n\n","sourceOld":"  final void addDocument(String segment, Document doc)\n          throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];    // init fieldLengths\n    fieldPositions = new int[fieldInfos.size()];  // init fieldPositions\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(doc, segment);\n\n  }\n\n","bugFix":null,"bugIntro":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1b54a9bc667895a2095a886184bf69a3179e63df","date":1172088096,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","sourceNew":"  final void addDocument(String segment, Document doc)\n          throws CorruptIndexException, IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];    // init fieldLengths\n    fieldPositions = new int[fieldInfos.size()];  // init fieldPositions\n    fieldOffsets = new int[fieldInfos.size()];    // init fieldOffsets\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(segment);\n\n  }\n\n","sourceOld":"  final void addDocument(String segment, Document doc)\n          throws IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];    // init fieldLengths\n    fieldPositions = new int[fieldInfos.size()];  // init fieldPositions\n    fieldOffsets = new int[fieldInfos.size()];    // init fieldOffsets\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(segment);\n\n  }\n\n","bugFix":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8522ae207a56c6db28ca06fe6cc33e70911c3600","date":1173935743,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","sourceNew":"  final void addDocument(String segment, Document doc)\n          throws CorruptIndexException, IOException {\n    // create field infos\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    \n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];    // init fieldLengths\n    fieldPositions = new int[fieldInfos.size()];  // init fieldPositions\n    fieldOffsets = new int[fieldInfos.size()];    // init fieldOffsets\n    fieldStoresPayloads = new BitSet(fieldInfos.size());\n    \n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    // Before we write the FieldInfos we invert the Document. The reason is that\n    // during invertion the TokenStreams of tokenized fields are being processed \n    // and we might encounter tokens that have payloads associated with them. In \n    // this case we have to update the FieldInfo of the particular field.\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n    \n    // write field infos \n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n    \n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(segment);\n\n  }\n\n","sourceOld":"  final void addDocument(String segment, Document doc)\n          throws CorruptIndexException, IOException {\n    // write field names\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n\n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];    // init fieldLengths\n    fieldPositions = new int[fieldInfos.size()];  // init fieldPositions\n    fieldOffsets = new int[fieldInfos.size()];    // init fieldOffsets\n\n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n\n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(segment);\n\n  }\n\n","bugFix":null,"bugIntro":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b443dda8f84ed86d67aeecb48ce98151c485dbe6","date":1179405523,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","sourceNew":"  final void addDocument(String segment, Document doc)\n          throws CorruptIndexException, IOException {\n    // create field infos\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    \n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];    // init fieldLengths\n    fieldPositions = new int[fieldInfos.size()];  // init fieldPositions\n    fieldOffsets = new int[fieldInfos.size()];    // init fieldOffsets\n    fieldStoresPayloads = new BitSet(fieldInfos.size());\n    \n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    try {\n    \n      // Before we write the FieldInfos we invert the Document. The reason is that\n      // during invertion the TokenStreams of tokenized fields are being processed \n      // and we might encounter tokens that have payloads associated with them. In \n      // this case we have to update the FieldInfo of the particular field.\n      invertDocument(doc);\n    \n      // sort postingTable into an array\n      Posting[] postings = sortPostingTable();\n    \n      // write field infos \n      fieldInfos.write(directory, segment + \".fnm\");\n\n      // write field values\n      FieldsWriter fieldsWriter =\n        new FieldsWriter(directory, segment, fieldInfos);\n      try {\n        fieldsWriter.addDocument(doc);\n      } finally {\n        fieldsWriter.close();\n      }\n    \n      /*\n      for (int i = 0; i < postings.length; i++) {\n        Posting posting = postings[i];\n        System.out.print(posting.term);\n        System.out.print(\" freq=\" + posting.freq);\n        System.out.print(\" pos=\");\n        System.out.print(posting.positions[0]);\n        for (int j = 1; j < posting.freq; j++)\n\t  System.out.print(\",\" + posting.positions[j]);\n        System.out.println(\"\");\n      }\n       */\n\n      // write postings\n      writePostings(postings, segment);\n\n      // write norms of indexed fields\n      writeNorms(segment);\n    } finally {\n      // close TokenStreams\n      IOException ex = null;\n      \n      Iterator it = openTokenStreams.iterator();\n      while (it.hasNext()) {\n        try {\n          ((TokenStream) it.next()).close();\n        } catch (IOException e) {\n          if (ex != null) {\n            ex = e;\n          }\n        }\n      }\n      openTokenStreams.clear();\n      \n      if (ex != null) {\n        throw ex;\n      }\n    }\n  }\n\n","sourceOld":"  final void addDocument(String segment, Document doc)\n          throws CorruptIndexException, IOException {\n    // create field infos\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    \n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];    // init fieldLengths\n    fieldPositions = new int[fieldInfos.size()];  // init fieldPositions\n    fieldOffsets = new int[fieldInfos.size()];    // init fieldOffsets\n    fieldStoresPayloads = new BitSet(fieldInfos.size());\n    \n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    // Before we write the FieldInfos we invert the Document. The reason is that\n    // during invertion the TokenStreams of tokenized fields are being processed \n    // and we might encounter tokens that have payloads associated with them. In \n    // this case we have to update the FieldInfo of the particular field.\n    invertDocument(doc);\n\n    // sort postingTable into an array\n    Posting[] postings = sortPostingTable();\n    \n    // write field infos \n    fieldInfos.write(directory, segment + \".fnm\");\n\n    // write field values\n    FieldsWriter fieldsWriter =\n            new FieldsWriter(directory, segment, fieldInfos);\n    try {\n      fieldsWriter.addDocument(doc);\n    } finally {\n      fieldsWriter.close();\n    }\n    \n    /*\n    for (int i = 0; i < postings.length; i++) {\n      Posting posting = postings[i];\n      System.out.print(posting.term);\n      System.out.print(\" freq=\" + posting.freq);\n      System.out.print(\" pos=\");\n      System.out.print(posting.positions[0]);\n      for (int j = 1; j < posting.freq; j++)\n\tSystem.out.print(\",\" + posting.positions[j]);\n      System.out.println(\"\");\n    }\n    */\n\n    // write postings\n    writePostings(postings, segment);\n\n    // write norms of indexed fields\n    writeNorms(segment);\n\n  }\n\n","bugFix":["03ae70812bc33b0270c1366378b2c2da95fe86a6","91109046a59c58ee0ee5d0d2767b08d1f30d6702","770281b8a8459cafcdd2354b6a06078fea2d83c9","8522ae207a56c6db28ca06fe6cc33e70911c3600","6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b20bed3506d9b128ea30a7a62e2a8b1d7df697b0","date":1185569419,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentWriter#addDocument(String,Document).mjava","sourceNew":null,"sourceOld":"  final void addDocument(String segment, Document doc)\n          throws CorruptIndexException, IOException {\n    // create field infos\n    fieldInfos = new FieldInfos();\n    fieldInfos.add(doc);\n    \n    // invert doc into postingTable\n    postingTable.clear();\t\t\t  // clear postingTable\n    fieldLengths = new int[fieldInfos.size()];    // init fieldLengths\n    fieldPositions = new int[fieldInfos.size()];  // init fieldPositions\n    fieldOffsets = new int[fieldInfos.size()];    // init fieldOffsets\n    fieldStoresPayloads = new BitSet(fieldInfos.size());\n    \n    fieldBoosts = new float[fieldInfos.size()];\t  // init fieldBoosts\n    Arrays.fill(fieldBoosts, doc.getBoost());\n\n    try {\n    \n      // Before we write the FieldInfos we invert the Document. The reason is that\n      // during invertion the TokenStreams of tokenized fields are being processed \n      // and we might encounter tokens that have payloads associated with them. In \n      // this case we have to update the FieldInfo of the particular field.\n      invertDocument(doc);\n    \n      // sort postingTable into an array\n      Posting[] postings = sortPostingTable();\n    \n      // write field infos \n      fieldInfos.write(directory, segment + \".fnm\");\n\n      // write field values\n      FieldsWriter fieldsWriter =\n        new FieldsWriter(directory, segment, fieldInfos);\n      try {\n        fieldsWriter.addDocument(doc);\n      } finally {\n        fieldsWriter.close();\n      }\n    \n      /*\n      for (int i = 0; i < postings.length; i++) {\n        Posting posting = postings[i];\n        System.out.print(posting.term);\n        System.out.print(\" freq=\" + posting.freq);\n        System.out.print(\" pos=\");\n        System.out.print(posting.positions[0]);\n        for (int j = 1; j < posting.freq; j++)\n\t  System.out.print(\",\" + posting.positions[j]);\n        System.out.println(\"\");\n      }\n       */\n\n      // write postings\n      writePostings(postings, segment);\n\n      // write norms of indexed fields\n      writeNorms(segment);\n    } finally {\n      // close TokenStreams\n      IOException ex = null;\n      \n      Iterator it = openTokenStreams.iterator();\n      while (it.hasNext()) {\n        try {\n          ((TokenStream) it.next()).close();\n        } catch (IOException e) {\n          if (ex != null) {\n            ex = e;\n          }\n        }\n      }\n      openTokenStreams.clear();\n      \n      if (ex != null) {\n        throw ex;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8522ae207a56c6db28ca06fe6cc33e70911c3600":["1b54a9bc667895a2095a886184bf69a3179e63df"],"b20bed3506d9b128ea30a7a62e2a8b1d7df697b0":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"1b54a9bc667895a2095a886184bf69a3179e63df":["6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["2ef9d8f446efc33e4fc919333c39cd20410ffa11"],"14aec0a40da5a9c26f8752701a5aa10f78f5017d":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"2ef9d8f446efc33e4fc919333c39cd20410ffa11":["66f8ac981e8707cfae011613a8168a2edeb0b6e3"],"b443dda8f84ed86d67aeecb48ce98151c485dbe6":["8522ae207a56c6db28ca06fe6cc33e70911c3600"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"03ae70812bc33b0270c1366378b2c2da95fe86a6":["14aec0a40da5a9c26f8752701a5aa10f78f5017d"],"66f8ac981e8707cfae011613a8168a2edeb0b6e3":["03ae70812bc33b0270c1366378b2c2da95fe86a6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b20bed3506d9b128ea30a7a62e2a8b1d7df697b0"]},"commit2Childs":{"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["14aec0a40da5a9c26f8752701a5aa10f78f5017d"],"8522ae207a56c6db28ca06fe6cc33e70911c3600":["b443dda8f84ed86d67aeecb48ce98151c485dbe6"],"1b54a9bc667895a2095a886184bf69a3179e63df":["8522ae207a56c6db28ca06fe6cc33e70911c3600"],"b20bed3506d9b128ea30a7a62e2a8b1d7df697b0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6177f0f28ace66d1538b1e6ac5f1773e5449a0b0":["1b54a9bc667895a2095a886184bf69a3179e63df"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["6177f0f28ace66d1538b1e6ac5f1773e5449a0b0"],"14aec0a40da5a9c26f8752701a5aa10f78f5017d":["03ae70812bc33b0270c1366378b2c2da95fe86a6"],"2ef9d8f446efc33e4fc919333c39cd20410ffa11":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"b443dda8f84ed86d67aeecb48ce98151c485dbe6":["b20bed3506d9b128ea30a7a62e2a8b1d7df697b0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"03ae70812bc33b0270c1366378b2c2da95fe86a6":["66f8ac981e8707cfae011613a8168a2edeb0b6e3"],"66f8ac981e8707cfae011613a8168a2edeb0b6e3":["2ef9d8f446efc33e4fc919333c39cd20410ffa11"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}