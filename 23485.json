{"path":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsReader#visitDocument(int,StoredFieldVisitor).mjava","commits":[{"id":"3d5291145ae0cea7e6e6a2379f3a32643bf71bf6","date":1411857884,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsReader#visitDocument(int,StoredFieldVisitor).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void visitDocument(int docID, StoredFieldVisitor visitor)\n      throws IOException {\n    fieldsStream.seek(indexReader.getStartPointer(docID));\n\n    final int docBase = fieldsStream.readVInt();\n    final int chunkDocs = fieldsStream.readVInt();\n    if (docID < docBase\n        || docID >= docBase + chunkDocs\n        || docBase + chunkDocs > numDocs) {\n      throw new CorruptIndexException(\"Corrupted: docID=\" + docID\n          + \", docBase=\" + docBase + \", chunkDocs=\" + chunkDocs\n          + \", numDocs=\" + numDocs, fieldsStream);\n    }\n\n    final int numStoredFields, offset, length, totalLength;\n    if (chunkDocs == 1) {\n      numStoredFields = fieldsStream.readVInt();\n      offset = 0;\n      length = fieldsStream.readVInt();\n      totalLength = length;\n    } else {\n      final int bitsPerStoredFields = fieldsStream.readVInt();\n      if (bitsPerStoredFields == 0) {\n        numStoredFields = fieldsStream.readVInt();\n      } else if (bitsPerStoredFields > 31) {\n        throw new CorruptIndexException(\"bitsPerStoredFields=\" + bitsPerStoredFields, fieldsStream);\n      } else {\n        final long filePointer = fieldsStream.getFilePointer();\n        final PackedInts.Reader reader = PackedInts.getDirectReaderNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields);\n        numStoredFields = (int) (reader.get(docID - docBase));\n        fieldsStream.seek(filePointer + PackedInts.Format.PACKED.byteCount(packedIntsVersion, chunkDocs, bitsPerStoredFields));\n      }\n\n      final int bitsPerLength = fieldsStream.readVInt();\n      if (bitsPerLength == 0) {\n        length = fieldsStream.readVInt();\n        offset = (docID - docBase) * length;\n        totalLength = chunkDocs * length;\n      } else if (bitsPerStoredFields > 31) {\n        throw new CorruptIndexException(\"bitsPerLength=\" + bitsPerLength, fieldsStream);\n      } else {\n        final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);\n        int off = 0;\n        for (int i = 0; i < docID - docBase; ++i) {\n          off += it.next();\n        }\n        offset = off;\n        length = (int) it.next();\n        off += length;\n        for (int i = docID - docBase + 1; i < chunkDocs; ++i) {\n          off += it.next();\n        }\n        totalLength = off;\n      }\n    }\n\n    if ((length == 0) != (numStoredFields == 0)) {\n      throw new CorruptIndexException(\"length=\" + length + \", numStoredFields=\" + numStoredFields, fieldsStream);\n    }\n    if (numStoredFields == 0) {\n      // nothing to do\n      return;\n    }\n\n    final DataInput documentInput;\n    if (version >= VERSION_BIG_CHUNKS && totalLength >= 2 * chunkSize) {\n      assert chunkSize > 0;\n      assert offset < chunkSize;\n\n      decompressor.decompress(fieldsStream, chunkSize, offset, Math.min(length, chunkSize - offset), bytes);\n      documentInput = new DataInput() {\n\n        int decompressed = bytes.length;\n\n        void fillBuffer() throws IOException {\n          assert decompressed <= length;\n          if (decompressed == length) {\n            throw new EOFException();\n          }\n          final int toDecompress = Math.min(length - decompressed, chunkSize);\n          decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, bytes);\n          decompressed += toDecompress;\n        }\n\n        @Override\n        public byte readByte() throws IOException {\n          if (bytes.length == 0) {\n            fillBuffer();\n          }\n          --bytes.length;\n          return bytes.bytes[bytes.offset++];\n        }\n\n        @Override\n        public void readBytes(byte[] b, int offset, int len) throws IOException {\n          while (len > bytes.length) {\n            System.arraycopy(bytes.bytes, bytes.offset, b, offset, bytes.length);\n            len -= bytes.length;\n            offset += bytes.length;\n            fillBuffer();\n          }\n          System.arraycopy(bytes.bytes, bytes.offset, b, offset, len);\n          bytes.offset += len;\n          bytes.length -= len;\n        }\n\n      };\n    } else {\n      final BytesRef bytes = totalLength <= BUFFER_REUSE_THRESHOLD ? this.bytes : new BytesRef();\n      decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);\n      assert bytes.length == length;\n      documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);\n    }\n\n    for (int fieldIDX = 0; fieldIDX < numStoredFields; fieldIDX++) {\n      final long infoAndBits = documentInput.readVLong();\n      final int fieldNumber = (int) (infoAndBits >>> TYPE_BITS);\n      final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n      final int bits = (int) (infoAndBits & TYPE_MASK);\n      assert bits <= NUMERIC_DOUBLE: \"bits=\" + Integer.toHexString(bits);\n\n      switch(visitor.needsField(fieldInfo)) {\n        case YES:\n          readField(documentInput, visitor, fieldInfo, bits);\n          break;\n        case NO:\n          skipField(documentInput, bits);\n          break;\n        case STOP:\n          return;\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsReader#visitDocument(int,StoredFieldVisitor).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void visitDocument(int docID, StoredFieldVisitor visitor)\n      throws IOException {\n    fieldsStream.seek(indexReader.getStartPointer(docID));\n\n    final int docBase = fieldsStream.readVInt();\n    final int chunkDocs = fieldsStream.readVInt();\n    if (docID < docBase\n        || docID >= docBase + chunkDocs\n        || docBase + chunkDocs > numDocs) {\n      throw new CorruptIndexException(\"Corrupted: docID=\" + docID\n          + \", docBase=\" + docBase + \", chunkDocs=\" + chunkDocs\n          + \", numDocs=\" + numDocs, fieldsStream);\n    }\n\n    final int numStoredFields, offset, length, totalLength;\n    if (chunkDocs == 1) {\n      numStoredFields = fieldsStream.readVInt();\n      offset = 0;\n      length = fieldsStream.readVInt();\n      totalLength = length;\n    } else {\n      final int bitsPerStoredFields = fieldsStream.readVInt();\n      if (bitsPerStoredFields == 0) {\n        numStoredFields = fieldsStream.readVInt();\n      } else if (bitsPerStoredFields > 31) {\n        throw new CorruptIndexException(\"bitsPerStoredFields=\" + bitsPerStoredFields, fieldsStream);\n      } else {\n        final long filePointer = fieldsStream.getFilePointer();\n        final PackedInts.Reader reader = PackedInts.getDirectReaderNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields);\n        numStoredFields = (int) (reader.get(docID - docBase));\n        fieldsStream.seek(filePointer + PackedInts.Format.PACKED.byteCount(packedIntsVersion, chunkDocs, bitsPerStoredFields));\n      }\n\n      final int bitsPerLength = fieldsStream.readVInt();\n      if (bitsPerLength == 0) {\n        length = fieldsStream.readVInt();\n        offset = (docID - docBase) * length;\n        totalLength = chunkDocs * length;\n      } else if (bitsPerStoredFields > 31) {\n        throw new CorruptIndexException(\"bitsPerLength=\" + bitsPerLength, fieldsStream);\n      } else {\n        final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);\n        int off = 0;\n        for (int i = 0; i < docID - docBase; ++i) {\n          off += it.next();\n        }\n        offset = off;\n        length = (int) it.next();\n        off += length;\n        for (int i = docID - docBase + 1; i < chunkDocs; ++i) {\n          off += it.next();\n        }\n        totalLength = off;\n      }\n    }\n\n    if ((length == 0) != (numStoredFields == 0)) {\n      throw new CorruptIndexException(\"length=\" + length + \", numStoredFields=\" + numStoredFields, fieldsStream);\n    }\n    if (numStoredFields == 0) {\n      // nothing to do\n      return;\n    }\n\n    final DataInput documentInput;\n    if (version >= VERSION_BIG_CHUNKS && totalLength >= 2 * chunkSize) {\n      assert chunkSize > 0;\n      assert offset < chunkSize;\n\n      decompressor.decompress(fieldsStream, chunkSize, offset, Math.min(length, chunkSize - offset), bytes);\n      documentInput = new DataInput() {\n\n        int decompressed = bytes.length;\n\n        void fillBuffer() throws IOException {\n          assert decompressed <= length;\n          if (decompressed == length) {\n            throw new EOFException();\n          }\n          final int toDecompress = Math.min(length - decompressed, chunkSize);\n          decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, bytes);\n          decompressed += toDecompress;\n        }\n\n        @Override\n        public byte readByte() throws IOException {\n          if (bytes.length == 0) {\n            fillBuffer();\n          }\n          --bytes.length;\n          return bytes.bytes[bytes.offset++];\n        }\n\n        @Override\n        public void readBytes(byte[] b, int offset, int len) throws IOException {\n          while (len > bytes.length) {\n            System.arraycopy(bytes.bytes, bytes.offset, b, offset, bytes.length);\n            len -= bytes.length;\n            offset += bytes.length;\n            fillBuffer();\n          }\n          System.arraycopy(bytes.bytes, bytes.offset, b, offset, len);\n          bytes.offset += len;\n          bytes.length -= len;\n        }\n\n      };\n    } else {\n      final BytesRef bytes = totalLength <= BUFFER_REUSE_THRESHOLD ? this.bytes : new BytesRef();\n      decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);\n      assert bytes.length == length;\n      documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);\n    }\n\n    for (int fieldIDX = 0; fieldIDX < numStoredFields; fieldIDX++) {\n      final long infoAndBits = documentInput.readVLong();\n      final int fieldNumber = (int) (infoAndBits >>> TYPE_BITS);\n      final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n      final int bits = (int) (infoAndBits & TYPE_MASK);\n      assert bits <= NUMERIC_DOUBLE: \"bits=\" + Integer.toHexString(bits);\n\n      switch(visitor.needsField(fieldInfo)) {\n        case YES:\n          readField(documentInput, visitor, fieldInfo, bits);\n          break;\n        case NO:\n          skipField(documentInput, bits);\n          break;\n        case STOP:\n          return;\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"71387d8cb6923eb831b17a8b734608ba2e21c653","date":1414126093,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsReader#visitDocument(int,StoredFieldVisitor).mjava","sourceNew":null,"sourceOld":"  @Override\n  public void visitDocument(int docID, StoredFieldVisitor visitor)\n      throws IOException {\n    fieldsStream.seek(indexReader.getStartPointer(docID));\n\n    final int docBase = fieldsStream.readVInt();\n    final int chunkDocs = fieldsStream.readVInt();\n    if (docID < docBase\n        || docID >= docBase + chunkDocs\n        || docBase + chunkDocs > numDocs) {\n      throw new CorruptIndexException(\"Corrupted: docID=\" + docID\n          + \", docBase=\" + docBase + \", chunkDocs=\" + chunkDocs\n          + \", numDocs=\" + numDocs, fieldsStream);\n    }\n\n    final int numStoredFields, offset, length, totalLength;\n    if (chunkDocs == 1) {\n      numStoredFields = fieldsStream.readVInt();\n      offset = 0;\n      length = fieldsStream.readVInt();\n      totalLength = length;\n    } else {\n      final int bitsPerStoredFields = fieldsStream.readVInt();\n      if (bitsPerStoredFields == 0) {\n        numStoredFields = fieldsStream.readVInt();\n      } else if (bitsPerStoredFields > 31) {\n        throw new CorruptIndexException(\"bitsPerStoredFields=\" + bitsPerStoredFields, fieldsStream);\n      } else {\n        final long filePointer = fieldsStream.getFilePointer();\n        final PackedInts.Reader reader = PackedInts.getDirectReaderNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields);\n        numStoredFields = (int) (reader.get(docID - docBase));\n        fieldsStream.seek(filePointer + PackedInts.Format.PACKED.byteCount(packedIntsVersion, chunkDocs, bitsPerStoredFields));\n      }\n\n      final int bitsPerLength = fieldsStream.readVInt();\n      if (bitsPerLength == 0) {\n        length = fieldsStream.readVInt();\n        offset = (docID - docBase) * length;\n        totalLength = chunkDocs * length;\n      } else if (bitsPerStoredFields > 31) {\n        throw new CorruptIndexException(\"bitsPerLength=\" + bitsPerLength, fieldsStream);\n      } else {\n        final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);\n        int off = 0;\n        for (int i = 0; i < docID - docBase; ++i) {\n          off += it.next();\n        }\n        offset = off;\n        length = (int) it.next();\n        off += length;\n        for (int i = docID - docBase + 1; i < chunkDocs; ++i) {\n          off += it.next();\n        }\n        totalLength = off;\n      }\n    }\n\n    if ((length == 0) != (numStoredFields == 0)) {\n      throw new CorruptIndexException(\"length=\" + length + \", numStoredFields=\" + numStoredFields, fieldsStream);\n    }\n    if (numStoredFields == 0) {\n      // nothing to do\n      return;\n    }\n\n    final DataInput documentInput;\n    if (version >= VERSION_BIG_CHUNKS && totalLength >= 2 * chunkSize) {\n      assert chunkSize > 0;\n      assert offset < chunkSize;\n\n      decompressor.decompress(fieldsStream, chunkSize, offset, Math.min(length, chunkSize - offset), bytes);\n      documentInput = new DataInput() {\n\n        int decompressed = bytes.length;\n\n        void fillBuffer() throws IOException {\n          assert decompressed <= length;\n          if (decompressed == length) {\n            throw new EOFException();\n          }\n          final int toDecompress = Math.min(length - decompressed, chunkSize);\n          decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, bytes);\n          decompressed += toDecompress;\n        }\n\n        @Override\n        public byte readByte() throws IOException {\n          if (bytes.length == 0) {\n            fillBuffer();\n          }\n          --bytes.length;\n          return bytes.bytes[bytes.offset++];\n        }\n\n        @Override\n        public void readBytes(byte[] b, int offset, int len) throws IOException {\n          while (len > bytes.length) {\n            System.arraycopy(bytes.bytes, bytes.offset, b, offset, bytes.length);\n            len -= bytes.length;\n            offset += bytes.length;\n            fillBuffer();\n          }\n          System.arraycopy(bytes.bytes, bytes.offset, b, offset, len);\n          bytes.offset += len;\n          bytes.length -= len;\n        }\n\n      };\n    } else {\n      final BytesRef bytes = totalLength <= BUFFER_REUSE_THRESHOLD ? this.bytes : new BytesRef();\n      decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);\n      assert bytes.length == length;\n      documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);\n    }\n\n    for (int fieldIDX = 0; fieldIDX < numStoredFields; fieldIDX++) {\n      final long infoAndBits = documentInput.readVLong();\n      final int fieldNumber = (int) (infoAndBits >>> TYPE_BITS);\n      final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);\n\n      final int bits = (int) (infoAndBits & TYPE_MASK);\n      assert bits <= NUMERIC_DOUBLE: \"bits=\" + Integer.toHexString(bits);\n\n      switch(visitor.needsField(fieldInfo)) {\n        case YES:\n          readField(documentInput, visitor, fieldInfo, bits);\n          break;\n        case NO:\n          skipField(documentInput, bits);\n          break;\n        case STOP:\n          return;\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9bb9a29a5e71a90295f175df8919802993142c9a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3d5291145ae0cea7e6e6a2379f3a32643bf71bf6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3d5291145ae0cea7e6e6a2379f3a32643bf71bf6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["9bb9a29a5e71a90295f175df8919802993142c9a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71387d8cb6923eb831b17a8b734608ba2e21c653"]},"commit2Childs":{"9bb9a29a5e71a90295f175df8919802993142c9a":["71387d8cb6923eb831b17a8b734608ba2e21c653"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9bb9a29a5e71a90295f175df8919802993142c9a","3d5291145ae0cea7e6e6a2379f3a32643bf71bf6"],"3d5291145ae0cea7e6e6a2379f3a32643bf71bf6":["9bb9a29a5e71a90295f175df8919802993142c9a"],"71387d8cb6923eb831b17a8b734608ba2e21c653":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}