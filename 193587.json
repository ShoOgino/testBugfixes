{"path":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54184c8fa13303a999acbb2f54de8b5d8d1b1bd7","date":1329818475,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f08557cdb6c60ac7b88a9342c983a20cd236e74f","date":1330954480,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bdb5e42b0cecd8dfb27767a02ada71899bf17917","date":1334100099,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5a238fc456663f685a9db1ed8d680e348bb45171","date":1334173266,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":["02331260bb246364779cb6f04919ca47900d01bb"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"02331260bb246364779cb6f04919ca47900d01bb","date":1343749884,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb07ab105350b80ed9d63ca64b117084ed7391bc","date":1344824719,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    Fields fields = MultiFields.getFields(knownSearcher.reader);\n    \n    DocsEnum docs = null;\n    for (String fieldName : fields) {\n      Terms terms = fields.terms(fieldName);\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    Fields fields = MultiFields.getFields(knownSearcher.reader);\n    \n    DocsEnum docs = null;\n    for (String fieldName : fields) {\n      Terms terms = fields.terms(fieldName);\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","date":1344867506,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    Fields fields = MultiFields.getFields(knownSearcher.reader);\n    \n    DocsEnum docs = null;\n    for (String fieldName : fields) {\n      Terms terms = fields.terms(fieldName);\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f21ce13f410ee015e1ba14687ab4b8518ac52a11","date":1359713213,"type":4,"author":"Adrien Grand","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":null,"sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    Fields fields = MultiFields.getFields(knownSearcher.reader);\n    \n    DocsEnum docs = null;\n    for (String fieldName : fields) {\n      Terms terms = fields.terms(fieldName);\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0fa6955ed1b1007ded1349ab72cea4555640432f","date":1359721908,"type":4,"author":"Robert Muir","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":null,"sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    Fields fields = MultiFields.getFields(knownSearcher.reader);\n    \n    DocsEnum docs = null;\n    for (String fieldName : fields) {\n      Terms terms = fields.terms(fieldName);\n      assertNotNull(terms); // NOTE: kinda sketchy assumptions, but ideally we would fix fieldsenum api... \n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random(), termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, DocsEnum.FLAG_FREQS);\n        \n        while (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.size());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bdb5e42b0cecd8dfb27767a02ada71899bf17917":["f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"5a238fc456663f685a9db1ed8d680e348bb45171":["f08557cdb6c60ac7b88a9342c983a20cd236e74f","bdb5e42b0cecd8dfb27767a02ada71899bf17917"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["54184c8fa13303a999acbb2f54de8b5d8d1b1bd7"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fb07ab105350b80ed9d63ca64b117084ed7391bc":["02331260bb246364779cb6f04919ca47900d01bb"],"0fa6955ed1b1007ded1349ab72cea4555640432f":["fb07ab105350b80ed9d63ca64b117084ed7391bc","f21ce13f410ee015e1ba14687ab4b8518ac52a11"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","fb07ab105350b80ed9d63ca64b117084ed7391bc"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["629c38c4ae4e303d0617e05fbfe508140b32f0a3","02331260bb246364779cb6f04919ca47900d01bb"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["629c38c4ae4e303d0617e05fbfe508140b32f0a3","02331260bb246364779cb6f04919ca47900d01bb"],"f21ce13f410ee015e1ba14687ab4b8518ac52a11":["fb07ab105350b80ed9d63ca64b117084ed7391bc"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["bdb5e42b0cecd8dfb27767a02ada71899bf17917"],"54184c8fa13303a999acbb2f54de8b5d8d1b1bd7":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["d6f074e73200c07d54f242d3880a8da5a35ff97b","fb07ab105350b80ed9d63ca64b117084ed7391bc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f21ce13f410ee015e1ba14687ab4b8518ac52a11"],"02331260bb246364779cb6f04919ca47900d01bb":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"]},"commit2Childs":{"bdb5e42b0cecd8dfb27767a02ada71899bf17917":["5a238fc456663f685a9db1ed8d680e348bb45171","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"5a238fc456663f685a9db1ed8d680e348bb45171":[],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["bdb5e42b0cecd8dfb27767a02ada71899bf17917","5a238fc456663f685a9db1ed8d680e348bb45171","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","54184c8fa13303a999acbb2f54de8b5d8d1b1bd7"],"fb07ab105350b80ed9d63ca64b117084ed7391bc":["0fa6955ed1b1007ded1349ab72cea4555640432f","c7869f64c874ebf7f317d22c00baf2b6857797a6","f21ce13f410ee015e1ba14687ab4b8518ac52a11","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"0fa6955ed1b1007ded1349ab72cea4555640432f":[],"c7869f64c874ebf7f317d22c00baf2b6857797a6":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"f21ce13f410ee015e1ba14687ab4b8518ac52a11":["0fa6955ed1b1007ded1349ab72cea4555640432f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","02331260bb246364779cb6f04919ca47900d01bb"],"54184c8fa13303a999acbb2f54de8b5d8d1b1bd7":["f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":[],"02331260bb246364779cb6f04919ca47900d01bb":["fb07ab105350b80ed9d63ca64b117084ed7391bc","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5a238fc456663f685a9db1ed8d680e348bb45171","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","0fa6955ed1b1007ded1349ab72cea4555640432f","c7869f64c874ebf7f317d22c00baf2b6857797a6","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}