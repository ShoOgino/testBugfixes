{"path":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(LeafReader...).mjava","commits":[{"id":"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa","date":1420599177,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(LeafReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(LeafReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(LeafReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (LeafReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(mergeDirectory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, mergeDirectory, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(IndexReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      final List<LeafReader> mergeReaders = new ArrayList<>();\n      for (IndexReader indexReader : readers) {\n        numDocs += indexReader.numDocs();\n        for (LeafReaderContext ctx : indexReader.leaves()) {\n          mergeReaders.add(ctx.reader());\n        }\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(mergeDirectory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, mergeDirectory, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"505bff044e47a553f461b6f4484d1d08faf4ac85","date":1420728783,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(LeafReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(mergeDirectory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, mergeDirectory, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(LeafReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(LeafReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (LeafReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(mergeDirectory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, mergeDirectory, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"505bff044e47a553f461b6f4484d1d08faf4ac85":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["505bff044e47a553f461b6f4484d1d08faf4ac85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa"],"0fa3fa46c67543546ab45142cc8ee7cf34fc9aaa":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"505bff044e47a553f461b6f4484d1d08faf4ac85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}