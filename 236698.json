{"path":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","commits":[{"id":"99eb4a732d1a908f4636ace52928876136bf1896","date":1413829552,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(Directory,FieldInfos,SegmentInfo,PostingsReaderBase,IOContext,String).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = state.segmentInfo.name;\n    String termsFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFileName, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      String indexFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFileName, state.context);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, in);\n        }\n        final int numBytes = in.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, in);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, in);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, in);\n        }\n        BytesRef minTerm, maxTerm;\n        if (version >= BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,\n                              PostingsReaderBase postingsReader, IOContext ioContext,\n                              String segmentSuffix)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = info.name;\n    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),\n                       ioContext);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),\n                                ioContext);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, in);\n        }\n        final int numBytes = in.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, in);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, in);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, in);\n        }\n        BytesRef minTerm, maxTerm;\n        if (version >= BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + info.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0628077afea69a2955260949478afabab8e500d8","date":1413915332,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = state.segmentInfo.name;\n    String termsFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFileName, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      String indexFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFileName, state.context);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, in);\n        }\n        final int numBytes = in.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, in);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, in);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, in);\n        }\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = state.segmentInfo.name;\n    String termsFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFileName, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      String indexFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFileName, state.context);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.checksumEntireFile(indexIn);\n      }\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      if (version >= BlockTreeTermsWriter.VERSION_CHECKSUM) {\n        CodecUtil.retrieveChecksum(in);\n      }\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, in);\n        }\n        final int numBytes = in.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, in);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, in);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = version >= BlockTreeTermsWriter.VERSION_META_ARRAY ? in.readVInt() : 0;\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, in);\n        }\n        BytesRef minTerm, maxTerm;\n        if (version >= BlockTreeTermsWriter.VERSION_MIN_MAX_TERMS) {\n          minTerm = readBytesRef(in);\n          maxTerm = readBytesRef(in);\n        } else {\n          minTerm = maxTerm = null;\n        }\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c6d238816bcdf9bbe4ec886226d89bd93834eb7e","date":1413925889,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkSegmentHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkSegmentHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state)\n    throws IOException {\n    \n    this.postingsReader = postingsReader;\n\n    this.segment = state.segmentInfo.name;\n    String termsFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION);\n    in = state.directory.openInput(termsFileName, state.context);\n\n    boolean success = false;\n    IndexInput indexIn = null;\n\n    try {\n      version = readHeader(in);\n      String indexFileName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexFileName, state.context);\n      int indexVersion = readIndexHeader(indexIn);\n      if (indexVersion != version) {\n        throw new CorruptIndexException(\"mixmatched version files: \" + in + \"=\" + version + \",\" + indexIn + \"=\" + indexVersion, indexIn);\n      }\n      \n      // verify\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(in, state);\n      \n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(in);\n\n      // Read per-field details\n      seekDir(in, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = in.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, in);\n      }\n\n      for(int i=0;i<numFields;i++) {\n        final int field = in.readVInt();\n        final long numTerms = in.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, in);\n        }\n        final int numBytes = in.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, in);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        in.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, in);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();\n        final long sumDocFreq = in.readVLong();\n        final int docCount = in.readVInt();\n        final int longsSize = in.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, in);\n        }\n        BytesRef minTerm = readBytesRef(in);\n        BytesRef maxTerm = readBytesRef(in);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), in);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, in);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, in);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, in);\n        }\n      }\n      indexIn.close();\n\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3384e6013a93e4d11b7d75388693f8d0388602bf","date":1413951663,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkSegmentHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkSegmentHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"db68c63cbfaa8698b9c4475f75ed2b9c9696d238","date":1414118621,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"/dev/null","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2bb2842e561df4e8e9ad89010605fc86ac265465","date":1414768208,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0267c69e2456a3477a1ad785723f2135da3117e","date":1425317087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b06445ae1731e049327712db0454e5643ca9b7fe","date":1425329139,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.getDocCount()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.getDocCount(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"59db5e5f780185e0155d296a323e440a6ecfd3b6","date":1435089559,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS) {\n        // Old (pre-5.2.0) index, no auto-prefix terms:\n        this.anyAutoPrefixTerms = false;\n      } else if (version == VERSION_AUTO_PREFIX_TERMS) {\n        // 5.2.x index, might have auto-prefix terms:\n        this.anyAutoPrefixTerms = true;\n      } else {\n        // 5.3.x index, we record up front if we may have written any auto-prefix terms:\n        assert version >= VERSION_AUTO_PREFIX_TERMS_COND;\n        byte b = termsIn.readByte();\n        if (b == 0) {\n          this.anyAutoPrefixTerms = false;\n        } else if (b == 1) {\n          this.anyAutoPrefixTerms = true;\n        } else {\n          throw new CorruptIndexException(\"invalid anyAutoPrefixTerms: expected 0 or 1 but got \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      \n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"25b8a027ba57951e63075a2ae9647c5c4a8c5c5f","date":1466407389,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS || version >= VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // Old (pre-5.2.0) or recent (6.2.0+) index, no auto-prefix terms:\n        this.anyAutoPrefixTerms = false;\n      } else if (version == VERSION_AUTO_PREFIX_TERMS) {\n        // 5.2.x index, might have auto-prefix terms:\n        this.anyAutoPrefixTerms = true;\n      } else {\n        // 5.3.x index, we record up front if we may have written any auto-prefix terms:\n        assert version == VERSION_AUTO_PREFIX_TERMS_COND;\n        byte b = termsIn.readByte();\n        if (b == 0) {\n          this.anyAutoPrefixTerms = false;\n        } else if (b == 1) {\n          this.anyAutoPrefixTerms = true;\n        } else {\n          throw new CorruptIndexException(\"invalid anyAutoPrefixTerms: expected 0 or 1 but got \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS) {\n        // Old (pre-5.2.0) index, no auto-prefix terms:\n        this.anyAutoPrefixTerms = false;\n      } else if (version == VERSION_AUTO_PREFIX_TERMS) {\n        // 5.2.x index, might have auto-prefix terms:\n        this.anyAutoPrefixTerms = true;\n      } else {\n        // 5.3.x index, we record up front if we may have written any auto-prefix terms:\n        assert version >= VERSION_AUTO_PREFIX_TERMS_COND;\n        byte b = termsIn.readByte();\n        if (b == 0) {\n          this.anyAutoPrefixTerms = false;\n        } else if (b == 1) {\n          this.anyAutoPrefixTerms = true;\n        } else {\n          throw new CorruptIndexException(\"invalid anyAutoPrefixTerms: expected 0 or 1 but got \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6aaba221b22442bdf0ef28770c25fe259dfb3f55","date":1466496193,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS || version >= VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // Old (pre-5.2.0) or recent (6.2.0+) index, no auto-prefix terms:\n        this.anyAutoPrefixTerms = false;\n      } else if (version == VERSION_AUTO_PREFIX_TERMS) {\n        // 5.2.x index, might have auto-prefix terms:\n        this.anyAutoPrefixTerms = true;\n      } else {\n        // 5.3.x index, we record up front if we may have written any auto-prefix terms:\n        assert version == VERSION_AUTO_PREFIX_TERMS_COND;\n        byte b = termsIn.readByte();\n        if (b == 0) {\n          this.anyAutoPrefixTerms = false;\n        } else if (b == 1) {\n          this.anyAutoPrefixTerms = true;\n        } else {\n          throw new CorruptIndexException(\"invalid anyAutoPrefixTerms: expected 0 or 1 but got \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS) {\n        // Old (pre-5.2.0) index, no auto-prefix terms:\n        this.anyAutoPrefixTerms = false;\n      } else if (version == VERSION_AUTO_PREFIX_TERMS) {\n        // 5.2.x index, might have auto-prefix terms:\n        this.anyAutoPrefixTerms = true;\n      } else {\n        // 5.3.x index, we record up front if we may have written any auto-prefix terms:\n        assert version >= VERSION_AUTO_PREFIX_TERMS_COND;\n        byte b = termsIn.readByte();\n        if (b == 0) {\n          this.anyAutoPrefixTerms = false;\n        } else if (b == 1) {\n          this.anyAutoPrefixTerms = true;\n        } else {\n          throw new CorruptIndexException(\"invalid anyAutoPrefixTerms: expected 0 or 1 but got \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS || version >= VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // Old (pre-5.2.0) or recent (6.2.0+) index, no auto-prefix terms:\n        this.anyAutoPrefixTerms = false;\n      } else if (version == VERSION_AUTO_PREFIX_TERMS) {\n        // 5.2.x index, might have auto-prefix terms:\n        this.anyAutoPrefixTerms = true;\n      } else {\n        // 5.3.x index, we record up front if we may have written any auto-prefix terms:\n        assert version == VERSION_AUTO_PREFIX_TERMS_COND;\n        byte b = termsIn.readByte();\n        if (b == 0) {\n          this.anyAutoPrefixTerms = false;\n        } else if (b == 1) {\n          this.anyAutoPrefixTerms = true;\n        } else {\n          throw new CorruptIndexException(\"invalid anyAutoPrefixTerms: expected 0 or 1 but got \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS) {\n        // Old (pre-5.2.0) index, no auto-prefix terms:\n        this.anyAutoPrefixTerms = false;\n      } else if (version == VERSION_AUTO_PREFIX_TERMS) {\n        // 5.2.x index, might have auto-prefix terms:\n        this.anyAutoPrefixTerms = true;\n      } else {\n        // 5.3.x index, we record up front if we may have written any auto-prefix terms:\n        assert version >= VERSION_AUTO_PREFIX_TERMS_COND;\n        byte b = termsIn.readByte();\n        if (b == 0) {\n          this.anyAutoPrefixTerms = false;\n        } else if (b == 1) {\n          this.anyAutoPrefixTerms = true;\n        } else {\n          throw new CorruptIndexException(\"invalid anyAutoPrefixTerms: expected 0 or 1 but got \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7d165dc88e14a2b2f1cc4ac8133ffdde44acfd5","date":1488285484,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS || version >= VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // Old (pre-5.2.0) or recent (6.2.0+) index, no auto-prefix terms:\n        this.anyAutoPrefixTerms = false;\n      } else if (version == VERSION_AUTO_PREFIX_TERMS) {\n        // 5.2.x index, might have auto-prefix terms:\n        this.anyAutoPrefixTerms = true;\n      } else {\n        // 5.3.x index, we record up front if we may have written any auto-prefix terms:\n        assert version == VERSION_AUTO_PREFIX_TERMS_COND;\n        byte b = termsIn.readByte();\n        if (b == 0) {\n          this.anyAutoPrefixTerms = false;\n        } else if (b == 1) {\n          this.anyAutoPrefixTerms = true;\n        } else {\n          throw new CorruptIndexException(\"invalid anyAutoPrefixTerms: expected 0 or 1 but got \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a8a04f59ee63f5c8b6c0486e823ed24346ddaa29","date":1496918402,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn, dirOffset);\n      seekDir(indexIn, indexDirOffset);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final int numBytes = termsIn.readVInt();\n        if (numBytes < 0) {\n          throw new CorruptIndexException(\"invalid rootCode for field number: \" + field + \", numBytes=\" + numBytes, termsIn);\n        }\n        final BytesRef rootCode = new BytesRef(new byte[numBytes]);\n        termsIn.readBytes(rootCode.bytes, 0, numBytes);\n        rootCode.length = numBytes;\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"086ffe31d8fba0110227db122974163709ecc1b4","date":1509678141,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d523b8189b211dd1630166aa77b8c88bb48b3fcc","date":1510144168,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? -1 : termsIn.readVLong();\n        final long sumDocFreq = termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dcc12263939c1d6c4b4a2015f67d1b6d97f375a4","date":1550598742,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    IndexInput indexIn = null;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      \n      indexIn.close();\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(indexIn, this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790693f23f4e88a59fbb25e47cc25f6d493b03cb","date":1553077690,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"938935e3efe6aaecb925448d7f992783247366de","date":1554389977,"type":5,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,Lucene50PostingsFormat.FSTLoadMode).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, Lucene50PostingsFormat.FSTLoadMode fstLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter, fstLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        final int longsSize = termsIn.readVInt();\n        if (longsSize < 0) {\n          throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fields.put(fieldInfo.name,       \n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, longsSize, indexIn, minTerm, maxTerm, state.openedFromWriter));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4e83191a3e02851a0b67e5335e6922f3e9ea86d","date":1583489709,"type":1,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState,FSTLoadMode).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        if (version < VERSION_META_LONGS_REMOVED) {\n          final int longsSize = termsIn.readVInt();\n          if (longsSize < 0) {\n            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n          }\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, indexIn, minTerm, maxTerm, state.openedFromWriter));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state, FSTLoadMode defaultLoadMode) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final FSTLoadMode fstLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY, defaultLoadMode);\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        if (version < VERSION_META_LONGS_REMOVED) {\n          final int longsSize = termsIn.readVInt();\n          if (longsSize < 0) {\n            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n          }\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final FSTLoadMode perFieldLoadMode = getLoadMode(state.readerAttributes, FST_MODE_KEY + \".\" + fieldInfo.name, fstLoadMode);\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, indexIn, minTerm, maxTerm, state.openedFromWriter, perFieldLoadMode));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bec68e7c41fed133827595747d853cad504e481e","date":1583501052,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        if (version < VERSION_META_LONGS_REMOVED) {\n          final int longsSize = termsIn.readVInt();\n          if (longsSize < 0) {\n            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n          }\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        if (version < VERSION_META_LONGS_REMOVED) {\n          final int longsSize = termsIn.readVInt();\n          if (longsSize < 0) {\n            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n          }\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, indexIn, minTerm, maxTerm, state.openedFromWriter));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"733b02d82d76739a6c079b56e3d1e11f93ed7b84","date":1583946929,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n\n      // Verifying the checksum against all bytes would be too costly, but for now we at least\n      // verify proper structure of the checksum footer. This is cheap and can detect some forms\n      // of corruption such as file truncation.\n      CodecUtil.retrieveChecksum(indexIn);\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        if (version < VERSION_META_LONGS_REMOVED) {\n          final int longsSize = termsIn.readVInt();\n          if (longsSize < 0) {\n            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n          }\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_AUTO_PREFIX_TERMS_REMOVED) {\n        // pre-6.2 index, records whether auto-prefix terms are enabled in the header\n        byte b = termsIn.readByte();\n        if (b != 0) {\n          throw new CorruptIndexException(\"Index header pretends the index has auto-prefix terms: \" + b, termsIn);\n        }\n      }\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n      CodecUtil.checksumEntireFile(indexIn);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n      \n      // NOTE: data file is too costly to verify checksum against all the bytes on open,\n      // but for now we at least verify proper structure of the checksum footer: which looks\n      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption\n      // such as file truncation.\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        if (version < VERSION_META_LONGS_REMOVED) {\n          final int longsSize = termsIn.readVInt();\n          if (longsSize < 0) {\n            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n          }\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"99f06f5dd087b1829e7b4139e4d014c786b92572","date":1592312728,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader#BlockTreeTermsReader(PostingsReaderBase,SegmentReadState).mjava","sourceNew":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n\n    try {\n      String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n\n      if (version < VERSION_META_FILE) {\n        // Have PostingsReader init itself\n        postingsReader.init(termsIn, state);\n\n        // Verifying the checksum against all bytes would be too costly, but for now we at least\n        // verify proper structure of the checksum footer. This is cheap and can detect some forms\n        // of corruption such as file truncation.\n        CodecUtil.retrieveChecksum(indexIn);\n        CodecUtil.retrieveChecksum(termsIn);\n      }\n\n      // Read per-field details\n      String metaName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_META_EXTENSION);\n      Map<String, FieldReader> fieldMap = null;\n      Throwable priorE = null;\n      long indexLength = -1, termsLength = -1;\n      try (ChecksumIndexInput metaIn = version >= VERSION_META_FILE ? state.directory.openChecksumInput(metaName, state.context) : null) {\n        try {\n          final IndexInput indexMetaIn, termsMetaIn;\n          if (version >= VERSION_META_FILE) {\n            CodecUtil.checkIndexHeader(metaIn, TERMS_META_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n            indexMetaIn = termsMetaIn = metaIn;\n            postingsReader.init(metaIn, state);\n          } else {\n            seekDir(termsIn);\n            seekDir(indexIn);\n            indexMetaIn = indexIn;\n            termsMetaIn = termsIn;\n          }\n\n          final int numFields = termsMetaIn.readVInt();\n          if (numFields < 0) {\n            throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsMetaIn);\n          }\n          fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n          for (int i = 0; i < numFields; ++i) {\n            final int field = termsMetaIn.readVInt();\n            final long numTerms = termsMetaIn.readVLong();\n            if (numTerms <= 0) {\n              throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsMetaIn);\n            }\n            final BytesRef rootCode = readBytesRef(termsMetaIn);\n            final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n            if (fieldInfo == null) {\n              throw new CorruptIndexException(\"invalid field number: \" + field, termsMetaIn);\n            }\n            final long sumTotalTermFreq = termsMetaIn.readVLong();\n            // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n            final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsMetaIn.readVLong();\n            final int docCount = termsMetaIn.readVInt();\n            if (version < VERSION_META_LONGS_REMOVED) {\n              final int longsSize = termsMetaIn.readVInt();\n              if (longsSize < 0) {\n                throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsMetaIn);\n              }\n            }\n            BytesRef minTerm = readBytesRef(termsMetaIn);\n            BytesRef maxTerm = readBytesRef(termsMetaIn);\n            if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n              throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsMetaIn);\n            }\n            if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n              throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsMetaIn);\n            }\n            if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n              throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsMetaIn);\n            }\n            final long indexStartFP = indexMetaIn.readVLong();\n            FieldReader previous = fieldMap.put(fieldInfo.name,\n                new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                    indexStartFP, indexMetaIn, indexIn, minTerm, maxTerm));\n            if (previous != null) {\n              throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsMetaIn);\n            }\n          }\n          if (version >= VERSION_META_FILE) {\n            indexLength = metaIn.readLong();\n            termsLength = metaIn.readLong();\n          }\n        } catch (Throwable exception) {\n          priorE = exception;\n        } finally {\n          if (metaIn != null) {\n            CodecUtil.checkFooter(metaIn, priorE);\n          } else if (priorE != null) {\n            IOUtils.rethrowAlways(priorE);\n          }\n        }\n      }\n      if (version >= VERSION_META_FILE) {\n        // At this point the checksum of the meta file has been verified so the lengths are likely correct\n        CodecUtil.retrieveChecksum(indexIn, indexLength);\n        CodecUtil.retrieveChecksum(termsIn, termsLength);\n      } else {\n        assert indexLength == -1 : indexLength;\n        assert termsLength == -1 : termsLength;\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldMap = fieldMap;\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","sourceOld":"  /** Sole constructor. */\n  public BlockTreeTermsReader(PostingsReaderBase postingsReader, SegmentReadState state) throws IOException {\n    boolean success = false;\n    \n    this.postingsReader = postingsReader;\n    this.segment = state.segmentInfo.name;\n    \n    String termsName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_EXTENSION);\n    try {\n      termsIn = state.directory.openInput(termsName, state.context);\n      version = CodecUtil.checkIndexHeader(termsIn, TERMS_CODEC_NAME, VERSION_START, VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n\n      String indexName = IndexFileNames.segmentFileName(segment, state.segmentSuffix, TERMS_INDEX_EXTENSION);\n      indexIn = state.directory.openInput(indexName, state.context);\n      CodecUtil.checkIndexHeader(indexIn, TERMS_INDEX_CODEC_NAME, version, version, state.segmentInfo.getId(), state.segmentSuffix);\n\n      // Have PostingsReader init itself\n      postingsReader.init(termsIn, state);\n\n      // Verifying the checksum against all bytes would be too costly, but for now we at least\n      // verify proper structure of the checksum footer. This is cheap and can detect some forms\n      // of corruption such as file truncation.\n      CodecUtil.retrieveChecksum(indexIn);\n      CodecUtil.retrieveChecksum(termsIn);\n\n      // Read per-field details\n      seekDir(termsIn);\n      seekDir(indexIn);\n\n      final int numFields = termsIn.readVInt();\n      if (numFields < 0) {\n        throw new CorruptIndexException(\"invalid numFields: \" + numFields, termsIn);\n      }\n      fieldMap = new HashMap<>((int) (numFields / 0.75f) + 1);\n      for (int i = 0; i < numFields; ++i) {\n        final int field = termsIn.readVInt();\n        final long numTerms = termsIn.readVLong();\n        if (numTerms <= 0) {\n          throw new CorruptIndexException(\"Illegal numTerms for field number: \" + field, termsIn);\n        }\n        final BytesRef rootCode = readBytesRef(termsIn);\n        final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);\n        if (fieldInfo == null) {\n          throw new CorruptIndexException(\"invalid field number: \" + field, termsIn);\n        }\n        final long sumTotalTermFreq = termsIn.readVLong();\n        // when frequencies are omitted, sumDocFreq=sumTotalTermFreq and only one value is written.\n        final long sumDocFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS ? sumTotalTermFreq : termsIn.readVLong();\n        final int docCount = termsIn.readVInt();\n        if (version < VERSION_META_LONGS_REMOVED) {\n          final int longsSize = termsIn.readVInt();\n          if (longsSize < 0) {\n            throw new CorruptIndexException(\"invalid longsSize for field: \" + fieldInfo.name + \", longsSize=\" + longsSize, termsIn);\n          }\n        }\n        BytesRef minTerm = readBytesRef(termsIn);\n        BytesRef maxTerm = readBytesRef(termsIn);\n        if (docCount < 0 || docCount > state.segmentInfo.maxDoc()) { // #docs with field must be <= #docs\n          throw new CorruptIndexException(\"invalid docCount: \" + docCount + \" maxDoc: \" + state.segmentInfo.maxDoc(), termsIn);\n        }\n        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field\n          throw new CorruptIndexException(\"invalid sumDocFreq: \" + sumDocFreq + \" docCount: \" + docCount, termsIn);\n        }\n        if (sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings\n          throw new CorruptIndexException(\"invalid sumTotalTermFreq: \" + sumTotalTermFreq + \" sumDocFreq: \" + sumDocFreq, termsIn);\n        }\n        final long indexStartFP = indexIn.readVLong();\n        FieldReader previous = fieldMap.put(fieldInfo.name,\n                                          new FieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,\n                                                          indexStartFP, indexIn, minTerm, maxTerm));\n        if (previous != null) {\n          throw new CorruptIndexException(\"duplicate field: \" + fieldInfo.name, termsIn);\n        }\n      }\n      List<String> fieldList = new ArrayList<>(fieldMap.keySet());\n      fieldList.sort(null);\n      this.fieldList = Collections.unmodifiableList(fieldList);\n      success = true;\n    } finally {\n      if (!success) {\n        // this.close() will close in:\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["28288370235ed02234a64753cdbf0c6ec096304a","086ffe31d8fba0110227db122974163709ecc1b4"],"99f06f5dd087b1829e7b4139e4d014c786b92572":["733b02d82d76739a6c079b56e3d1e11f93ed7b84"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["938935e3efe6aaecb925448d7f992783247366de"],"c6d238816bcdf9bbe4ec886226d89bd93834eb7e":["0628077afea69a2955260949478afabab8e500d8"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["2bb2842e561df4e8e9ad89010605fc86ac265465","b0267c69e2456a3477a1ad785723f2135da3117e"],"086ffe31d8fba0110227db122974163709ecc1b4":["28288370235ed02234a64753cdbf0c6ec096304a"],"59db5e5f780185e0155d296a323e440a6ecfd3b6":["b0267c69e2456a3477a1ad785723f2135da3117e"],"b06445ae1731e049327712db0454e5643ca9b7fe":["2bb2842e561df4e8e9ad89010605fc86ac265465","b0267c69e2456a3477a1ad785723f2135da3117e"],"733b02d82d76739a6c079b56e3d1e11f93ed7b84":["bec68e7c41fed133827595747d853cad504e481e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7d165dc88e14a2b2f1cc4ac8133ffdde44acfd5":["6aaba221b22442bdf0ef28770c25fe259dfb3f55"],"a8a04f59ee63f5c8b6c0486e823ed24346ddaa29":["b7d165dc88e14a2b2f1cc4ac8133ffdde44acfd5"],"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["dcc12263939c1d6c4b4a2015f67d1b6d97f375a4"],"99eb4a732d1a908f4636ace52928876136bf1896":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b0267c69e2456a3477a1ad785723f2135da3117e":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"938935e3efe6aaecb925448d7f992783247366de":["790693f23f4e88a59fbb25e47cc25f6d493b03cb"],"bec68e7c41fed133827595747d853cad504e481e":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["c6d238816bcdf9bbe4ec886226d89bd93834eb7e"],"0628077afea69a2955260949478afabab8e500d8":["99eb4a732d1a908f4636ace52928876136bf1896"],"28288370235ed02234a64753cdbf0c6ec096304a":["b7d165dc88e14a2b2f1cc4ac8133ffdde44acfd5","a8a04f59ee63f5c8b6c0486e823ed24346ddaa29"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["59db5e5f780185e0155d296a323e440a6ecfd3b6","6aaba221b22442bdf0ef28770c25fe259dfb3f55"],"dcc12263939c1d6c4b4a2015f67d1b6d97f375a4":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"6aaba221b22442bdf0ef28770c25fe259dfb3f55":["59db5e5f780185e0155d296a323e440a6ecfd3b6","25b8a027ba57951e63075a2ae9647c5c4a8c5c5f"],"25b8a027ba57951e63075a2ae9647c5c4a8c5c5f":["59db5e5f780185e0155d296a323e440a6ecfd3b6"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3384e6013a93e4d11b7d75388693f8d0388602bf"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["b7d165dc88e14a2b2f1cc4ac8133ffdde44acfd5","a8a04f59ee63f5c8b6c0486e823ed24346ddaa29"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["99f06f5dd087b1829e7b4139e4d014c786b92572"]},"commit2Childs":{"d523b8189b211dd1630166aa77b8c88bb48b3fcc":["dcc12263939c1d6c4b4a2015f67d1b6d97f375a4"],"99f06f5dd087b1829e7b4139e4d014c786b92572":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2bb2842e561df4e8e9ad89010605fc86ac265465":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","b0267c69e2456a3477a1ad785723f2135da3117e"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["bec68e7c41fed133827595747d853cad504e481e"],"c6d238816bcdf9bbe4ec886226d89bd93834eb7e":["3384e6013a93e4d11b7d75388693f8d0388602bf"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"086ffe31d8fba0110227db122974163709ecc1b4":["d523b8189b211dd1630166aa77b8c88bb48b3fcc"],"59db5e5f780185e0155d296a323e440a6ecfd3b6":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","6aaba221b22442bdf0ef28770c25fe259dfb3f55","25b8a027ba57951e63075a2ae9647c5c4a8c5c5f"],"b06445ae1731e049327712db0454e5643ca9b7fe":[],"733b02d82d76739a6c079b56e3d1e11f93ed7b84":["99f06f5dd087b1829e7b4139e4d014c786b92572"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["99eb4a732d1a908f4636ace52928876136bf1896","db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"b7d165dc88e14a2b2f1cc4ac8133ffdde44acfd5":["a8a04f59ee63f5c8b6c0486e823ed24346ddaa29","28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"a8a04f59ee63f5c8b6c0486e823ed24346ddaa29":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["938935e3efe6aaecb925448d7f992783247366de"],"99eb4a732d1a908f4636ace52928876136bf1896":["0628077afea69a2955260949478afabab8e500d8"],"b0267c69e2456a3477a1ad785723f2135da3117e":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","59db5e5f780185e0155d296a323e440a6ecfd3b6","b06445ae1731e049327712db0454e5643ca9b7fe"],"938935e3efe6aaecb925448d7f992783247366de":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"bec68e7c41fed133827595747d853cad504e481e":["733b02d82d76739a6c079b56e3d1e11f93ed7b84"],"3384e6013a93e4d11b7d75388693f8d0388602bf":["db68c63cbfaa8698b9c4475f75ed2b9c9696d238"],"0628077afea69a2955260949478afabab8e500d8":["c6d238816bcdf9bbe4ec886226d89bd93834eb7e"],"28288370235ed02234a64753cdbf0c6ec096304a":["d523b8189b211dd1630166aa77b8c88bb48b3fcc","086ffe31d8fba0110227db122974163709ecc1b4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"dcc12263939c1d6c4b4a2015f67d1b6d97f375a4":["790693f23f4e88a59fbb25e47cc25f6d493b03cb"],"6aaba221b22442bdf0ef28770c25fe259dfb3f55":["b7d165dc88e14a2b2f1cc4ac8133ffdde44acfd5","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"25b8a027ba57951e63075a2ae9647c5c4a8c5c5f":["6aaba221b22442bdf0ef28770c25fe259dfb3f55"],"db68c63cbfaa8698b9c4475f75ed2b9c9696d238":["2bb2842e561df4e8e9ad89010605fc86ac265465"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}