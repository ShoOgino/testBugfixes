{"path":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","commits":[{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":1,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertDoubles();\n\n    waitForThingsToLevelOut(30000); // TODO: why whould we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n      pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n\n    sanityCheckAssertDoubles();\n\n    waitForThingsToLevelOut(30000); // TODO: why whould we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n      pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dbdfda95da145c32af2267b537c92481acc7a522","date":1435597588,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why whould we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n      pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertDoubles();\n\n    waitForThingsToLevelOut(30000); // TODO: why whould we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n      pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":["4c18a95e9168a30be855f5d5e7d61a863186fdc5"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"103fcde6b6238d15212a5524ae65e1f43603db44","date":1437759248,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why whould we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why whould we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n      pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_PIVOT, buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac9de183adbc9483681f275ac1e2d92ed19f52e1","date":1452414626,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why whould we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"25107e8bc3cbaed9427c9c7092c430cf4ce48b1b","date":1464030817,"type":3,"author":"Dennis Gove","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_DISTRIB_MCO, \"true\");\n        // trace param for validation\n        baseP.add(TRACE_DISTRIB_MIN, \"true\");\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e616cfbf284ecbd0f40b4b9e4196daa27e92abd5","date":1464066565,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_DISTRIB_MCO, \"true\");\n        // trace param for validation\n        baseP.add(TRACE_DISTRIB_MIN, \"true\");\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_DISTRIB_MCO, \"true\");\n        // trace param for validation\n        baseP.add(TRACE_DISTRIB_MIN, \"true\");\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ef57266eb048f5d440958e9b0612adf7535e5e8b","date":1513013215,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_DISTRIB_MCO, \"true\");\n        // trace param for validation\n        baseP.add(TRACE_DISTRIB_MIN, \"true\");\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"927e6deec13723e5fd6fb8f1e0457e8597dae15e","date":1513167013,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        pivotP.add(FACET_DISTRIB_MCO, \"true\");\n        // trace param for validation\n        baseP.add(TRACE_DISTRIB_MIN, \"true\");\n      }\n\n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"11d6f92dfa9251d9da6d80ec5963a9cbecc90180","date":1530559969,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 28-June-2018\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 28-June-2018\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 28-June-2018\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108","date":1533256859,"type":3,"author":"Erick","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  //commented 2-Aug-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 28-June-2018\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 28-June-2018\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab69ebfb1ee96d15d8a6b2ed13c2ac96c82ee0d2","date":1541135000,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  //commented 2-Aug-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 28-June-2018\n  public void test() throws Exception {\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2 \"+USE_STATS+\"}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3 \"+USE_STATS+\"}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4 \"+USE_STATS+\"}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  //commented 2-Aug-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 28-June-2018\n  public void test() throws Exception {\n\n    sanityCheckAssertNumerics();\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"add1e7dd742ea533ff4318cea83ca0a1f669f662","date":1585262285,"type":3,"author":"Mike Drob","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/TestCloudPivotFacet#test().mjava","sourceNew":"  @Test\n  //commented 2-Aug-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 28-June-2018\n  public void test() throws Exception {\n\n    waitForThingsToLevelOut(30, TimeUnit.SECONDS); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2 \"+USE_STATS+\"}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3 \"+USE_STATS+\"}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4 \"+USE_STATS+\"}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","sourceOld":"  @Test\n  //commented 2-Aug-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 28-June-2018\n  public void test() throws Exception {\n\n    waitForThingsToLevelOut(30000); // TODO: why would we have to wait?\n    // \n    handle.clear();\n    handle.put(\"QTime\", SKIPVAL);\n    handle.put(\"timestamp\", SKIPVAL);\n    \n    final Set<String> fieldNameSet = new HashSet<>();\n    \n    // build up a randomized index\n    final int numDocs = atLeast(500);\n    log.info(\"numDocs: {}\", numDocs);\n\n    for (int i = 1; i <= numDocs; i++) {\n      SolrInputDocument doc = buildRandomDocument(i);\n\n      // not efficient, but it guarantees that even if people change buildRandomDocument\n      // we'll always have the full list of fields w/o needing to keep code in sync\n      fieldNameSet.addAll(doc.getFieldNames());\n\n      cloudClient.add(doc);\n    }\n    cloudClient.commit();\n\n    fieldNameSet.remove(\"id\");\n    assertTrue(\"WTF, bogus field exists?\", fieldNameSet.add(\"bogus_not_in_any_doc_s\"));\n\n    final String[] fieldNames = fieldNameSet.toArray(new String[fieldNameSet.size()]);\n    Arrays.sort(fieldNames); // need determinism when picking random fields\n\n    for (int i = 0; i < 5; i++) {\n\n      String q = \"*:*\";\n      if (random().nextBoolean()) {\n        q = \"id:[* TO \" + TestUtil.nextInt(random(),300,numDocs) + \"]\";\n      }\n      ModifiableSolrParams baseP = params(\"rows\", \"0\", \"q\", q);\n      \n      if (random().nextBoolean()) {\n        baseP.add(\"fq\", \"id:[* TO \" + TestUtil.nextInt(random(),200,numDocs) + \"]\");\n      }\n\n      final boolean stats = random().nextBoolean();\n      if (stats) {\n        baseP.add(StatsParams.STATS, \"true\");\n        \n        // if we are doing stats, then always generated the same # of STATS_FIELD\n        // params, using multiple tags from a fixed set, but with diff fieldName values.\n        // later, each pivot will randomly pick a tag.\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk1 tag=st1,st2 \"+USE_STATS+\"}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk2 tag=st2,st3 \"+USE_STATS+\"}\" +\n                  pickRandomStatsFields(fieldNames));\n        baseP.add(StatsParams.STATS_FIELD, \"{!key=sk3 tag=st3,st4 \"+USE_STATS+\"}\" +\n                  pickRandomStatsFields(fieldNames));\n        // NOTE: there's a chance that some of those stats field names\n        // will be the same, but if so, all the better to test that edge case\n      }\n      \n      ModifiableSolrParams pivotP = params(FACET,\"true\");\n\n      // put our FACET_PIVOT params in a set in case we just happen to pick the same one twice\n      LinkedHashSet<String> pivotParamValues = new LinkedHashSet<String>();\n      pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n                 \n      if (random().nextBoolean()) {\n        pivotParamValues.add(buildPivotParamValue(buildRandomPivot(fieldNames)));\n      }\n      pivotP.set(FACET_PIVOT, pivotParamValues.toArray(new String[pivotParamValues.size()]));\n\n      // keep limit low - lots of unique values, and lots of depth in pivots\n      pivotP.add(FACET_LIMIT, \"\"+TestUtil.nextInt(random(),1,17));\n\n      // sometimes use an offset\n      if (random().nextBoolean()) {\n        pivotP.add(FACET_OFFSET, \"\"+TestUtil.nextInt(random(),0,7));\n      }\n\n      if (random().nextBoolean()) {\n        String min = \"\"+TestUtil.nextInt(random(),0,numDocs+10);\n        pivotP.add(FACET_PIVOT_MINCOUNT, min);\n        // trace param for validation\n        baseP.add(TRACE_MIN, min);\n      }\n      \n      if (random().nextBoolean()) {\n        String missing = \"\"+random().nextBoolean();\n        pivotP.add(FACET_MISSING, missing);\n        // trace param for validation\n        baseP.add(TRACE_MISS, missing);\n      }\n\n      if (random().nextBoolean()) {\n        String sort = random().nextBoolean() ? \"index\" : \"count\";\n        pivotP.add(FACET_SORT, sort);\n        // trace param for validation\n        baseP.add(TRACE_SORT, sort);\n      }\n\n      // overrequest\n      //\n      // NOTE: since this test focuses on accuracy of refinement, and doesn't do \n      // control collection comparisons, there isn't a lot of need for excessive\n      // overrequesting -- we focus here on trying to exercise the various edge cases\n      // involved as different values are used with overrequest\n      if (0 == TestUtil.nextInt(random(),0,4)) {\n        // we want a decent chance of no overrequest at all\n        pivotP.add(FACET_OVERREQUEST_COUNT, \"0\");\n        pivotP.add(FACET_OVERREQUEST_RATIO, \"0\");\n      } else {\n        if (random().nextBoolean()) {\n          pivotP.add(FACET_OVERREQUEST_COUNT, \"\"+TestUtil.nextInt(random(),0,5));\n        }\n        if (random().nextBoolean()) {\n          // sometimes give a ratio less then 1, code should be smart enough to deal\n          float ratio = 0.5F + random().nextFloat();\n          // sometimes go negative\n          if (random().nextBoolean()) {\n            ratio *= -1;\n          }\n          pivotP.add(FACET_OVERREQUEST_RATIO, \"\"+ratio);\n        }\n      }\n      \n      assertPivotCountsAreCorrect(baseP, pivotP);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108":["11d6f92dfa9251d9da6d80ec5963a9cbecc90180"],"abb23fcc2461782ab204e61213240feb77d355aa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ef57266eb048f5d440958e9b0612adf7535e5e8b":["e616cfbf284ecbd0f40b4b9e4196daa27e92abd5"],"103fcde6b6238d15212a5524ae65e1f43603db44":["dbdfda95da145c32af2267b537c92481acc7a522"],"ac9de183adbc9483681f275ac1e2d92ed19f52e1":["103fcde6b6238d15212a5524ae65e1f43603db44"],"927e6deec13723e5fd6fb8f1e0457e8597dae15e":["e616cfbf284ecbd0f40b4b9e4196daa27e92abd5","ef57266eb048f5d440958e9b0612adf7535e5e8b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["ac9de183adbc9483681f275ac1e2d92ed19f52e1","e616cfbf284ecbd0f40b4b9e4196daa27e92abd5"],"11d6f92dfa9251d9da6d80ec5963a9cbecc90180":["927e6deec13723e5fd6fb8f1e0457e8597dae15e"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["927e6deec13723e5fd6fb8f1e0457e8597dae15e","11d6f92dfa9251d9da6d80ec5963a9cbecc90180"],"ab69ebfb1ee96d15d8a6b2ed13c2ac96c82ee0d2":["05a3c9b5f1dfb39879069eb1dac3ca104d3e4108"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"add1e7dd742ea533ff4318cea83ca0a1f669f662":["ab69ebfb1ee96d15d8a6b2ed13c2ac96c82ee0d2"],"e616cfbf284ecbd0f40b4b9e4196daa27e92abd5":["ac9de183adbc9483681f275ac1e2d92ed19f52e1","25107e8bc3cbaed9427c9c7092c430cf4ce48b1b"],"25107e8bc3cbaed9427c9c7092c430cf4ce48b1b":["ac9de183adbc9483681f275ac1e2d92ed19f52e1"],"dbdfda95da145c32af2267b537c92481acc7a522":["abb23fcc2461782ab204e61213240feb77d355aa"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["add1e7dd742ea533ff4318cea83ca0a1f669f662"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["927e6deec13723e5fd6fb8f1e0457e8597dae15e","11d6f92dfa9251d9da6d80ec5963a9cbecc90180"]},"commit2Childs":{"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108":["ab69ebfb1ee96d15d8a6b2ed13c2ac96c82ee0d2"],"abb23fcc2461782ab204e61213240feb77d355aa":["dbdfda95da145c32af2267b537c92481acc7a522"],"ef57266eb048f5d440958e9b0612adf7535e5e8b":["927e6deec13723e5fd6fb8f1e0457e8597dae15e"],"103fcde6b6238d15212a5524ae65e1f43603db44":["ac9de183adbc9483681f275ac1e2d92ed19f52e1"],"ac9de183adbc9483681f275ac1e2d92ed19f52e1":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","e616cfbf284ecbd0f40b4b9e4196daa27e92abd5","25107e8bc3cbaed9427c9c7092c430cf4ce48b1b"],"927e6deec13723e5fd6fb8f1e0457e8597dae15e":["11d6f92dfa9251d9da6d80ec5963a9cbecc90180","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"11d6f92dfa9251d9da6d80ec5963a9cbecc90180":["05a3c9b5f1dfb39879069eb1dac3ca104d3e4108","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"ab69ebfb1ee96d15d8a6b2ed13c2ac96c82ee0d2":["add1e7dd742ea533ff4318cea83ca0a1f669f662"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["abb23fcc2461782ab204e61213240feb77d355aa"],"add1e7dd742ea533ff4318cea83ca0a1f669f662":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e616cfbf284ecbd0f40b4b9e4196daa27e92abd5":["ef57266eb048f5d440958e9b0612adf7535e5e8b","927e6deec13723e5fd6fb8f1e0457e8597dae15e","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"25107e8bc3cbaed9427c9c7092c430cf4ce48b1b":["e616cfbf284ecbd0f40b4b9e4196daa27e92abd5"],"dbdfda95da145c32af2267b537c92481acc7a522":["103fcde6b6238d15212a5524ae65e1f43603db44"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}