{"path":"lucene/core/src/test/org/apache/lucene/index/TestFieldInvertState#testRandom().mjava","commits":[{"id":"e72e3ade782716457071fee4033f18689acc4c4f","date":1496770651,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestFieldInvertState#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n    int numUniqueTokens = TestUtil.nextInt(random(), 1, 25);\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setSimilarity(NeverForgetsSimilarity.INSTANCE);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    Document doc = new Document();\n\n    int numTokens = atLeast(10000);\n    Token[] tokens = new Token[numTokens];\n    Map<Character,Integer> counts = new HashMap<>();\n    int numStacked = 0;\n    int maxTermFreq = 0;\n    int pos = -1;\n    for (int i=0;i<numTokens;i++) {\n      char tokenChar = (char) ('a' + random().nextInt(numUniqueTokens));\n      Integer oldCount = counts.get(tokenChar);\n      int newCount;\n      if (oldCount == null) {\n        newCount = 1;\n      } else {\n        newCount = 1 + oldCount;\n      }\n      counts.put(tokenChar, newCount);\n      maxTermFreq = Math.max(maxTermFreq, newCount);\n      \n      Token token = new Token(Character.toString(tokenChar), 2*i, 2*i+1);\n      \n      if (i > 0 && random().nextInt(7) == 3) {\n        token.setPositionIncrement(0);\n        numStacked++;\n      } else {\n        pos++;\n      }\n      tokens[i] = token;\n    }\n\n    Field field = new Field(\"field\",\n                            new CannedTokenStream(tokens),\n                            TextField.TYPE_NOT_STORED);\n    doc.add(field);\n    w.addDocument(doc);\n    FieldInvertState fis = NeverForgetsSimilarity.INSTANCE.lastState;\n    assertEquals(maxTermFreq, fis.getMaxTermFrequency());\n    assertEquals(counts.size(), fis.getUniqueTermCount());\n    assertEquals(numStacked, fis.getNumOverlap());\n    assertEquals(numTokens, fis.getLength());\n    assertEquals(pos, fis.getPosition());\n    \n    IOUtils.close(w, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f344bb33ca91f48e99c061980115b46fa84fc8f5","date":1496903283,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestFieldInvertState#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n    int numUniqueTokens = TestUtil.nextInt(random(), 1, 25);\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setSimilarity(NeverForgetsSimilarity.INSTANCE);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    Document doc = new Document();\n\n    int numTokens = atLeast(10000);\n    Token[] tokens = new Token[numTokens];\n    Map<Character,Integer> counts = new HashMap<>();\n    int numStacked = 0;\n    int maxTermFreq = 0;\n    int pos = -1;\n    for (int i=0;i<numTokens;i++) {\n      char tokenChar = (char) ('a' + random().nextInt(numUniqueTokens));\n      Integer oldCount = counts.get(tokenChar);\n      int newCount;\n      if (oldCount == null) {\n        newCount = 1;\n      } else {\n        newCount = 1 + oldCount;\n      }\n      counts.put(tokenChar, newCount);\n      maxTermFreq = Math.max(maxTermFreq, newCount);\n      \n      Token token = new Token(Character.toString(tokenChar), 2*i, 2*i+1);\n      \n      if (i > 0 && random().nextInt(7) == 3) {\n        token.setPositionIncrement(0);\n        numStacked++;\n      } else {\n        pos++;\n      }\n      tokens[i] = token;\n    }\n\n    Field field = new Field(\"field\",\n                            new CannedTokenStream(tokens),\n                            TextField.TYPE_NOT_STORED);\n    doc.add(field);\n    w.addDocument(doc);\n    FieldInvertState fis = NeverForgetsSimilarity.INSTANCE.lastState;\n    assertEquals(maxTermFreq, fis.getMaxTermFrequency());\n    assertEquals(counts.size(), fis.getUniqueTermCount());\n    assertEquals(numStacked, fis.getNumOverlap());\n    assertEquals(numTokens, fis.getLength());\n    assertEquals(pos, fis.getPosition());\n    \n    IOUtils.close(w, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","date":1498028748,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestFieldInvertState#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n    int numUniqueTokens = TestUtil.nextInt(random(), 1, 25);\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setSimilarity(NeverForgetsSimilarity.INSTANCE);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    Document doc = new Document();\n\n    int numTokens = atLeast(10000);\n    Token[] tokens = new Token[numTokens];\n    Map<Character,Integer> counts = new HashMap<>();\n    int numStacked = 0;\n    int maxTermFreq = 0;\n    int pos = -1;\n    for (int i=0;i<numTokens;i++) {\n      char tokenChar = (char) ('a' + random().nextInt(numUniqueTokens));\n      Integer oldCount = counts.get(tokenChar);\n      int newCount;\n      if (oldCount == null) {\n        newCount = 1;\n      } else {\n        newCount = 1 + oldCount;\n      }\n      counts.put(tokenChar, newCount);\n      maxTermFreq = Math.max(maxTermFreq, newCount);\n      \n      Token token = new Token(Character.toString(tokenChar), 2*i, 2*i+1);\n      \n      if (i > 0 && random().nextInt(7) == 3) {\n        token.setPositionIncrement(0);\n        numStacked++;\n      } else {\n        pos++;\n      }\n      tokens[i] = token;\n    }\n\n    Field field = new Field(\"field\",\n                            new CannedTokenStream(tokens),\n                            TextField.TYPE_NOT_STORED);\n    doc.add(field);\n    w.addDocument(doc);\n    FieldInvertState fis = NeverForgetsSimilarity.INSTANCE.lastState;\n    assertEquals(maxTermFreq, fis.getMaxTermFrequency());\n    assertEquals(counts.size(), fis.getUniqueTermCount());\n    assertEquals(numStacked, fis.getNumOverlap());\n    assertEquals(numTokens, fis.getLength());\n    assertEquals(pos, fis.getPosition());\n    \n    IOUtils.close(w, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestFieldInvertState#testRandom().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom() throws Exception {\n    int numUniqueTokens = TestUtil.nextInt(random(), 1, 25);\n    Directory dir = newDirectory();\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setSimilarity(NeverForgetsSimilarity.INSTANCE);\n    IndexWriter w = new IndexWriter(dir, iwc);\n    Document doc = new Document();\n\n    int numTokens = atLeast(10000);\n    Token[] tokens = new Token[numTokens];\n    Map<Character,Integer> counts = new HashMap<>();\n    int numStacked = 0;\n    int maxTermFreq = 0;\n    int pos = -1;\n    for (int i=0;i<numTokens;i++) {\n      char tokenChar = (char) ('a' + random().nextInt(numUniqueTokens));\n      Integer oldCount = counts.get(tokenChar);\n      int newCount;\n      if (oldCount == null) {\n        newCount = 1;\n      } else {\n        newCount = 1 + oldCount;\n      }\n      counts.put(tokenChar, newCount);\n      maxTermFreq = Math.max(maxTermFreq, newCount);\n      \n      Token token = new Token(Character.toString(tokenChar), 2*i, 2*i+1);\n      \n      if (i > 0 && random().nextInt(7) == 3) {\n        token.setPositionIncrement(0);\n        numStacked++;\n      } else {\n        pos++;\n      }\n      tokens[i] = token;\n    }\n\n    Field field = new Field(\"field\",\n                            new CannedTokenStream(tokens),\n                            TextField.TYPE_NOT_STORED);\n    doc.add(field);\n    w.addDocument(doc);\n    FieldInvertState fis = NeverForgetsSimilarity.INSTANCE.lastState;\n    assertEquals(maxTermFreq, fis.getMaxTermFrequency());\n    assertEquals(counts.size(), fis.getUniqueTermCount());\n    assertEquals(numStacked, fis.getNumOverlap());\n    assertEquals(numTokens, fis.getLength());\n    assertEquals(pos, fis.getPosition());\n    \n    IOUtils.close(w, dir);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e72e3ade782716457071fee4033f18689acc4c4f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"f344bb33ca91f48e99c061980115b46fa84fc8f5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e72e3ade782716457071fee4033f18689acc4c4f"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f344bb33ca91f48e99c061980115b46fa84fc8f5"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f344bb33ca91f48e99c061980115b46fa84fc8f5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28288370235ed02234a64753cdbf0c6ec096304a"]},"commit2Childs":{"e72e3ade782716457071fee4033f18689acc4c4f":["f344bb33ca91f48e99c061980115b46fa84fc8f5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e72e3ade782716457071fee4033f18689acc4c4f","f344bb33ca91f48e99c061980115b46fa84fc8f5","28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"f344bb33ca91f48e99c061980115b46fa84fc8f5":["28288370235ed02234a64753cdbf0c6ec096304a","2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9"],"28288370235ed02234a64753cdbf0c6ec096304a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2a3ed3f77cdd034e789d00d1ca8bb7054c9fb8e9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}