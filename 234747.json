{"path":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","commits":[{"id":"893269407f5c988a4b2ee13c7ffc01ec43136c1d","date":1268598046,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    final String[] terms = si.lookup;\n    final int[] termNum = si.order;\n\n    if (prefix!=null && prefix.length()==0) prefix=null;\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = Arrays.binarySearch(terms,prefix+\"\\uffff\\uffff\\uffff\\uffff\",nullStrComparator);\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=terms.length;\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = termNum[iter.nextDoc()];\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<String,Integer>> queue = new BoundedTreeSet<CountPair<String,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<String,Integer>(terms[startTermIndex+i], c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<String,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(p.key), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(terms[startTermIndex+i]), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","date":1268599006,"type":4,"author":"Mark Robert Miller","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":null,"sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    final String[] terms = si.lookup;\n    final int[] termNum = si.order;\n\n    if (prefix!=null && prefix.length()==0) prefix=null;\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = Arrays.binarySearch(terms,prefix+\"\\uffff\\uffff\\uffff\\uffff\",nullStrComparator);\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=terms.length;\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = termNum[iter.nextDoc()];\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<String,Integer>> queue = new BoundedTreeSet<CountPair<String,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<String,Integer>(terms[startTermIndex+i], c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<String,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(p.key), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(terms[startTermIndex+i]), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    final String[] terms = si.lookup;\n    final int[] termNum = si.order;\n\n    if (prefix!=null && prefix.length()==0) prefix=null;\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = Arrays.binarySearch(terms,prefix+\"\\uffff\\uffff\\uffff\\uffff\",nullStrComparator);\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=terms.length;\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = termNum[iter.nextDoc()];\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<String,Integer>> queue = new BoundedTreeSet<CountPair<String,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<String,Integer>(terms[startTermIndex+i], c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<String,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(p.key), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(terms[startTermIndex+i]), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    final String[] terms = si.lookup;\n    final int[] termNum = si.order;\n\n    if (prefix!=null && prefix.length()==0) prefix=null;\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = Arrays.binarySearch(terms,prefix+\"\\uffff\\uffff\\uffff\\uffff\",nullStrComparator);\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=terms.length;\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = termNum[iter.nextDoc()];\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<String,Integer>> queue = new BoundedTreeSet<CountPair<String,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<String,Integer>(terms[startTermIndex+i], c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<String,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(p.key), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(terms[startTermIndex+i]), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1da8d55113b689b06716246649de6f62430f15c0","date":1453508340,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    final String[] terms = si.lookup;\n    final int[] termNum = si.order;\n\n    if (prefix!=null && prefix.length()==0) prefix=null;\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = Arrays.binarySearch(terms,prefix+\"\\uffff\\uffff\\uffff\\uffff\",nullStrComparator);\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=terms.length;\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = termNum[iter.nextDoc()];\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<String,Integer>> queue = new BoundedTreeSet<CountPair<String,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<String,Integer>(terms[startTermIndex+i], c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<String,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(p.key), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(terms[startTermIndex+i]), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"be20f9fed1d3edcb1c84abcc39df87a90fab22df","date":1275590285,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    CharArr spare = new CharArr();\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<BytesRef,Integer>(si.lookup(startTermIndex+i, new BytesRef()), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<BytesRef,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(p.key, spare);\n          res.add(spare.toString(), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.StringIndex si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), fieldName);\n    final String[] terms = si.lookup;\n    final int[] termNum = si.order;\n\n    if (prefix!=null && prefix.length()==0) prefix=null;\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = Arrays.binarySearch(terms,prefix,nullStrComparator);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = Arrays.binarySearch(terms,prefix+\"\\uffff\\uffff\\uffff\\uffff\",nullStrComparator);\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=terms.length;\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = termNum[iter.nextDoc()];\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<String,Integer>> queue = new BoundedTreeSet<CountPair<String,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<String,Integer>(terms[startTermIndex+i], c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<String,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(p.key), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          res.add(ft.indexedToReadable(terms[startTermIndex+i]), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fd8b97815b7381ece49de90a42c654c24169a26c","date":1276979385,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    CharArr spare = new CharArr();\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<BytesRef,Integer>(si.lookup(startTermIndex+i, new BytesRef()), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<BytesRef,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(p.key, spare);\n          res.add(spare.toString(), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    CharArr spare = new CharArr();\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<BytesRef,Integer>(si.lookup(startTermIndex+i, new BytesRef()), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<BytesRef,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(p.key, spare);\n          res.add(spare.toString(), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    CharArr spare = new CharArr();\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<BytesRef,Integer>(si.lookup(startTermIndex+i, new BytesRef()), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<BytesRef,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(p.key, spare);\n          res.add(spare.toString(), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=1;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    CharArr spare = new CharArr();\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int term = si.getOrd(iter.nextDoc());\n        int arrIdx = term-startTermIndex;\n        if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<BytesRef,Integer>(si.lookup(startTermIndex+i, new BytesRef()), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<BytesRef,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(p.key, spare);\n          res.add(spare.toString(), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e789a6ee3a5fa41394cbed2293ff68c3712c32a9","date":1283533946,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount < lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    CharArr spare = new CharArr();\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<BytesRef,Integer>(si.lookup(startTermIndex+i, new BytesRef()), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<BytesRef,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(p.key, spare);\n          res.add(spare.toString(), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98807419200815580d853d74ffde5d2af0406d30","date":1283539270,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount < lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"627ce218a5a68018115c2deb6559b41e3665b8ab","date":1284500689,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      // find the end term.  \\uffff isn't a legal unicode char, but only compareTo\n      // is used, so it should be fine, and is guaranteed to be bigger than legal chars.\n      endTermIndex = si.binarySearchLookup(new BytesRef(prefix+\"\\uffff\\uffff\\uffff\\uffff\"), br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n\n    CharArr spare = new CharArr();\n\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        final BoundedTreeSet<CountPair<BytesRef,Integer>> queue = new BoundedTreeSet<CountPair<BytesRef,Integer>>(maxsize);\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n            queue.add(new CountPair<BytesRef,Integer>(si.lookup(startTermIndex+i, new BytesRef()), c));\n            if (queue.size()>=maxsize) min=queue.last().val;\n          }\n        }\n        // now select the right page from the results\n        for (CountPair<BytesRef,Integer> p : queue) {\n          if (--off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(p.key, spare);\n          res.add(spare.toString(), p.val);\n        }\n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      res.add(null, getFieldMissingCount(searcher,docs,fieldName));\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"755f2f419306d7297c8feee10d1897addf4b2dd0","date":1294442354,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c220849f876de24a79f756f65b3eb045db59f63f","date":1294902803,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList res = new NamedList();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1b3a24d5d9b47345473ff564f5cc127a7b526b4","date":1306277076,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(ByteUtils.bigTerm);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n\n    CharArr spare = new CharArr();\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), spare);\n          res.add(spare.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          spare.reset();\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), spare);\n          res.add(spare.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/SimpleFacets#getFieldCacheCounts(SolrIndexSearcher,DocSet,String,int,int,int,boolean,String,String).mjava","sourceNew":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","sourceOld":"  /**\n   * Use the Lucene FieldCache to get counts for each unique field value in <code>docs</code>.\n   * The field must have at most one indexed token per document.\n   */\n  public static NamedList<Integer> getFieldCacheCounts(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort, String prefix) throws IOException {\n    // TODO: If the number of terms is high compared to docs.size(), and zeros==false,\n    //  we should use an alternate strategy to avoid\n    //  1) creating another huge int[] for the counts\n    //  2) looping over that huge int[] looking for the rare non-zeros.\n    //\n    // Yet another variation: if docs.size() is small and termvectors are stored,\n    // then use them instead of the FieldCache.\n    //\n\n    // TODO: this function is too big and could use some refactoring, but\n    // we also need a facet cache, and refactoring of SimpleFacets instead of\n    // trying to pass all the various params around.\n\n    FieldType ft = searcher.getSchema().getFieldType(fieldName);\n    NamedList<Integer> res = new NamedList<Integer>();\n\n    FieldCache.DocTermsIndex si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), fieldName);\n\n    final BytesRef prefixRef;\n    if (prefix == null) {\n      prefixRef = null;\n    } else if (prefix.length()==0) {\n      prefix = null;\n      prefixRef = null;\n    } else {\n      prefixRef = new BytesRef(prefix);\n    }\n\n    final BytesRef br = new BytesRef();\n\n    int startTermIndex, endTermIndex;\n    if (prefix!=null) {\n      startTermIndex = si.binarySearchLookup(prefixRef, br);\n      if (startTermIndex<0) startTermIndex=-startTermIndex-1;\n      prefixRef.append(UnicodeUtil.BIG_TERM);\n      endTermIndex = si.binarySearchLookup(prefixRef, br);\n      assert endTermIndex < 0;\n      endTermIndex = -endTermIndex-1;\n    } else {\n      startTermIndex=0;\n      endTermIndex=si.numOrd();\n    }\n\n    final int nTerms=endTermIndex-startTermIndex;\n    int missingCount = -1; \n    final CharsRef charsRef = new CharsRef(10);\n    if (nTerms>0 && docs.size() >= mincount) {\n\n      // count collection array only needs to be as big as the number of terms we are\n      // going to collect counts for.\n      final int[] counts = new int[nTerms];\n\n      DocIterator iter = docs.iterator();\n\n      PackedInts.Reader ordReader = si.getDocToOrd();\n      if (ordReader instanceof Direct32) {\n        int[] ords = ((Direct32)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()]]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()];\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct16) {\n        short[] ords = ((Direct16)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xffff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xffff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else if (ordReader instanceof Direct8) {\n        byte[] ords = ((Direct8)ordReader).getArray();\n        if (prefix==null) {\n          while (iter.hasNext()) {\n            counts[ords[iter.nextDoc()] & 0xff]++;\n          }\n        } else {\n          while (iter.hasNext()) {\n            int term = ords[iter.nextDoc()] & 0xff;\n            int arrIdx = term-startTermIndex;\n            if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n          }\n        }\n      } else {\n        while (iter.hasNext()) {\n          int term = si.getOrd(iter.nextDoc());\n          int arrIdx = term-startTermIndex;\n          if (arrIdx>=0 && arrIdx<nTerms) counts[arrIdx]++;\n        }\n      }\n\n      if (startTermIndex == 0) {\n        missingCount = counts[0];\n      }\n\n      // IDEA: we could also maintain a count of \"other\"... everything that fell outside\n      // of the top 'N'\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, nTerms);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        for (int i=(startTermIndex==0)?1:0; i<nTerms; i++) {\n          int c = counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n          ft.indexedToReadable(si.lookup(startTermIndex+tnum, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      \n      } else {\n        // add results in index order\n        int i=(startTermIndex==0)?1:0;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i+=off;\n          off=0;\n        }\n\n        for (; i<nTerms; i++) {          \n          int c = counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n          ft.indexedToReadable(si.lookup(startTermIndex+i, br), charsRef);\n          res.add(charsRef.toString(), c);\n        }\n      }\n    }\n\n    if (missing) {\n      if (missingCount < 0) {\n        missingCount = getFieldMissingCount(searcher,docs,fieldName);\n      }\n      res.add(null, missingCount);\n    }\n    \n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"627ce218a5a68018115c2deb6559b41e3665b8ab":["98807419200815580d853d74ffde5d2af0406d30"],"be20f9fed1d3edcb1c84abcc39df87a90fab22df":["1da8d55113b689b06716246649de6f62430f15c0"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"c26f00b574427b55127e869b935845554afde1fa":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"5f4e87790277826a2aea119328600dfb07761f32":["be20f9fed1d3edcb1c84abcc39df87a90fab22df","fd8b97815b7381ece49de90a42c654c24169a26c"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","627ce218a5a68018115c2deb6559b41e3665b8ab"],"1da8d55113b689b06716246649de6f62430f15c0":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","ad94625fb8d088209f46650c8097196fec67f00c"],"c220849f876de24a79f756f65b3eb045db59f63f":["755f2f419306d7297c8feee10d1897addf4b2dd0"],"fd8b97815b7381ece49de90a42c654c24169a26c":["be20f9fed1d3edcb1c84abcc39df87a90fab22df"],"e789a6ee3a5fa41394cbed2293ff68c3712c32a9":["fd8b97815b7381ece49de90a42c654c24169a26c"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["c220849f876de24a79f756f65b3eb045db59f63f","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"755f2f419306d7297c8feee10d1897addf4b2dd0":["627ce218a5a68018115c2deb6559b41e3665b8ab"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["627ce218a5a68018115c2deb6559b41e3665b8ab","c220849f876de24a79f756f65b3eb045db59f63f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["c220849f876de24a79f756f65b3eb045db59f63f"],"ad94625fb8d088209f46650c8097196fec67f00c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"98807419200815580d853d74ffde5d2af0406d30":["e789a6ee3a5fa41394cbed2293ff68c3712c32a9"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c220849f876de24a79f756f65b3eb045db59f63f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["29ef99d61cda9641b6250bf9567329a6e65f901d","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"]},"commit2Childs":{"627ce218a5a68018115c2deb6559b41e3665b8ab":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","755f2f419306d7297c8feee10d1897addf4b2dd0","29ef99d61cda9641b6250bf9567329a6e65f901d"],"be20f9fed1d3edcb1c84abcc39df87a90fab22df":["5f4e87790277826a2aea119328600dfb07761f32","fd8b97815b7381ece49de90a42c654c24169a26c"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["1da8d55113b689b06716246649de6f62430f15c0"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ad94625fb8d088209f46650c8097196fec67f00c"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"1da8d55113b689b06716246649de6f62430f15c0":["be20f9fed1d3edcb1c84abcc39df87a90fab22df"],"fd8b97815b7381ece49de90a42c654c24169a26c":["5f4e87790277826a2aea119328600dfb07761f32","e789a6ee3a5fa41394cbed2293ff68c3712c32a9"],"c220849f876de24a79f756f65b3eb045db59f63f":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","29ef99d61cda9641b6250bf9567329a6e65f901d","a1b3a24d5d9b47345473ff564f5cc127a7b526b4","868da859b43505d9d2a023bfeae6dd0c795f5295"],"e789a6ee3a5fa41394cbed2293ff68c3712c32a9":["98807419200815580d853d74ffde5d2af0406d30"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"755f2f419306d7297c8feee10d1897addf4b2dd0":["c220849f876de24a79f756f65b3eb045db59f63f"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["c26f00b574427b55127e869b935845554afde1fa","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","a258fbb26824fd104ed795e5d9033d2d040049ee","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"ad94625fb8d088209f46650c8097196fec67f00c":["1da8d55113b689b06716246649de6f62430f15c0"],"98807419200815580d853d74ffde5d2af0406d30":["627ce218a5a68018115c2deb6559b41e3665b8ab"],"868da859b43505d9d2a023bfeae6dd0c795f5295":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[]},"heads":["a258fbb26824fd104ed795e5d9033d2d040049ee","868da859b43505d9d2a023bfeae6dd0c795f5295","cd5edd1f2b162a5cfa08efd17851a07373a96817","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}