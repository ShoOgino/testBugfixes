{"path":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = Codec.getDefault().getName().equals(\"Lucene3x\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = Codec.getDefault().getName().equals(\"Lucene3x\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a78a90fc9701e511308346ea29f4f5e548bb39fe","date":1329489995,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = Codec.getDefault().getName().equals(\"Lucene3x\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = Codec.getDefault().getName().equals(\"Lucene3x\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":["60ba444201d2570214b6fcf1d15600dc1a01f548"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = Codec.getDefault().getName().equals(\"Lucene3x\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = Codec.getDefault().getName().equals(\"Lucene3x\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random, MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random);\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"57ae3024996ccdb3c36c42cb890e1efb37df4ce8","date":1338343651,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  public void setUp() throws Exception {\n    super.setUp();\n    // we generate aweful regexps: good for testing.\n    // but for preflex codec, the test can be very slow, so use less iterations.\n    numIterations = Codec.getDefault().getName().equals(\"Lucene3x\") ? 10 * RANDOM_MULTIPLIER : atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newField(\"field\", \"\", StringField.TYPE_STORED);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d94feb02e9c604630d8a6758abcb40cbfa91f5d","date":1340964157,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = DaciukMihovAutomatonBuilder.build(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7530de27b87b961b51f01bd1299b7004d46e8823","date":1355236261,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = _TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<BytesRef>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac34f0c5bb9274821fb0cb18075234e02002e9bf","date":1402508126,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnionLight(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4ca1c732df8923f5624f6c06b1dcca9e69d98c96","date":1402957391,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = Automata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnionLight(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c84485629d80d203608e8975a1139de9933cc38","date":1403166128,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = Automata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = BasicAutomata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = Automata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(TEST_VERSION_CURRENT,\n            new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = Automata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2#setUp().mjava","sourceNew":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = Automata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.close();\n  }\n\n","sourceOld":"  @Override\n  public void setUp() throws Exception {\n    super.setUp();\n    numIterations = atLeast(50);\n    dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,\n        newIndexWriterConfig(new MockAnalyzer(random(), MockTokenizer.KEYWORD, false))\n            .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    Field field = newStringField(\"field\", \"\", Field.Store.YES);\n    doc.add(field);\n    terms = new TreeSet<>();\n \n    int num = atLeast(200);\n    for (int i = 0; i < num; i++) {\n      String s = TestUtil.randomUnicodeString(random());\n      field.setStringValue(s);\n      terms.add(new BytesRef(s));\n      writer.addDocument(doc);\n    }\n    \n    termsAutomaton = Automata.makeStringUnion(terms);\n    \n    reader = writer.getReader();\n    searcher = newSearcher(reader);\n    writer.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["8d94feb02e9c604630d8a6758abcb40cbfa91f5d","7530de27b87b961b51f01bd1299b7004d46e8823"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6613659748fe4411a7dcf85266e55db1f95f7315"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["5c84485629d80d203608e8975a1139de9933cc38"],"6613659748fe4411a7dcf85266e55db1f95f7315":["7530de27b87b961b51f01bd1299b7004d46e8823"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8d94feb02e9c604630d8a6758abcb40cbfa91f5d":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["04f07771a2a7dd3a395700665ed839c3dae2def2","8d94feb02e9c604630d8a6758abcb40cbfa91f5d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4ca1c732df8923f5624f6c06b1dcca9e69d98c96":["ac34f0c5bb9274821fb0cb18075234e02002e9bf"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"5c84485629d80d203608e8975a1139de9933cc38":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a","4ca1c732df8923f5624f6c06b1dcca9e69d98c96"],"7530de27b87b961b51f01bd1299b7004d46e8823":["8d94feb02e9c604630d8a6758abcb40cbfa91f5d"],"ac34f0c5bb9274821fb0cb18075234e02002e9bf":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d0ef034a4f10871667ae75181537775ddcf8ade4"]},"commit2Childs":{"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"6613659748fe4411a7dcf85266e55db1f95f7315":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"8d94feb02e9c604630d8a6758abcb40cbfa91f5d":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","fe33227f6805edab2036cbb80645cc4e2d1fa424","7530de27b87b961b51f01bd1299b7004d46e8823"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["8d94feb02e9c604630d8a6758abcb40cbfa91f5d","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"57ae3024996ccdb3c36c42cb890e1efb37df4ce8":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"4ca1c732df8923f5624f6c06b1dcca9e69d98c96":["5c84485629d80d203608e8975a1139de9933cc38"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["5c84485629d80d203608e8975a1139de9933cc38","ac34f0c5bb9274821fb0cb18075234e02002e9bf"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["57ae3024996ccdb3c36c42cb890e1efb37df4ce8"],"5c84485629d80d203608e8975a1139de9933cc38":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"7530de27b87b961b51f01bd1299b7004d46e8823":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","6613659748fe4411a7dcf85266e55db1f95f7315"],"ac34f0c5bb9274821fb0cb18075234e02002e9bf":["4ca1c732df8923f5624f6c06b1dcca9e69d98c96"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","fe33227f6805edab2036cbb80645cc4e2d1fa424","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}