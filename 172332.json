{"path":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","commits":[{"id":"8598a11db0eb9efa116ba7656c437f5bed7de0f7","date":1272964265,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"solr/src/test/org/apache/solr/analysis/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(DEFAULT_VERSION, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(DEFAULT_VERSION,\n            new WhitespaceTokenizer(DEFAULT_VERSION, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f080986da691a3bba7b757f43ab72cdc82b57ce","date":1273069619,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","pathOld":"lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilter#testPositionIncrements().mjava","sourceNew":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","sourceOld":"  @Test\n  public void testPositionIncrements() throws Exception {\n    final CharArraySet protWords = new CharArraySet(TEST_VERSION_CURRENT, new HashSet<String>(Arrays.asList(\"NUTCH\")), false);\n    \n    /* analyzer that uses whitespace + wdf */\n    Analyzer a = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    /* in this case, works as expected. */\n    assertAnalyzesTo(a, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 1 });\n    \n    /* only in this case, posInc of 2 ?! */\n    assertAnalyzesTo(a, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 1, 1, 0 });\n    \n    assertAnalyzesTo(a, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 1, 1 });\n    \n    /* analyzer that will consume tokens with large position increments */\n    Analyzer a2 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        return new WordDelimiterFilter(\n            new LargePosIncTokenFilter(\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader)),\n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n    \n    /* increment of \"largegap\" is preserved */\n    assertAnalyzesTo(a2, \"LUCENE largegap SOLR\", new String[] { \"LUCENE\", \"largegap\", \"SOLR\" },\n        new int[] { 0, 7, 16 },\n        new int[] { 6, 15, 20 },\n        new int[] { 1, 10, 1 });\n    \n    /* the \"/\" had a position increment of 10, where did it go?!?!! */\n    assertAnalyzesTo(a2, \"LUCENE / SOLR\", new String[] { \"LUCENE\", \"SOLR\" },\n        new int[] { 0, 9 },\n        new int[] { 6, 13 },\n        new int[] { 1, 11 });\n    \n    /* in this case, the increment of 10 from the \"/\" is carried over */\n    assertAnalyzesTo(a2, \"LUCENE / solR\", new String[] { \"LUCENE\", \"sol\", \"R\", \"solR\" },\n        new int[] { 0, 9, 12, 9 },\n        new int[] { 6, 12, 13, 13 },\n        new int[] { 1, 11, 1, 0 });\n    \n    assertAnalyzesTo(a2, \"LUCENE / NUTCH SOLR\", new String[] { \"LUCENE\", \"NUTCH\", \"SOLR\" },\n        new int[] { 0, 9, 15 },\n        new int[] { 6, 14, 19 },\n        new int[] { 1, 11, 1 });\n\n    Analyzer a3 = new Analyzer() {\n      public TokenStream tokenStream(String field, Reader reader) {\n        StopFilter filter = new StopFilter(TEST_VERSION_CURRENT,\n            new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader), StandardAnalyzer.STOP_WORDS_SET);\n        filter.setEnablePositionIncrements(true);\n        return new WordDelimiterFilter(filter, \n            1, 1, 0, 0, 1, 1, 0, 1, 1, protWords);\n      }\n    };\n\n    assertAnalyzesTo(a3, \"lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" },\n        new int[] { 0, 7, 0 },\n        new int[] { 6, 11, 11 },\n        new int[] { 1, 1, 0 });\n\n    /* the stopword should add a gap here */\n    assertAnalyzesTo(a3, \"the lucene.solr\", \n        new String[] { \"lucene\", \"solr\", \"lucenesolr\" }, \n        new int[] { 4, 11, 4 }, \n        new int[] { 10, 15, 15 },\n        new int[] { 2, 1, 0 });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"8598a11db0eb9efa116ba7656c437f5bed7de0f7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["8598a11db0eb9efa116ba7656c437f5bed7de0f7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0f080986da691a3bba7b757f43ab72cdc82b57ce"]},"commit2Childs":{"8598a11db0eb9efa116ba7656c437f5bed7de0f7":["0f080986da691a3bba7b757f43ab72cdc82b57ce"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8598a11db0eb9efa116ba7656c437f5bed7de0f7"],"0f080986da691a3bba7b757f43ab72cdc82b57ce":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}