{"path":"lucene/src/java/org/apache/lucene/analysis/tokenattributes/TermToBytesRefAttribute#fillBytesRef().mjava","commits":[{"id":"b3d07f1ae3b58102f36f3393c397d78ba4e547a4","date":1300715535,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/analysis/tokenattributes/TermToBytesRefAttribute#fillBytesRef().mjava","pathOld":"lucene/src/java/org/apache/lucene/analysis/tokenattributes/TermToBytesRefAttribute#toBytesRef(BytesRef).mjava","sourceNew":"  /** \n   * Updates the bytes {@link #getBytesRef()} to contain this term's\n   * final encoding, and returns its hashcode.\n   * @return the hashcode as defined by {@link BytesRef#hashCode}:\n   * <pre>\n   *  int hash = 0;\n   *  for (int i = termBytes.offset; i &lt; termBytes.offset+termBytes.length; i++) {\n   *    hash = 31*hash + termBytes.bytes[i];\n   *  }\n   * </pre>\n   * Implement this for performance reasons, if your code can calculate\n   * the hash on-the-fly. If this is not the case, just return\n   * {@code termBytes.hashCode()}.\n   */\n  public int fillBytesRef();\n\n","sourceOld":"  /** Copies the token's term text into the given {@link BytesRef}.\n   * @param termBytes destination to write the bytes to (UTF-8 for text terms).\n   * The length of the BytesRef's buffer may be not large enough, so you need to grow.\n   * The parameters' {@code bytes} is guaranteed to be not {@code null}.\n   * @return the hashcode as defined by {@link BytesRef#hashCode}:\n   * <pre>\n   *  int hash = 0;\n   *  for (int i = termBytes.offset; i &lt; termBytes.offset+termBytes.length; i++) {\n   *    hash = 31*hash + termBytes.bytes[i];\n   *  }\n   * </pre>\n   * Implement this for performance reasons, if your code can calculate\n   * the hash on-the-fly. If this is not the case, just return\n   * {@code termBytes.hashCode()}.\n   */\n  public int toBytesRef(BytesRef termBytes);\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/analysis/tokenattributes/TermToBytesRefAttribute#fillBytesRef().mjava","pathOld":"/dev/null","sourceNew":"  /** \n   * Updates the bytes {@link #getBytesRef()} to contain this term's\n   * final encoding, and returns its hashcode.\n   * @return the hashcode as defined by {@link BytesRef#hashCode}:\n   * <pre>\n   *  int hash = 0;\n   *  for (int i = termBytes.offset; i &lt; termBytes.offset+termBytes.length; i++) {\n   *    hash = 31*hash + termBytes.bytes[i];\n   *  }\n   * </pre>\n   * Implement this for performance reasons, if your code can calculate\n   * the hash on-the-fly. If this is not the case, just return\n   * {@code termBytes.hashCode()}.\n   */\n  public int fillBytesRef();\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c0ef0193974807e4bddf5432a6b0287fe4d6c9df","date":1301476645,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/analysis/tokenattributes/TermToBytesRefAttribute#fillBytesRef().mjava","pathOld":"/dev/null","sourceNew":"  /** \n   * Updates the bytes {@link #getBytesRef()} to contain this term's\n   * final encoding, and returns its hashcode.\n   * @return the hashcode as defined by {@link BytesRef#hashCode}:\n   * <pre>\n   *  int hash = 0;\n   *  for (int i = termBytes.offset; i &lt; termBytes.offset+termBytes.length; i++) {\n   *    hash = 31*hash + termBytes.bytes[i];\n   *  }\n   * </pre>\n   * Implement this for performance reasons, if your code can calculate\n   * the hash on-the-fly. If this is not the case, just return\n   * {@code termBytes.hashCode()}.\n   */\n  public int fillBytesRef();\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/TermToBytesRefAttribute#fillBytesRef().mjava","pathOld":"lucene/src/java/org/apache/lucene/analysis/tokenattributes/TermToBytesRefAttribute#fillBytesRef().mjava","sourceNew":"  /** \n   * Updates the bytes {@link #getBytesRef()} to contain this term's\n   * final encoding, and returns its hashcode.\n   * @return the hashcode as defined by {@link BytesRef#hashCode}:\n   * <pre>\n   *  int hash = 0;\n   *  for (int i = termBytes.offset; i &lt; termBytes.offset+termBytes.length; i++) {\n   *    hash = 31*hash + termBytes.bytes[i];\n   *  }\n   * </pre>\n   * Implement this for performance reasons, if your code can calculate\n   * the hash on-the-fly. If this is not the case, just return\n   * {@code termBytes.hashCode()}.\n   */\n  public int fillBytesRef();\n\n","sourceOld":"  /** \n   * Updates the bytes {@link #getBytesRef()} to contain this term's\n   * final encoding, and returns its hashcode.\n   * @return the hashcode as defined by {@link BytesRef#hashCode}:\n   * <pre>\n   *  int hash = 0;\n   *  for (int i = termBytes.offset; i &lt; termBytes.offset+termBytes.length; i++) {\n   *    hash = 31*hash + termBytes.bytes[i];\n   *  }\n   * </pre>\n   * Implement this for performance reasons, if your code can calculate\n   * the hash on-the-fly. If this is not the case, just return\n   * {@code termBytes.hashCode()}.\n   */\n  public int fillBytesRef();\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b3d07f1ae3b58102f36f3393c397d78ba4e547a4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"c0ef0193974807e4bddf5432a6b0287fe4d6c9df":[],"d619839baa8ce5503e496b94a9e42ad6f079293f":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","b3d07f1ae3b58102f36f3393c397d78ba4e547a4"],"b3d07f1ae3b58102f36f3393c397d78ba4e547a4":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c0ef0193974807e4bddf5432a6b0287fe4d6c9df","d619839baa8ce5503e496b94a9e42ad6f079293f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}