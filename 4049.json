{"path":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","commits":[{"id":"b7acd952b8ec320606434716bd02faaec540c885","date":1376495743,"type":1,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testForkLastToken().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, indexAnalyzer, queryAnalyzer, 3) {\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    TermFreqPayload keys[] = new TermFreqPayload[] {\n      new TermFreqPayload(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new TermFreqPayloadArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testForkLastToken() throws Exception {\n    Analyzer a = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          // ForkLastTokenFilter is a bit evil:\n          tokens.setEnableChecks(false);\n          return new TokenStreamComponents(tokens,\n                                           new StopKeywordFilter(TEST_VERSION_CURRENT,\n                                                                 new ForkLastTokenFilter(tokens), StopKeywordFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\")));\n        }\n      };\n\n    TermFreqPayload keys[] = new TermFreqPayload[] {\n      new TermFreqPayload(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, a, a, 3) {\n        @Override\n        protected Query finishQuery(BooleanQuery in, boolean allTermsRequired) {\n          List<BooleanClause> clauses = in.clauses();\n          if (clauses.size() >= 2 && allTermsRequired) {\n            String t1 = getTerm(clauses.get(clauses.size()-2).getQuery());\n            String t2 = getTerm(clauses.get(clauses.size()-1).getQuery());\n            if (t1.equals(t2)) {\n              // The last 2 tokens came from\n              // ForkLastTokenFilter; we remove them and\n              // replace them with a MUST BooleanQuery that\n              // SHOULDs the two of them together:\n              BooleanQuery sub = new BooleanQuery();\n              BooleanClause other = clauses.get(clauses.size()-2);\n              sub.add(new BooleanClause(clauses.get(clauses.size()-2).getQuery(), BooleanClause.Occur.SHOULD));\n              sub.add(new BooleanClause(clauses.get(clauses.size()-1).getQuery(), BooleanClause.Occur.SHOULD));\n              clauses.subList(clauses.size()-2, clauses.size()).clear();\n              clauses.add(new BooleanClause(sub, BooleanClause.Occur.MUST));\n            }\n          }\n          return in;\n        }\n\n        private String getTerm(Query query) {\n          if (query instanceof TermQuery) {\n            return ((TermQuery) query).getTerm().text();\n          } else if (query instanceof PrefixQuery) {\n            return ((PrefixQuery) query).getPrefix().text();\n          } else {\n            return null;\n          }\n        }\n\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    suggester.build(new TermFreqPayloadArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"/dev/null","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, indexAnalyzer, queryAnalyzer, 3) {\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    TermFreqPayload keys[] = new TermFreqPayload[] {\n      new TermFreqPayload(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new TermFreqPayloadArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"41aee74b5f91a096e3fd950f4a336bc763f0e7a7","date":1381772070,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, indexAnalyzer, queryAnalyzer, 3) {\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, indexAnalyzer, queryAnalyzer, 3) {\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    TermFreqPayload keys[] = new TermFreqPayload[] {\n      new TermFreqPayload(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new TermFreqPayloadArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, indexAnalyzer, queryAnalyzer, 3) {\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          MockTokenizer tokens = new MockTokenizer(reader);\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, indexAnalyzer, queryAnalyzer, 3) {\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    File tempDir = TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, indexAnalyzer, queryAnalyzer, 3) {\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    File tempDir = _TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, indexAnalyzer, queryAnalyzer, 3) {\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":["4637747f71df783fc2014ef1f1e0418466e3bed6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f1e7da8a91a92330e8f04b171b83e655a4a25c31","date":1394125906,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, newDirectory(), indexAnalyzer, queryAnalyzer, 3);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    File tempDir = TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, indexAnalyzer, queryAnalyzer, 3) {\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4637747f71df783fc2014ef1f1e0418466e3bed6","date":1394196311,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, newDirectory(), indexAnalyzer, queryAnalyzer, 3);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    File tempDir = TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, indexAnalyzer, queryAnalyzer, 3) {\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":["b7acd952b8ec320606434716bd02faaec540c885","6613659748fe4411a7dcf85266e55db1f95f7315"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, newDirectory(), indexAnalyzer, queryAnalyzer, 3);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    File tempDir = TestUtil.getTempDir(\"AnalyzingInfixSuggesterTest\");\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, tempDir, indexAnalyzer, queryAnalyzer, 3) {\n        @Override\n        protected Directory getDirectory(File path) {\n          return newDirectory();\n        }\n      };\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ff4227bb146f97aabae888091c19e48c88dbb0db","date":1406758576,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, newDirectory(), indexAnalyzer, queryAnalyzer, 3);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, newDirectory(), indexAnalyzer, queryAnalyzer, 3);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cdab62f058ea765dd33deb05b4f19b7d626c801","date":1406803479,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, newDirectory(), indexAnalyzer, queryAnalyzer, 3);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, newDirectory(), indexAnalyzer, queryAnalyzer, 3);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"379db3ad24c4f0214f30a122265a6d6be003a99d","date":1407537768,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, newDirectory(), indexAnalyzer, queryAnalyzer, 3);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(TEST_VERSION_CURRENT, \"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(TEST_VERSION_CURRENT, tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, newDirectory(), indexAnalyzer, queryAnalyzer, 3);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6549d5ea6b7b25525309b981de3ec92b4dff99d1","date":1408666035,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, newDirectory(), indexAnalyzer, queryAnalyzer, 3, false);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, newDirectory(), indexAnalyzer, queryAnalyzer, 3);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"19e497fe4da591a79332da97681b8017d9c61165","date":1409030374,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(newDirectory(), indexAnalyzer, queryAnalyzer, 3, false);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(TEST_VERSION_CURRENT, newDirectory(), indexAnalyzer, queryAnalyzer, 3, false);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ec083aa3f3ecd55f91c47009d49e45553f99bd77","date":1416002645,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(newDirectory(), indexAnalyzer, queryAnalyzer, 3, false);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for apples\", results.get(0).key);\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).highlightKey);\n    suggester.close();\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(newDirectory(), indexAnalyzer, queryAnalyzer, 3, false);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).key);\n    suggester.close();\n  }\n\n","bugFix":["b7acd952b8ec320606434716bd02faaec540c885"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a56958d7f71a28824f20031ffbb2e13502a0274e","date":1425573902,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(newDirectory(), indexAnalyzer, queryAnalyzer, 3, false);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for apples\", results.get(0).key);\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).highlightKey);\n    suggester.close();\n    IOUtils.close(suggester, indexAnalyzer, queryAnalyzer);\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(newDirectory(), indexAnalyzer, queryAnalyzer, 3, false);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for apples\", results.get(0).key);\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).highlightKey);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","pathOld":"lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggesterTest#testSuggestStopFilter().mjava","sourceNew":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(newDirectory(), indexAnalyzer, queryAnalyzer, 3, false);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for apples\", results.get(0).key);\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).highlightKey);\n    suggester.close();\n    IOUtils.close(suggester, indexAnalyzer, queryAnalyzer);\n  }\n\n","sourceOld":"  public void testSuggestStopFilter() throws Exception {\n    final CharArraySet stopWords = StopFilter.makeStopSet(\"a\");\n    Analyzer indexAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new StopFilter(tokens, stopWords));\n        }\n      };\n\n    Analyzer queryAnalyzer = new Analyzer() {\n        @Override\n        protected TokenStreamComponents createComponents(String fieldName) {\n          MockTokenizer tokens = new MockTokenizer();\n          return new TokenStreamComponents(tokens,\n                                           new SuggestStopFilter(tokens, stopWords));\n        }\n      };\n\n    AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(newDirectory(), indexAnalyzer, queryAnalyzer, 3, false);\n\n    Input keys[] = new Input[] {\n      new Input(\"a bob for apples\", 10, new BytesRef(\"foobaz\")),\n    };\n\n    suggester.build(new InputArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(TestUtil.stringToCharSequence(\"a\", random()), 10, true, true);\n    assertEquals(1, results.size());\n    assertEquals(\"a bob for apples\", results.get(0).key);\n    assertEquals(\"a bob for <b>a</b>pples\", results.get(0).highlightKey);\n    suggester.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ff4227bb146f97aabae888091c19e48c88dbb0db":["4637747f71df783fc2014ef1f1e0418466e3bed6"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["6613659748fe4411a7dcf85266e55db1f95f7315","4637747f71df783fc2014ef1f1e0418466e3bed6"],"41aee74b5f91a096e3fd950f4a336bc763f0e7a7":["b7acd952b8ec320606434716bd02faaec540c885"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["41aee74b5f91a096e3fd950f4a336bc763f0e7a7"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b7acd952b8ec320606434716bd02faaec540c885"],"6549d5ea6b7b25525309b981de3ec92b4dff99d1":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"f1e7da8a91a92330e8f04b171b83e655a4a25c31":["6613659748fe4411a7dcf85266e55db1f95f7315"],"19e497fe4da591a79332da97681b8017d9c61165":["6549d5ea6b7b25525309b981de3ec92b4dff99d1"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["ec083aa3f3ecd55f91c47009d49e45553f99bd77","a56958d7f71a28824f20031ffbb2e13502a0274e"],"b7acd952b8ec320606434716bd02faaec540c885":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ec083aa3f3ecd55f91c47009d49e45553f99bd77":["19e497fe4da591a79332da97681b8017d9c61165"],"a56958d7f71a28824f20031ffbb2e13502a0274e":["ec083aa3f3ecd55f91c47009d49e45553f99bd77"],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["ff4227bb146f97aabae888091c19e48c88dbb0db"],"4637747f71df783fc2014ef1f1e0418466e3bed6":["6613659748fe4411a7dcf85266e55db1f95f7315","f1e7da8a91a92330e8f04b171b83e655a4a25c31"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a56958d7f71a28824f20031ffbb2e13502a0274e"]},"commit2Childs":{"ff4227bb146f97aabae888091c19e48c88dbb0db":["5cdab62f058ea765dd33deb05b4f19b7d626c801"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"41aee74b5f91a096e3fd950f4a336bc763f0e7a7":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["6613659748fe4411a7dcf85266e55db1f95f7315"],"6613659748fe4411a7dcf85266e55db1f95f7315":["96ea64d994d340044e0d57aeb6a5871539d10ca5","f1e7da8a91a92330e8f04b171b83e655a4a25c31","4637747f71df783fc2014ef1f1e0418466e3bed6"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"6549d5ea6b7b25525309b981de3ec92b4dff99d1":["19e497fe4da591a79332da97681b8017d9c61165"],"f1e7da8a91a92330e8f04b171b83e655a4a25c31":["4637747f71df783fc2014ef1f1e0418466e3bed6"],"19e497fe4da591a79332da97681b8017d9c61165":["ec083aa3f3ecd55f91c47009d49e45553f99bd77"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"b7acd952b8ec320606434716bd02faaec540c885":["41aee74b5f91a096e3fd950f4a336bc763f0e7a7","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","b7acd952b8ec320606434716bd02faaec540c885"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["6549d5ea6b7b25525309b981de3ec92b4dff99d1"],"ec083aa3f3ecd55f91c47009d49e45553f99bd77":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","a56958d7f71a28824f20031ffbb2e13502a0274e"],"4637747f71df783fc2014ef1f1e0418466e3bed6":["ff4227bb146f97aabae888091c19e48c88dbb0db","96ea64d994d340044e0d57aeb6a5871539d10ca5"],"a56958d7f71a28824f20031ffbb2e13502a0274e":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"5cdab62f058ea765dd33deb05b4f19b7d626c801":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}