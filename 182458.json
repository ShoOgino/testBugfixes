{"path":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","sourceNew":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random.nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedDeletesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedDeletesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random.nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedDeletesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedDeletesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","sourceNew":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedDeletesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedDeletesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random.nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedDeletesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedDeletesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0791b41f65aecff2e75db0c1ebf95d745a5ab1b1","date":1338332414,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","sourceNew":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedDeletesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedDeletesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedDeletesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedDeletesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","sourceNew":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedDeletesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedDeletesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","sourceNew":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.shutdown();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","sourceNew":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT,\n        new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.shutdown();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","sourceNew":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    writer.flush(false, false);\n\n    // deletes are now resolved on flush, so there shouldn't be\n    // any deletes after flush\n    assertFalse(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","sourceNew":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    writer.flush(false, false);\n\n    // deletes are now resolved on flush, so there shouldn't be\n    // any deletes after flush\n    assertFalse(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","sourceNew":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    writer.flush(false, false);\n\n    // deletes are now resolved on flush, so there shouldn't be\n    // any deletes after flush\n    assertFalse(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    // flushing without applying deletes means\n    // there will still be deletes in the segment infos\n    writer.flush(false, false);\n    assertTrue(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"631ea3d1607299c59f33edef140ffc19a81f07a0","date":1532450367,"type":3,"author":"Nhat Nguyen","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","sourceNew":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.listOfSegmentCommitInfos().size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.listOfSegmentCommitInfos().size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    writer.flush(false, false);\n\n    // deletes are now resolved on flush, so there shouldn't be\n    // any deletes after flush\n    assertFalse(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.listOfSegmentCommitInfos().size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.segmentInfos.size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.segmentInfos.size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    writer.flush(false, false);\n\n    // deletes are now resolved on flush, so there shouldn't be\n    // any deletes after flush\n    assertFalse(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.segmentInfos.size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d77dafd89756a5161d244985903e3487ca109182","date":1548679743,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","sourceNew":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new ByteBuffersDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.listOfSegmentCommitInfos().size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.listOfSegmentCommitInfos().size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    writer.flush(false, false);\n\n    // deletes are now resolved on flush, so there shouldn't be\n    // any deletes after flush\n    assertFalse(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.listOfSegmentCommitInfos().size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new RAMDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.listOfSegmentCommitInfos().size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.listOfSegmentCommitInfos().size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    writer.flush(false, false);\n\n    // deletes are now resolved on flush, so there shouldn't be\n    // any deletes after flush\n    assertFalse(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.listOfSegmentCommitInfos().size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2","date":1588002560,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes#testDeletes1().mjava","sourceNew":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new ByteBuffersDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.cloneSegmentInfos().size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.cloneSegmentInfos().size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    writer.flush(false, false);\n\n    // deletes are now resolved on flush, so there shouldn't be\n    // any deletes after flush\n    assertFalse(writer.hasChangesInRam());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.hasChangesInRam());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.cloneSegmentInfos().size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDeletes1() throws Exception {\n    //IndexWriter.debug2 = System.out;\n    Directory dir = new MockDirectoryWrapper(new Random(random().nextLong()), new ByteBuffersDirectory());\n    IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    iwc.setMaxBufferedDocs(5000);\n    iwc.setRAMBufferSizeMB(100);\n    RangeMergePolicy fsmp = new RangeMergePolicy(false);\n    iwc.setMergePolicy(fsmp);\n    IndexWriter writer = new IndexWriter(dir, iwc);\n    for (int x = 0; x < 5; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"1\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit1\");\n    writer.commit();\n    assertEquals(1, writer.listOfSegmentCommitInfos().size());\n    for (int x = 5; x < 10; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"2\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    //System.out.println(\"commit2\");\n    writer.commit();\n    assertEquals(2, writer.listOfSegmentCommitInfos().size());\n\n    for (int x = 10; x < 15; x++) {\n      writer.addDocument(DocHelper.createDocument(x, \"3\", 2));\n      //System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n\n    writer.deleteDocuments(new Term(\"id\", \"1\"));\n\n    writer.deleteDocuments(new Term(\"id\", \"11\"));\n\n    writer.flush(false, false);\n\n    // deletes are now resolved on flush, so there shouldn't be\n    // any deletes after flush\n    assertFalse(writer.bufferedUpdatesStream.any());\n\n    // get reader flushes pending deletes\n    // so there should not be anymore\n    IndexReader r1 = writer.getReader();\n    assertFalse(writer.bufferedUpdatesStream.any());\n    r1.close();\n\n    // delete id:2 from the first segment\n    // merge segments 0 and 1\n    // which should apply the delete id:2\n    writer.deleteDocuments(new Term(\"id\", \"2\"));\n    writer.flush(false, false);\n    fsmp = (RangeMergePolicy) writer.getConfig().getMergePolicy();\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    writer.maybeMerge();\n\n    assertEquals(2, writer.listOfSegmentCommitInfos().size());\n\n    // id:2 shouldn't exist anymore because\n    // it's been applied in the merge and now it's gone\n    IndexReader r2 = writer.getReader();\n    int[] id2docs = toDocsArray(new Term(\"id\", \"2\"), null, r2);\n    assertTrue(id2docs == null);\n    r2.close();\n\n    /**\n    // added docs are in the ram buffer\n    for (int x = 15; x < 20; x++) {\n      writer.addDocument(TestIndexWriterReader.createDocument(x, \"4\", 2));\n      System.out.println(\"numRamDocs(\" + x + \")\" + writer.numRamDocs());\n    }\n    assertTrue(writer.numRamDocs() > 0);\n    // delete from the ram buffer\n    writer.deleteDocuments(new Term(\"id\", Integer.toString(13)));\n\n    Term id3 = new Term(\"id\", Integer.toString(3));\n\n    // delete from the 1st segment\n    writer.deleteDocuments(id3);\n\n    assertTrue(writer.numRamDocs() > 0);\n\n    //System.out\n    //    .println(\"segdels1:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    // we cause a merge to happen\n    fsmp.doMerge = true;\n    fsmp.start = 0;\n    fsmp.length = 2;\n    System.out.println(\"maybeMerge \"+writer.segmentInfos);\n\n    SegmentInfo info0 = writer.segmentInfos.info(0);\n    SegmentInfo info1 = writer.segmentInfos.info(1);\n\n    writer.maybeMerge();\n    System.out.println(\"maybeMerge after \"+writer.segmentInfos);\n    // there should be docs in RAM\n    assertTrue(writer.numRamDocs() > 0);\n\n    // assert we've merged the 1 and 2 segments\n    // and still have a segment leftover == 2\n    assertEquals(2, writer.segmentInfos.size());\n    assertFalse(segThere(info0, writer.segmentInfos));\n    assertFalse(segThere(info1, writer.segmentInfos));\n\n    //System.out.println(\"segdels2:\" + writer.docWriter.deletesToString());\n\n    //assertTrue(writer.docWriter.segmentDeletes.size() > 0);\n\n    IndexReader r = writer.getReader();\n    IndexReader r1 = r.getSequentialSubReaders()[0];\n    printDelDocs(r1.getLiveDocs());\n    int[] docs = toDocsArray(id3, null, r);\n    System.out.println(\"id3 docs:\"+Arrays.toString(docs));\n    // there shouldn't be any docs for id:3\n    assertTrue(docs == null);\n    r.close();\n\n    part2(writer, fsmp);\n    **/\n    // System.out.println(\"segdels2:\"+writer.docWriter.segmentDeletes.toString());\n    //System.out.println(\"close\");\n    writer.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["0791b41f65aecff2e75db0c1ebf95d745a5ab1b1"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"d77dafd89756a5161d244985903e3487ca109182":["631ea3d1607299c59f33edef140ffc19a81f07a0"],"0791b41f65aecff2e75db0c1ebf95d745a5ab1b1":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["d0ef034a4f10871667ae75181537775ddcf8ade4","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["d0ef034a4f10871667ae75181537775ddcf8ade4","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"631ea3d1607299c59f33edef140ffc19a81f07a0":["28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["d77dafd89756a5161d244985903e3487ca109182"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"]},"commit2Childs":{"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"0791b41f65aecff2e75db0c1ebf95d745a5ab1b1":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"d77dafd89756a5161d244985903e3487ca109182":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["631ea3d1607299c59f33edef140ffc19a81f07a0"],"631ea3d1607299c59f33edef140ffc19a81f07a0":["d77dafd89756a5161d244985903e3487ca109182"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["0791b41f65aecff2e75db0c1ebf95d745a5ab1b1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}