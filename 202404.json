{"path":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f08557cdb6c60ac7b88a9342c983a20cd236e74f","date":1330954480,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != termPositions.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d153abcf92dc5329d98571a8c3035df9bd80648","date":1337702630,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"76fa9933adb0556e752e8af9734c4d0ae14622ff","date":1339178321,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new Payload(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newField(\"f1\", \"a 5 a a\", TextField.TYPE_STORED));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"322360ac5185a8446d3e0b530b2068bef67cd3d5","date":1343669494,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"), false);\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552","date":1344797146,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","date":1344867506,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertEquals(true, termPositions.hasPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertEquals(false, termPositions.hasPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a45bec74b98f6fc05f52770cfb425739e6563960","date":1375119292,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","date":1376366778,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentInfoPerCommit info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.shutdown();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.shutdown();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.shutdown();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.shutdown();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f4464508ee83288c8c4585b533f9faaa93aa314","date":1435240759,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiFields.getTermPositionsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31741cf1390044e38a2ec3127cf302ba841bfd75","date":1491292636,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, Version.LATEST.major, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiFields.getTermPositionsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiFields.getTermPositionsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92212fd254551a0b1156aafc3a1a6ed1a43932ad","date":1491296431,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, Version.LATEST.major, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiFields.getTermPositionsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiFields.getTermPositionsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, Version.LATEST.major, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiTerms.getTermPostingsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, Version.LATEST.major, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiFields.getTermPositionsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790693f23f4e88a59fbb25e47cc25f6d493b03cb","date":1553077690,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, Version.LATEST.major, false, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiTerms.getTermPostingsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, Version.LATEST.major, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiTerms.getTermPostingsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"763da4a9605e47013078edc323b9d4b608f0f9e0","date":1555353576,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, Version.LATEST.major, false, newIOContext(random()), Collections.emptyMap());\n\n    PostingsEnum termPositions = MultiTerms.getTermPostingsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, Version.LATEST.major, false, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiTerms.getTermPostingsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4e83191a3e02851a0b67e5335e6922f3e9ea86d","date":1583489709,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, Version.LATEST.major, false, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiTerms.getTermPostingsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, Version.LATEST.major, false, newIOContext(random()), Collections.emptyMap());\n\n    PostingsEnum termPositions = MultiTerms.getTermPostingsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bec68e7c41fed133827595747d853cad504e481e","date":1583501052,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter#testTokenReuse().mjava","sourceNew":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, Version.LATEST.major, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiTerms.getTermPostingsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","sourceOld":"  public void testTokenReuse() throws IOException {\n    Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        return new TokenStreamComponents(tokenizer, new TokenFilter(tokenizer) {\n          boolean first = true;\n          AttributeSource.State state;\n\n          @Override\n          public boolean incrementToken() throws IOException {\n            if (state != null) {\n              restoreState(state);\n              payloadAtt.setPayload(null);\n              posIncrAtt.setPositionIncrement(0);\n              termAtt.setEmpty().append(\"b\");\n              state = null;\n              return true;\n            }\n\n            boolean hasNext = input.incrementToken();\n            if (!hasNext) return false;\n            if (Character.isDigit(termAtt.buffer()[0])) {\n              posIncrAtt.setPositionIncrement(termAtt.buffer()[0] - '0');\n            }\n            if (first) {\n              // set payload on first position only\n              payloadAtt.setPayload(new BytesRef(new byte[]{100}));\n              first = false;\n            }\n\n            // index a \"synonym\" for every token\n            state = captureState();\n            return true;\n\n          }\n\n          @Override\n          public void reset() throws IOException {\n            super.reset();\n            first = true;\n            state = null;\n          }\n\n          final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);\n          final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);\n          final PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);\n        });\n      }\n    };\n\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(analyzer));\n\n    Document doc = new Document();\n    doc.add(newTextField(\"f1\", \"a 5 a a\", Field.Store.YES));\n\n    writer.addDocument(doc);\n    writer.commit();\n    SegmentCommitInfo info = writer.newestSegment();\n    writer.close();\n    SegmentReader reader = new SegmentReader(info, Version.LATEST.major, false, newIOContext(random()));\n\n    PostingsEnum termPositions = MultiTerms.getTermPostingsEnum(reader, \"f1\", new BytesRef(\"a\"));\n    assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);\n    int freq = termPositions.freq();\n    assertEquals(3, freq);\n    assertEquals(0, termPositions.nextPosition());\n    assertNotNull(termPositions.getPayload());\n    assertEquals(6, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    assertEquals(7, termPositions.nextPosition());\n    assertNull(termPositions.getPayload());\n    reader.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["a45bec74b98f6fc05f52770cfb425739e6563960"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["790693f23f4e88a59fbb25e47cc25f6d493b03cb"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["76fa9933adb0556e752e8af9734c4d0ae14622ff"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["322360ac5185a8446d3e0b530b2068bef67cd3d5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"76fa9933adb0556e752e8af9734c4d0ae14622ff":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":["d6f074e73200c07d54f242d3880a8da5a35ff97b","2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"a45bec74b98f6fc05f52770cfb425739e6563960":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"bec68e7c41fed133827595747d853cad504e481e":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"51f5280f31484820499077f41fcdfe92d527d9dc":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["51f5280f31484820499077f41fcdfe92d527d9dc"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["629c38c4ae4e303d0617e05fbfe508140b32f0a3","9d153abcf92dc5329d98571a8c3035df9bd80648"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["04f07771a2a7dd3a395700665ed839c3dae2def2","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["04f07771a2a7dd3a395700665ed839c3dae2def2","322360ac5185a8446d3e0b530b2068bef67cd3d5"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["31741cf1390044e38a2ec3127cf302ba841bfd75"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["bec68e7c41fed133827595747d853cad504e481e"]},"commit2Childs":{"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","f08557cdb6c60ac7b88a9342c983a20cd236e74f"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["bec68e7c41fed133827595747d853cad504e481e"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["322360ac5185a8446d3e0b530b2068bef67cd3d5","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":[],"2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552":["c7869f64c874ebf7f317d22c00baf2b6857797a6","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","a45bec74b98f6fc05f52770cfb425739e6563960"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee":[],"76fa9933adb0556e752e8af9734c4d0ae14622ff":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":[],"322360ac5185a8446d3e0b530b2068bef67cd3d5":["2b4c7e630332c5e9e7d7a70f4ace4b3ffd3fc552","b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9":[],"790693f23f4e88a59fbb25e47cc25f6d493b03cb":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"a45bec74b98f6fc05f52770cfb425739e6563960":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"f08557cdb6c60ac7b88a9342c983a20cd236e74f":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"9d153abcf92dc5329d98571a8c3035df9bd80648":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"bec68e7c41fed133827595747d853cad504e481e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"51f5280f31484820499077f41fcdfe92d527d9dc":["0f4464508ee83288c8c4585b533f9faaa93aa314"],"0f4464508ee83288c8c4585b533f9faaa93aa314":["31741cf1390044e38a2ec3127cf302ba841bfd75","92212fd254551a0b1156aafc3a1a6ed1a43932ad"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["76fa9933adb0556e752e8af9734c4d0ae14622ff"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["c7869f64c874ebf7f317d22c00baf2b6857797a6"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["790693f23f4e88a59fbb25e47cc25f6d493b03cb"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["9d153abcf92dc5329d98571a8c3035df9bd80648","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","c7869f64c874ebf7f317d22c00baf2b6857797a6","8989a9672fc1bb2d9a549a4f9005a7d0b0d728ee","92212fd254551a0b1156aafc3a1a6ed1a43932ad","d0ba34ddeec9e4ab657150c29a5614a7bfbb53c9","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}