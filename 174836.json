{"path":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","commits":[{"id":"120dcb9902dc31423bf7d82c10c5439b88325390","date":1442349554,"type":0,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"/dev/null","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.updateClusterState(); // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName);\n    assertDocExists(leaderSolr, testCollectionName, \"3\");\n\n    for (Replica notLeader : notLeaders) {\n      HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName);\n      assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["344b0840364d990b29b97467bfcc766ff8325d11"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f","date":1457343183,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName);\n    assertDocExists(leaderSolr, testCollectionName, \"3\");\n\n    for (Replica notLeader : notLeaders) {\n      HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName);\n      assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.updateClusterState(); // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName);\n    assertDocExists(leaderSolr, testCollectionName, \"3\");\n\n    for (Replica notLeader : notLeaders) {\n      HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName);\n      assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"859081acf00749f5dd462772c571d611d4a4d2db","date":1459527719,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName);\n    assertDocExists(leaderSolr, testCollectionName, \"3\");\n\n    for (Replica notLeader : notLeaders) {\n      HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName);\n      assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"197bbedf08450ade98a11f4a0001448059666bec","date":1498534625,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","date":1498540685,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"344b0840364d990b29b97467bfcc766ff8325d11","date":1501574100,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getCollection(testCollectionName).getActiveSlices();\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":["120dcb9902dc31423bf7d82c10c5439b88325390"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","date":1502192746,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getCollection(testCollectionName).getActiveSlices();\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getActiveSlices(testCollectionName);\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f09dfe4eae83c6f3ce87c6267cb774e4da0a2d73","date":1504185139,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    log.info(\"partitioning replica :  \" + notLeaders.get(0));\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getCollection(testCollectionName).getActiveSlices();\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n    log.info(\"un-partitioning replica :  \" + notLeaders.get(0));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getCollection(testCollectionName).getActiveSlices();\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a7809d1d753b67f48b1a706e17034bf8b624ea3","date":1504366927,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    log.info(\"partitioning replica :  \" + notLeaders.get(0));\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getCollection(testCollectionName).getActiveSlices();\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n    log.info(\"un-partitioning replica :  \" + notLeaders.get(0));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getCollection(testCollectionName).getActiveSlices();\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"104a3f62ee393d48b5596de76ed4d9a4e0ea6de7","date":1504848000,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    log.info(\"partitioning replica :  \" + notLeaders.get(0));\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getCollection(testCollectionName).getActiveSlices();\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n    log.info(\"un-partitioning replica :  \" + notLeaders.get(0));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getCollection(testCollectionName).getActiveSlices();\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84f20f331d8001864545c7021812d8c6509c7593","date":1517216128,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    log.info(\"partitioning replica :  \" + notLeaders.get(0));\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n\n    proxy0.close();\n    // leader still can connect to replica 2, by closing leaderProxy, replica 1 can not do recovery\n    leaderProxy.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2, leaderJetty);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n    try (ZkShardTerms zkShardTerms = new ZkShardTerms(testCollectionName, \"shard1\", cloudClient.getZkStateReader().getZkClient())) {\n      assertFalse(zkShardTerms.canBecomeLeader(notLeaders.get(0).getName()));\n    }\n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy0.reopen();\n    leaderProxy.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n    leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    leaderProxy.close();\n\n    achievedRf = sendDoc(3, 2, leaderJetty);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n    leaderProxy.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    log.info(\"partitioning replica :  \" + notLeaders.get(0));\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n\n    proxy0.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replica is DOWN\n    ZkStateReader zkr = cloudClient.getZkStateReader();\n    zkr.forceUpdateCollection(testCollectionName);; // force the state to be fresh\n    ClusterState cs = zkr.getClusterState();\n    Collection<Slice> slices = cs.getCollection(testCollectionName).getActiveSlices();\n    Slice slice = slices.iterator().next();\n    Replica partitionedReplica = slice.getReplica(notLeaders.get(0).getName());\n    assertEquals(\"The partitioned replica did not get marked down\",\n        Replica.State.DOWN.toString(), partitionedReplica.getStr(ZkStateReader.STATE_PROP));\n    log.info(\"un-partitioning replica :  \" + notLeaders.get(0));\n\n    proxy0.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"67144a5cdd5ecbe2f8ca846956214f360fec12f5","date":1521599751,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    // term of the core still be watched even when the core is reloaded\n    CollectionAdminRequest.reloadCollection(testCollectionName).process(cloudClient);\n\n    sendDoc(1, 2);\n\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    log.info(\"partitioning replica :  \" + notLeaders.get(0));\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n\n    proxy0.close();\n    // leader still can connect to replica 2, by closing leaderProxy, replica 1 can not do recovery\n    leaderProxy.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2, leaderJetty);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n    try (ZkShardTerms zkShardTerms = new ZkShardTerms(testCollectionName, \"shard1\", cloudClient.getZkStateReader().getZkClient())) {\n      assertFalse(zkShardTerms.canBecomeLeader(notLeaders.get(0).getName()));\n    }\n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy0.reopen();\n    leaderProxy.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n    leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    leaderProxy.close();\n\n    achievedRf = sendDoc(3, 2, leaderJetty);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n    leaderProxy.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    log.info(\"partitioning replica :  \" + notLeaders.get(0));\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n\n    proxy0.close();\n    // leader still can connect to replica 2, by closing leaderProxy, replica 1 can not do recovery\n    leaderProxy.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2, leaderJetty);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n    try (ZkShardTerms zkShardTerms = new ZkShardTerms(testCollectionName, \"shard1\", cloudClient.getZkStateReader().getZkClient())) {\n      assertFalse(zkShardTerms.canBecomeLeader(notLeaders.get(0).getName()));\n    }\n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy0.reopen();\n    leaderProxy.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n    leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    leaderProxy.close();\n\n    achievedRf = sendDoc(3, 2, leaderJetty);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n    leaderProxy.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","date":1521731438,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    // term of the core still be watched even when the core is reloaded\n    CollectionAdminRequest.reloadCollection(testCollectionName).process(cloudClient);\n\n    sendDoc(1, 2);\n\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    log.info(\"partitioning replica :  \" + notLeaders.get(0));\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n\n    proxy0.close();\n    // leader still can connect to replica 2, by closing leaderProxy, replica 1 can not do recovery\n    leaderProxy.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2, leaderJetty);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n    try (ZkShardTerms zkShardTerms = new ZkShardTerms(testCollectionName, \"shard1\", cloudClient.getZkStateReader().getZkClient())) {\n      assertFalse(zkShardTerms.canBecomeLeader(notLeaders.get(0).getName()));\n    }\n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy0.reopen();\n    leaderProxy.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n    leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    leaderProxy.close();\n\n    achievedRf = sendDoc(3, 2, leaderJetty);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n    leaderProxy.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    sendDoc(1, 2);\n\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    log.info(\"partitioning replica :  \" + notLeaders.get(0));\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n\n    proxy0.close();\n    // leader still can connect to replica 2, by closing leaderProxy, replica 1 can not do recovery\n    leaderProxy.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2, leaderJetty);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n    try (ZkShardTerms zkShardTerms = new ZkShardTerms(testCollectionName, \"shard1\", cloudClient.getZkStateReader().getZkClient())) {\n      assertFalse(zkShardTerms.canBecomeLeader(notLeaders.get(0).getName()));\n    }\n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy0.reopen();\n    leaderProxy.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n    leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    leaderProxy.close();\n\n    achievedRf = sendDoc(3, 2, leaderJetty);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n    leaderProxy.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43f5f8344e80b4bfb2069917069430266753d2f0","date":1538584815,"type":4,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testMinRf().mjava","sourceNew":null,"sourceOld":"  protected void testMinRf() throws Exception {\n    // create a collection that has 1 shard and 3 replicas\n    String testCollectionName = \"collMinRf_1x3\";\n    createCollection(testCollectionName, \"conf1\", 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n\n    // term of the core still be watched even when the core is reloaded\n    CollectionAdminRequest.reloadCollection(testCollectionName).process(cloudClient);\n\n    sendDoc(1, 2);\n\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n    List<Replica> notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 non-leader replicas for collection \" + testCollectionName\n            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n            + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 1);\n\n    // Now introduce a network partition between the leader and 1 replica, so a minRf of 2 is still achieved\n    log.info(\"partitioning replica :  \" + notLeaders.get(0));\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n\n    proxy0.close();\n    // leader still can connect to replica 2, by closing leaderProxy, replica 1 can not do recovery\n    leaderProxy.close();\n\n    // indexing during a partition\n    int achievedRf = sendDoc(2, 2, leaderJetty);\n    assertEquals(\"Unexpected achieved replication factor\", 2, achievedRf);\n    try (ZkShardTerms zkShardTerms = new ZkShardTerms(testCollectionName, \"shard1\", cloudClient.getZkStateReader().getZkClient())) {\n      assertFalse(zkShardTerms.canBecomeLeader(notLeaders.get(0).getName()));\n    }\n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy0.reopen();\n    leaderProxy.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Since minRf is achieved, we expect recovery, so we expect seeing 2 documents\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 2);\n\n    // Now introduce a network partition between the leader and both of its replicas, so a minRf of 2 is NOT achieved\n    proxy0 = getProxyForReplica(notLeaders.get(0));\n    proxy0.close();\n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n    leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    leaderProxy.close();\n\n    achievedRf = sendDoc(3, 2, leaderJetty);\n    assertEquals(\"Unexpected achieved replication factor\", 1, achievedRf);\n\n    Thread.sleep(sleepMsBeforeHealPartition);\n\n    // Verify that the partitioned replicas are NOT DOWN since minRf wasn't achieved\n    ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, 1);\n\n    proxy0.reopen();\n    proxy1.reopen();\n    leaderProxy.reopen();\n\n    notLeaders =\n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n\n    // Check that doc 3 is on the leader but not on the notLeaders\n    Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, \"shard1\", 10000);\n    try (HttpSolrClient leaderSolr = getHttpSolrClient(leader, testCollectionName)) {\n      assertDocExists(leaderSolr, testCollectionName, \"3\");\n    }\n\n    for (Replica notLeader : notLeaders) {\n      try (HttpSolrClient notLeaderSolr = getHttpSolrClient(notLeader, testCollectionName)) {\n        assertDocNotExists(notLeaderSolr, testCollectionName, \"3\");\n      }\n    }\n\n    // Retry sending doc 3\n    achievedRf = sendDoc(3, 2);\n    assertEquals(\"Unexpected achieved replication factor\", 3, achievedRf);\n\n    // Now doc 3 should be on all replicas\n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 3);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["84f20f331d8001864545c7021812d8c6509c7593","67144a5cdd5ecbe2f8ca846956214f360fec12f5"],"43f5f8344e80b4bfb2069917069430266753d2f0":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"197bbedf08450ade98a11f4a0001448059666bec":["859081acf00749f5dd462772c571d611d4a4d2db"],"344b0840364d990b29b97467bfcc766ff8325d11":["28288370235ed02234a64753cdbf0c6ec096304a"],"84f20f331d8001864545c7021812d8c6509c7593":["104a3f62ee393d48b5596de76ed4d9a4e0ea6de7"],"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f":["120dcb9902dc31423bf7d82c10c5439b88325390"],"28288370235ed02234a64753cdbf0c6ec096304a":["859081acf00749f5dd462772c571d611d4a4d2db","197bbedf08450ade98a11f4a0001448059666bec"],"859081acf00749f5dd462772c571d611d4a4d2db":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f"],"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac":["fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","344b0840364d990b29b97467bfcc766ff8325d11"],"104a3f62ee393d48b5596de76ed4d9a4e0ea6de7":["344b0840364d990b29b97467bfcc766ff8325d11","f09dfe4eae83c6f3ce87c6267cb774e4da0a2d73"],"3a7809d1d753b67f48b1a706e17034bf8b624ea3":["7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","f09dfe4eae83c6f3ce87c6267cb774e4da0a2d73"],"f09dfe4eae83c6f3ce87c6267cb774e4da0a2d73":["344b0840364d990b29b97467bfcc766ff8325d11"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"67144a5cdd5ecbe2f8ca846956214f360fec12f5":["84f20f331d8001864545c7021812d8c6509c7593"],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":["859081acf00749f5dd462772c571d611d4a4d2db","197bbedf08450ade98a11f4a0001448059666bec"],"120dcb9902dc31423bf7d82c10c5439b88325390":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["43f5f8344e80b4bfb2069917069430266753d2f0"]},"commit2Childs":{"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["43f5f8344e80b4bfb2069917069430266753d2f0"],"43f5f8344e80b4bfb2069917069430266753d2f0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"197bbedf08450ade98a11f4a0001448059666bec":["28288370235ed02234a64753cdbf0c6ec096304a","fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4"],"344b0840364d990b29b97467bfcc766ff8325d11":["7a23cf16c8fa265dc0a564adcabb55e3f054e0ac","104a3f62ee393d48b5596de76ed4d9a4e0ea6de7","f09dfe4eae83c6f3ce87c6267cb774e4da0a2d73"],"84f20f331d8001864545c7021812d8c6509c7593":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","67144a5cdd5ecbe2f8ca846956214f360fec12f5"],"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f":["859081acf00749f5dd462772c571d611d4a4d2db"],"28288370235ed02234a64753cdbf0c6ec096304a":["344b0840364d990b29b97467bfcc766ff8325d11"],"859081acf00749f5dd462772c571d611d4a4d2db":["197bbedf08450ade98a11f4a0001448059666bec","28288370235ed02234a64753cdbf0c6ec096304a","fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4"],"7a23cf16c8fa265dc0a564adcabb55e3f054e0ac":["3a7809d1d753b67f48b1a706e17034bf8b624ea3"],"104a3f62ee393d48b5596de76ed4d9a4e0ea6de7":["84f20f331d8001864545c7021812d8c6509c7593"],"3a7809d1d753b67f48b1a706e17034bf8b624ea3":[],"f09dfe4eae83c6f3ce87c6267cb774e4da0a2d73":["104a3f62ee393d48b5596de76ed4d9a4e0ea6de7","3a7809d1d753b67f48b1a706e17034bf8b624ea3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["120dcb9902dc31423bf7d82c10c5439b88325390"],"67144a5cdd5ecbe2f8ca846956214f360fec12f5":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":["7a23cf16c8fa265dc0a564adcabb55e3f054e0ac"],"120dcb9902dc31423bf7d82c10c5439b88325390":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["3a7809d1d753b67f48b1a706e17034bf8b624ea3","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}