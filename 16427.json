{"path":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","commits":[{"id":"e7fb6d70db034a5456ae560175dd1b821eea9ff4","date":1066759157,"type":0,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"/dev/null","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(directory, mergedName, false);\n\n    if (segmentInfos.size() == 1)                 // add existing index, if any\n      merger.add(new SegmentReader(segmentInfos.info(0)));\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(\"commit.lock\")) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    return null;\n\t  }\n\t}.run();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["dbb18b6a222f2507f22fab7cc7eed06658d59772","1b54a9bc667895a2095a886184bf69a3179e63df","d72db039743bd6a2da9be6306f57c71654ca1bf6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"683a9c890d06f2af26082bbd5a71c6b9954f9ae1","date":1077214323,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(directory, mergedName, false);\n\n    if (segmentInfos.size() == 1)                 // add existing index, if any\n      merger.add(new SegmentReader(segmentInfos.info(0)));\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(\"commit.lock\"), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    return null;\n\t  }\n\t}.run();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(directory, mergedName, false);\n\n    if (segmentInfos.size() == 1)                 // add existing index, if any\n      merger.add(new SegmentReader(segmentInfos.info(0)));\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(\"commit.lock\")) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    return null;\n\t  }\n\t}.run();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"42a7d0b384f0022c5a29e562b809ebf73991d7e6","date":1082489615,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(directory, mergedName, false);\n\n    if (segmentInfos.size() == 1)                 // add existing index, if any\n      merger.add(new SegmentReader(segmentInfos.info(0)));\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(\"commit.lock\"), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    return null;\n\t  }\n\t}.run();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(directory, mergedName, false);\n\n    if (segmentInfos.size() == 1)                 // add existing index, if any\n      merger.add(new SegmentReader(segmentInfos.info(0)));\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(\"commit.lock\"), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    return null;\n\t  }\n\t}.run();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28f95f0e6ecb2efc0a13d0131c57f7530f38c705","date":1091794884,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(directory, mergedName, useCompoundFile);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = new SegmentReader(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(\"commit.lock\"), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\t    return null;\n\t  }\n\t}.run();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(directory, mergedName, false);\n\n    if (segmentInfos.size() == 1)                 // add existing index, if any\n      merger.add(new SegmentReader(segmentInfos.info(0)));\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(\"commit.lock\"), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    return null;\n\t  }\n\t}.run();\n    }\n  }\n\n","bugFix":null,"bugIntro":["1507a324c1f939ed71e01297733a49b9c36e5688","d72db039743bd6a2da9be6306f57c71654ca1bf6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"79fc67d5cdece593cd3b3b6c7ef195ee2625522c","date":1091970333,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    String mergedName = newSegmentName();\n    final SegmentMerger merger = new SegmentMerger(directory, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = new SegmentReader(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(\"commit.lock\"), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\t    if(useCompoundFile)\n\t        merger.createCompoundFile();\n\t    return null;\n\t  }\n\t}.run();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(directory, mergedName, useCompoundFile);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = new SegmentReader(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(\"commit.lock\"), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\t    return null;\n\t  }\n\t}.run();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6263f219dcdfc6e861ecffaecf5e1e195f1aaaa7","date":1092245915,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(directory, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = new SegmentReader(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\t    return null;\n\t  }\n\t}.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    String mergedName = newSegmentName();\n    final SegmentMerger merger = new SegmentMerger(directory, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = new SegmentReader(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(\"commit.lock\"), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\t    if(useCompoundFile)\n\t        merger.createCompoundFile();\n\t    return null;\n\t  }\n\t}.run();\n    }\n  }\n\n","bugFix":null,"bugIntro":["1507a324c1f939ed71e01297733a49b9c36e5688"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6e5d88b55f1b57feab6da94a5c635a224539bd2a","date":1095877947,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(directory, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\t    return null;\n\t  }\n\t}.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(directory, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = new SegmentReader(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\t    return null;\n\t  }\n\t}.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3afaca6e0770734b01e3bc663bec3ffa71b6f87b","date":1110394706,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\t    return null;\n\t  }\n\t}.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(directory, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\t    return null;\n\t  }\n\t}.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"14e0bcefcbabf34d28e4fd7060dabc9a9fe37cea","date":1142635892,"type":3,"author":"Daniel Naber","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\t    return null;\n\t  }\n\t}.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\t    return null;\n\t  }\n\t}.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), COMMIT_LOCK_TIMEOUT) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8c3ae2a9b6bdc01b2b6665e79fdfc71ae4a8b1ca","date":1148660052,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    return null;\n\t  }\n\t}.run();\n    }\n    \n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment \n      deleteFiles(filesToDelete);   \n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\t    return null;\n\t  }\n\t}.run();\n    }\n    \n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            // delete now unused files of segment \n            deleteFiles(filesToDelete);   \n            return null;\n          }\n        }.run();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["1507a324c1f939ed71e01297733a49b9c36e5688"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1507a324c1f939ed71e01297733a49b9c36e5688","date":1155783141,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n\n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    return null;\n\t  }\n\t}.run();\n    }\n\n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment\n      deleteFiles(filesToDelete);\n    }\n\n    // testInvariants();\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n      \n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n    \n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    return null;\n\t  }\n\t}.run();\n    }\n    \n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment \n      deleteFiles(filesToDelete);   \n    }\n  }\n\n","bugFix":["8c3ae2a9b6bdc01b2b6665e79fdfc71ae4a8b1ca","28f95f0e6ecb2efc0a13d0131c57f7530f38c705","6263f219dcdfc6e861ecffaecf5e1e195f1aaaa7"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d7052f725a053aa55424f966831826f61b798bf1","date":1158258681,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n\n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    return null;\n\t  }\n\t}.run();\n    }\n\n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment\n      deleteFiles(filesToDelete);\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n\n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    return null;\n\t  }\n\t}.run();\n    }\n\n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment\n      deleteFiles(filesToDelete);\n    }\n\n    // testInvariants();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"284c1d3c8b19931bf6f312fae7470487f5d9e580","date":1163805527,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    SegmentInfo info = new SegmentInfo(mergedName, docCount, directory, false);\n    segmentInfos.addElement(info);\n\n    if(sReader != null)\n        sReader.close();\n\n    String segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n    segmentInfos.write(directory);         // commit changes\n\n    deleter.deleteFile(segmentsInfosFileName);    // delete old segments_N file\n    deleter.deleteSegments(segmentsToDelete);     // delete now-unused segments\n\n    if (useCompoundFile) {\n      Vector filesToDelete = merger.createCompoundFile(mergedName + \".cfs\");\n      segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n      info.setUseCompoundFile(true);\n      segmentInfos.write(directory);     // commit again so readers know we've switched this segment to a compound file\n\n      deleter.deleteFile(segmentsInfosFileName);  // delete old segments_N file\n      deleter.deleteFiles(filesToDelete); // delete now unused files of segment \n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    segmentInfos.addElement(new SegmentInfo(mergedName, docCount, directory));\n\n    if(sReader != null)\n        sReader.close();\n\n    synchronized (directory) {\t\t\t  // in- & inter-process sync\n      new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n\t  public Object doBody() throws IOException {\n\t    segmentInfos.write(directory);\t  // commit changes\n\t    return null;\n\t  }\n\t}.run();\n    }\n\n    deleteSegments(segmentsToDelete);  // delete now-unused segments\n\n    if (useCompoundFile) {\n      final Vector filesToDelete = merger.createCompoundFile(mergedName + \".tmp\");\n      synchronized (directory) { // in- & inter-process sync\n        new Lock.With(directory.makeLock(COMMIT_LOCK_NAME), commitLockTimeout) {\n          public Object doBody() throws IOException {\n            // make compound file visible for SegmentReaders\n            directory.renameFile(mergedName + \".tmp\", mergedName + \".cfs\");\n            return null;\n          }\n        }.run();\n      }\n\n      // delete now unused files of segment\n      deleteFiles(filesToDelete);\n    }\n  }\n\n","bugFix":null,"bugIntro":["d72db039743bd6a2da9be6306f57c71654ca1bf6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"eeefd99c477417e5c7c574228461ebafe92469d4","date":1166460329,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    SegmentInfo info;\n\n    String segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      int docCount = merger.merge();                // merge 'em\n\n      segmentInfos.setSize(0);                      // pop old infos & add new\n      info = new SegmentInfo(mergedName, docCount, directory, false);\n      segmentInfos.addElement(info);\n      commitPending = true;\n\n      if(sReader != null)\n        sReader.close();\n\n      success = true;\n\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      } else {\n        commitTransaction();\n      }\n    }\n\n    deleter.deleteFile(segmentsInfosFileName);    // delete old segments_N file\n    deleter.deleteSegments(segmentsToDelete);     // delete now-unused segments\n\n    if (useCompoundFile) {\n      success = false;\n\n      segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n      Vector filesToDelete;\n\n      startTransaction();\n\n      try {\n\n        filesToDelete = merger.createCompoundFile(mergedName + \".cfs\");\n\n        info.setUseCompoundFile(true);\n        commitPending = true;\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n\n      deleter.deleteFile(segmentsInfosFileName);  // delete old segments_N file\n      deleter.deleteFiles(filesToDelete); // delete now unused files of segment \n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    int docCount = merger.merge();                // merge 'em\n\n    segmentInfos.setSize(0);                      // pop old infos & add new\n    SegmentInfo info = new SegmentInfo(mergedName, docCount, directory, false);\n    segmentInfos.addElement(info);\n\n    if(sReader != null)\n        sReader.close();\n\n    String segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n    segmentInfos.write(directory);         // commit changes\n\n    deleter.deleteFile(segmentsInfosFileName);    // delete old segments_N file\n    deleter.deleteSegments(segmentsToDelete);     // delete now-unused segments\n\n    if (useCompoundFile) {\n      Vector filesToDelete = merger.createCompoundFile(mergedName + \".cfs\");\n      segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n      info.setUseCompoundFile(true);\n      segmentInfos.write(directory);     // commit again so readers know we've switched this segment to a compound file\n\n      deleter.deleteFile(segmentsInfosFileName);  // delete old segments_N file\n      deleter.deleteFiles(filesToDelete); // delete now unused files of segment \n    }\n  }\n\n","bugFix":null,"bugIntro":["d72db039743bd6a2da9be6306f57c71654ca1bf6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8969a184df55d25d61e85be785987fbf830d4028","date":1168143561,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    SegmentInfo info;\n\n    String segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      int docCount = merger.merge();                // merge 'em\n\n      segmentInfos.setSize(0);                      // pop old infos & add new\n      info = new SegmentInfo(mergedName, docCount, directory, false, true);\n      segmentInfos.addElement(info);\n      commitPending = true;\n\n      if(sReader != null)\n        sReader.close();\n\n      success = true;\n\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      } else {\n        commitTransaction();\n      }\n    }\n\n    deleter.deleteFile(segmentsInfosFileName);    // delete old segments_N file\n    deleter.deleteSegments(segmentsToDelete);     // delete now-unused segments\n\n    if (useCompoundFile) {\n      success = false;\n\n      segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n      Vector filesToDelete;\n\n      startTransaction();\n\n      try {\n\n        filesToDelete = merger.createCompoundFile(mergedName + \".cfs\");\n\n        info.setUseCompoundFile(true);\n        commitPending = true;\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n\n      deleter.deleteFile(segmentsInfosFileName);  // delete old segments_N file\n      deleter.deleteFiles(filesToDelete); // delete now unused files of segment \n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    SegmentInfo info;\n\n    String segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      int docCount = merger.merge();                // merge 'em\n\n      segmentInfos.setSize(0);                      // pop old infos & add new\n      info = new SegmentInfo(mergedName, docCount, directory, false);\n      segmentInfos.addElement(info);\n      commitPending = true;\n\n      if(sReader != null)\n        sReader.close();\n\n      success = true;\n\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      } else {\n        commitTransaction();\n      }\n    }\n\n    deleter.deleteFile(segmentsInfosFileName);    // delete old segments_N file\n    deleter.deleteSegments(segmentsToDelete);     // delete now-unused segments\n\n    if (useCompoundFile) {\n      success = false;\n\n      segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n      Vector filesToDelete;\n\n      startTransaction();\n\n      try {\n\n        filesToDelete = merger.createCompoundFile(mergedName + \".cfs\");\n\n        info.setUseCompoundFile(true);\n        commitPending = true;\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n\n      deleter.deleteFile(segmentsInfosFileName);  // delete old segments_N file\n      deleter.deleteFiles(filesToDelete); // delete now unused files of segment \n    }\n  }\n\n","bugFix":null,"bugIntro":["d72db039743bd6a2da9be6306f57c71654ca1bf6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1b54a9bc667895a2095a886184bf69a3179e63df","date":1172088096,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    SegmentInfo info;\n\n    String segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      int docCount = merger.merge();                // merge 'em\n\n      segmentInfos.setSize(0);                      // pop old infos & add new\n      info = new SegmentInfo(mergedName, docCount, directory, false, true);\n      segmentInfos.addElement(info);\n      commitPending = true;\n\n      if(sReader != null)\n        sReader.close();\n\n      success = true;\n\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      } else {\n        commitTransaction();\n      }\n    }\n\n    deleter.deleteFile(segmentsInfosFileName);    // delete old segments_N file\n    deleter.deleteSegments(segmentsToDelete);     // delete now-unused segments\n\n    if (useCompoundFile) {\n      success = false;\n\n      segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n      Vector filesToDelete;\n\n      startTransaction();\n\n      try {\n\n        filesToDelete = merger.createCompoundFile(mergedName + \".cfs\");\n\n        info.setUseCompoundFile(true);\n        commitPending = true;\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n\n      deleter.deleteFile(segmentsInfosFileName);  // delete old segments_N file\n      deleter.deleteFiles(filesToDelete); // delete now unused files of segment \n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    SegmentInfo info;\n\n    String segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      int docCount = merger.merge();                // merge 'em\n\n      segmentInfos.setSize(0);                      // pop old infos & add new\n      info = new SegmentInfo(mergedName, docCount, directory, false, true);\n      segmentInfos.addElement(info);\n      commitPending = true;\n\n      if(sReader != null)\n        sReader.close();\n\n      success = true;\n\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      } else {\n        commitTransaction();\n      }\n    }\n\n    deleter.deleteFile(segmentsInfosFileName);    // delete old segments_N file\n    deleter.deleteSegments(segmentsToDelete);     // delete now-unused segments\n\n    if (useCompoundFile) {\n      success = false;\n\n      segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n      Vector filesToDelete;\n\n      startTransaction();\n\n      try {\n\n        filesToDelete = merger.createCompoundFile(mergedName + \".cfs\");\n\n        info.setUseCompoundFile(true);\n        commitPending = true;\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n\n      deleter.deleteFile(segmentsInfosFileName);  // delete old segments_N file\n      deleter.deleteFiles(filesToDelete); // delete now unused files of segment \n    }\n  }\n\n","bugFix":["e7fb6d70db034a5456ae560175dd1b821eea9ff4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d72db039743bd6a2da9be6306f57c71654ca1bf6","date":1173217255,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    SegmentInfo info;\n    String segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n\n    IndexReader sReader = null;\n    try {\n      if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n      }\n\n      for (int i = 0; i < readers.length; i++)      // add new indexes\n        merger.add(readers[i]);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = merger.merge();                // merge 'em\n\n        segmentInfos.setSize(0);                      // pop old infos & add new\n        info = new SegmentInfo(mergedName, docCount, directory, false, true);\n        segmentInfos.addElement(info);\n        commitPending = true;\n\n        if(sReader != null) {\n          sReader.close();\n          sReader = null;\n        }\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    } finally {\n      if (sReader != null) {\n        sReader.close();\n      }\n    }\n\n    deleter.deleteFile(segmentsInfosFileName);    // delete old segments_N file\n    deleter.deleteSegments(segmentsToDelete);     // delete now-unused segments\n\n    if (useCompoundFile) {\n      boolean success = false;\n\n      segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n      Vector filesToDelete;\n\n      startTransaction();\n\n      try {\n\n        filesToDelete = merger.createCompoundFile(mergedName + \".cfs\");\n\n        info.setUseCompoundFile(true);\n        commitPending = true;\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n\n      deleter.deleteFile(segmentsInfosFileName);  // delete old segments_N file\n      deleter.deleteFiles(filesToDelete); // delete now unused files of segment \n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    IndexReader sReader = null;\n    if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n    }\n\n    for (int i = 0; i < readers.length; i++)      // add new indexes\n      merger.add(readers[i]);\n\n    SegmentInfo info;\n\n    String segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n\n    boolean success = false;\n\n    startTransaction();\n\n    try {\n      int docCount = merger.merge();                // merge 'em\n\n      segmentInfos.setSize(0);                      // pop old infos & add new\n      info = new SegmentInfo(mergedName, docCount, directory, false, true);\n      segmentInfos.addElement(info);\n      commitPending = true;\n\n      if(sReader != null)\n        sReader.close();\n\n      success = true;\n\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      } else {\n        commitTransaction();\n      }\n    }\n\n    deleter.deleteFile(segmentsInfosFileName);    // delete old segments_N file\n    deleter.deleteSegments(segmentsToDelete);     // delete now-unused segments\n\n    if (useCompoundFile) {\n      success = false;\n\n      segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n      Vector filesToDelete;\n\n      startTransaction();\n\n      try {\n\n        filesToDelete = merger.createCompoundFile(mergedName + \".cfs\");\n\n        info.setUseCompoundFile(true);\n        commitPending = true;\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n\n      deleter.deleteFile(segmentsInfosFileName);  // delete old segments_N file\n      deleter.deleteFiles(filesToDelete); // delete now unused files of segment \n    }\n  }\n\n","bugFix":["eeefd99c477417e5c7c574228461ebafe92469d4","284c1d3c8b19931bf6f312fae7470487f5d9e580","28f95f0e6ecb2efc0a13d0131c57f7530f38c705","e7fb6d70db034a5456ae560175dd1b821eea9ff4","8969a184df55d25d61e85be785987fbf830d4028"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8b6187898fc4413ccd18229711786550a280383c","date":1173776782,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    SegmentInfo info;\n\n    IndexReader sReader = null;\n    try {\n      if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n      }\n\n      for (int i = 0; i < readers.length; i++)      // add new indexes\n        merger.add(readers[i]);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = merger.merge();                // merge 'em\n\n        if(sReader != null) {\n          sReader.close();\n          sReader = null;\n        }\n\n        segmentInfos.setSize(0);                      // pop old infos & add new\n        info = new SegmentInfo(mergedName, docCount, directory, false, true);\n        segmentInfos.addElement(info);\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    } finally {\n      if (sReader != null) {\n        sReader.close();\n      }\n    }\n    \n    if (useCompoundFile) {\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        merger.createCompoundFile(mergedName + \".cfs\");\n        info.setUseCompoundFile(true);\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    final Vector segmentsToDelete = new Vector();\n    SegmentInfo info;\n    String segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n\n    IndexReader sReader = null;\n    try {\n      if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n        segmentsToDelete.addElement(sReader);   // queue segment for deletion\n      }\n\n      for (int i = 0; i < readers.length; i++)      // add new indexes\n        merger.add(readers[i]);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = merger.merge();                // merge 'em\n\n        segmentInfos.setSize(0);                      // pop old infos & add new\n        info = new SegmentInfo(mergedName, docCount, directory, false, true);\n        segmentInfos.addElement(info);\n        commitPending = true;\n\n        if(sReader != null) {\n          sReader.close();\n          sReader = null;\n        }\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    } finally {\n      if (sReader != null) {\n        sReader.close();\n      }\n    }\n\n    deleter.deleteFile(segmentsInfosFileName);    // delete old segments_N file\n    deleter.deleteSegments(segmentsToDelete);     // delete now-unused segments\n\n    if (useCompoundFile) {\n      boolean success = false;\n\n      segmentsInfosFileName = segmentInfos.getCurrentSegmentFileName();\n      Vector filesToDelete;\n\n      startTransaction();\n\n      try {\n\n        filesToDelete = merger.createCompoundFile(mergedName + \".cfs\");\n\n        info.setUseCompoundFile(true);\n        commitPending = true;\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n\n      deleter.deleteFile(segmentsInfosFileName);  // delete old segments_N file\n      deleter.deleteFiles(filesToDelete); // delete now unused files of segment \n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"328c1568e471f0c6eaa49ec00334ca59e573710f","date":1173897963,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    SegmentInfo info;\n\n    IndexReader sReader = null;\n    try {\n      if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n      }\n\n      for (int i = 0; i < readers.length; i++)      // add new indexes\n        merger.add(readers[i]);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = merger.merge();                // merge 'em\n\n        if(sReader != null) {\n          sReader.close();\n          sReader = null;\n        }\n\n        segmentInfos.setSize(0);                      // pop old infos & add new\n        info = new SegmentInfo(mergedName, docCount, directory, false, true);\n        segmentInfos.addElement(info);\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    } finally {\n      if (sReader != null) {\n        sReader.close();\n      }\n    }\n    \n    if (useCompoundFile) {\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        merger.createCompoundFile(mergedName + \".cfs\");\n        info.setUseCompoundFile(true);\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    SegmentInfo info;\n\n    IndexReader sReader = null;\n    try {\n      if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n      }\n\n      for (int i = 0; i < readers.length; i++)      // add new indexes\n        merger.add(readers[i]);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = merger.merge();                // merge 'em\n\n        if(sReader != null) {\n          sReader.close();\n          sReader = null;\n        }\n\n        segmentInfos.setSize(0);                      // pop old infos & add new\n        info = new SegmentInfo(mergedName, docCount, directory, false, true);\n        segmentInfos.addElement(info);\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    } finally {\n      if (sReader != null) {\n        sReader.close();\n      }\n    }\n    \n    if (useCompoundFile) {\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        merger.createCompoundFile(mergedName + \".cfs\");\n        info.setUseCompoundFile(true);\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4350b17bd363cd13a95171b8df1ca62ea4c3e71c","date":1183562198,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    SegmentInfo info;\n\n    IndexReader sReader = null;\n    try {\n      if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n      }\n\n      for (int i = 0; i < readers.length; i++)      // add new indexes\n        merger.add(readers[i]);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = merger.merge();                // merge 'em\n\n        if(sReader != null) {\n          sReader.close();\n          sReader = null;\n        }\n\n        segmentInfos.setSize(0);                      // pop old infos & add new\n        info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                               -1, null, false);\n        segmentInfos.addElement(info);\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    } finally {\n      if (sReader != null) {\n        sReader.close();\n      }\n    }\n    \n    if (useCompoundFile) {\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        merger.createCompoundFile(mergedName + \".cfs\");\n        info.setUseCompoundFile(true);\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    SegmentInfo info;\n\n    IndexReader sReader = null;\n    try {\n      if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n      }\n\n      for (int i = 0; i < readers.length; i++)      // add new indexes\n        merger.add(readers[i]);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = merger.merge();                // merge 'em\n\n        if(sReader != null) {\n          sReader.close();\n          sReader = null;\n        }\n\n        segmentInfos.setSize(0);                      // pop old infos & add new\n        info = new SegmentInfo(mergedName, docCount, directory, false, true);\n        segmentInfos.addElement(info);\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    } finally {\n      if (sReader != null) {\n        sReader.close();\n      }\n    }\n    \n    if (useCompoundFile) {\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        merger.createCompoundFile(mergedName + \".cfs\");\n        info.setUseCompoundFile(true);\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b1405362241b561f5590ff4a87d5d6e173bcd9cf","date":1190107634,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    SegmentInfo info;\n\n    IndexReader sReader = null;\n    try {\n      if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n      }\n\n      for (int i = 0; i < readers.length; i++)      // add new indexes\n        merger.add(readers[i]);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = merger.merge();                // merge 'em\n\n        if(sReader != null) {\n          sReader.close();\n          sReader = null;\n        }\n\n        segmentInfos.setSize(0);                      // pop old infos & add new\n        info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                               -1, null, false);\n        segmentInfos.addElement(info);\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    } finally {\n      if (sReader != null) {\n        sReader.close();\n      }\n    }\n    \n    if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        merger.createCompoundFile(mergedName + \".cfs\");\n        info.setUseCompoundFile(true);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception building compound file in addIndexes during merge\");\n\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    SegmentInfo info;\n\n    IndexReader sReader = null;\n    try {\n      if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n      }\n\n      for (int i = 0; i < readers.length; i++)      // add new indexes\n        merger.add(readers[i]);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = merger.merge();                // merge 'em\n\n        if(sReader != null) {\n          sReader.close();\n          sReader = null;\n        }\n\n        segmentInfos.setSize(0);                      // pop old infos & add new\n        info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                               -1, null, false);\n        segmentInfos.addElement(info);\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    } finally {\n      if (sReader != null) {\n        sReader.close();\n      }\n    }\n    \n    if (useCompoundFile) {\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        merger.createCompoundFile(mergedName + \".cfs\");\n        info.setUseCompoundFile(true);\n      } finally {\n        if (!success) {\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"346d5897e4c4e77ed5dbd31f7730ff30973d5971","date":1198317988,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n    SegmentInfo info;\n\n    IndexReader sReader = null;\n    try {\n      if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n      }\n\n      for (int i = 0; i < readers.length; i++)      // add new indexes\n        merger.add(readers[i]);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = merger.merge();                // merge 'em\n\n        if(sReader != null) {\n          sReader.close();\n          sReader = null;\n        }\n\n        segmentInfos.setSize(0);                      // pop old infos & add new\n        info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                               -1, null, false);\n        segmentInfos.addElement(info);\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    } finally {\n      if (sReader != null) {\n        sReader.close();\n      }\n    }\n    \n    if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        merger.createCompoundFile(mergedName + \".cfs\");\n        info.setUseCompoundFile(true);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception building compound file in addIndexes during merge\");\n\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName);\n\n    SegmentInfo info;\n\n    IndexReader sReader = null;\n    try {\n      if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n      }\n\n      for (int i = 0; i < readers.length; i++)      // add new indexes\n        merger.add(readers[i]);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = merger.merge();                // merge 'em\n\n        if(sReader != null) {\n          sReader.close();\n          sReader = null;\n        }\n\n        segmentInfos.setSize(0);                      // pop old infos & add new\n        info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                               -1, null, false);\n        segmentInfos.addElement(info);\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    } finally {\n      if (sReader != null) {\n        sReader.close();\n      }\n    }\n    \n    if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        merger.createCompoundFile(mergedName + \".cfs\");\n        info.setUseCompoundFile(true);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception building compound file in addIndexes during merge\");\n\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    }\n  }\n\n","bugFix":["3afaca6e0770734b01e3bc663bec3ffa71b6f87b"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63","date":1204234542,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        if (segmentInfos.size() == 1){ // add existing index, if any\n          sReader = SegmentReader.get(segmentInfos.info(0));\n          merger.add(sReader);\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          segmentInfos.setSize(0);                      // pop old infos & add new\n          info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                 -1, null, false);\n          segmentInfos.addElement(info);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          info.setUseCompoundFile(true);\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n    final String mergedName = newSegmentName();\n    SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n    SegmentInfo info;\n\n    IndexReader sReader = null;\n    try {\n      if (segmentInfos.size() == 1){ // add existing index, if any\n        sReader = SegmentReader.get(segmentInfos.info(0));\n        merger.add(sReader);\n      }\n\n      for (int i = 0; i < readers.length; i++)      // add new indexes\n        merger.add(readers[i]);\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        int docCount = merger.merge();                // merge 'em\n\n        if(sReader != null) {\n          sReader.close();\n          sReader = null;\n        }\n\n        segmentInfos.setSize(0);                      // pop old infos & add new\n        info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                               -1, null, false);\n        segmentInfos.addElement(info);\n\n        success = true;\n\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    } finally {\n      if (sReader != null) {\n        sReader.close();\n      }\n    }\n    \n    if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n      boolean success = false;\n\n      startTransaction();\n\n      try {\n        merger.createCompoundFile(mergedName + \".cfs\");\n        info.setUseCompoundFile(true);\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception building compound file in addIndexes during merge\");\n\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["dbb18b6a222f2507f22fab7cc7eed06658d59772","cd488f50316362b01a7f67b11a96796b9652e3e5","c48871ed951104729f5e17a8ee1091b43fa18980"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be","date":1204801324,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        if (segmentInfos.size() == 1){ // add existing index, if any\n          sReader = SegmentReader.get(segmentInfos.info(0));\n          merger.add(sReader);\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          segmentInfos.setSize(0);                      // pop old infos & add new\n          info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                 -1, null, false);\n          segmentInfos.addElement(info);\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          info.setUseCompoundFile(true);\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        if (segmentInfos.size() == 1){ // add existing index, if any\n          sReader = SegmentReader.get(segmentInfos.info(0));\n          merger.add(sReader);\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          segmentInfos.setSize(0);                      // pop old infos & add new\n          info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                 -1, null, false);\n          segmentInfos.addElement(info);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          info.setUseCompoundFile(true);\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"dbb18b6a222f2507f22fab7cc7eed06658d59772","date":1204804366,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        synchronized(this) {\n          if (segmentInfos.size() == 1){ // add existing index, if any\n            sReader = SegmentReader.get(segmentInfos.info(0));\n            merger.add(sReader);\n          }\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.setSize(0);                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false);\n            segmentInfos.addElement(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          synchronized(this) {\n            info.setUseCompoundFile(true);\n          }\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public synchronized void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        if (segmentInfos.size() == 1){ // add existing index, if any\n          sReader = SegmentReader.get(segmentInfos.info(0));\n          merger.add(sReader);\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          segmentInfos.setSize(0);                      // pop old infos & add new\n          info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                 -1, null, false);\n          segmentInfos.addElement(info);\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          info.setUseCompoundFile(true);\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    }\n  }\n\n","bugFix":["01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63","e7fb6d70db034a5456ae560175dd1b821eea9ff4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7a577d3327a52745f4d422afe87eb854ebb8a3f2","date":1205441852,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        synchronized(this) {\n          if (segmentInfos.size() == 1){ // add existing index, if any\n            sReader = SegmentReader.get(segmentInfos.info(0));\n            merger.add(sReader);\n          }\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.setSize(0);                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false);\n            segmentInfos.addElement(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          synchronized(this) {\n            info.setUseCompoundFile(true);\n          }\n          \n          success = true;\n          \n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        synchronized(this) {\n          if (segmentInfos.size() == 1){ // add existing index, if any\n            sReader = SegmentReader.get(segmentInfos.info(0));\n            merger.add(sReader);\n          }\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.setSize(0);                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false);\n            segmentInfos.addElement(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          synchronized(this) {\n            info.setUseCompoundFile(true);\n          }\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"902ba79f4590a41c663c447756d2e5041cbbdda9","date":1217956662,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        synchronized(this) {\n          if (segmentInfos.size() == 1){ // add existing index, if any\n            sReader = SegmentReader.get(segmentInfos.info(0));\n            merger.add(sReader);\n          }\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.setSize(0);                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.addElement(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          synchronized(this) {\n            info.setUseCompoundFile(true);\n          }\n          \n          success = true;\n          \n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        synchronized(this) {\n          if (segmentInfos.size() == 1){ // add existing index, if any\n            sReader = SegmentReader.get(segmentInfos.info(0));\n            merger.add(sReader);\n          }\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.setSize(0);                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false);\n            segmentInfos.addElement(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          synchronized(this) {\n            info.setUseCompoundFile(true);\n          }\n          \n          success = true;\n          \n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"268d26e9bb23249b6a418a01d52fcbe19ee33a1f","date":1219498325,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        synchronized(this) {\n          if (segmentInfos.size() == 1){ // add existing index, if any\n            sReader = SegmentReader.get(segmentInfos.info(0));\n            merger.add(sReader);\n          }\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.setSize(0);                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.addElement(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          synchronized(this) {\n            info.setUseCompoundFile(true);\n          }\n          \n          success = true;\n          \n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexes(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        synchronized(this) {\n          if (segmentInfos.size() == 1){ // add existing index, if any\n            sReader = SegmentReader.get(segmentInfos.info(0));\n            merger.add(sReader);\n          }\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.setSize(0);                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.addElement(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          synchronized(this) {\n            info.setUseCompoundFile(true);\n          }\n          \n          success = true;\n          \n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5712975970a595c28f1988efd007e1b8a617a92f","date":1219499238,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        synchronized(this) {\n          if (segmentInfos.size() == 1){ // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n            merger.add(sReader);\n          }\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.setSize(0);                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.addElement(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          synchronized(this) {\n            info.setUseCompoundFile(true);\n          }\n          \n          success = true;\n          \n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        synchronized(this) {\n          if (segmentInfos.size() == 1){ // add existing index, if any\n            sReader = SegmentReader.get(segmentInfos.info(0));\n            merger.add(sReader);\n          }\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.setSize(0);                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.addElement(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          synchronized(this) {\n            info.setUseCompoundFile(true);\n          }\n          \n          success = true;\n          \n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2586f96f60332eb97ecd2934b0763791462568b2","date":1220116589,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire the write lock here (and not in\n    // startTransaction below) so that no other addIndexes\n    // is allowed to start up after we have flushed &\n    // optimized but before we then start our transaction.\n    // This is because the merging below requires that only\n    // one segment is present in the index:\n    acquireWrite();\n\n    try {\n\n      boolean success = false;\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the write lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseWrite();\n      }\n\n      // true means we already have write lock; if this call\n      // hits an exception it will release the write lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        IndexReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n          }\n        }\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.setSize(0);                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.addElement(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            sReader.close();\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    try {\n      optimize();\t\t\t\t\t  // start with zero or 1 seg\n\n      final String mergedName = newSegmentName();\n      SegmentMerger merger = new SegmentMerger(this, mergedName, null);\n\n      SegmentInfo info;\n\n      IndexReader sReader = null;\n      try {\n        synchronized(this) {\n          if (segmentInfos.size() == 1){ // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n            merger.add(sReader);\n          }\n        }\n\n        for (int i = 0; i < readers.length; i++)      // add new indexes\n          merger.add(readers[i]);\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.setSize(0);                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.addElement(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      } finally {\n        if (sReader != null) {\n          sReader.close();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        boolean success = false;\n\n        startTransaction();\n\n        try {\n          merger.createCompoundFile(mergedName + \".cfs\");\n          synchronized(this) {\n            info.setUseCompoundFile(true);\n          }\n          \n          success = true;\n          \n        } finally {\n          if (!success) {\n            if (infoStream != null)\n              message(\"hit exception building compound file in addIndexes during merge\");\n\n            rollbackTransaction();\n          } else {\n            commitTransaction();\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":["7ea195c05e197e45a9dd84b46e809370e8e51a69","5fff8d3ee91620a83b0a3ac1c9f85b384c7cf32a"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c7b6cdc70e097da94da79a655ed8f94477ff69f5","date":1220815360,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire the write lock here (and not in\n    // startTransaction below) so that no other addIndexes\n    // is allowed to start up after we have flushed &\n    // optimized but before we then start our transaction.\n    // This is because the merging below requires that only\n    // one segment is present in the index:\n    acquireWrite();\n\n    try {\n\n      boolean success = false;\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the write lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseWrite();\n      }\n\n      // true means we already have write lock; if this call\n      // hits an exception it will release the write lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        IndexReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n          }\n        }\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            sReader.close();\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire the write lock here (and not in\n    // startTransaction below) so that no other addIndexes\n    // is allowed to start up after we have flushed &\n    // optimized but before we then start our transaction.\n    // This is because the merging below requires that only\n    // one segment is present in the index:\n    acquireWrite();\n\n    try {\n\n      boolean success = false;\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the write lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseWrite();\n      }\n\n      // true means we already have write lock; if this call\n      // hits an exception it will release the write lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        IndexReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n          }\n        }\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.setSize(0);                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.addElement(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            sReader.close();\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9665d17707cc21b1db995118ff36129723139ab","date":1225384420,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire the write lock here (and not in\n    // startTransaction below) so that no other addIndexes\n    // is allowed to start up after we have flushed &\n    // optimized but before we then start our transaction.\n    // This is because the merging below requires that only\n    // one segment is present in the index:\n    acquireWrite();\n\n    try {\n\n      boolean success = false;\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the write lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseWrite();\n      }\n\n      // true means we already have write lock; if this call\n      // hits an exception it will release the write lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        IndexReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n          }\n        }\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            sReader.close();\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire the write lock here (and not in\n    // startTransaction below) so that no other addIndexes\n    // is allowed to start up after we have flushed &\n    // optimized but before we then start our transaction.\n    // This is because the merging below requires that only\n    // one segment is present in the index:\n    acquireWrite();\n\n    try {\n\n      boolean success = false;\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the write lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseWrite();\n      }\n\n      // true means we already have write lock; if this call\n      // hits an exception it will release the write lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        IndexReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n          }\n        }\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            sReader.close();\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7ea195c05e197e45a9dd84b46e809370e8e51a69","date":1235305227,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        IndexReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n          }\n        }\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            sReader.close();\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire the write lock here (and not in\n    // startTransaction below) so that no other addIndexes\n    // is allowed to start up after we have flushed &\n    // optimized but before we then start our transaction.\n    // This is because the merging below requires that only\n    // one segment is present in the index:\n    acquireWrite();\n\n    try {\n\n      boolean success = false;\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the write lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseWrite();\n      }\n\n      // true means we already have write lock; if this call\n      // hits an exception it will release the write lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        IndexReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n          }\n        }\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            sReader.close();\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":["2586f96f60332eb97ecd2934b0763791462568b2"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6a7626d7c0746aa25a85a8c7a27742a411e9b9fd","date":1236202493,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        IndexReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            sReader.close();\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        IndexReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n          }\n        }\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            sReader.close();\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e0b538625874e1e8dd57caf8ddf8482ebdd4b705","date":1236338695,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        IndexReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            sReader.close();\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n\n   * <p><b>NOTE:</b> the index in each Directory must not be\n   * changed (opened by a writer) while this method is\n   * running.  This method does not acquire a write lock in\n   * each input Directory, so it is up to the caller to\n   * enforce this.\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        IndexReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            sReader.close();\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c4ff8864209d2e972cb4393600c26082f9a6533d","date":1239297466,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        IndexReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = SegmentReader.get(true, segmentInfos.info(0));\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          if(sReader != null) {\n            sReader.close();\n            sReader = null;\n          }\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            sReader.close();\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      docWriter.resumeAllThreads();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cd488f50316362b01a7f67b11a96796b9652e3e5","date":1241121034,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader[])\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      hitOOM = true;\n      throw oom;\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":["01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3ab91f3bb602daf6393fa7f78b11afd3400d669","date":1243282044,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader[])\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader[])\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader[])\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"961159f13aece73fbb30aea720e77a2237e8bafd","date":1247258916,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader[])\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader[])\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader[])\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader[])\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7","date":1255555265,"type":5,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader...).mjava","pathOld":"src/java/org/apache/lucene/index/IndexWriter#addIndexes(IndexReader[]).mjava","sourceNew":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader... readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader...)\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader...)\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","sourceOld":"  /** Merges the provided indexes into this index.\n   * <p>After this completes, the index is optimized. </p>\n   * <p>The provided IndexReaders are not closed.</p>\n   *\n   * <p><b>NOTE:</b> while this is running, any attempts to\n   * add or delete documents (with another thread) will be\n   * paused until this method completes.\n   *\n   * <p>See {@link #addIndexesNoOptimize(Directory[])} for\n   * details on transactional semantics, temporary free\n   * space required in the Directory, and non-CFS segments\n   * on an Exception.</p>\n   *\n   * <p><b>NOTE</b>: if this method hits an OutOfMemoryError\n   * you should immediately close the writer.  See <a\n   * href=\"#OOME\">above</a> for details.</p>\n   *\n   * @throws CorruptIndexException if the index is corrupt\n   * @throws IOException if there is a low-level IO error\n   */\n  public void addIndexes(IndexReader[] readers)\n    throws CorruptIndexException, IOException {\n\n    ensureOpen();\n\n    // Do not allow add docs or deletes while we are running:\n    docWriter.pauseAllThreads();\n\n    // We must pre-acquire a read lock here (and upgrade to\n    // write lock in startTransaction below) so that no\n    // other addIndexes is allowed to start up after we have\n    // flushed & optimized but before we then start our\n    // transaction.  This is because the merging below\n    // requires that only one segment is present in the\n    // index:\n    acquireRead();\n\n    try {\n\n      SegmentInfo info = null;\n      String mergedName = null;\n      SegmentMerger merger = null;\n\n      boolean success = false;\n\n      try {\n        flush(true, false, true);\n        optimize();\t\t\t\t\t  // start with zero or 1 seg\n        success = true;\n      } finally {\n        // Take care to release the read lock if we hit an\n        // exception before starting the transaction\n        if (!success)\n          releaseRead();\n      }\n\n      // true means we already have a read lock; if this\n      // call hits an exception it will release the write\n      // lock:\n      startTransaction(true);\n\n      try {\n        mergedName = newSegmentName();\n        merger = new SegmentMerger(this, mergedName, null);\n\n        SegmentReader sReader = null;\n        synchronized(this) {\n          if (segmentInfos.size() == 1) { // add existing index, if any\n            sReader = readerPool.get(segmentInfos.info(0), true, BufferedIndexInput.BUFFER_SIZE, -1);\n          }\n        }\n        \n        success = false;\n\n        try {\n          if (sReader != null)\n            merger.add(sReader);\n\n          for (int i = 0; i < readers.length; i++)      // add new indexes\n            merger.add(readers[i]);\n\n          int docCount = merger.merge();                // merge 'em\n\n          synchronized(this) {\n            segmentInfos.clear();                      // pop old infos & add new\n            info = new SegmentInfo(mergedName, docCount, directory, false, true,\n                                   -1, null, false, merger.hasProx());\n            setDiagnostics(info, \"addIndexes(IndexReader[])\");\n            segmentInfos.add(info);\n          }\n\n          // Notify DocumentsWriter that the flushed count just increased\n          docWriter.updateFlushedDocCount(docCount);\n\n          success = true;\n\n        } finally {\n          if (sReader != null) {\n            readerPool.release(sReader);\n          }\n        }\n      } finally {\n        if (!success) {\n          if (infoStream != null)\n            message(\"hit exception in addIndexes during merge\");\n          rollbackTransaction();\n        } else {\n          commitTransaction();\n        }\n      }\n    \n      if (mergePolicy instanceof LogMergePolicy && getUseCompoundFile()) {\n\n        List files = null;\n\n        synchronized(this) {\n          // Must incRef our files so that if another thread\n          // is running merge/optimize, it doesn't delete our\n          // segment's files before we have a change to\n          // finish making the compound file.\n          if (segmentInfos.contains(info)) {\n            files = info.files();\n            deleter.incRef(files);\n          }\n        }\n\n        if (files != null) {\n\n          success = false;\n\n          startTransaction(false);\n\n          try {\n            merger.createCompoundFile(mergedName + \".cfs\");\n            synchronized(this) {\n              info.setUseCompoundFile(true);\n            }\n          \n            success = true;\n          \n          } finally {\n\n            deleter.decRef(files);\n\n            if (!success) {\n              if (infoStream != null)\n                message(\"hit exception building compound file in addIndexes during merge\");\n\n              rollbackTransaction();\n            } else {\n              commitTransaction();\n            }\n          }\n        }\n      }\n    } catch (OutOfMemoryError oom) {\n      handleOOM(oom, \"addIndexes(IndexReader[])\");\n    } finally {\n      if (docWriter != null) {\n        docWriter.resumeAllThreads();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"6a7626d7c0746aa25a85a8c7a27742a411e9b9fd":["7ea195c05e197e45a9dd84b46e809370e8e51a69"],"328c1568e471f0c6eaa49ec00334ca59e573710f":["8b6187898fc4413ccd18229711786550a280383c"],"5712975970a595c28f1988efd007e1b8a617a92f":["268d26e9bb23249b6a418a01d52fcbe19ee33a1f"],"e7fb6d70db034a5456ae560175dd1b821eea9ff4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c4ff8864209d2e972cb4393600c26082f9a6533d":["e0b538625874e1e8dd57caf8ddf8482ebdd4b705"],"6263f219dcdfc6e861ecffaecf5e1e195f1aaaa7":["79fc67d5cdece593cd3b3b6c7ef195ee2625522c"],"902ba79f4590a41c663c447756d2e5041cbbdda9":["7a577d3327a52745f4d422afe87eb854ebb8a3f2"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["2586f96f60332eb97ecd2934b0763791462568b2"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["328c1568e471f0c6eaa49ec00334ca59e573710f"],"dbb18b6a222f2507f22fab7cc7eed06658d59772":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"eeefd99c477417e5c7c574228461ebafe92469d4":["284c1d3c8b19931bf6f312fae7470487f5d9e580"],"1507a324c1f939ed71e01297733a49b9c36e5688":["8c3ae2a9b6bdc01b2b6665e79fdfc71ae4a8b1ca"],"79fc67d5cdece593cd3b3b6c7ef195ee2625522c":["28f95f0e6ecb2efc0a13d0131c57f7530f38c705"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8b6187898fc4413ccd18229711786550a280383c":["d72db039743bd6a2da9be6306f57c71654ca1bf6"],"2586f96f60332eb97ecd2934b0763791462568b2":["5712975970a595c28f1988efd007e1b8a617a92f"],"268d26e9bb23249b6a418a01d52fcbe19ee33a1f":["902ba79f4590a41c663c447756d2e5041cbbdda9"],"7a577d3327a52745f4d422afe87eb854ebb8a3f2":["dbb18b6a222f2507f22fab7cc7eed06658d59772"],"d7052f725a053aa55424f966831826f61b798bf1":["1507a324c1f939ed71e01297733a49b9c36e5688"],"14e0bcefcbabf34d28e4fd7060dabc9a9fe37cea":["3afaca6e0770734b01e3bc663bec3ffa71b6f87b"],"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"1b54a9bc667895a2095a886184bf69a3179e63df":["8969a184df55d25d61e85be785987fbf830d4028"],"683a9c890d06f2af26082bbd5a71c6b9954f9ae1":["e7fb6d70db034a5456ae560175dd1b821eea9ff4"],"e0b538625874e1e8dd57caf8ddf8482ebdd4b705":["6a7626d7c0746aa25a85a8c7a27742a411e9b9fd"],"42a7d0b384f0022c5a29e562b809ebf73991d7e6":["683a9c890d06f2af26082bbd5a71c6b9954f9ae1"],"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7":["961159f13aece73fbb30aea720e77a2237e8bafd"],"28f95f0e6ecb2efc0a13d0131c57f7530f38c705":["42a7d0b384f0022c5a29e562b809ebf73991d7e6"],"e9665d17707cc21b1db995118ff36129723139ab":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"cd488f50316362b01a7f67b11a96796b9652e3e5":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"7ea195c05e197e45a9dd84b46e809370e8e51a69":["e9665d17707cc21b1db995118ff36129723139ab"],"284c1d3c8b19931bf6f312fae7470487f5d9e580":["d7052f725a053aa55424f966831826f61b798bf1"],"961159f13aece73fbb30aea720e77a2237e8bafd":["d3ab91f3bb602daf6393fa7f78b11afd3400d669"],"6e5d88b55f1b57feab6da94a5c635a224539bd2a":["6263f219dcdfc6e861ecffaecf5e1e195f1aaaa7"],"d3ab91f3bb602daf6393fa7f78b11afd3400d669":["cd488f50316362b01a7f67b11a96796b9652e3e5"],"d72db039743bd6a2da9be6306f57c71654ca1bf6":["1b54a9bc667895a2095a886184bf69a3179e63df"],"8c3ae2a9b6bdc01b2b6665e79fdfc71ae4a8b1ca":["14e0bcefcbabf34d28e4fd7060dabc9a9fe37cea"],"8969a184df55d25d61e85be785987fbf830d4028":["eeefd99c477417e5c7c574228461ebafe92469d4"],"3afaca6e0770734b01e3bc663bec3ffa71b6f87b":["6e5d88b55f1b57feab6da94a5c635a224539bd2a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7"]},"commit2Childs":{"6a7626d7c0746aa25a85a8c7a27742a411e9b9fd":["e0b538625874e1e8dd57caf8ddf8482ebdd4b705"],"328c1568e471f0c6eaa49ec00334ca59e573710f":["4350b17bd363cd13a95171b8df1ca62ea4c3e71c"],"5712975970a595c28f1988efd007e1b8a617a92f":["2586f96f60332eb97ecd2934b0763791462568b2"],"e7fb6d70db034a5456ae560175dd1b821eea9ff4":["683a9c890d06f2af26082bbd5a71c6b9954f9ae1"],"c4ff8864209d2e972cb4393600c26082f9a6533d":["cd488f50316362b01a7f67b11a96796b9652e3e5"],"6263f219dcdfc6e861ecffaecf5e1e195f1aaaa7":["6e5d88b55f1b57feab6da94a5c635a224539bd2a"],"902ba79f4590a41c663c447756d2e5041cbbdda9":["268d26e9bb23249b6a418a01d52fcbe19ee33a1f"],"c7b6cdc70e097da94da79a655ed8f94477ff69f5":["e9665d17707cc21b1db995118ff36129723139ab"],"a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be":["dbb18b6a222f2507f22fab7cc7eed06658d59772"],"4350b17bd363cd13a95171b8df1ca62ea4c3e71c":["b1405362241b561f5590ff4a87d5d6e173bcd9cf"],"dbb18b6a222f2507f22fab7cc7eed06658d59772":["7a577d3327a52745f4d422afe87eb854ebb8a3f2"],"eeefd99c477417e5c7c574228461ebafe92469d4":["8969a184df55d25d61e85be785987fbf830d4028"],"79fc67d5cdece593cd3b3b6c7ef195ee2625522c":["6263f219dcdfc6e861ecffaecf5e1e195f1aaaa7"],"1507a324c1f939ed71e01297733a49b9c36e5688":["d7052f725a053aa55424f966831826f61b798bf1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e7fb6d70db034a5456ae560175dd1b821eea9ff4"],"8b6187898fc4413ccd18229711786550a280383c":["328c1568e471f0c6eaa49ec00334ca59e573710f"],"2586f96f60332eb97ecd2934b0763791462568b2":["c7b6cdc70e097da94da79a655ed8f94477ff69f5"],"268d26e9bb23249b6a418a01d52fcbe19ee33a1f":["5712975970a595c28f1988efd007e1b8a617a92f"],"7a577d3327a52745f4d422afe87eb854ebb8a3f2":["902ba79f4590a41c663c447756d2e5041cbbdda9"],"d7052f725a053aa55424f966831826f61b798bf1":["284c1d3c8b19931bf6f312fae7470487f5d9e580"],"14e0bcefcbabf34d28e4fd7060dabc9a9fe37cea":["8c3ae2a9b6bdc01b2b6665e79fdfc71ae4a8b1ca"],"346d5897e4c4e77ed5dbd31f7730ff30973d5971":["01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63"],"b1405362241b561f5590ff4a87d5d6e173bcd9cf":["346d5897e4c4e77ed5dbd31f7730ff30973d5971"],"1b54a9bc667895a2095a886184bf69a3179e63df":["d72db039743bd6a2da9be6306f57c71654ca1bf6"],"683a9c890d06f2af26082bbd5a71c6b9954f9ae1":["42a7d0b384f0022c5a29e562b809ebf73991d7e6"],"e0b538625874e1e8dd57caf8ddf8482ebdd4b705":["c4ff8864209d2e972cb4393600c26082f9a6533d"],"42a7d0b384f0022c5a29e562b809ebf73991d7e6":["28f95f0e6ecb2efc0a13d0131c57f7530f38c705"],"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"28f95f0e6ecb2efc0a13d0131c57f7530f38c705":["79fc67d5cdece593cd3b3b6c7ef195ee2625522c"],"e9665d17707cc21b1db995118ff36129723139ab":["7ea195c05e197e45a9dd84b46e809370e8e51a69"],"01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63":["a2fc4b864a5dc2c630bb1fa94091e89e69f8f8be"],"cd488f50316362b01a7f67b11a96796b9652e3e5":["d3ab91f3bb602daf6393fa7f78b11afd3400d669"],"7ea195c05e197e45a9dd84b46e809370e8e51a69":["6a7626d7c0746aa25a85a8c7a27742a411e9b9fd"],"284c1d3c8b19931bf6f312fae7470487f5d9e580":["eeefd99c477417e5c7c574228461ebafe92469d4"],"961159f13aece73fbb30aea720e77a2237e8bafd":["560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7"],"6e5d88b55f1b57feab6da94a5c635a224539bd2a":["3afaca6e0770734b01e3bc663bec3ffa71b6f87b"],"d3ab91f3bb602daf6393fa7f78b11afd3400d669":["961159f13aece73fbb30aea720e77a2237e8bafd"],"8c3ae2a9b6bdc01b2b6665e79fdfc71ae4a8b1ca":["1507a324c1f939ed71e01297733a49b9c36e5688"],"d72db039743bd6a2da9be6306f57c71654ca1bf6":["8b6187898fc4413ccd18229711786550a280383c"],"8969a184df55d25d61e85be785987fbf830d4028":["1b54a9bc667895a2095a886184bf69a3179e63df"],"3afaca6e0770734b01e3bc663bec3ffa71b6f87b":["14e0bcefcbabf34d28e4fd7060dabc9a9fe37cea"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}