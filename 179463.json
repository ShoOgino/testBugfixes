{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymGraphFilter#bufferOutputTokens(BytesRef,int).mjava","commits":[{"id":"24a98f5fdd23e04f85819dbc63b47a12f7c44311","date":1482439157,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymGraphFilter#bufferOutputTokens(BytesRef,int).mjava","pathOld":"/dev/null","sourceNew":"  /** Expands the output graph into the necessary tokens, adding\n   *  synonyms as side paths parallel to the input tokens, and\n   *  buffers them in the output token buffer. */\n  private void bufferOutputTokens(BytesRef bytes, int matchInputLength) {\n    bytesReader.reset(bytes.bytes, bytes.offset, bytes.length);\n\n    final int code = bytesReader.readVInt();\n    final boolean keepOrig = (code & 0x1) == 0;\n    //System.out.println(\"  buffer: keepOrig=\" + keepOrig + \" matchInputLength=\" + matchInputLength);\n\n    // How many nodes along all paths; we need this to assign the\n    // node ID for the final end node where all paths merge back:\n    int totalPathNodes;\n    if (keepOrig) {\n      assert matchInputLength > 0;\n      totalPathNodes = matchInputLength - 1;\n    } else {\n      totalPathNodes = 0;\n    }\n\n    // How many synonyms we will insert over this match:\n    final int count = code >>> 1;\n\n    // TODO: we could encode this instead into the FST:\n\n    // 1st pass: count how many new nodes we need\n    List<List<String>> paths = new ArrayList<>();\n    for(int outputIDX=0;outputIDX<count;outputIDX++) {\n      int wordID = bytesReader.readVInt();\n      synonyms.words.get(wordID, scratchBytes);\n      scratchChars.copyUTF8Bytes(scratchBytes);\n      int lastStart = 0;\n\n      List<String> path = new ArrayList<>();\n      paths.add(path);\n      int chEnd = scratchChars.length();\n      for(int chUpto=0; chUpto<=chEnd; chUpto++) {\n        if (chUpto == chEnd || scratchChars.charAt(chUpto) == SynonymMap.WORD_SEPARATOR) {\n          path.add(new String(scratchChars.chars(), lastStart, chUpto - lastStart));\n          lastStart = 1 + chUpto;\n        }\n      }\n\n      assert path.size() > 0;\n      totalPathNodes += path.size() - 1;\n    }\n    //System.out.println(\"  totalPathNodes=\" + totalPathNodes);\n\n    // 2nd pass: buffer tokens for the graph fragment\n\n    // NOTE: totalPathNodes will be 0 in the case where the matched\n    // input is a single token and all outputs are also a single token\n\n    // We \"spawn\" a side-path for each of the outputs for this matched\n    // synonym, all ending back at this end node:\n\n    int startNode = nextNodeOut;\n\n    int endNode = startNode + totalPathNodes + 1;\n    //System.out.println(\"  \" + paths.size() + \" new side-paths\");\n\n    // First, fanout all tokens departing start node for these new side paths:\n    int newNodeCount = 0;\n    for(List<String> path : paths) {\n      int pathEndNode;\n      //System.out.println(\"    path size=\" + path.size());\n      if (path.size() == 1) {\n        // Single token output, so there are no intermediate nodes:\n        pathEndNode = endNode;\n      } else {\n        pathEndNode = nextNodeOut + newNodeCount + 1;\n        newNodeCount += path.size() - 1;\n      }\n      outputBuffer.add(new BufferedOutputToken(null, path.get(0), startNode, pathEndNode));\n    }\n\n    // We must do the original tokens last, else the offsets \"go backwards\":\n    if (keepOrig) {\n      BufferedInputToken token = lookahead.get(lookaheadNextRead);\n      int inputEndNode;\n      if (matchInputLength == 1) {\n        // Single token matched input, so there are no intermediate nodes:\n        inputEndNode = endNode;\n      } else {\n        inputEndNode = nextNodeOut + newNodeCount + 1;\n      }\n\n      //System.out.println(\"    keepOrig first token: \" + token.term);\n\n      outputBuffer.add(new BufferedOutputToken(token.state, token.term.toString(), startNode, inputEndNode));\n    }\n\n    nextNodeOut = endNode;\n\n    // Do full side-path for each syn output:\n    for(int pathID=0;pathID<paths.size();pathID++) {\n      List<String> path = paths.get(pathID);\n      if (path.size() > 1) {\n        int lastNode = outputBuffer.get(pathID).endNode;\n        for(int i=1;i<path.size()-1;i++) {\n          outputBuffer.add(new BufferedOutputToken(null, path.get(i), lastNode, lastNode+1));\n          lastNode++;\n        }\n        outputBuffer.add(new BufferedOutputToken(null, path.get(path.size()-1), lastNode, endNode));\n      }\n    }\n\n    if (keepOrig && matchInputLength > 1) {\n      // Do full \"side path\" with the original tokens:\n      int lastNode = outputBuffer.get(paths.size()).endNode;\n      for(int i=1;i<matchInputLength-1;i++) {\n        BufferedInputToken token = lookahead.get(lookaheadNextRead + i);\n        outputBuffer.add(new BufferedOutputToken(token.state, token.term.toString(), lastNode, lastNode+1));\n        lastNode++;\n      }\n      BufferedInputToken token = lookahead.get(lookaheadNextRead + matchInputLength - 1);\n      outputBuffer.add(new BufferedOutputToken(token.state, token.term.toString(), lastNode, endNode));\n    }\n\n    /*\n    System.out.println(\"  after buffer: \" + outputBuffer.size() + \" tokens:\");\n    for(BufferedOutputToken token : outputBuffer) {\n      System.out.println(\"    tok: \" + token.term + \" startNode=\" + token.startNode + \" endNode=\" + token.endNode);\n    }\n    */\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymGraphFilter#bufferOutputTokens(BytesRef,int).mjava","pathOld":"/dev/null","sourceNew":"  /** Expands the output graph into the necessary tokens, adding\n   *  synonyms as side paths parallel to the input tokens, and\n   *  buffers them in the output token buffer. */\n  private void bufferOutputTokens(BytesRef bytes, int matchInputLength) {\n    bytesReader.reset(bytes.bytes, bytes.offset, bytes.length);\n\n    final int code = bytesReader.readVInt();\n    final boolean keepOrig = (code & 0x1) == 0;\n    //System.out.println(\"  buffer: keepOrig=\" + keepOrig + \" matchInputLength=\" + matchInputLength);\n\n    // How many nodes along all paths; we need this to assign the\n    // node ID for the final end node where all paths merge back:\n    int totalPathNodes;\n    if (keepOrig) {\n      assert matchInputLength > 0;\n      totalPathNodes = matchInputLength - 1;\n    } else {\n      totalPathNodes = 0;\n    }\n\n    // How many synonyms we will insert over this match:\n    final int count = code >>> 1;\n\n    // TODO: we could encode this instead into the FST:\n\n    // 1st pass: count how many new nodes we need\n    List<List<String>> paths = new ArrayList<>();\n    for(int outputIDX=0;outputIDX<count;outputIDX++) {\n      int wordID = bytesReader.readVInt();\n      synonyms.words.get(wordID, scratchBytes);\n      scratchChars.copyUTF8Bytes(scratchBytes);\n      int lastStart = 0;\n\n      List<String> path = new ArrayList<>();\n      paths.add(path);\n      int chEnd = scratchChars.length();\n      for(int chUpto=0; chUpto<=chEnd; chUpto++) {\n        if (chUpto == chEnd || scratchChars.charAt(chUpto) == SynonymMap.WORD_SEPARATOR) {\n          path.add(new String(scratchChars.chars(), lastStart, chUpto - lastStart));\n          lastStart = 1 + chUpto;\n        }\n      }\n\n      assert path.size() > 0;\n      totalPathNodes += path.size() - 1;\n    }\n    //System.out.println(\"  totalPathNodes=\" + totalPathNodes);\n\n    // 2nd pass: buffer tokens for the graph fragment\n\n    // NOTE: totalPathNodes will be 0 in the case where the matched\n    // input is a single token and all outputs are also a single token\n\n    // We \"spawn\" a side-path for each of the outputs for this matched\n    // synonym, all ending back at this end node:\n\n    int startNode = nextNodeOut;\n\n    int endNode = startNode + totalPathNodes + 1;\n    //System.out.println(\"  \" + paths.size() + \" new side-paths\");\n\n    // First, fanout all tokens departing start node for these new side paths:\n    int newNodeCount = 0;\n    for(List<String> path : paths) {\n      int pathEndNode;\n      //System.out.println(\"    path size=\" + path.size());\n      if (path.size() == 1) {\n        // Single token output, so there are no intermediate nodes:\n        pathEndNode = endNode;\n      } else {\n        pathEndNode = nextNodeOut + newNodeCount + 1;\n        newNodeCount += path.size() - 1;\n      }\n      outputBuffer.add(new BufferedOutputToken(null, path.get(0), startNode, pathEndNode));\n    }\n\n    // We must do the original tokens last, else the offsets \"go backwards\":\n    if (keepOrig) {\n      BufferedInputToken token = lookahead.get(lookaheadNextRead);\n      int inputEndNode;\n      if (matchInputLength == 1) {\n        // Single token matched input, so there are no intermediate nodes:\n        inputEndNode = endNode;\n      } else {\n        inputEndNode = nextNodeOut + newNodeCount + 1;\n      }\n\n      //System.out.println(\"    keepOrig first token: \" + token.term);\n\n      outputBuffer.add(new BufferedOutputToken(token.state, token.term.toString(), startNode, inputEndNode));\n    }\n\n    nextNodeOut = endNode;\n\n    // Do full side-path for each syn output:\n    for(int pathID=0;pathID<paths.size();pathID++) {\n      List<String> path = paths.get(pathID);\n      if (path.size() > 1) {\n        int lastNode = outputBuffer.get(pathID).endNode;\n        for(int i=1;i<path.size()-1;i++) {\n          outputBuffer.add(new BufferedOutputToken(null, path.get(i), lastNode, lastNode+1));\n          lastNode++;\n        }\n        outputBuffer.add(new BufferedOutputToken(null, path.get(path.size()-1), lastNode, endNode));\n      }\n    }\n\n    if (keepOrig && matchInputLength > 1) {\n      // Do full \"side path\" with the original tokens:\n      int lastNode = outputBuffer.get(paths.size()).endNode;\n      for(int i=1;i<matchInputLength-1;i++) {\n        BufferedInputToken token = lookahead.get(lookaheadNextRead + i);\n        outputBuffer.add(new BufferedOutputToken(token.state, token.term.toString(), lastNode, lastNode+1));\n        lastNode++;\n      }\n      BufferedInputToken token = lookahead.get(lookaheadNextRead + matchInputLength - 1);\n      outputBuffer.add(new BufferedOutputToken(token.state, token.term.toString(), lastNode, endNode));\n    }\n\n    /*\n    System.out.println(\"  after buffer: \" + outputBuffer.size() + \" tokens:\");\n    for(BufferedOutputToken token : outputBuffer) {\n      System.out.println(\"    tok: \" + token.term + \" startNode=\" + token.startNode + \" endNode=\" + token.endNode);\n    }\n    */\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["24a98f5fdd23e04f85819dbc63b47a12f7c44311"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","24a98f5fdd23e04f85819dbc63b47a12f7c44311"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["24a98f5fdd23e04f85819dbc63b47a12f7c44311","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["cd5edd1f2b162a5cfa08efd17851a07373a96817","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"f03e4bed5023ec3ef93a771b8888cae991cf448d":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}