{"path":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"modules/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish().pack(2, 100000);\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish().pack(2, 100000);\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"edb74c83fff94196b864e08ca033d92823252cb7","date":1339593164,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish().pack(2, 100000, PackedInts.DEFAULT);\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish().pack(2, 100000);\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"49f4d9c4e29f2345e789073801e7945431a23ca3","date":1355344131,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish().pack(2, 100000, PackedInts.DEFAULT);\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish().pack(2, 100000, PackedInts.DEFAULT);\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb8af2aec0a8574cf50cad6939d4475179595eca","date":1357675799,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4e6354dd7c71fe122926fc53d7d29f715b1283db","date":1357915185,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3c397b1515e745d8b12d70edfc0e17cb7eac7f0","date":1358188277,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, PackedInts.DEFAULT, true, 15);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8917bfede3b4ca30f4305c1e391e9218959cd723","date":1358189662,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, PackedInts.DEFAULT, true, 15);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, true);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6b1e6a1c51433ebaa34dc0b76d7ab1876072be39","date":1370266419,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, PackedInts.DEFAULT, true, 15);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton(true);\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, PackedInts.DEFAULT, true, 15);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, PackedInts.DEFAULT, true, 15);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<String[]>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, PackedInts.DEFAULT, true, 15);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8405d98acebb7e287bf7ac40e937ba05b8661285","date":1401433291,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, PackedInts.DEFAULT, true, 15);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, PackedInts.DEFAULT, true, 15);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.sizeInBytes() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":["6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f838187609fee3a1afa5f162f93c796046242c84","date":1406216791,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, PackedInts.DEFAULT, true, 15);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, null, true, PackedInts.DEFAULT, true, 15);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","date":1407854805,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, PackedInts.DEFAULT, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, PackedInts.DEFAULT, true, 15);\n    IntsRef scratch = new IntsRef();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.length = token.length();\n        for (int i = 0; i < token.length(); i++) {\n          scratch.ints[i] = (int) token.charAt(i);\n        }\n        fstBuilder.add(scratch, ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":["b3be20ca1091c0b7cdb2308b9023606a5e451cec","6ae8b8ec55786d06eb9b03fc7bc86a907e1a3ae2"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1abb939fb41b2fe4f89fd518f3da288c0213341d","date":1435657417,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, PackedInts.DEFAULT, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, PackedInts.DEFAULT, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fst.getNodeCount() + \" nodes, \" + fst.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6244579a467d5f2673ac98265d74bddbea1a8114","date":1478786509,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, PackedInts.DEFAULT, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"199dfa410f1fdbfd3294106b04096cce5ed34b21","date":1478812506,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, PackedInts.DEFAULT, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae67e1f82a53594208ca929f382ee861dad3d7a8","date":1557134375,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15, false);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7923aefdc3cf4457114880722d650b9b1a37ae28","date":1561070324,"type":3,"author":"Michael Sokolov","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (13 field values expected): \" + line);\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15, false);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          System.out.println(\"Entry in CSV is not valid: \" + line);\n          continue;\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15, false);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6aef50ee9987a6332ef151b4c1b6bfc7f830fc3b","date":1561080017,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (13 field values expected): \" + line);\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalForm != null) {\n          if (Normalizer.isNormalized(entry[0], normalForm)){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = Normalizer.normalize(entry[i], normalForm);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15, false);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (13 field values expected): \" + line);\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalizeEntries) {\n          if (normalizer.isNormalized(entry[0])){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = normalizer.normalize(entry[i]);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15, false);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05feb7eb5e91cb64742ce32b2ca3f02433530446","date":1561602840,"type":5,"author":"Michael Sokolov","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[Path]).mjava","pathOld":"lucene/analysis/kuromoji/src/tools/java/org/apache/lucene/analysis/ja/util/TokenInfoDictionaryBuilder#buildDictionary(List[File]).mjava","sourceNew":"  private TokenInfoDictionaryWriter buildDictionary(List<Path> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    Charset cs = Charset.forName(encoding);\n    // all lines in the file\n    List<String[]> lines = new ArrayList<>(400000);\n    for (Path path : csvFiles) {\n      try (BufferedReader reader = Files.newBufferedReader(path, cs)) {\n        String line;\n        while ((line = reader.readLine()) != null) {\n          String[] entry = CSVUtil.parse(line);\n\n          if (entry.length < 13) {\n            throw new IllegalArgumentException(\"Entry in CSV is not valid (13 field values expected): \" + line);\n          }\n\n          lines.add(formatEntry(entry));\n\n          if (normalForm != null) {\n            if (Normalizer.isNormalized(entry[0], normalForm)) {\n              continue;\n            }\n            String[] normalizedEntry = new String[entry.length];\n            for (int i = 0; i < entry.length; i++) {\n              normalizedEntry[i] = Normalizer.normalize(entry[i], normalForm);\n            }\n            lines.add(formatEntry(normalizedEntry));\n          }\n        }\n      }\n    }\n    \n    // sort by term: we sorted the files already and use a stable sort.\n    lines.sort(Comparator.comparing(entry -> entry[0]));\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15, false);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build token info dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        throw new IllegalStateException(\"Failed to process line: \" + Arrays.toString(entry));\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int) ord, offset);\n      offset = next;\n    }\n    dictionary.setFST(fstBuilder.finish());\n    return dictionary;\n  }\n\n","sourceOld":"  public TokenInfoDictionaryWriter buildDictionary(List<File> csvFiles) throws IOException {\n    TokenInfoDictionaryWriter dictionary = new TokenInfoDictionaryWriter(10 * 1024 * 1024);\n    \n    // all lines in the file\n    System.out.println(\"  parse...\");\n    List<String[]> lines = new ArrayList<>(400000);\n    for (File file : csvFiles){\n      FileInputStream inputStream = new FileInputStream(file);\n      Charset cs = Charset.forName(encoding);\n      CharsetDecoder decoder = cs.newDecoder()\n          .onMalformedInput(CodingErrorAction.REPORT)\n          .onUnmappableCharacter(CodingErrorAction.REPORT);\n      InputStreamReader streamReader = new InputStreamReader(inputStream, decoder);\n      BufferedReader reader = new BufferedReader(streamReader);\n      \n      String line = null;\n      while ((line = reader.readLine()) != null) {\n        String[] entry = CSVUtil.parse(line);\n\n        if(entry.length < 13) {\n          throw new IllegalArgumentException(\"Entry in CSV is not valid (13 field values expected): \" + line);\n        }\n        \n        String[] formatted = formatEntry(entry);\n        lines.add(formatted);\n        \n        // NFKC normalize dictionary entry\n        if (normalForm != null) {\n          if (Normalizer.isNormalized(entry[0], normalForm)){\n            continue;\n          }\n          String[] normalizedEntry = new String[entry.length];\n          for (int i = 0; i < entry.length; i++) {\n            normalizedEntry[i] = Normalizer.normalize(entry[i], normalForm);\n          }\n          \n          formatted = formatEntry(normalizedEntry);\n          lines.add(formatted);\n        }\n      }\n    }\n    \n    System.out.println(\"  sort...\");\n\n    // sort by term: we sorted the files already and use a stable sort.\n    Collections.sort(lines, new Comparator<String[]>() {\n      public int compare(String[] left, String[] right) {\n        return left[0].compareTo(right[0]);\n      }\n    });\n    \n    System.out.println(\"  encode...\");\n\n    PositiveIntOutputs fstOutput = PositiveIntOutputs.getSingleton();\n    Builder<Long> fstBuilder = new Builder<>(FST.INPUT_TYPE.BYTE2, 0, 0, true, true, Integer.MAX_VALUE, fstOutput, true, 15, false);\n    IntsRefBuilder scratch = new IntsRefBuilder();\n    long ord = -1; // first ord will be 0\n    String lastValue = null;\n\n    // build tokeninfo dictionary\n    for (String[] entry : lines) {\n      int next = dictionary.put(entry);\n        \n      if(next == offset){\n        System.out.println(\"Failed to process line: \" + Arrays.toString(entry));\n        continue;\n      }\n      \n      String token = entry[0];\n      if (!token.equals(lastValue)) {\n        // new word to add to fst\n        ord++;\n        lastValue = token;\n        scratch.grow(token.length());\n        scratch.setLength(token.length());\n        for (int i = 0; i < token.length(); i++) {\n          scratch.setIntAt(i, (int) token.charAt(i));\n        }\n        fstBuilder.add(scratch.get(), ord);\n      }\n      dictionary.addMapping((int)ord, offset);\n      offset = next;\n    }\n    \n    final FST<Long> fst = fstBuilder.finish();\n    \n    System.out.print(\"  \" + fstBuilder.getNodeCount() + \" nodes, \" + fstBuilder.getArcCount() + \" arcs, \" + fst.ramBytesUsed() + \" bytes...  \");\n    dictionary.setFST(fst);\n    System.out.println(\" done\");\n    \n    return dictionary;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1abb939fb41b2fe4f89fd518f3da288c0213341d":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["edb74c83fff94196b864e08ca033d92823252cb7","49f4d9c4e29f2345e789073801e7945431a23ca3"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6b1e6a1c51433ebaa34dc0b76d7ab1876072be39"],"8405d98acebb7e287bf7ac40e937ba05b8661285":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"d3c397b1515e745d8b12d70edfc0e17cb7eac7f0":["fb8af2aec0a8574cf50cad6939d4475179595eca"],"05feb7eb5e91cb64742ce32b2ca3f02433530446":["6aef50ee9987a6332ef151b4c1b6bfc7f830fc3b"],"8917bfede3b4ca30f4305c1e391e9218959cd723":["4e6354dd7c71fe122926fc53d7d29f715b1283db","d3c397b1515e745d8b12d70edfc0e17cb7eac7f0"],"6244579a467d5f2673ac98265d74bddbea1a8114":["1abb939fb41b2fe4f89fd518f3da288c0213341d"],"fb8af2aec0a8574cf50cad6939d4475179595eca":["49f4d9c4e29f2345e789073801e7945431a23ca3"],"199dfa410f1fdbfd3294106b04096cce5ed34b21":["1abb939fb41b2fe4f89fd518f3da288c0213341d","6244579a467d5f2673ac98265d74bddbea1a8114"],"ae67e1f82a53594208ca929f382ee861dad3d7a8":["6244579a467d5f2673ac98265d74bddbea1a8114"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["f838187609fee3a1afa5f162f93c796046242c84"],"f838187609fee3a1afa5f162f93c796046242c84":["8405d98acebb7e287bf7ac40e937ba05b8661285"],"6b1e6a1c51433ebaa34dc0b76d7ab1876072be39":["d3c397b1515e745d8b12d70edfc0e17cb7eac7f0"],"7923aefdc3cf4457114880722d650b9b1a37ae28":["ae67e1f82a53594208ca929f382ee861dad3d7a8"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"edb74c83fff94196b864e08ca033d92823252cb7":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"4e6354dd7c71fe122926fc53d7d29f715b1283db":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","fb8af2aec0a8574cf50cad6939d4475179595eca"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["05feb7eb5e91cb64742ce32b2ca3f02433530446"],"6aef50ee9987a6332ef151b4c1b6bfc7f830fc3b":["7923aefdc3cf4457114880722d650b9b1a37ae28"],"49f4d9c4e29f2345e789073801e7945431a23ca3":["edb74c83fff94196b864e08ca033d92823252cb7"]},"commit2Childs":{"1abb939fb41b2fe4f89fd518f3da288c0213341d":["6244579a467d5f2673ac98265d74bddbea1a8114","199dfa410f1fdbfd3294106b04096cce5ed34b21"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["4e6354dd7c71fe122926fc53d7d29f715b1283db"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["8405d98acebb7e287bf7ac40e937ba05b8661285"],"8405d98acebb7e287bf7ac40e937ba05b8661285":["f838187609fee3a1afa5f162f93c796046242c84"],"d3c397b1515e745d8b12d70edfc0e17cb7eac7f0":["8917bfede3b4ca30f4305c1e391e9218959cd723","6b1e6a1c51433ebaa34dc0b76d7ab1876072be39"],"05feb7eb5e91cb64742ce32b2ca3f02433530446":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"8917bfede3b4ca30f4305c1e391e9218959cd723":[],"fb8af2aec0a8574cf50cad6939d4475179595eca":["d3c397b1515e745d8b12d70edfc0e17cb7eac7f0","4e6354dd7c71fe122926fc53d7d29f715b1283db"],"6244579a467d5f2673ac98265d74bddbea1a8114":["199dfa410f1fdbfd3294106b04096cce5ed34b21","ae67e1f82a53594208ca929f382ee861dad3d7a8"],"199dfa410f1fdbfd3294106b04096cce5ed34b21":[],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["1abb939fb41b2fe4f89fd518f3da288c0213341d"],"ae67e1f82a53594208ca929f382ee861dad3d7a8":["7923aefdc3cf4457114880722d650b9b1a37ae28"],"f838187609fee3a1afa5f162f93c796046242c84":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"6b1e6a1c51433ebaa34dc0b76d7ab1876072be39":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"7923aefdc3cf4457114880722d650b9b1a37ae28":["6aef50ee9987a6332ef151b4c1b6bfc7f830fc3b"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["edb74c83fff94196b864e08ca033d92823252cb7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"edb74c83fff94196b864e08ca033d92823252cb7":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","49f4d9c4e29f2345e789073801e7945431a23ca3"],"4e6354dd7c71fe122926fc53d7d29f715b1283db":["8917bfede3b4ca30f4305c1e391e9218959cd723"],"49f4d9c4e29f2345e789073801e7945431a23ca3":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","fb8af2aec0a8574cf50cad6939d4475179595eca"],"6aef50ee9987a6332ef151b4c1b6bfc7f830fc3b":["05feb7eb5e91cb64742ce32b2ca3f02433530446"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["8917bfede3b4ca30f4305c1e391e9218959cd723","199dfa410f1fdbfd3294106b04096cce5ed34b21","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}