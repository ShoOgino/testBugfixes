{"path":"solr/core/src/test/org/apache/solr/cloud/api/collections/ShardSplitTest#testSplitStaticIndexReplication().mjava","commits":[{"id":"6146c07c0dee1ae1e42926167acd127fed5ef59d","date":1516129420,"type":1,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ShardSplitTest#testSplitStaticIndexReplication().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#testSplitStaticIndexReplication().mjava","sourceNew":"  /*\n  Creates a collection with replicationFactor=1, splits a shard. Restarts the sub-shard leader node.\n  Add a replica. Ensure count matches in leader and replica.\n   */\n  public void testSplitStaticIndexReplication() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    DocCollection defCol = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n    Replica replica = defCol.getReplicas().get(0);\n    String nodeName = replica.getNodeName();\n\n    String collectionName = \"testSplitStaticIndexReplication\";\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName, \"conf1\", 1, 1);\n    create.setMaxShardsPerNode(5); // some high number so we can create replicas without hindrance\n    create.setCreateNodeSet(nodeName); // we want to create the leader on a fixed node so that we know which one to restart later\n    create.process(cloudClient);\n    try (CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress(), true, cloudClient.getLbClient().getHttpClient())) {\n      client.setDefaultCollection(collectionName);\n      StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, client, \"i1\", true);\n      try {\n        thread.start();\n        Thread.sleep(1000); // give the indexer sometime to do its work\n        thread.safeStop();\n        thread.join();\n        client.commit();\n        controlClient.commit();\n\n        CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(collectionName);\n        splitShard.setShardName(SHARD1);\n        String asyncId = splitShard.processAsync(client);\n        RequestStatusState state = CollectionAdminRequest.requestStatus(asyncId).waitFor(client, 120);\n        if (state == RequestStatusState.COMPLETED)  {\n          waitForRecoveriesToFinish(collectionName, true);\n          // let's wait to see parent shard become inactive\n          CountDownLatch latch = new CountDownLatch(1);\n          client.getZkStateReader().registerCollectionStateWatcher(collectionName, new CollectionStateWatcher() {\n            @Override\n            public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n              Slice parent = collectionState.getSlice(SHARD1);\n              Slice slice10 = collectionState.getSlice(SHARD1_0);\n              Slice slice11 = collectionState.getSlice(SHARD1_1);\n              if (slice10 != null && slice11 != null &&\n                  parent.getState() == Slice.State.INACTIVE &&\n                  slice10.getState() == Slice.State.ACTIVE &&\n                  slice11.getState() == Slice.State.ACTIVE) {\n                latch.countDown();\n                return true; // removes the watch\n              }\n              return false;\n            }\n          });\n          latch.await(1, TimeUnit.MINUTES);\n          if (latch.getCount() != 0)  {\n            // sanity check\n            fail(\"Sub-shards did not become active even after waiting for 1 minute\");\n          }\n\n          int liveNodeCount = client.getZkStateReader().getClusterState().getLiveNodes().size();\n\n          // restart the sub-shard leader node\n          boolean restarted = false;\n          for (JettySolrRunner jetty : jettys) {\n            int port = jetty.getBaseUrl().getPort();\n            if (replica.getStr(BASE_URL_PROP).contains(\":\" + port))  {\n              ChaosMonkey.kill(jetty);\n              ChaosMonkey.start(jetty);\n              restarted = true;\n              break;\n            }\n          }\n          if (!restarted) {\n            // sanity check\n            fail(\"We could not find a jetty to kill for replica: \" + replica.getCoreUrl());\n          }\n\n          // add a new replica for the sub-shard\n          CollectionAdminRequest.AddReplica addReplica = CollectionAdminRequest.addReplicaToShard(collectionName, SHARD1_0);\n          // use control client because less chances of it being the node being restarted\n          // this is to avoid flakiness of test because of NoHttpResponseExceptions\n          String control_collection = client.getZkStateReader().getClusterState().getCollection(\"control_collection\").getReplicas().get(0).getStr(BASE_URL_PROP);\n          try (HttpSolrClient control = new HttpSolrClient.Builder(control_collection).withHttpClient(client.getLbClient().getHttpClient()).build())  {\n            state = addReplica.processAndWait(control, 30);\n          }\n          if (state == RequestStatusState.COMPLETED)  {\n            CountDownLatch newReplicaLatch = new CountDownLatch(1);\n            client.getZkStateReader().registerCollectionStateWatcher(collectionName, new CollectionStateWatcher() {\n              @Override\n              public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n                if (liveNodes.size() != liveNodeCount)  {\n                  return false;\n                }\n                Slice slice = collectionState.getSlice(SHARD1_0);\n                if (slice.getReplicas().size() == 2)  {\n                  if (!slice.getReplicas().stream().anyMatch(r -> r.getState() == Replica.State.RECOVERING)) {\n                    // we see replicas and none of them are recovering\n                    newReplicaLatch.countDown();\n                    return true;\n                  }\n                }\n                return false;\n              }\n            });\n            newReplicaLatch.await(30, TimeUnit.SECONDS);\n            // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n            // handle new shards/replica so well.\n            ClusterState clusterState = client.getZkStateReader().getClusterState();\n            DocCollection collection = clusterState.getCollection(collectionName);\n            int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n            assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n          } else  {\n            fail(\"Adding a replica to sub-shard did not complete even after waiting for 30 seconds!. Saw state = \" + state.getKey());\n          }\n        } else {\n          fail(\"We expected shard split to succeed on a static index but it didn't. Found state = \" + state.getKey());\n        }\n      } finally {\n        thread.safeStop();\n        thread.join();\n      }\n    }\n  }\n\n","sourceOld":"  /*\n  Creates a collection with replicationFactor=1, splits a shard. Restarts the sub-shard leader node.\n  Add a replica. Ensure count matches in leader and replica.\n   */\n  public void testSplitStaticIndexReplication() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    DocCollection defCol = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n    Replica replica = defCol.getReplicas().get(0);\n    String nodeName = replica.getNodeName();\n\n    String collectionName = \"testSplitStaticIndexReplication\";\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName, \"conf1\", 1, 1);\n    create.setMaxShardsPerNode(5); // some high number so we can create replicas without hindrance\n    create.setCreateNodeSet(nodeName); // we want to create the leader on a fixed node so that we know which one to restart later\n    create.process(cloudClient);\n    try (CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress(), true, cloudClient.getLbClient().getHttpClient())) {\n      client.setDefaultCollection(collectionName);\n      StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, client, \"i1\", true);\n      try {\n        thread.start();\n        Thread.sleep(1000); // give the indexer sometime to do its work\n        thread.safeStop();\n        thread.join();\n        client.commit();\n        controlClient.commit();\n\n        CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(collectionName);\n        splitShard.setShardName(SHARD1);\n        String asyncId = splitShard.processAsync(client);\n        RequestStatusState state = CollectionAdminRequest.requestStatus(asyncId).waitFor(client, 120);\n        if (state == RequestStatusState.COMPLETED)  {\n          waitForRecoveriesToFinish(collectionName, true);\n          // let's wait to see parent shard become inactive\n          CountDownLatch latch = new CountDownLatch(1);\n          client.getZkStateReader().registerCollectionStateWatcher(collectionName, new CollectionStateWatcher() {\n            @Override\n            public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n              Slice parent = collectionState.getSlice(SHARD1);\n              Slice slice10 = collectionState.getSlice(SHARD1_0);\n              Slice slice11 = collectionState.getSlice(SHARD1_1);\n              if (slice10 != null && slice11 != null &&\n                  parent.getState() == Slice.State.INACTIVE &&\n                  slice10.getState() == Slice.State.ACTIVE &&\n                  slice11.getState() == Slice.State.ACTIVE) {\n                latch.countDown();\n                return true; // removes the watch\n              }\n              return false;\n            }\n          });\n          latch.await(1, TimeUnit.MINUTES);\n          if (latch.getCount() != 0)  {\n            // sanity check\n            fail(\"Sub-shards did not become active even after waiting for 1 minute\");\n          }\n\n          int liveNodeCount = client.getZkStateReader().getClusterState().getLiveNodes().size();\n\n          // restart the sub-shard leader node\n          boolean restarted = false;\n          for (JettySolrRunner jetty : jettys) {\n            int port = jetty.getBaseUrl().getPort();\n            if (replica.getStr(BASE_URL_PROP).contains(\":\" + port))  {\n              ChaosMonkey.kill(jetty);\n              ChaosMonkey.start(jetty);\n              restarted = true;\n              break;\n            }\n          }\n          if (!restarted) {\n            // sanity check\n            fail(\"We could not find a jetty to kill for replica: \" + replica.getCoreUrl());\n          }\n\n          // add a new replica for the sub-shard\n          CollectionAdminRequest.AddReplica addReplica = CollectionAdminRequest.addReplicaToShard(collectionName, SHARD1_0);\n          // use control client because less chances of it being the node being restarted\n          // this is to avoid flakiness of test because of NoHttpResponseExceptions\n          String control_collection = client.getZkStateReader().getClusterState().getCollection(\"control_collection\").getReplicas().get(0).getStr(BASE_URL_PROP);\n          try (HttpSolrClient control = new HttpSolrClient.Builder(control_collection).withHttpClient(client.getLbClient().getHttpClient()).build())  {\n            state = addReplica.processAndWait(control, 30);\n          }\n          if (state == RequestStatusState.COMPLETED)  {\n            CountDownLatch newReplicaLatch = new CountDownLatch(1);\n            client.getZkStateReader().registerCollectionStateWatcher(collectionName, new CollectionStateWatcher() {\n              @Override\n              public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n                if (liveNodes.size() != liveNodeCount)  {\n                  return false;\n                }\n                Slice slice = collectionState.getSlice(SHARD1_0);\n                if (slice.getReplicas().size() == 2)  {\n                  if (!slice.getReplicas().stream().anyMatch(r -> r.getState() == Replica.State.RECOVERING)) {\n                    // we see replicas and none of them are recovering\n                    newReplicaLatch.countDown();\n                    return true;\n                  }\n                }\n                return false;\n              }\n            });\n            newReplicaLatch.await(30, TimeUnit.SECONDS);\n            // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n            // handle new shards/replica so well.\n            ClusterState clusterState = client.getZkStateReader().getClusterState();\n            DocCollection collection = clusterState.getCollection(collectionName);\n            int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n            assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n          } else  {\n            fail(\"Adding a replica to sub-shard did not complete even after waiting for 30 seconds!. Saw state = \" + state.getKey());\n          }\n        } else {\n          fail(\"We expected shard split to succeed on a static index but it didn't. Found state = \" + state.getKey());\n        }\n      } finally {\n        thread.safeStop();\n        thread.join();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ShardSplitTest#testSplitStaticIndexReplication().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ShardSplitTest#testSplitStaticIndexReplication().mjava","sourceNew":"  /*\n  Creates a collection with replicationFactor=1, splits a shard. Restarts the sub-shard leader node.\n  Add a replica. Ensure count matches in leader and replica.\n   */\n  public void testSplitStaticIndexReplication() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    DocCollection defCol = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n    Replica replica = defCol.getReplicas().get(0);\n    String nodeName = replica.getNodeName();\n\n    String collectionName = \"testSplitStaticIndexReplication\";\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName, \"conf1\", 1, 1);\n    create.setMaxShardsPerNode(5); // some high number so we can create replicas without hindrance\n    create.setCreateNodeSet(nodeName); // we want to create the leader on a fixed node so that we know which one to restart later\n    create.process(cloudClient);\n    try (CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress(), true, cloudClient.getLbClient().getHttpClient())) {\n      client.setDefaultCollection(collectionName);\n      StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, client, \"i1\", true);\n      try {\n        thread.start();\n        Thread.sleep(1000); // give the indexer sometime to do its work\n        thread.safeStop();\n        thread.join();\n        client.commit();\n        controlClient.commit();\n\n        CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(collectionName);\n        splitShard.setShardName(SHARD1);\n        String asyncId = splitShard.processAsync(client);\n        RequestStatusState state = CollectionAdminRequest.requestStatus(asyncId).waitFor(client, 120);\n        if (state == RequestStatusState.COMPLETED)  {\n          waitForRecoveriesToFinish(collectionName, true);\n          // let's wait to see parent shard become inactive\n          CountDownLatch latch = new CountDownLatch(1);\n          client.getZkStateReader().registerCollectionStateWatcher(collectionName, new CollectionStateWatcher() {\n            @Override\n            public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n              Slice parent = collectionState.getSlice(SHARD1);\n              Slice slice10 = collectionState.getSlice(SHARD1_0);\n              Slice slice11 = collectionState.getSlice(SHARD1_1);\n              if (slice10 != null && slice11 != null &&\n                  parent.getState() == Slice.State.INACTIVE &&\n                  slice10.getState() == Slice.State.ACTIVE &&\n                  slice11.getState() == Slice.State.ACTIVE) {\n                latch.countDown();\n                return true; // removes the watch\n              }\n              return false;\n            }\n          });\n          latch.await(1, TimeUnit.MINUTES);\n          if (latch.getCount() != 0)  {\n            // sanity check\n            fail(\"Sub-shards did not become active even after waiting for 1 minute\");\n          }\n\n          int liveNodeCount = client.getZkStateReader().getClusterState().getLiveNodes().size();\n\n          // restart the sub-shard leader node\n          boolean restarted = false;\n          for (JettySolrRunner jetty : jettys) {\n            int port = jetty.getBaseUrl().getPort();\n            if (replica.getStr(BASE_URL_PROP).contains(\":\" + port))  {\n              ChaosMonkey.kill(jetty);\n              ChaosMonkey.start(jetty);\n              restarted = true;\n              break;\n            }\n          }\n          if (!restarted) {\n            // sanity check\n            fail(\"We could not find a jetty to kill for replica: \" + replica.getCoreUrl());\n          }\n\n          // add a new replica for the sub-shard\n          CollectionAdminRequest.AddReplica addReplica = CollectionAdminRequest.addReplicaToShard(collectionName, SHARD1_0);\n          // use control client because less chances of it being the node being restarted\n          // this is to avoid flakiness of test because of NoHttpResponseExceptions\n          String control_collection = client.getZkStateReader().getClusterState().getCollection(\"control_collection\").getReplicas().get(0).getStr(BASE_URL_PROP);\n          try (HttpSolrClient control = new HttpSolrClient.Builder(control_collection).withHttpClient(client.getLbClient().getHttpClient()).build())  {\n            state = addReplica.processAndWait(control, 30);\n          }\n          if (state == RequestStatusState.COMPLETED)  {\n            CountDownLatch newReplicaLatch = new CountDownLatch(1);\n            client.getZkStateReader().registerCollectionStateWatcher(collectionName, new CollectionStateWatcher() {\n              @Override\n              public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n                if (liveNodes.size() != liveNodeCount)  {\n                  return false;\n                }\n                Slice slice = collectionState.getSlice(SHARD1_0);\n                if (slice.getReplicas().size() == 2)  {\n                  if (!slice.getReplicas().stream().anyMatch(r -> r.getState() == Replica.State.RECOVERING)) {\n                    // we see replicas and none of them are recovering\n                    newReplicaLatch.countDown();\n                    return true;\n                  }\n                }\n                return false;\n              }\n            });\n            newReplicaLatch.await(30, TimeUnit.SECONDS);\n            // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n            // handle new shards/replica so well.\n            ClusterState clusterState = client.getZkStateReader().getClusterState();\n            DocCollection collection = clusterState.getCollection(collectionName);\n            int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n            assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n          } else  {\n            fail(\"Adding a replica to sub-shard did not complete even after waiting for 30 seconds!. Saw state = \" + state.getKey());\n          }\n        } else {\n          fail(\"We expected shard split to succeed on a static index but it didn't. Found state = \" + state.getKey());\n        }\n      } finally {\n        thread.safeStop();\n        thread.join();\n      }\n    }\n  }\n\n","sourceOld":"  /*\n  Creates a collection with replicationFactor=1, splits a shard. Restarts the sub-shard leader node.\n  Add a replica. Ensure count matches in leader and replica.\n   */\n  public void testSplitStaticIndexReplication() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    DocCollection defCol = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n    Replica replica = defCol.getReplicas().get(0);\n    String nodeName = replica.getNodeName();\n\n    String collectionName = \"testSplitStaticIndexReplication\";\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName, \"conf1\", 1, 1);\n    create.setMaxShardsPerNode(5); // some high number so we can create replicas without hindrance\n    create.setCreateNodeSet(nodeName); // we want to create the leader on a fixed node so that we know which one to restart later\n    create.process(cloudClient);\n    try (CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress(), true, cloudClient.getLbClient().getHttpClient())) {\n      client.setDefaultCollection(collectionName);\n      StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, client, \"i1\", true);\n      try {\n        thread.start();\n        Thread.sleep(1000); // give the indexer sometime to do its work\n        thread.safeStop();\n        thread.join();\n        client.commit();\n        controlClient.commit();\n\n        CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(collectionName);\n        splitShard.setShardName(SHARD1);\n        String asyncId = splitShard.processAsync(client);\n        RequestStatusState state = CollectionAdminRequest.requestStatus(asyncId).waitFor(client, 120);\n        if (state == RequestStatusState.COMPLETED)  {\n          waitForRecoveriesToFinish(collectionName, true);\n          // let's wait to see parent shard become inactive\n          CountDownLatch latch = new CountDownLatch(1);\n          client.getZkStateReader().registerCollectionStateWatcher(collectionName, new CollectionStateWatcher() {\n            @Override\n            public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n              Slice parent = collectionState.getSlice(SHARD1);\n              Slice slice10 = collectionState.getSlice(SHARD1_0);\n              Slice slice11 = collectionState.getSlice(SHARD1_1);\n              if (slice10 != null && slice11 != null &&\n                  parent.getState() == Slice.State.INACTIVE &&\n                  slice10.getState() == Slice.State.ACTIVE &&\n                  slice11.getState() == Slice.State.ACTIVE) {\n                latch.countDown();\n                return true; // removes the watch\n              }\n              return false;\n            }\n          });\n          latch.await(1, TimeUnit.MINUTES);\n          if (latch.getCount() != 0)  {\n            // sanity check\n            fail(\"Sub-shards did not become active even after waiting for 1 minute\");\n          }\n\n          int liveNodeCount = client.getZkStateReader().getClusterState().getLiveNodes().size();\n\n          // restart the sub-shard leader node\n          boolean restarted = false;\n          for (JettySolrRunner jetty : jettys) {\n            int port = jetty.getBaseUrl().getPort();\n            if (replica.getStr(BASE_URL_PROP).contains(\":\" + port))  {\n              ChaosMonkey.kill(jetty);\n              ChaosMonkey.start(jetty);\n              restarted = true;\n              break;\n            }\n          }\n          if (!restarted) {\n            // sanity check\n            fail(\"We could not find a jetty to kill for replica: \" + replica.getCoreUrl());\n          }\n\n          // add a new replica for the sub-shard\n          CollectionAdminRequest.AddReplica addReplica = CollectionAdminRequest.addReplicaToShard(collectionName, SHARD1_0);\n          // use control client because less chances of it being the node being restarted\n          // this is to avoid flakiness of test because of NoHttpResponseExceptions\n          String control_collection = client.getZkStateReader().getClusterState().getCollection(\"control_collection\").getReplicas().get(0).getStr(BASE_URL_PROP);\n          try (HttpSolrClient control = new HttpSolrClient.Builder(control_collection).withHttpClient(client.getLbClient().getHttpClient()).build())  {\n            state = addReplica.processAndWait(control, 30);\n          }\n          if (state == RequestStatusState.COMPLETED)  {\n            CountDownLatch newReplicaLatch = new CountDownLatch(1);\n            client.getZkStateReader().registerCollectionStateWatcher(collectionName, new CollectionStateWatcher() {\n              @Override\n              public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n                if (liveNodes.size() != liveNodeCount)  {\n                  return false;\n                }\n                Slice slice = collectionState.getSlice(SHARD1_0);\n                if (slice.getReplicas().size() == 2)  {\n                  if (!slice.getReplicas().stream().anyMatch(r -> r.getState() == Replica.State.RECOVERING)) {\n                    // we see replicas and none of them are recovering\n                    newReplicaLatch.countDown();\n                    return true;\n                  }\n                }\n                return false;\n              }\n            });\n            newReplicaLatch.await(30, TimeUnit.SECONDS);\n            // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n            // handle new shards/replica so well.\n            ClusterState clusterState = client.getZkStateReader().getClusterState();\n            DocCollection collection = clusterState.getCollection(collectionName);\n            int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n            assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n          } else  {\n            fail(\"Adding a replica to sub-shard did not complete even after waiting for 30 seconds!. Saw state = \" + state.getKey());\n          }\n        } else {\n          fail(\"We expected shard split to succeed on a static index but it didn't. Found state = \" + state.getKey());\n        }\n      } finally {\n        thread.safeStop();\n        thread.join();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"20c968c14aace7cf49843bf2c1fafc7fd3845659","date":1533133859,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/api/collections/ShardSplitTest#testSplitStaticIndexReplication().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/api/collections/ShardSplitTest#testSplitStaticIndexReplication().mjava","sourceNew":"  /*\n  Creates a collection with replicationFactor=1, splits a shard. Restarts the sub-shard leader node.\n  Add a replica. Ensure count matches in leader and replica.\n   */\n  @Test\n  public void testSplitStaticIndexReplication() throws Exception {\n    doSplitStaticIndexReplication(SolrIndexSplitter.SplitMethod.REWRITE);\n  }\n\n","sourceOld":"  /*\n  Creates a collection with replicationFactor=1, splits a shard. Restarts the sub-shard leader node.\n  Add a replica. Ensure count matches in leader and replica.\n   */\n  public void testSplitStaticIndexReplication() throws Exception {\n    waitForThingsToLevelOut(15);\n\n    DocCollection defCol = cloudClient.getZkStateReader().getClusterState().getCollection(AbstractDistribZkTestBase.DEFAULT_COLLECTION);\n    Replica replica = defCol.getReplicas().get(0);\n    String nodeName = replica.getNodeName();\n\n    String collectionName = \"testSplitStaticIndexReplication\";\n    CollectionAdminRequest.Create create = CollectionAdminRequest.createCollection(collectionName, \"conf1\", 1, 1);\n    create.setMaxShardsPerNode(5); // some high number so we can create replicas without hindrance\n    create.setCreateNodeSet(nodeName); // we want to create the leader on a fixed node so that we know which one to restart later\n    create.process(cloudClient);\n    try (CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress(), true, cloudClient.getLbClient().getHttpClient())) {\n      client.setDefaultCollection(collectionName);\n      StoppableIndexingThread thread = new StoppableIndexingThread(controlClient, client, \"i1\", true);\n      try {\n        thread.start();\n        Thread.sleep(1000); // give the indexer sometime to do its work\n        thread.safeStop();\n        thread.join();\n        client.commit();\n        controlClient.commit();\n\n        CollectionAdminRequest.SplitShard splitShard = CollectionAdminRequest.splitShard(collectionName);\n        splitShard.setShardName(SHARD1);\n        String asyncId = splitShard.processAsync(client);\n        RequestStatusState state = CollectionAdminRequest.requestStatus(asyncId).waitFor(client, 120);\n        if (state == RequestStatusState.COMPLETED)  {\n          waitForRecoveriesToFinish(collectionName, true);\n          // let's wait to see parent shard become inactive\n          CountDownLatch latch = new CountDownLatch(1);\n          client.getZkStateReader().registerCollectionStateWatcher(collectionName, new CollectionStateWatcher() {\n            @Override\n            public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n              Slice parent = collectionState.getSlice(SHARD1);\n              Slice slice10 = collectionState.getSlice(SHARD1_0);\n              Slice slice11 = collectionState.getSlice(SHARD1_1);\n              if (slice10 != null && slice11 != null &&\n                  parent.getState() == Slice.State.INACTIVE &&\n                  slice10.getState() == Slice.State.ACTIVE &&\n                  slice11.getState() == Slice.State.ACTIVE) {\n                latch.countDown();\n                return true; // removes the watch\n              }\n              return false;\n            }\n          });\n          latch.await(1, TimeUnit.MINUTES);\n          if (latch.getCount() != 0)  {\n            // sanity check\n            fail(\"Sub-shards did not become active even after waiting for 1 minute\");\n          }\n\n          int liveNodeCount = client.getZkStateReader().getClusterState().getLiveNodes().size();\n\n          // restart the sub-shard leader node\n          boolean restarted = false;\n          for (JettySolrRunner jetty : jettys) {\n            int port = jetty.getBaseUrl().getPort();\n            if (replica.getStr(BASE_URL_PROP).contains(\":\" + port))  {\n              ChaosMonkey.kill(jetty);\n              ChaosMonkey.start(jetty);\n              restarted = true;\n              break;\n            }\n          }\n          if (!restarted) {\n            // sanity check\n            fail(\"We could not find a jetty to kill for replica: \" + replica.getCoreUrl());\n          }\n\n          // add a new replica for the sub-shard\n          CollectionAdminRequest.AddReplica addReplica = CollectionAdminRequest.addReplicaToShard(collectionName, SHARD1_0);\n          // use control client because less chances of it being the node being restarted\n          // this is to avoid flakiness of test because of NoHttpResponseExceptions\n          String control_collection = client.getZkStateReader().getClusterState().getCollection(\"control_collection\").getReplicas().get(0).getStr(BASE_URL_PROP);\n          try (HttpSolrClient control = new HttpSolrClient.Builder(control_collection).withHttpClient(client.getLbClient().getHttpClient()).build())  {\n            state = addReplica.processAndWait(control, 30);\n          }\n          if (state == RequestStatusState.COMPLETED)  {\n            CountDownLatch newReplicaLatch = new CountDownLatch(1);\n            client.getZkStateReader().registerCollectionStateWatcher(collectionName, new CollectionStateWatcher() {\n              @Override\n              public boolean onStateChanged(Set<String> liveNodes, DocCollection collectionState) {\n                if (liveNodes.size() != liveNodeCount)  {\n                  return false;\n                }\n                Slice slice = collectionState.getSlice(SHARD1_0);\n                if (slice.getReplicas().size() == 2)  {\n                  if (!slice.getReplicas().stream().anyMatch(r -> r.getState() == Replica.State.RECOVERING)) {\n                    // we see replicas and none of them are recovering\n                    newReplicaLatch.countDown();\n                    return true;\n                  }\n                }\n                return false;\n              }\n            });\n            newReplicaLatch.await(30, TimeUnit.SECONDS);\n            // check consistency of sub-shard replica explicitly because checkShardConsistency methods doesn't\n            // handle new shards/replica so well.\n            ClusterState clusterState = client.getZkStateReader().getClusterState();\n            DocCollection collection = clusterState.getCollection(collectionName);\n            int numReplicasChecked = assertConsistentReplicas(collection.getSlice(SHARD1_0));\n            assertEquals(\"We should have checked consistency for exactly 2 replicas of shard1_0\", 2, numReplicasChecked);\n          } else  {\n            fail(\"Adding a replica to sub-shard did not complete even after waiting for 30 seconds!. Saw state = \" + state.getKey());\n          }\n        } else {\n          fail(\"We expected shard split to succeed on a static index but it didn't. Found state = \" + state.getKey());\n        }\n      } finally {\n        thread.safeStop();\n        thread.join();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b94236357aaa22b76c10629851fe4e376e0cea82":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","6146c07c0dee1ae1e42926167acd127fed5ef59d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"6146c07c0dee1ae1e42926167acd127fed5ef59d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"20c968c14aace7cf49843bf2c1fafc7fd3845659":["b94236357aaa22b76c10629851fe4e376e0cea82"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["20c968c14aace7cf49843bf2c1fafc7fd3845659"]},"commit2Childs":{"b94236357aaa22b76c10629851fe4e376e0cea82":["20c968c14aace7cf49843bf2c1fafc7fd3845659"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b94236357aaa22b76c10629851fe4e376e0cea82","6146c07c0dee1ae1e42926167acd127fed5ef59d"],"6146c07c0dee1ae1e42926167acd127fed5ef59d":["b94236357aaa22b76c10629851fe4e376e0cea82"],"20c968c14aace7cf49843bf2c1fafc7fd3845659":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}