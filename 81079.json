{"path":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(\"meat\");\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(\"meat\");\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","date":1305207152,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(\"meat\");\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(\"meat\");\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(\"meat\");\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(\"meat\");\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(\"meat\");\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(\"meat\");\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dfd4d352ddf04b37253ad97ce1aad1448253f0f7","date":1310173878,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(\"meat\");\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"63639dd66fd5bd9b90bc24dd596ae01575f27cc4","date":1310237454,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(\"meat\");\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cf7efd82433f3f64684711c16edfd149db6af111","date":1317013128,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.reusableTokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME, tokenStream,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"69e043c521d4e8db770cc140c63f5ef51f03426a","date":1317187614,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.reusableTokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random, MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest#testMaxSizeHighlight().mjava","sourceNew":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","sourceOld":"  public void testMaxSizeHighlight() throws Exception {\n    final MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.SIMPLE, true, MockTokenFilter.ENGLISH_STOPSET, true);\n    // we disable MockTokenizer checks because we will forcefully limit the \n    // tokenstream and call end() before incrementToken() returns false.\n    analyzer.setEnableChecks(false);\n    TestHighlightRunner helper = new TestHighlightRunner() {\n\n      @Override\n      public void run() throws Exception {\n        numHighlights = 0;\n        doSearching(new TermQuery(new Term(FIELD_NAME, \"meat\")));\n        TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(texts[0]));\n        Highlighter highlighter = getHighlighter(query, FIELD_NAME,\n            HighlighterTest.this);// new Highlighter(this, new\n        // QueryTermScorer(query));\n        highlighter.setMaxDocCharsToAnalyze(30);\n\n        highlighter.getBestFragment(tokenStream, texts[0]);\n        assertTrue(\"Setting MaxDocBytesToAnalyze should have prevented \"\n            + \"us from finding matches for this record: \" + numHighlights + \" found\",\n            numHighlights == 0);\n      }\n    };\n\n    helper.start();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"dfd4d352ddf04b37253ad97ce1aad1448253f0f7":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a3776dccca01c11e7046323cfad46a3b4a471233":["9454a6510e2db155fb01faa5c049b06ece95fab9","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["9454a6510e2db155fb01faa5c049b06ece95fab9","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"cf7efd82433f3f64684711c16edfd149db6af111":["dfd4d352ddf04b37253ad97ce1aad1448253f0f7"],"63639dd66fd5bd9b90bc24dd596ae01575f27cc4":["a3776dccca01c11e7046323cfad46a3b4a471233","dfd4d352ddf04b37253ad97ce1aad1448253f0f7"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["cf7efd82433f3f64684711c16edfd149db6af111"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"dfd4d352ddf04b37253ad97ce1aad1448253f0f7":["cf7efd82433f3f64684711c16edfd149db6af111","63639dd66fd5bd9b90bc24dd596ae01575f27cc4"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["dfd4d352ddf04b37253ad97ce1aad1448253f0f7","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"a3776dccca01c11e7046323cfad46a3b4a471233":["63639dd66fd5bd9b90bc24dd596ae01575f27cc4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":[],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cf7efd82433f3f64684711c16edfd149db6af111":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"63639dd66fd5bd9b90bc24dd596ae01575f27cc4":[],"69e043c521d4e8db770cc140c63f5ef51f03426a":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb","63639dd66fd5bd9b90bc24dd596ae01575f27cc4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}