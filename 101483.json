{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","commits":[{"id":"b7465988fd0a9c673dcb88f51473300c41d630f0","date":1311685662,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", count+\"\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      if (dir.fileExists(\"_0_1.del\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","sourceNew":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", count+\"\"));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      if (dir.fileExists(\"_0_1.del\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new Field(\"id\", count+\"\", Field.Store.NO, Field.Index.NOT_ANALYZED));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      if (dir.fileExists(\"_0_1.del\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","sourceNew":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", count+\"\"));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      if (dir.fileExists(\"_0_1.del\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", count+\"\"));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      if (dir.fileExists(\"_0_1.del\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2972b4f2d23be6887a2f48be21a969d8a98610d6","date":1327078547,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","sourceNew":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", count+\"\"));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      // TODO: fix this test\n      if (dir.fileExists(\"_0_1.del\") || dir.fileExists(\"_0_1.liv\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", count+\"\"));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      if (dir.fileExists(\"_0_1.del\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31","date":1327836826,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","sourceNew":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", count+\"\"));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      // TODO: fix this test\n      if (dir.fileExists(\"_0_1.del\") || dir.fileExists(\"_0_1.liv\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", count+\"\"));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      if (dir.fileExists(\"_0_1.del\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fd92b8bcc88e969302510acf77bd6970da3994c4","date":1327839530,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","sourceNew":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", count+\"\"));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      // TODO: fix this test\n      if (dir.fileExists(\"_0_1.del\") || dir.fileExists(\"_0_1.liv\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", count+\"\"));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      if (dir.fileExists(\"_0_1.del\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testFlushPushedDeletesByCount().mjava","sourceNew":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", count+\"\"));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      // TODO: fix this test\n      if (dir.fileExists(\"_0_1.del\") || dir.fileExists(\"_0_1.liv\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-3340: make sure deletes that we don't apply\n  // during flush (ie are just pushed into the stream) are\n  // in fact later flushed due to their RAM usage:\n  public void testFlushPushedDeletesByCount() throws Exception {\n    Directory dir = newDirectory();\n    // Cannot use RandomIndexWriter because we don't want to\n    // ever call commit() for this test:\n    final int flushAtDelCount = atLeast(1020);\n    IndexWriter w = new IndexWriter(dir,\n                                    newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n                                    setMaxBufferedDeleteTerms(flushAtDelCount).setMaxBufferedDocs(1000).setRAMBufferSizeMB(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES).setReaderPooling(false));\n    int count = 0;\n    while(true) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", count+\"\"));\n      final Term delTerm;\n      if (count == 1010) {\n        // This is the only delete that applies\n        delTerm = new Term(\"id\", \"\"+0);\n      } else {\n        // These get buffered, taking up RAM, but delete\n        // nothing when applied:\n        delTerm = new Term(\"id\", \"x\" + count);\n      }\n      w.updateDocument(delTerm, doc);\n      // Eventually segment 0 should get a del docs:\n      // TODO: fix this test\n      if (dir.fileExists(\"_0_1.del\") || dir.fileExists(\"_0_1.liv\")) {\n        break;\n      }\n      count++;\n      if (count > flushAtDelCount) {\n        fail(\"delete's were not applied at count=\" + flushAtDelCount);\n      }\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"2972b4f2d23be6887a2f48be21a969d8a98610d6":["06584e6e98d592b34e1329b384182f368d2025e8"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"06584e6e98d592b34e1329b384182f368d2025e8":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"b7465988fd0a9c673dcb88f51473300c41d630f0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fd92b8bcc88e969302510acf77bd6970da3994c4":["06584e6e98d592b34e1329b384182f368d2025e8","c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31":["06584e6e98d592b34e1329b384182f368d2025e8","2972b4f2d23be6887a2f48be21a969d8a98610d6"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["b7465988fd0a9c673dcb88f51473300c41d630f0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"2972b4f2d23be6887a2f48be21a969d8a98610d6":["c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"06584e6e98d592b34e1329b384182f368d2025e8":["2972b4f2d23be6887a2f48be21a969d8a98610d6","fd92b8bcc88e969302510acf77bd6970da3994c4","c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b7465988fd0a9c673dcb88f51473300c41d630f0":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b7465988fd0a9c673dcb88f51473300c41d630f0"],"fd92b8bcc88e969302510acf77bd6970da3994c4":[],"c9f2d6bb11ccaac366d9b7652b2feb0c715d9c31":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","fd92b8bcc88e969302510acf77bd6970da3994c4"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["06584e6e98d592b34e1329b384182f368d2025e8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["fd92b8bcc88e969302510acf77bd6970da3994c4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}