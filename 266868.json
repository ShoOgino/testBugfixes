{"path":"lucene/classification/src/java/org/apache/lucene/classification/utils/NearestFuzzyQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","commits":[{"id":"b9626938d906e220bc834fca668189e7cdf4985d","date":1498723309,"type":0,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/NearestFuzzyQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"/dev/null","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n              atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n              fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              if (st != null) {\n                st.score = (st.score * st.score) * idf(df, corpusNumDocs);\n                q.insertWithOverflow(st);\n              }\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/NearestFuzzyQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"/dev/null","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n              atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n              fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              if (st != null) {\n                st.score = (st.score * st.score) * idf(df, corpusNumDocs);\n                q.insertWithOverflow(st);\n              }\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30c8e5574b55d57947e989443dfde611646530ee","date":1499131153,"type":0,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/NearestFuzzyQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"/dev/null","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n              atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n              fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              if (st != null) {\n                st.score = (st.score * st.score) * idf(df, corpusNumDocs);\n                q.insertWithOverflow(st);\n              }\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/NearestFuzzyQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/NearestFuzzyQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiTerms.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n              atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n              fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              if (st != null) {\n                st.score = (st.score * st.score) * idf(df, corpusNumDocs);\n                q.insertWithOverflow(st);\n              }\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiFields.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n              atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n              fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              if (st != null) {\n                st.score = (st.score * st.score) * idf(df, corpusNumDocs);\n                q.insertWithOverflow(st);\n              }\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e99d92de6748e3bbd2dd7b72695cdb952b2d835","date":1579100291,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/utils/NearestFuzzyQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/utils/NearestFuzzyQuery#addTerms(IndexReader,FieldVals,ScoreTermQueue).mjava","sourceNew":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiTerms.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = fe.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            fe.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              if (st != null) {\n                st.score = (st.score * st.score) * idf(df, corpusNumDocs);\n                q.insertWithOverflow(st);\n              }\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","sourceOld":"  private void addTerms(IndexReader reader, FieldVals f, ScoreTermQueue q) throws IOException {\n    if (f.queryString == null) return;\n    final Terms terms = MultiTerms.getTerms(reader, f.fieldName);\n    if (terms == null) {\n      return;\n    }\n    try (TokenStream ts = analyzer.tokenStream(f.fieldName, f.queryString)) {\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n\n      int corpusNumDocs = reader.numDocs();\n      HashSet<String> processedTerms = new HashSet<>();\n      ts.reset();\n      while (ts.incrementToken()) {\n        String term = termAtt.toString();\n        if (!processedTerms.contains(term)) {\n          processedTerms.add(term);\n          ScoreTermQueue variantsQ = new ScoreTermQueue(MAX_VARIANTS_PER_TERM); //maxNum variants considered for any one term\n          float minScore = 0;\n          Term startTerm = new Term(f.fieldName, term);\n          AttributeSource atts = new AttributeSource();\n          MaxNonCompetitiveBoostAttribute maxBoostAtt =\n              atts.addAttribute(MaxNonCompetitiveBoostAttribute.class);\n          FuzzyTermsEnum fe = new FuzzyTermsEnum(terms, atts, startTerm, f.maxEdits, f.prefixLength, true);\n          //store the df so all variants use same idf\n          int df = reader.docFreq(startTerm);\n          int numVariants = 0;\n          int totalVariantDocFreqs = 0;\n          BytesRef possibleMatch;\n          BoostAttribute boostAtt =\n              fe.attributes().addAttribute(BoostAttribute.class);\n          while ((possibleMatch = fe.next()) != null) {\n            numVariants++;\n            totalVariantDocFreqs += fe.docFreq();\n            float score = boostAtt.getBoost();\n            if (variantsQ.size() < MAX_VARIANTS_PER_TERM || score > minScore) {\n              ScoreTerm st = new ScoreTerm(new Term(startTerm.field(), BytesRef.deepCopyOf(possibleMatch)), score, startTerm);\n              variantsQ.insertWithOverflow(st);\n              minScore = variantsQ.top().score; // maintain minScore\n            }\n            maxBoostAtt.setMaxNonCompetitiveBoost(variantsQ.size() >= MAX_VARIANTS_PER_TERM ? minScore : Float.NEGATIVE_INFINITY);\n          }\n\n          if (numVariants > 0) {\n            int avgDf = totalVariantDocFreqs / numVariants;\n            if (df == 0)//no direct match we can use as df for all variants\n            {\n              df = avgDf; //use avg df of all variants\n            }\n\n            // take the top variants (scored by edit distance) and reset the score\n            // to include an IDF factor then add to the global queue for ranking\n            // overall top query terms\n            int size = variantsQ.size();\n            for (int i = 0; i < size; i++) {\n              ScoreTerm st = variantsQ.pop();\n              if (st != null) {\n                st.score = (st.score * st.score) * idf(df, corpusNumDocs);\n                q.insertWithOverflow(st);\n              }\n            }\n          }\n        }\n      }\n      ts.end();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3e99d92de6748e3bbd2dd7b72695cdb952b2d835":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"b9626938d906e220bc834fca668189e7cdf4985d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"30c8e5574b55d57947e989443dfde611646530ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","28288370235ed02234a64753cdbf0c6ec096304a"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["28288370235ed02234a64753cdbf0c6ec096304a"],"28288370235ed02234a64753cdbf0c6ec096304a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b9626938d906e220bc834fca668189e7cdf4985d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3e99d92de6748e3bbd2dd7b72695cdb952b2d835"]},"commit2Childs":{"3e99d92de6748e3bbd2dd7b72695cdb952b2d835":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b9626938d906e220bc834fca668189e7cdf4985d":["28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b9626938d906e220bc834fca668189e7cdf4985d","30c8e5574b55d57947e989443dfde611646530ee","28288370235ed02234a64753cdbf0c6ec096304a"],"30c8e5574b55d57947e989443dfde611646530ee":[],"04e775de416dd2d8067b10db1c8af975a1d5017e":["3e99d92de6748e3bbd2dd7b72695cdb952b2d835"],"28288370235ed02234a64753cdbf0c6ec096304a":["30c8e5574b55d57947e989443dfde611646530ee","04e775de416dd2d8067b10db1c8af975a1d5017e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["30c8e5574b55d57947e989443dfde611646530ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}