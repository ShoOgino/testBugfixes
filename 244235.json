{"path":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = IndexReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = IndexReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = IndexReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random, directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = IndexReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random, new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random, new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random, dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random, dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random, 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":["302be0cc5e6a28ebcebcac98aa81a92be2e94370"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4356000e349e38c9fb48034695b7c309abd54557","date":1337460341,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = IndexReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = IndexReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"615ddbd81799980d0fdd95e0238e1c498b6f47b0","date":1338233290,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = IndexReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = IndexReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    do {\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = IndexReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newField(field, docFields[i], TextField.TYPE_UNSTORED));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newField(\"field2\", \"xxx\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newField(\"field2\", \"big bad bug\", TextField.TYPE_UNSTORED));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b0c73b4e6b72cca35c7f115ab543ce9dcf50d8b5","date":1344439048,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":["a3a0403b45dfe384fae4a1b6e96c3265d000c498"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7869f64c874ebf7f317d22c00baf2b6857797a6","date":1344856617,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = new IndexSearcher(littleReader);\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(_TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.shutdown();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.shutdown();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.shutdown();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.shutdown();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.shutdown();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.shutdown();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.shutdown();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.shutdown();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.shutdown();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.shutdown();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.shutdown();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"302be0cc5e6a28ebcebcac98aa81a92be2e94370","date":1423848654,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), new RAMDirectory(directory, IOContext.DEFAULT));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), new RAMDirectory(dir2, IOContext.DEFAULT));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"81d0720146de53dd3a4a023d2a3d1089d86d748d","date":1442268215,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new DefaultSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c82b0d4b7bf499a159eeff92add20bac6599cc1","date":1465223716,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap betwen docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    \n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    singleSegmentDirectory = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":["97358022f8fb21ca48fc5f2eb205c98a484e2174","29d6eadc26bef1d3f5d3804b90cb7ba77162d007"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"191128ac5b85671b1671e2c857437694283b6ebf","date":1465297861,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap betwen docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    \n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    singleSegmentDirectory = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7524d27129b064aa52a7feecc8e26f4c13ccc6cc","date":1466330609,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap betwen docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    for (String fileName : directory.listAll()) {\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap betwen docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    \n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    singleSegmentDirectory = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"468103c3bb7b4f2ceb6716d84d698c8ff47dd2cd","date":1466349084,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap betwen docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    for (String fileName : directory.listAll()) {\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap betwen docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    for (String fileName : directory.listAll()) {\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2ea96381deceb3f8a6fdcee2a6377ed9c108e056","date":1466350600,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap betwen docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap betwen docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    for (String fileName : directory.listAll()) {\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9dea8da13fd1a227ae1071e8f4ce66bff42174de","date":1471439735,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap betwen docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","date":1471496851,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap betwen docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"403d05f7f8d69b65659157eff1bc1d2717f04c66","date":1471692961,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap betwen docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9263c82c39ef86ad7ca7397a4edb52b3b8013353","date":1473692931,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene62\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"89424def13674ea17829b41c5883c54ecc31a132","date":1473767373,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene62\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c602b9782dfa18eed3bae1d161ae67c1be2378da","date":1474034057,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene62\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene62\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene62\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n    \n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"084884d4602f4d1c7411eab29e897e349ce62675","date":1475571034,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene70\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene62\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1967bed916cc89da82a1c2085f27976da6d08cbd","date":1475588750,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene70\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene62\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene62\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene70\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    directory = newDirectory();\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, newIndexWriterConfig(new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));\n    for (int i = 0; i < docFields.length; i++) {\n      Document doc = new Document();\n      doc.add(newTextField(field, docFields[i], Field.Store.NO));\n      writer.addDocument(doc);\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // Make big index\n    dir2 = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(directory));\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = new MockDirectoryWrapper(random(), TestUtil.ramCopyOf(dir2));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2);\n      w.addIndexes(copy);\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000);\n\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, \n        newIndexWriterConfig(new MockAnalyzer(random()))\n        .setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000)));\n    Document doc = new Document();\n    doc.add(newTextField(\"field2\", \"xxx\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(newTextField(\"field2\", \"big bad bug\", Field.Store.NO));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"68d6cb7f0f019661a784bd0e5a21e85b5f812af6","date":1515075216,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity());\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene70\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene70\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity());\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene70\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity(true));\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene70\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6930fc653c2b86e857fce0af7ec99993593d0d89","date":1535014305,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene80\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene80\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity());\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene80\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene80\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity());\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene70\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene70\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"feb4029567b43f074ed7b6eb8fb126d355075dfd","date":1544812585,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene80\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene80\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity());\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene80\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.getDocStats().maxDoc;\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene80\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene80\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene80\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity());\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene80\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.maxDoc();\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene80\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9a0cc86697753659ac5e7f55243ccdcbf44d8a36","date":1574100512,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene84\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene84\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity());\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene84\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.getDocStats().maxDoc;\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene84\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene80\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene80\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity());\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene80\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.getDocStats().maxDoc;\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene80\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"773bf150032d3ef6c95997a154fb914b82875cb8","date":1590150786,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene86\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene86\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity());\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene86\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.getDocStats().maxDoc;\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene86\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene84\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene84\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity());\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene84\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.getDocStats().maxDoc;\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene84\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0db8230c9dbe474fc18e1e71cf04c1ada8046ec9","date":1598432674,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestBoolean2#beforeClass().mjava","sourceNew":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(TestUtil.getDefaultCodec());\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(TestUtil.getDefaultCodec());\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity());\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(TestUtil.getDefaultCodec());\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.getDocStats().maxDoc;\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(TestUtil.getDefaultCodec());\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void beforeClass() throws Exception {\n    // in some runs, test immediate adjacency of matches - in others, force a full bucket gap between docs\n    NUM_FILLER_DOCS = random().nextBoolean() ? 0 : BooleanScorer.SIZE;\n    PRE_FILLER_DOCS = TestUtil.nextInt(random(), 0, (NUM_FILLER_DOCS / 2));\n    if (VERBOSE) {\n      System.out.println(\"TEST: NUM_FILLER_DOCS=\" + NUM_FILLER_DOCS + \" PRE_FILLER_DOCS=\" + PRE_FILLER_DOCS);\n    }\n\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      directory = newFSDirectory(createTempDir());\n    } else {\n      directory = newDirectory();\n    }\n\n    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene86\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    RandomIndexWriter writer= new RandomIndexWriter(random(), directory, iwc);\n    // we'll make a ton of docs, disable store/norms/vectors\n    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);\n    ft.setOmitNorms(true);\n    \n    Document doc = new Document();\n    for (int filler = 0; filler < PRE_FILLER_DOCS; filler++) {\n      writer.addDocument(doc);\n    }\n    for (int i = 0; i < docFields.length; i++) {\n      doc.add(new Field(field, docFields[i], ft));\n      writer.addDocument(doc);\n      \n      doc = new Document();\n      for (int filler = 0; filler < NUM_FILLER_DOCS; filler++) {\n        writer.addDocument(doc);\n      }\n    }\n    writer.close();\n    littleReader = DirectoryReader.open(directory);\n    searcher = newSearcher(littleReader);\n    // this is intentionally using the baseline sim, because it compares against bigSearcher (which uses a random one)\n    searcher.setSimilarity(new ClassicSimilarity());\n\n    // make a copy of our index using a single segment\n    if (NUM_FILLER_DOCS * PRE_FILLER_DOCS > 100000) {\n      singleSegmentDirectory = newFSDirectory(createTempDir());\n    } else {\n      singleSegmentDirectory = newDirectory();\n    }\n\n    // TODO: this test does not need to be doing this crazy stuff. please improve it!\n    for (String fileName : directory.listAll()) {\n      if (fileName.startsWith(\"extra\")) {\n        continue;\n      }\n      singleSegmentDirectory.copyFrom(directory, fileName, fileName, IOContext.DEFAULT);\n      singleSegmentDirectory.sync(Collections.singleton(fileName));\n    }\n    \n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    // we need docID order to be preserved:\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene86\"));\n    iwc.setMergePolicy(newLogMergePolicy());\n    try (IndexWriter w = new IndexWriter(singleSegmentDirectory, iwc)) {\n      w.forceMerge(1, true);\n    }\n    singleSegmentReader = DirectoryReader.open(singleSegmentDirectory);\n    singleSegmentSearcher = newSearcher(singleSegmentReader);\n    singleSegmentSearcher.setSimilarity(searcher.getSimilarity());\n    \n    // Make big index\n    dir2 = copyOf(directory);\n\n    // First multiply small test index:\n    mulFactor = 1;\n    int docCount = 0;\n    if (VERBOSE) {\n      System.out.println(\"\\nTEST: now copy index...\");\n    }\n    do {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: cycle...\");\n      }\n      final Directory copy = copyOf(dir2);\n\n      iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n      // randomized codecs are sometimes too costly for this test:\n      iwc.setCodec(Codec.forName(\"Lucene86\"));\n      RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n      w.addIndexes(copy);\n      copy.close();\n      docCount = w.getDocStats().maxDoc;\n      w.close();\n      mulFactor *= 2;\n    } while(docCount < 3000 * NUM_FILLER_DOCS);\n\n    iwc = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc.setMaxBufferedDocs(TestUtil.nextInt(random(), 50, 1000));\n    // randomized codecs are sometimes too costly for this test:\n    iwc.setCodec(Codec.forName(\"Lucene86\"));\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir2, iwc);\n\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"xxx\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    doc = new Document();\n    doc.add(new Field(\"field2\", \"big bad bug\", ft));\n    for(int i=0;i<NUM_EXTRA_DOCS/2;i++) {\n      w.addDocument(doc);\n    }\n    reader = w.getReader();\n    bigSearcher = newSearcher(reader);\n    w.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b94236357aaa22b76c10629851fe4e376e0cea82":["1967bed916cc89da82a1c2085f27976da6d08cbd","68d6cb7f0f019661a784bd0e5a21e85b5f812af6"],"1967bed916cc89da82a1c2085f27976da6d08cbd":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","084884d4602f4d1c7411eab29e897e349ce62675"],"6930fc653c2b86e857fce0af7ec99993593d0d89":["b94236357aaa22b76c10629851fe4e376e0cea82"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0db8230c9dbe474fc18e1e71cf04c1ada8046ec9":["773bf150032d3ef6c95997a154fb914b82875cb8"],"7524d27129b064aa52a7feecc8e26f4c13ccc6cc":["191128ac5b85671b1671e2c857437694283b6ebf"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["2ea96381deceb3f8a6fdcee2a6377ed9c108e056","9dea8da13fd1a227ae1071e8f4ce66bff42174de"],"191128ac5b85671b1671e2c857437694283b6ebf":["81d0720146de53dd3a4a023d2a3d1089d86d748d","7c82b0d4b7bf499a159eeff92add20bac6599cc1"],"b0c73b4e6b72cca35c7f115ab543ce9dcf50d8b5":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"81d0720146de53dd3a4a023d2a3d1089d86d748d":["302be0cc5e6a28ebcebcac98aa81a92be2e94370"],"9263c82c39ef86ad7ca7397a4edb52b3b8013353":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"89424def13674ea17829b41c5883c54ecc31a132":["403d05f7f8d69b65659157eff1bc1d2717f04c66","9263c82c39ef86ad7ca7397a4edb52b3b8013353"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"468103c3bb7b4f2ceb6716d84d698c8ff47dd2cd":["7524d27129b064aa52a7feecc8e26f4c13ccc6cc"],"4356000e349e38c9fb48034695b7c309abd54557":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":["04f07771a2a7dd3a395700665ed839c3dae2def2","b0c73b4e6b72cca35c7f115ab543ce9dcf50d8b5"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"773bf150032d3ef6c95997a154fb914b82875cb8":["9a0cc86697753659ac5e7f55243ccdcbf44d8a36"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["403d05f7f8d69b65659157eff1bc1d2717f04c66","c602b9782dfa18eed3bae1d161ae67c1be2378da"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["6613659748fe4411a7dcf85266e55db1f95f7315"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["2ea96381deceb3f8a6fdcee2a6377ed9c108e056","2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"9a0cc86697753659ac5e7f55243ccdcbf44d8a36":["feb4029567b43f074ed7b6eb8fb126d355075dfd"],"6613659748fe4411a7dcf85266e55db1f95f7315":["b0c73b4e6b72cca35c7f115ab543ce9dcf50d8b5"],"9dea8da13fd1a227ae1071e8f4ce66bff42174de":["2ea96381deceb3f8a6fdcee2a6377ed9c108e056"],"c602b9782dfa18eed3bae1d161ae67c1be2378da":["89424def13674ea17829b41c5883c54ecc31a132"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["81d0720146de53dd3a4a023d2a3d1089d86d748d","1967bed916cc89da82a1c2085f27976da6d08cbd"],"084884d4602f4d1c7411eab29e897e349ce62675":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"68d6cb7f0f019661a784bd0e5a21e85b5f812af6":["1967bed916cc89da82a1c2085f27976da6d08cbd"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["629c38c4ae4e303d0617e05fbfe508140b32f0a3","4356000e349e38c9fb48034695b7c309abd54557"],"7c82b0d4b7bf499a159eeff92add20bac6599cc1":["81d0720146de53dd3a4a023d2a3d1089d86d748d"],"feb4029567b43f074ed7b6eb8fb126d355075dfd":["6930fc653c2b86e857fce0af7ec99993593d0d89"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["04f07771a2a7dd3a395700665ed839c3dae2def2","b0c73b4e6b72cca35c7f115ab543ce9dcf50d8b5"],"2ea96381deceb3f8a6fdcee2a6377ed9c108e056":["468103c3bb7b4f2ceb6716d84d698c8ff47dd2cd"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0db8230c9dbe474fc18e1e71cf04c1ada8046ec9"],"302be0cc5e6a28ebcebcac98aa81a92be2e94370":["d0ef034a4f10871667ae75181537775ddcf8ade4"]},"commit2Childs":{"b94236357aaa22b76c10629851fe4e376e0cea82":["6930fc653c2b86e857fce0af7ec99993593d0d89"],"1967bed916cc89da82a1c2085f27976da6d08cbd":["b94236357aaa22b76c10629851fe4e376e0cea82","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","68d6cb7f0f019661a784bd0e5a21e85b5f812af6"],"6930fc653c2b86e857fce0af7ec99993593d0d89":["feb4029567b43f074ed7b6eb8fb126d355075dfd"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"0db8230c9dbe474fc18e1e71cf04c1ada8046ec9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7524d27129b064aa52a7feecc8e26f4c13ccc6cc":["468103c3bb7b4f2ceb6716d84d698c8ff47dd2cd"],"2c8bedceb91e64a3f0e831450058fc4a76d2c0a6":["403d05f7f8d69b65659157eff1bc1d2717f04c66"],"191128ac5b85671b1671e2c857437694283b6ebf":["7524d27129b064aa52a7feecc8e26f4c13ccc6cc"],"b0c73b4e6b72cca35c7f115ab543ce9dcf50d8b5":["c7869f64c874ebf7f317d22c00baf2b6857797a6","6613659748fe4411a7dcf85266e55db1f95f7315","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"81d0720146de53dd3a4a023d2a3d1089d86d748d":["191128ac5b85671b1671e2c857437694283b6ebf","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","7c82b0d4b7bf499a159eeff92add20bac6599cc1"],"9263c82c39ef86ad7ca7397a4edb52b3b8013353":["89424def13674ea17829b41c5883c54ecc31a132"],"89424def13674ea17829b41c5883c54ecc31a132":["c602b9782dfa18eed3bae1d161ae67c1be2378da"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["b0c73b4e6b72cca35c7f115ab543ce9dcf50d8b5","c7869f64c874ebf7f317d22c00baf2b6857797a6","d6f074e73200c07d54f242d3880a8da5a35ff97b"],"468103c3bb7b4f2ceb6716d84d698c8ff47dd2cd":["2ea96381deceb3f8a6fdcee2a6377ed9c108e056"],"4356000e349e38c9fb48034695b7c309abd54557":["615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"c7869f64c874ebf7f317d22c00baf2b6857797a6":[],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["1967bed916cc89da82a1c2085f27976da6d08cbd","084884d4602f4d1c7411eab29e897e349ce62675"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"773bf150032d3ef6c95997a154fb914b82875cb8":["0db8230c9dbe474fc18e1e71cf04c1ada8046ec9"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"403d05f7f8d69b65659157eff1bc1d2717f04c66":["9263c82c39ef86ad7ca7397a4edb52b3b8013353","89424def13674ea17829b41c5883c54ecc31a132","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"9a0cc86697753659ac5e7f55243ccdcbf44d8a36":["773bf150032d3ef6c95997a154fb914b82875cb8"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"9dea8da13fd1a227ae1071e8f4ce66bff42174de":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6"],"c602b9782dfa18eed3bae1d161ae67c1be2378da":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"084884d4602f4d1c7411eab29e897e349ce62675":["1967bed916cc89da82a1c2085f27976da6d08cbd"],"68d6cb7f0f019661a784bd0e5a21e85b5f812af6":["b94236357aaa22b76c10629851fe4e376e0cea82"],"615ddbd81799980d0fdd95e0238e1c498b6f47b0":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"7c82b0d4b7bf499a159eeff92add20bac6599cc1":["191128ac5b85671b1671e2c857437694283b6ebf"],"feb4029567b43f074ed7b6eb8fb126d355075dfd":["9a0cc86697753659ac5e7f55243ccdcbf44d8a36"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["302be0cc5e6a28ebcebcac98aa81a92be2e94370"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"2ea96381deceb3f8a6fdcee2a6377ed9c108e056":["2c8bedceb91e64a3f0e831450058fc4a76d2c0a6","403d05f7f8d69b65659157eff1bc1d2717f04c66","9dea8da13fd1a227ae1071e8f4ce66bff42174de"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["4356000e349e38c9fb48034695b7c309abd54557","615ddbd81799980d0fdd95e0238e1c498b6f47b0"],"302be0cc5e6a28ebcebcac98aa81a92be2e94370":["81d0720146de53dd3a4a023d2a3d1089d86d748d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c7869f64c874ebf7f317d22c00baf2b6857797a6","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d6f074e73200c07d54f242d3880a8da5a35ff97b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}