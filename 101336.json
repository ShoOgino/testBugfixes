{"path":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","commits":[{"id":"0d49a158012a8ff48f328a4558e4bfcffbaed16f","date":1453677440,"type":1,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          anyChanges = docWriter.flushAllThreads();\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              System.out.println(\"IW: now readerPool.commit\");\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes) throws IOException {\n    ensureOpen();\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          anyChanges = docWriter.flushAllThreads();\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70303239aff48a883d655b94899202818b0aecd6","date":1455032616,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          anyChanges = docWriter.flushAllThreads();\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          anyChanges = docWriter.flushAllThreads();\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              System.out.println(\"IW: now readerPool.commit\");\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"68496c2200e559fb7802f7575427b7a482659afb","date":1455207618,"type":1,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          anyChanges = docWriter.flushAllThreads();\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes) throws IOException {\n    ensureOpen();\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          anyChanges = docWriter.flushAllThreads();\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f492fe129985750df09c8dac738aecc503158bb3","date":1464099630,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // nocommit should we make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          anyChanges = docWriter.flushAllThreads();\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9972d96003bc59c07a44e73de3cdd505dc08fd17","date":1464216081,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // nocommit should we make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6483e4260c08168709c02238ae083a51519a28dd","date":1465117546,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          anyChanges = docWriter.flushAllThreads();\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"191128ac5b85671b1671e2c857437694283b6ebf","date":1465297861,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          anyChanges = docWriter.flushAllThreads();\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          anyChanges = docWriter.flushAllThreads();\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n\n          processEvents(false, true);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            readerPool.writeAllDocValuesUpdates();\n\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":["f372764a5bd3ebacde5b99ee3303153eb5ec0d2f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n\n          processEvents(false, true);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            readerPool.writeAllDocValuesUpdates();\n\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n\n          processEvents(false, true);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            readerPool.writeAllDocValuesUpdates();\n\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (!anyChanges) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          // Prevent segmentInfos from changing while opening the\n          // reader; in theory we could instead do similar retry logic,\n          // just like we do when loading segments_N\n          synchronized(this) {\n            anyChanges |= maybeApplyDeletes(applyAllDeletes);\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"845b760a99e5f369fcd0a5d723a87b8def6a3f56","date":1521117993,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            readerPool.writeAllDocValuesUpdates();\n\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n\n          processEvents(false, true);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            readerPool.writeAllDocValuesUpdates();\n\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false, true);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (AbortingException | VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      // never reached but javac disagrees:\n      return null;\n    } finally {\n      if (!success2) {\n        IOUtils.closeWhileHandlingException(r);\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1926100d9b67becc9701c54266fee3ba7878a5f0","date":1524472150,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            if (readerPool.writeAllDocValuesUpdates()) {\n              checkpoint();\n            }\n\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              if (readerPool.commit(segmentInfos)) {\n                checkpointNoSIS();\n              }\n            }\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    poolReaders = true;\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            readerPool.writeAllDocValuesUpdates();\n\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              readerPool.commit(segmentInfos);\n            }\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":["f372764a5bd3ebacde5b99ee3303153eb5ec0d2f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7","date":1524496660,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads(this);\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            if (readerPool.writeAllDocValuesUpdates()) {\n              checkpoint();\n            }\n\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              if (readerPool.commit(segmentInfos)) {\n                checkpointNoSIS();\n              }\n            }\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            if (readerPool.writeAllDocValuesUpdates()) {\n              checkpoint();\n            }\n\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              if (readerPool.commit(segmentInfos)) {\n                checkpointNoSIS();\n              }\n            }\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6b8498afacfc8322268ca0d659d274fcce08d557","date":1524577248,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          publishFlushedSegments(true);\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            if (readerPool.writeAllDocValuesUpdates()) {\n              checkpoint();\n            }\n\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              if (readerPool.commit(segmentInfos)) {\n                checkpointNoSIS();\n              }\n            }\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          assert holdsFullFlushLock();\n          docWriter.finishFullFlush(success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads(this);\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            if (readerPool.writeAllDocValuesUpdates()) {\n              checkpoint();\n            }\n\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              if (readerPool.commit(segmentInfos)) {\n                checkpointNoSIS();\n              }\n            }\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          docWriter.finishFullFlush(this, success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f372764a5bd3ebacde5b99ee3303153eb5ec0d2f","date":1525347515,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          publishFlushedSegments(true);\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n           writeReaderPool(writeAllDeletes);\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          assert holdsFullFlushLock();\n          docWriter.finishFullFlush(success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          publishFlushedSegments(true);\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            if (readerPool.writeAllDocValuesUpdates()) {\n              checkpoint();\n            }\n\n            if (writeAllDeletes) {\n              // Must move the deletes to disk:\n              if (readerPool.commit(segmentInfos)) {\n                checkpointNoSIS();\n              }\n            }\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          assert holdsFullFlushLock();\n          docWriter.finishFullFlush(success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","bugFix":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","1926100d9b67becc9701c54266fee3ba7878a5f0","0d49a158012a8ff48f328a4558e4bfcffbaed16f"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"763da4a9605e47013078edc323b9d4b608f0f9e0","date":1555353576,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          publishFlushedSegments(true);\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n           writeReaderPool(writeAllDeletes);\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes, config.getReaderAttributes());\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          assert holdsFullFlushLock();\n          docWriter.finishFullFlush(success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          publishFlushedSegments(true);\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n           writeReaderPool(writeAllDeletes);\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          assert holdsFullFlushLock();\n          docWriter.finishFullFlush(success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a4e83191a3e02851a0b67e5335e6922f3e9ea86d","date":1583489709,"type":3,"author":"Bruno Roustant","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          publishFlushedSegments(true);\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n           writeReaderPool(writeAllDeletes);\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          assert holdsFullFlushLock();\n          docWriter.finishFullFlush(success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          publishFlushedSegments(true);\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n           writeReaderPool(writeAllDeletes);\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes, config.getReaderAttributes());\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          assert holdsFullFlushLock();\n          docWriter.finishFullFlush(success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2","date":1588002560,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make the seqNo available in the returned NRT reader?\n          anyChanges = docWriter.flushAllThreads() < 0;\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          publishFlushedSegments(true);\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n           writeReaderPool(writeAllDeletes);\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          assert Thread.holdsLock(fullFlushLock);\n          docWriter.finishFullFlush(success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make this available in the returned NRT reader?\n          long seqNo = docWriter.flushAllThreads();\n          if (seqNo < 0) {\n            anyChanges = true;\n            seqNo = -seqNo;\n          } else {\n            anyChanges = false;\n          }\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          publishFlushedSegments(true);\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n           writeReaderPool(writeAllDeletes);\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          assert holdsFullFlushLock();\n          docWriter.finishFullFlush(success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f354ba79a5a3e8491ec2953f14f365a02c058ac","date":1598293148,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#getReader(boolean,boolean).mjava","sourceNew":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    StandardDirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges;\n    final long maxFullFlushMergeWaitMillis = config.getMaxFullFlushMergeWaitMillis();\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    MergePolicy.MergeSpecification onGetReaderMerges = null;\n    final AtomicBoolean stopCollectingMergedReaders = new AtomicBoolean(false);\n    final Map<String, SegmentReader> mergedReaders = new HashMap<>();\n    final Map<String, SegmentReader> openedReadOnlyClones = new HashMap<>();\n    // this function is used to control which SR are opened in order to keep track of them\n    // and to reuse them in the case we wait for merges in this getReader call.\n    IOUtils.IOFunction<SegmentCommitInfo, SegmentReader> readerFactory = sci -> {\n      final ReadersAndUpdates rld = getPooledInstance(sci, true);\n      try {\n        assert Thread.holdsLock(IndexWriter.this);\n        SegmentReader segmentReader = rld.getReadOnlyClone(IOContext.READ);\n        if (maxFullFlushMergeWaitMillis > 0) { // only track this if we actually do fullFlush merges\n          openedReadOnlyClones.put(sci.info.name, segmentReader);\n        }\n        return segmentReader;\n      } finally {\n        release(rld);\n      }\n    };\n    Closeable onGetReaderMergeResources = null;\n    SegmentInfos openingSegmentInfos = null;\n    boolean success2 = false;\n    try {\n      /* this is the essential part of the getReader method. We need to take care of the following things:\n       *  - flush all currently in-memory DWPTs to disk\n       *  - apply all deletes & updates to new and to the existing DWPTs\n       *  - prevent flushes and applying deletes of concurrently indexing DWPTs to be applied\n       *  - open a SDR on the updated SIS\n       *\n       * in order to prevent concurrent flushes we call DocumentsWriter#flushAllThreads that swaps out the deleteQueue\n       *  (this enforces a happens before relationship between this and the subsequent full flush) and informs the\n       * FlushControl (#markForFullFlush()) that it should prevent any new DWPTs from flushing until we are \\\n       * done (DocumentsWriter#finishFullFlush(boolean)). All this is guarded by the fullFlushLock to prevent multiple\n       * full flushes from happening concurrently. Once the DocWriter has initiated a full flush we can sequentially flush\n       * and apply deletes & updates to the written segments without worrying about concurrently indexing DWPTs. The important\n       * aspect is that it all happens between DocumentsWriter#flushAllThread() and DocumentsWriter#finishFullFlush(boolean)\n       * since once the flush is marked as done deletes start to be applied to the segments on disk without guarantees that\n       * the corresponding added documents (in the update case) are flushed and visible when opening a SDR.\n       *\n       */\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make the seqNo available in the returned NRT reader?\n          anyChanges = docWriter.flushAllThreads() < 0;\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          publishFlushedSegments(true);\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n            writeReaderPool(writeAllDeletes);\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            r = StandardDirectoryReader.open(this, readerFactory, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n            if (maxFullFlushMergeWaitMillis > 0) {\n              // we take the SIS from the reader which has already pruned away fully deleted readers\n              // this makes pulling the readers below after the merge simpler since we can be safe that\n              // they are not closed. Every segment has a corresponding SR in the SDR we opened if we use\n              // this SIS\n              // we need to do this rather complicated management of SRs and infos since we can't wait for merges\n              // while we hold the fullFlushLock since the merge might hit a tragic event and that must not be reported\n              // while holding that lock. Merging outside of the lock ie. after calling docWriter.finishFullFlush(boolean) would\n              // yield wrong results because deletes might sneak in during the merge\n              openingSegmentInfos = r.getSegmentInfos().clone();\n              onGetReaderMerges = preparePointInTimeMerge(openingSegmentInfos, stopCollectingMergedReaders::get, MergeTrigger.GET_READER,\n                  sci -> {\n                    assert stopCollectingMergedReaders.get() == false : \"illegal state  merge reader must be not pulled since we already stopped waiting for merges\";\n                    SegmentReader apply = readerFactory.apply(sci);\n                    mergedReaders.put(sci.info.name, apply);\n                    // we need to incRef the files of the opened SR otherwise it's possible that another merge\n                    // removes the segment before we pass it on to the SDR\n                    deleter.incRef(sci.files());\n                  });\n              onGetReaderMergeResources = () -> {\n                // this needs to be closed once after we are done. In the case of an exception it releases\n                // all resources, closes the merged readers and decrements the files references.\n                // this only happens for readers that haven't been removed from the mergedReaders and release elsewhere\n                synchronized (this) {\n                  stopCollectingMergedReaders.set(true);\n                  IOUtils.close(mergedReaders.values().stream().map(sr -> (Closeable) () -> {\n                    try {\n                      deleter.decRef(sr.getSegmentInfo().files());\n                    } finally {\n                      sr.close();\n                    }\n                  }).collect(Collectors.toList()));\n                }\n              };\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          assert Thread.holdsLock(fullFlushLock);\n          docWriter.finishFullFlush(success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      if (onGetReaderMerges != null) { // only relevant if we do merge on getReader\n        StandardDirectoryReader mergedReader = finishGetReaderMerge(stopCollectingMergedReaders, mergedReaders,\n            openedReadOnlyClones, openingSegmentInfos, applyAllDeletes,\n            writeAllDeletes, onGetReaderMerges, maxFullFlushMergeWaitMillis);\n        if (mergedReader != null) {\n          try {\n            r.close();\n          } finally {\n            r = mergedReader;\n          }\n        }\n      }\n\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r, onGetReaderMergeResources);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      } else {\n        IOUtils.close(onGetReaderMergeResources);\n      }\n    }\n    return r;\n  }\n\n","sourceOld":"  /**\n   * Expert: returns a readonly reader, covering all\n   * committed as well as un-committed changes to the index.\n   * This provides \"near real-time\" searching, in that\n   * changes made during an IndexWriter session can be\n   * quickly made available for searching without closing\n   * the writer nor calling {@link #commit}.\n   *\n   * <p>Note that this is functionally equivalent to calling\n   * {#flush} and then opening a new reader.  But the turnaround time of this\n   * method should be faster since it avoids the potentially\n   * costly {@link #commit}.</p>\n   *\n   * <p>You must close the {@link IndexReader} returned by\n   * this method once you are done using it.</p>\n   *\n   * <p>It's <i>near</i> real-time because there is no hard\n   * guarantee on how quickly you can get a new reader after\n   * making changes with IndexWriter.  You'll have to\n   * experiment in your situation to determine if it's\n   * fast enough.  As this is a new and experimental\n   * feature, please report back on your findings so we can\n   * learn, improve and iterate.</p>\n   *\n   * <p>The resulting reader supports {@link\n   * DirectoryReader#openIfChanged}, but that call will simply forward\n   * back to this method (though this may change in the\n   * future).</p>\n   *\n   * <p>The very first time this method is called, this\n   * writer instance will make every effort to pool the\n   * readers that it opens for doing merges, applying\n   * deletes, etc.  This means additional resources (RAM,\n   * file descriptors, CPU time) will be consumed.</p>\n   *\n   * <p>For lower latency on reopening a reader, you should\n   * call {@link IndexWriterConfig#setMergedSegmentWarmer} to\n   * pre-warm a newly merged segment before it's committed\n   * to the index.  This is important for minimizing\n   * index-to-search delay after a large merge.  </p>\n   *\n   * <p>If an addIndexes* call is running in another thread,\n   * then this reader will only search those segments from\n   * the foreign index that have been successfully copied\n   * over, so far</p>.\n   *\n   * <p><b>NOTE</b>: Once the writer is closed, any\n   * outstanding readers may continue to be used.  However,\n   * if you attempt to reopen any of those readers, you'll\n   * hit an {@link AlreadyClosedException}.</p>\n   *\n   * @lucene.experimental\n   *\n   * @return IndexReader that covers entire index plus all\n   * changes made so far by this IndexWriter instance\n   *\n   * @throws IOException If there is a low-level I/O error\n   */\n  DirectoryReader getReader(boolean applyAllDeletes, boolean writeAllDeletes) throws IOException {\n    ensureOpen();\n\n    if (writeAllDeletes && applyAllDeletes == false) {\n      throw new IllegalArgumentException(\"applyAllDeletes must be true when writeAllDeletes=true\");\n    }\n\n    final long tStart = System.currentTimeMillis();\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"flush at getReader\");\n    }\n    // Do this up front before flushing so that the readers\n    // obtained during this flush are pooled, the first time\n    // this method is called:\n    readerPool.enableReaderPooling();\n    DirectoryReader r = null;\n    doBeforeFlush();\n    boolean anyChanges = false;\n    /*\n     * for releasing a NRT reader we must ensure that \n     * DW doesn't add any segments or deletes until we are\n     * done with creating the NRT DirectoryReader. \n     * We release the two stage full flush after we are done opening the\n     * directory reader!\n     */\n    boolean success2 = false;\n    try {\n      boolean success = false;\n      synchronized (fullFlushLock) {\n        try {\n          // TODO: should we somehow make the seqNo available in the returned NRT reader?\n          anyChanges = docWriter.flushAllThreads() < 0;\n          if (anyChanges == false) {\n            // prevent double increment since docWriter#doFlush increments the flushcount\n            // if we flushed anything.\n            flushCount.incrementAndGet();\n          }\n          publishFlushedSegments(true);\n          processEvents(false);\n\n          if (applyAllDeletes) {\n            applyAllDeletesAndUpdates();\n          }\n\n          synchronized(this) {\n\n            // NOTE: we cannot carry doc values updates in memory yet, so we always must write them through to disk and re-open each\n            // SegmentReader:\n\n            // TODO: we could instead just clone SIS and pull/incref readers in sync'd block, and then do this w/o IW's lock?\n            // Must do this sync'd on IW to prevent a merge from completing at the last second and failing to write its DV updates:\n           writeReaderPool(writeAllDeletes);\n\n            // Prevent segmentInfos from changing while opening the\n            // reader; in theory we could instead do similar retry logic,\n            // just like we do when loading segments_N\n            \n            r = StandardDirectoryReader.open(this, segmentInfos, applyAllDeletes, writeAllDeletes);\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"return reader version=\" + r.getVersion() + \" reader=\" + r);\n            }\n          }\n          success = true;\n        } finally {\n          // Done: finish the full flush!\n          assert Thread.holdsLock(fullFlushLock);\n          docWriter.finishFullFlush(success);\n          if (success) {\n            processEvents(false);\n            doAfterFlush();\n          } else {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception during NRT reader\");\n            }\n          }\n        }\n      }\n      anyChanges |= maybeMerge.getAndSet(false);\n      if (anyChanges) {\n        maybeMerge(config.getMergePolicy(), MergeTrigger.FULL_FLUSH, UNBOUNDED_MAX_MERGE_SEGMENTS);\n      }\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"getReader took \" + (System.currentTimeMillis() - tStart) + \" msec\");\n      }\n      success2 = true;\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"getReader\");\n      throw tragedy;\n    } finally {\n      if (!success2) {\n        try {\n          IOUtils.closeWhileHandlingException(r);\n        } finally {\n          maybeCloseOnTragicEvent();\n        }\n      }\n    }\n    return r;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["28288370235ed02234a64753cdbf0c6ec096304a"],"f372764a5bd3ebacde5b99ee3303153eb5ec0d2f":["6b8498afacfc8322268ca0d659d274fcce08d557"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["f372764a5bd3ebacde5b99ee3303153eb5ec0d2f"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["1926100d9b67becc9701c54266fee3ba7878a5f0"],"70303239aff48a883d655b94899202818b0aecd6":["0d49a158012a8ff48f328a4558e4bfcffbaed16f"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["191128ac5b85671b1671e2c857437694283b6ebf"],"6483e4260c08168709c02238ae083a51519a28dd":["68496c2200e559fb7802f7575427b7a482659afb","9972d96003bc59c07a44e73de3cdd505dc08fd17"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"191128ac5b85671b1671e2c857437694283b6ebf":["68496c2200e559fb7802f7575427b7a482659afb","6483e4260c08168709c02238ae083a51519a28dd"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["191128ac5b85671b1671e2c857437694283b6ebf","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["191128ac5b85671b1671e2c857437694283b6ebf","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["68496c2200e559fb7802f7575427b7a482659afb","191128ac5b85671b1671e2c857437694283b6ebf"],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"6b8498afacfc8322268ca0d659d274fcce08d557":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"f492fe129985750df09c8dac738aecc503158bb3":["68496c2200e559fb7802f7575427b7a482659afb"],"68496c2200e559fb7802f7575427b7a482659afb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","70303239aff48a883d655b94899202818b0aecd6"],"9972d96003bc59c07a44e73de3cdd505dc08fd17":["f492fe129985750df09c8dac738aecc503158bb3"],"3f354ba79a5a3e8491ec2953f14f365a02c058ac":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3f354ba79a5a3e8491ec2953f14f365a02c058ac"],"0d49a158012a8ff48f328a4558e4bfcffbaed16f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["1926100d9b67becc9701c54266fee3ba7878a5f0"],"f372764a5bd3ebacde5b99ee3303153eb5ec0d2f":["763da4a9605e47013078edc323b9d4b608f0f9e0"],"763da4a9605e47013078edc323b9d4b608f0f9e0":["a4e83191a3e02851a0b67e5335e6922f3e9ea86d"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["6b8498afacfc8322268ca0d659d274fcce08d557"],"70303239aff48a883d655b94899202818b0aecd6":["68496c2200e559fb7802f7575427b7a482659afb"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"6483e4260c08168709c02238ae083a51519a28dd":["191128ac5b85671b1671e2c857437694283b6ebf"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"191128ac5b85671b1671e2c857437694283b6ebf":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"a4e83191a3e02851a0b67e5335e6922f3e9ea86d":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"6b8498afacfc8322268ca0d659d274fcce08d557":["f372764a5bd3ebacde5b99ee3303153eb5ec0d2f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["68496c2200e559fb7802f7575427b7a482659afb","0d49a158012a8ff48f328a4558e4bfcffbaed16f"],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["3f354ba79a5a3e8491ec2953f14f365a02c058ac"],"f492fe129985750df09c8dac738aecc503158bb3":["9972d96003bc59c07a44e73de3cdd505dc08fd17"],"68496c2200e559fb7802f7575427b7a482659afb":["6483e4260c08168709c02238ae083a51519a28dd","191128ac5b85671b1671e2c857437694283b6ebf","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","f492fe129985750df09c8dac738aecc503158bb3"],"9972d96003bc59c07a44e73de3cdd505dc08fd17":["6483e4260c08168709c02238ae083a51519a28dd"],"3f354ba79a5a3e8491ec2953f14f365a02c058ac":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"0d49a158012a8ff48f328a4558e4bfcffbaed16f":["70303239aff48a883d655b94899202818b0aecd6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}