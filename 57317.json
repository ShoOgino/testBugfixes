{"path":"lucene/core/src/test/org/apache/lucene/analysis/TestStopFilter#testTokenPositionsWithConcatenatedStopwordFilters().mjava","commits":[{"id":"cb5529b83dfe8f452a536b7afe14b0f26b33033c","date":1544111330,"type":0,"author":"Diego Ceccarelli","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/analysis/TestStopFilter#testTokenPositionsWithConcatenatedStopwordFilters().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Check that the positions of the terms in a document keep into account the fact\n   * that some of the words were filtered by two StopwordFilters concatenated together.\n   */\n  public void testTokenPositionsWithConcatenatedStopwordFilters() throws IOException {\n    // at least 1 token\n    final int numberOfTokens = random().nextInt(MAX_NUMBER_OF_TOKENS-1)+1;\n    StringBuilder sb = new StringBuilder();\n    List<String> stopwords = new ArrayList<>(numberOfTokens);\n    List<Integer> stopwordPositions = new ArrayList<>();\n    generateTestSetWithStopwordsAndStopwordPositions(numberOfTokens, sb, stopwords, stopwordPositions);\n\n    // we want to make sure that concatenating two list of stopwords\n    // produce the same results of using one unique list of stopwords.\n    // So we first generate a list of stopwords:\n    // e.g.: [a, b, c, d, e]\n    // and then we split the list in two disjoint partitions\n    // e.g. [a, c, e] [b, d]\n    int partition = random().nextInt(stopwords.size());\n    Collections.shuffle(stopwords, random());\n    final List<String> stopwordsRandomPartition = stopwords.subList(0, partition);\n    final Set<String> stopwordsRemaining = new HashSet<>(stopwords);\n    stopwordsRemaining.removeAll(stopwordsRandomPartition); // remove the first partition from all the stopwords\n\n    CharArraySet firstStopSet = StopFilter.makeStopSet(stopwordsRandomPartition);\n    logStopwords(\"Stopwords-first\", stopwordsRandomPartition);\n    CharArraySet secondStopSet = StopFilter.makeStopSet(new ArrayList<>(stopwordsRemaining), false);\n    logStopwords(\"Stopwords-second\", stopwordsRemaining);\n\n    Reader reader = new StringReader(sb.toString());\n    final MockTokenizer in1 = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n    in1.setReader(reader);\n\n    // Here we create a stopFilter with the stopwords in the first partition and then we\n    // concatenate it with the stopFilter created with the stopwords in the second partition\n    StopFilter stopFilter = new StopFilter(in1, firstStopSet); // first part of the set\n    StopFilter concatenatedStopFilter = new StopFilter(stopFilter, secondStopSet); // two stop filters concatenated!\n\n    // ... and finally we check that the positions of the filtered tokens matched using the concatenated\n    // stopFilters match the positions of the filtered tokens using the unique original list of stopwords\n    doTestStopwordsPositions(concatenatedStopFilter, stopwordPositions, numberOfTokens);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"cb5529b83dfe8f452a536b7afe14b0f26b33033c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["cb5529b83dfe8f452a536b7afe14b0f26b33033c"]},"commit2Childs":{"cb5529b83dfe8f452a536b7afe14b0f26b33033c":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cb5529b83dfe8f452a536b7afe14b0f26b33033c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}