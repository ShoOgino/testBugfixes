{"path":"solr/core/src/java/org/apache/solr/handler/ClassifyStream#read().mjava","commits":[{"id":"b5fa1c8367f821057f943ece929329485ec708ba","date":1475186606,"type":0,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/ClassifyStream#read().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Tuple read() throws IOException {\n    if (modelTuple == null) {\n\n      modelTuple = modelStream.read();\n      if (modelTuple == null || modelTuple.EOF) {\n        throw new IOException(\"Model tuple not found for classify stream!\");\n      }\n\n      termToIndex = new HashMap<>();\n\n      List<String> terms = modelTuple.getStrings(\"terms_ss\");\n\n      for (int i = 0; i < terms.size(); i++) {\n        termToIndex.put(terms.get(i), i);\n      }\n\n      idfs = modelTuple.getDoubles(\"idfs_ds\");\n      modelWeights = modelTuple.getDoubles(\"weights_ds\");\n    }\n\n    Tuple docTuple = docStream.read();\n    if (docTuple.EOF) return docTuple;\n\n    String text = docTuple.getString(field);\n\n    double tfs[] = new double[termToIndex.size()];\n\n    TokenStream tokenStream = analyzer.tokenStream(analyzerField, text);\n    CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n\n    int termCount = 0;\n    while (tokenStream.incrementToken()) {\n      termCount++;\n      if (termToIndex.containsKey(termAtt.toString())) {\n        tfs[termToIndex.get(termAtt.toString())]++;\n      }\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    List<Double> tfidfs = new ArrayList<>(termToIndex.size());\n    tfidfs.add(1.0);\n    for (int i = 0; i < tfs.length; i++) {\n      if (tfs[i] != 0) {\n        tfs[i] = 1 + Math.log(tfs[i]);\n      }\n      tfidfs.add(this.idfs.get(i) * tfs[i]);\n    }\n\n    double total = 0.0;\n    for (int i = 0; i < tfidfs.size(); i++) {\n      total += tfidfs.get(i) * modelWeights.get(i);\n    }\n\n    double score = total * ((float) (1.0 / Math.sqrt(termCount)));\n    double positiveProb = sigmoid(total);\n\n    docTuple.put(\"probability_d\", positiveProb);\n    docTuple.put(\"score_d\",  score);\n\n    return docTuple;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/ClassifyStream#read().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Tuple read() throws IOException {\n    if (modelTuple == null) {\n\n      modelTuple = modelStream.read();\n      if (modelTuple == null || modelTuple.EOF) {\n        throw new IOException(\"Model tuple not found for classify stream!\");\n      }\n\n      termToIndex = new HashMap<>();\n\n      List<String> terms = modelTuple.getStrings(\"terms_ss\");\n\n      for (int i = 0; i < terms.size(); i++) {\n        termToIndex.put(terms.get(i), i);\n      }\n\n      idfs = modelTuple.getDoubles(\"idfs_ds\");\n      modelWeights = modelTuple.getDoubles(\"weights_ds\");\n    }\n\n    Tuple docTuple = docStream.read();\n    if (docTuple.EOF) return docTuple;\n\n    String text = docTuple.getString(field);\n\n    double tfs[] = new double[termToIndex.size()];\n\n    TokenStream tokenStream = analyzer.tokenStream(analyzerField, text);\n    CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n\n    int termCount = 0;\n    while (tokenStream.incrementToken()) {\n      termCount++;\n      if (termToIndex.containsKey(termAtt.toString())) {\n        tfs[termToIndex.get(termAtt.toString())]++;\n      }\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    List<Double> tfidfs = new ArrayList<>(termToIndex.size());\n    tfidfs.add(1.0);\n    for (int i = 0; i < tfs.length; i++) {\n      if (tfs[i] != 0) {\n        tfs[i] = 1 + Math.log(tfs[i]);\n      }\n      tfidfs.add(this.idfs.get(i) * tfs[i]);\n    }\n\n    double total = 0.0;\n    for (int i = 0; i < tfidfs.size(); i++) {\n      total += tfidfs.get(i) * modelWeights.get(i);\n    }\n\n    double score = total * ((float) (1.0 / Math.sqrt(termCount)));\n    double positiveProb = sigmoid(total);\n\n    docTuple.put(\"probability_d\", positiveProb);\n    docTuple.put(\"score_d\",  score);\n\n    return docTuple;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/ClassifyStream#read().mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Tuple read() throws IOException {\n    if (modelTuple == null) {\n\n      modelTuple = modelStream.read();\n      if (modelTuple == null || modelTuple.EOF) {\n        throw new IOException(\"Model tuple not found for classify stream!\");\n      }\n\n      termToIndex = new HashMap<>();\n\n      List<String> terms = modelTuple.getStrings(\"terms_ss\");\n\n      for (int i = 0; i < terms.size(); i++) {\n        termToIndex.put(terms.get(i), i);\n      }\n\n      idfs = modelTuple.getDoubles(\"idfs_ds\");\n      modelWeights = modelTuple.getDoubles(\"weights_ds\");\n    }\n\n    Tuple docTuple = docStream.read();\n    if (docTuple.EOF) return docTuple;\n\n    String text = docTuple.getString(field);\n\n    double tfs[] = new double[termToIndex.size()];\n\n    TokenStream tokenStream = analyzer.tokenStream(analyzerField, text);\n    CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n\n    int termCount = 0;\n    while (tokenStream.incrementToken()) {\n      termCount++;\n      if (termToIndex.containsKey(termAtt.toString())) {\n        tfs[termToIndex.get(termAtt.toString())]++;\n      }\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    List<Double> tfidfs = new ArrayList<>(termToIndex.size());\n    tfidfs.add(1.0);\n    for (int i = 0; i < tfs.length; i++) {\n      if (tfs[i] != 0) {\n        tfs[i] = 1 + Math.log(tfs[i]);\n      }\n      tfidfs.add(this.idfs.get(i) * tfs[i]);\n    }\n\n    double total = 0.0;\n    for (int i = 0; i < tfidfs.size(); i++) {\n      total += tfidfs.get(i) * modelWeights.get(i);\n    }\n\n    double score = total * ((float) (1.0 / Math.sqrt(termCount)));\n    double positiveProb = sigmoid(total);\n\n    docTuple.put(\"probability_d\", positiveProb);\n    docTuple.put(\"score_d\",  score);\n\n    return docTuple;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b5fa1c8367f821057f943ece929329485ec708ba":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","b5fa1c8367f821057f943ece929329485ec708ba"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"]},"commit2Childs":{"b5fa1c8367f821057f943ece929329485ec708ba":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b5fa1c8367f821057f943ece929329485ec708ba","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}