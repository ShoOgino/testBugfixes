{"path":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricManager,String,String,String).mjava","commits":[{"id":"bfc52860e6d13d034226a760813c59d984c6817a","date":1522229027,"type":1,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricManager,String,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricManager,String,String).mjava","sourceNew":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricManager manager, String registryName, String tag, String scope) {\n    this.metricManager = manager;\n    this.registryName = registryName;\n    registry = manager.registry(registryName);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            logger.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    manager.registerGauge(this, registryName, metricsMap, tag, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","sourceOld":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricManager manager, String registryName, String scope) {\n    registry = manager.registry(registryName);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            logger.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    manager.registerGauge(this, registryName, metricsMap, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"43564cbb30b064675027cfb569564e8531096e97","date":1522334265,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricManager,String,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricManager,String,String).mjava","sourceNew":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricManager manager, String registryName, String tag, String scope) {\n    this.metricManager = manager;\n    this.registryName = registryName;\n    registry = manager.registry(registryName);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            logger.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    manager.registerGauge(this, registryName, metricsMap, tag, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","sourceOld":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricManager manager, String registryName, String scope) {\n    registry = manager.registry(registryName);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            logger.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    manager.registerGauge(this, registryName, metricsMap, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c80f6f4fe2e841ba56e6ce200951063ab91196d3","date":1533052731,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricManager,String,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricManager,String,String,String).mjava","sourceNew":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricManager manager, String registryName, String tag, String scope) {\n    this.metricManager = manager;\n    this.registryName = registryName;\n    registry = manager.registry(registryName);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    manager.registerGauge(this, registryName, metricsMap, tag, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","sourceOld":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricManager manager, String registryName, String tag, String scope) {\n    this.metricManager = manager;\n    this.registryName = registryName;\n    registry = manager.registry(registryName);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            logger.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    manager.registerGauge(this, registryName, metricsMap, tag, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c959a64c5b52cc12eb8daa17f4f0ed9cf2dfcaaa","date":1571411704,"type":5,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricsContext,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/store/hdfs/HdfsLocalityReporter#initializeMetrics(SolrMetricManager,String,String,String).mjava","sourceNew":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricsContext parentContext, String scope) {\n    solrMetricsContext = parentContext.getChildContext(this);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    solrMetricsContext.gauge(this, metricsMap, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","sourceOld":"  /**\n   * Provide statistics on HDFS block locality, both in terms of bytes and block counts.\n   */\n  @Override\n  public void initializeMetrics(SolrMetricManager manager, String registryName, String tag, String scope) {\n    this.metricManager = manager;\n    this.registryName = registryName;\n    registry = manager.registry(registryName);\n    MetricsMap metricsMap = new MetricsMap((detailed, map) -> {\n      long totalBytes = 0;\n      long localBytes = 0;\n      int totalCount = 0;\n      int localCount = 0;\n\n      for (Iterator<HdfsDirectory> iterator = cache.keySet().iterator(); iterator.hasNext();) {\n        HdfsDirectory hdfsDirectory = iterator.next();\n\n        if (hdfsDirectory.isClosed()) {\n          iterator.remove();\n        } else {\n          try {\n            refreshDirectory(hdfsDirectory);\n            Map<FileStatus,BlockLocation[]> blockMap = cache.get(hdfsDirectory);\n\n            // For every block in every file in this directory, count it\n            for (BlockLocation[] locations : blockMap.values()) {\n              for (BlockLocation bl : locations) {\n                totalBytes += bl.getLength();\n                totalCount++;\n\n                if (Arrays.asList(bl.getHosts()).contains(hostname)) {\n                  localBytes += bl.getLength();\n                  localCount++;\n                }\n              }\n            }\n          } catch (IOException e) {\n            log.warn(\"Could not retrieve locality information for {} due to exception: {}\",\n                hdfsDirectory.getHdfsDirPath(), e);\n          }\n        }\n      }\n      map.put(LOCALITY_BYTES_TOTAL, totalBytes);\n      map.put(LOCALITY_BYTES_LOCAL, localBytes);\n      if (localBytes == 0) {\n        map.put(LOCALITY_BYTES_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BYTES_RATIO, localBytes / (double) totalBytes);\n      }\n      map.put(LOCALITY_BLOCKS_TOTAL, totalCount);\n      map.put(LOCALITY_BLOCKS_LOCAL, localCount);\n      if (localCount == 0) {\n        map.put(LOCALITY_BLOCKS_RATIO, 0);\n      } else {\n        map.put(LOCALITY_BLOCKS_RATIO, localCount / (double) totalCount);\n      }\n    });\n    manager.registerGauge(this, registryName, metricsMap, tag, true, \"hdfsLocality\", getCategory().toString(), scope);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"bfc52860e6d13d034226a760813c59d984c6817a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c959a64c5b52cc12eb8daa17f4f0ed9cf2dfcaaa":["c80f6f4fe2e841ba56e6ce200951063ab91196d3"],"43564cbb30b064675027cfb569564e8531096e97":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","bfc52860e6d13d034226a760813c59d984c6817a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c80f6f4fe2e841ba56e6ce200951063ab91196d3":["43564cbb30b064675027cfb569564e8531096e97"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c959a64c5b52cc12eb8daa17f4f0ed9cf2dfcaaa"]},"commit2Childs":{"bfc52860e6d13d034226a760813c59d984c6817a":["43564cbb30b064675027cfb569564e8531096e97"],"c959a64c5b52cc12eb8daa17f4f0ed9cf2dfcaaa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"43564cbb30b064675027cfb569564e8531096e97":["c80f6f4fe2e841ba56e6ce200951063ab91196d3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["bfc52860e6d13d034226a760813c59d984c6817a","43564cbb30b064675027cfb569564e8531096e97"],"c80f6f4fe2e841ba56e6ce200951063ab91196d3":["c959a64c5b52cc12eb8daa17f4f0ed9cf2dfcaaa"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}