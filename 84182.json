{"path":"contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph#generateBiSegGraph(SegGraph).mjava","commits":[{"id":"dd745d580729e528151b58aeda87ef82f1b95c9b","date":1248369082,"type":1,"author":"Simon Willnauer","isMerge":false,"pathNew":"contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph#generateBiSegGraph(SegGraph).mjava","pathOld":"contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph#generateBiSegGraph(SegGraph).mjava","sourceNew":"  /*\n   * Generate a BiSegGraph based upon a SegGraph\n   */\n  private void generateBiSegGraph(SegGraph segGraph) {\n    double smooth = 0.1;\n    int wordPairFreq = 0;\n    int maxStart = segGraph.getMaxStart();\n    double oneWordFreq, weight, tinyDouble = 1.0 / Utility.MAX_FREQUENCE;\n\n    int next;\n    char[] idBuffer;\n    // get the list of tokens ordered and indexed\n    segTokenList = segGraph.makeIndex();\n    // 因为startToken（\"始##始\"）的起始位置是-1因此key为-1时可以取出startToken\n    int key = -1;\n    List nextTokens = null;\n    while (key < maxStart) {\n      if (segGraph.isStartExist(key)) {\n\n        List tokenList = segGraph.getStartList(key);\n\n        // 为某一个key对应的所有Token都计算一次\n        for (Iterator iter = tokenList.iterator(); iter.hasNext();) {\n          SegToken t1 = (SegToken) iter.next();\n          oneWordFreq = t1.weight;\n          next = t1.endOffset;\n          nextTokens = null;\n          // 找到下一个对应的Token，例如“阳光海岸”，当前Token是“阳光”， 下一个Token可以是“海”或者“海岸”\n          // 如果找不到下一个Token，则说明到了末尾，重新循环。\n          while (next <= maxStart) {\n            // 因为endToken的起始位置是sentenceLen，因此等于sentenceLen是可以找到endToken\n            if (segGraph.isStartExist(next)) {\n              nextTokens = segGraph.getStartList(next);\n              break;\n            }\n            next++;\n          }\n          if (nextTokens == null) {\n            break;\n          }\n          for (Iterator iter2 = nextTokens.iterator(); iter2.hasNext();) {\n            SegToken t2 = (SegToken) iter2.next();\n            idBuffer = new char[t1.charArray.length + t2.charArray.length + 1];\n            System.arraycopy(t1.charArray, 0, idBuffer, 0, t1.charArray.length);\n            idBuffer[t1.charArray.length] = BigramDictionary.WORD_SEGMENT_CHAR;\n            System.arraycopy(t2.charArray, 0, idBuffer,\n                t1.charArray.length + 1, t2.charArray.length);\n\n            // Two linked Words frequency\n            wordPairFreq = bigramDict.getFrequency(idBuffer);\n\n            // Smoothing\n\n            // -log{a*P(Ci-1)+(1-a)P(Ci|Ci-1)} Note 0<a<1\n            weight = -Math\n                .log(smooth\n                    * (1.0 + oneWordFreq)\n                    / (Utility.MAX_FREQUENCE + 0.0)\n                    + (1.0 - smooth)\n                    * ((1.0 - tinyDouble) * wordPairFreq / (1.0 + oneWordFreq) + tinyDouble));\n\n            SegTokenPair tokenPair = new SegTokenPair(idBuffer, t1.index,\n                t2.index, weight);\n            this.addSegTokenPair(tokenPair);\n          }\n        }\n      }\n      key++;\n    }\n\n  }\n\n","sourceOld":"  /*\n   * Generate a BiSegGraph based upon a SegGraph\n   */\n  private void generateBiSegGraph(SegGraph segGraph) {\n    double smooth = 0.1;\n    int wordPairFreq = 0;\n    int maxStart = segGraph.getMaxStart();\n    double oneWordFreq, weight, tinyDouble = 1.0 / Utility.MAX_FREQUENCE;\n\n    int next;\n    char[] idBuffer;\n    // get the list of tokens ordered and indexed\n    segTokenList = segGraph.makeIndex();\n    // 因为startToken（\"始##始\"）的起始位置是-1因此key为-1时可以取出startToken\n    int key = -1;\n    List nextTokens = null;\n    while (key < maxStart) {\n      if (segGraph.isStartExist(key)) {\n\n        List tokenList = segGraph.getStartList(key);\n\n        // 为某一个key对应的所有Token都计算一次\n        for (Iterator iter = tokenList.iterator(); iter.hasNext();) {\n          SegToken t1 = (SegToken) iter.next();\n          oneWordFreq = t1.weight;\n          next = t1.endOffset;\n          nextTokens = null;\n          // 找到下一个对应的Token，例如“阳光海岸”，当前Token是“阳光”， 下一个Token可以是“海”或者“海岸”\n          // 如果找不到下一个Token，则说明到了末尾，重新循环。\n          while (next <= maxStart) {\n            // 因为endToken的起始位置是sentenceLen，因此等于sentenceLen是可以找到endToken\n            if (segGraph.isStartExist(next)) {\n              nextTokens = segGraph.getStartList(next);\n              break;\n            }\n            next++;\n          }\n          if (nextTokens == null) {\n            break;\n          }\n          for (Iterator iter2 = nextTokens.iterator(); iter2.hasNext();) {\n            SegToken t2 = (SegToken) iter2.next();\n            idBuffer = new char[t1.charArray.length + t2.charArray.length + 1];\n            System.arraycopy(t1.charArray, 0, idBuffer, 0, t1.charArray.length);\n            idBuffer[t1.charArray.length] = BigramDictionary.WORD_SEGMENT_CHAR;\n            System.arraycopy(t2.charArray, 0, idBuffer,\n                t1.charArray.length + 1, t2.charArray.length);\n\n            // Two linked Words frequency\n            wordPairFreq = bigramDict.getFrequency(idBuffer);\n\n            // Smoothing\n\n            // -log{a*P(Ci-1)+(1-a)P(Ci|Ci-1)} Note 0<a<1\n            weight = -Math\n                .log(smooth\n                    * (1.0 + oneWordFreq)\n                    / (Utility.MAX_FREQUENCE + 0.0)\n                    + (1.0 - smooth)\n                    * ((1.0 - tinyDouble) * wordPairFreq / (1.0 + oneWordFreq) + tinyDouble));\n\n            SegTokenPair tokenPair = new SegTokenPair(idBuffer, t1.index,\n                t2.index, weight);\n            this.addSegTokenPair(tokenPair);\n          }\n        }\n      }\n      key++;\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55fa7b85159e79f79dfdca119db7f0f4cb6a2a74","date":1254579885,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph#generateBiSegGraph(SegGraph).mjava","pathOld":"contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph#generateBiSegGraph(SegGraph).mjava","sourceNew":"  /*\n   * Generate a BiSegGraph based upon a SegGraph\n   */\n  private void generateBiSegGraph(SegGraph segGraph) {\n    double smooth = 0.1;\n    int wordPairFreq = 0;\n    int maxStart = segGraph.getMaxStart();\n    double oneWordFreq, weight, tinyDouble = 1.0 / Utility.MAX_FREQUENCE;\n\n    int next;\n    char[] idBuffer;\n    // get the list of tokens ordered and indexed\n    segTokenList = segGraph.makeIndex();\n    // Because the beginning position of startToken is -1, therefore startToken can be obtained when key = -1\n    int key = -1;\n    List nextTokens = null;\n    while (key < maxStart) {\n      if (segGraph.isStartExist(key)) {\n\n        List tokenList = segGraph.getStartList(key);\n\n        // Calculate all tokens for a given key.\n        for (Iterator iter = tokenList.iterator(); iter.hasNext();) {\n          SegToken t1 = (SegToken) iter.next();\n          oneWordFreq = t1.weight;\n          next = t1.endOffset;\n          nextTokens = null;\n          // Find the next corresponding Token.\n          // For example: \"Sunny seashore\", the present Token is \"sunny\", next one should be \"sea\" or \"seashore\".\n          // If we cannot find the next Token, then go to the end and repeat the same cycle.\n          while (next <= maxStart) {\n            // Because the beginning position of endToken is sentenceLen, so equal to sentenceLen can find endToken.\n            if (segGraph.isStartExist(next)) {\n              nextTokens = segGraph.getStartList(next);\n              break;\n            }\n            next++;\n          }\n          if (nextTokens == null) {\n            break;\n          }\n          for (Iterator iter2 = nextTokens.iterator(); iter2.hasNext();) {\n            SegToken t2 = (SegToken) iter2.next();\n            idBuffer = new char[t1.charArray.length + t2.charArray.length + 1];\n            System.arraycopy(t1.charArray, 0, idBuffer, 0, t1.charArray.length);\n            idBuffer[t1.charArray.length] = BigramDictionary.WORD_SEGMENT_CHAR;\n            System.arraycopy(t2.charArray, 0, idBuffer,\n                t1.charArray.length + 1, t2.charArray.length);\n\n            // Two linked Words frequency\n            wordPairFreq = bigramDict.getFrequency(idBuffer);\n\n            // Smoothing\n\n            // -log{a*P(Ci-1)+(1-a)P(Ci|Ci-1)} Note 0<a<1\n            weight = -Math\n                .log(smooth\n                    * (1.0 + oneWordFreq)\n                    / (Utility.MAX_FREQUENCE + 0.0)\n                    + (1.0 - smooth)\n                    * ((1.0 - tinyDouble) * wordPairFreq / (1.0 + oneWordFreq) + tinyDouble));\n\n            SegTokenPair tokenPair = new SegTokenPair(idBuffer, t1.index,\n                t2.index, weight);\n            this.addSegTokenPair(tokenPair);\n          }\n        }\n      }\n      key++;\n    }\n\n  }\n\n","sourceOld":"  /*\n   * Generate a BiSegGraph based upon a SegGraph\n   */\n  private void generateBiSegGraph(SegGraph segGraph) {\n    double smooth = 0.1;\n    int wordPairFreq = 0;\n    int maxStart = segGraph.getMaxStart();\n    double oneWordFreq, weight, tinyDouble = 1.0 / Utility.MAX_FREQUENCE;\n\n    int next;\n    char[] idBuffer;\n    // get the list of tokens ordered and indexed\n    segTokenList = segGraph.makeIndex();\n    // 因为startToken（\"始##始\"）的起始位置是-1因此key为-1时可以取出startToken\n    int key = -1;\n    List nextTokens = null;\n    while (key < maxStart) {\n      if (segGraph.isStartExist(key)) {\n\n        List tokenList = segGraph.getStartList(key);\n\n        // 为某一个key对应的所有Token都计算一次\n        for (Iterator iter = tokenList.iterator(); iter.hasNext();) {\n          SegToken t1 = (SegToken) iter.next();\n          oneWordFreq = t1.weight;\n          next = t1.endOffset;\n          nextTokens = null;\n          // 找到下一个对应的Token，例如“阳光海岸”，当前Token是“阳光”， 下一个Token可以是“海”或者“海岸”\n          // 如果找不到下一个Token，则说明到了末尾，重新循环。\n          while (next <= maxStart) {\n            // 因为endToken的起始位置是sentenceLen，因此等于sentenceLen是可以找到endToken\n            if (segGraph.isStartExist(next)) {\n              nextTokens = segGraph.getStartList(next);\n              break;\n            }\n            next++;\n          }\n          if (nextTokens == null) {\n            break;\n          }\n          for (Iterator iter2 = nextTokens.iterator(); iter2.hasNext();) {\n            SegToken t2 = (SegToken) iter2.next();\n            idBuffer = new char[t1.charArray.length + t2.charArray.length + 1];\n            System.arraycopy(t1.charArray, 0, idBuffer, 0, t1.charArray.length);\n            idBuffer[t1.charArray.length] = BigramDictionary.WORD_SEGMENT_CHAR;\n            System.arraycopy(t2.charArray, 0, idBuffer,\n                t1.charArray.length + 1, t2.charArray.length);\n\n            // Two linked Words frequency\n            wordPairFreq = bigramDict.getFrequency(idBuffer);\n\n            // Smoothing\n\n            // -log{a*P(Ci-1)+(1-a)P(Ci|Ci-1)} Note 0<a<1\n            weight = -Math\n                .log(smooth\n                    * (1.0 + oneWordFreq)\n                    / (Utility.MAX_FREQUENCE + 0.0)\n                    + (1.0 - smooth)\n                    * ((1.0 - tinyDouble) * wordPairFreq / (1.0 + oneWordFreq) + tinyDouble));\n\n            SegTokenPair tokenPair = new SegTokenPair(idBuffer, t1.index,\n                t2.index, weight);\n            this.addSegTokenPair(tokenPair);\n          }\n        }\n      }\n      key++;\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"573b8638be27860dcb2ce77889c694e5f8a76106","date":1256855390,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph#generateBiSegGraph(SegGraph).mjava","pathOld":"contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph#generateBiSegGraph(SegGraph).mjava","sourceNew":"  /*\n   * Generate a BiSegGraph based upon a SegGraph\n   */\n  private void generateBiSegGraph(SegGraph segGraph) {\n    double smooth = 0.1;\n    int wordPairFreq = 0;\n    int maxStart = segGraph.getMaxStart();\n    double oneWordFreq, weight, tinyDouble = 1.0 / Utility.MAX_FREQUENCE;\n\n    int next;\n    char[] idBuffer;\n    // get the list of tokens ordered and indexed\n    segTokenList = segGraph.makeIndex();\n    // Because the beginning position of startToken is -1, therefore startToken can be obtained when key = -1\n    int key = -1;\n    List<SegToken> nextTokens = null;\n    while (key < maxStart) {\n      if (segGraph.isStartExist(key)) {\n\n        List<SegToken> tokenList = segGraph.getStartList(key);\n\n        // Calculate all tokens for a given key.\n        for (SegToken t1 : tokenList) {\n          oneWordFreq = t1.weight;\n          next = t1.endOffset;\n          nextTokens = null;\n          // Find the next corresponding Token.\n          // For example: \"Sunny seashore\", the present Token is \"sunny\", next one should be \"sea\" or \"seashore\".\n          // If we cannot find the next Token, then go to the end and repeat the same cycle.\n          while (next <= maxStart) {\n            // Because the beginning position of endToken is sentenceLen, so equal to sentenceLen can find endToken.\n            if (segGraph.isStartExist(next)) {\n              nextTokens = segGraph.getStartList(next);\n              break;\n            }\n            next++;\n          }\n          if (nextTokens == null) {\n            break;\n          }\n          for (SegToken t2 : nextTokens) {\n            idBuffer = new char[t1.charArray.length + t2.charArray.length + 1];\n            System.arraycopy(t1.charArray, 0, idBuffer, 0, t1.charArray.length);\n            idBuffer[t1.charArray.length] = BigramDictionary.WORD_SEGMENT_CHAR;\n            System.arraycopy(t2.charArray, 0, idBuffer,\n                t1.charArray.length + 1, t2.charArray.length);\n\n            // Two linked Words frequency\n            wordPairFreq = bigramDict.getFrequency(idBuffer);\n\n            // Smoothing\n\n            // -log{a*P(Ci-1)+(1-a)P(Ci|Ci-1)} Note 0<a<1\n            weight = -Math\n                .log(smooth\n                    * (1.0 + oneWordFreq)\n                    / (Utility.MAX_FREQUENCE + 0.0)\n                    + (1.0 - smooth)\n                    * ((1.0 - tinyDouble) * wordPairFreq / (1.0 + oneWordFreq) + tinyDouble));\n\n            SegTokenPair tokenPair = new SegTokenPair(idBuffer, t1.index,\n                t2.index, weight);\n            this.addSegTokenPair(tokenPair);\n          }\n        }\n      }\n      key++;\n    }\n\n  }\n\n","sourceOld":"  /*\n   * Generate a BiSegGraph based upon a SegGraph\n   */\n  private void generateBiSegGraph(SegGraph segGraph) {\n    double smooth = 0.1;\n    int wordPairFreq = 0;\n    int maxStart = segGraph.getMaxStart();\n    double oneWordFreq, weight, tinyDouble = 1.0 / Utility.MAX_FREQUENCE;\n\n    int next;\n    char[] idBuffer;\n    // get the list of tokens ordered and indexed\n    segTokenList = segGraph.makeIndex();\n    // Because the beginning position of startToken is -1, therefore startToken can be obtained when key = -1\n    int key = -1;\n    List nextTokens = null;\n    while (key < maxStart) {\n      if (segGraph.isStartExist(key)) {\n\n        List tokenList = segGraph.getStartList(key);\n\n        // Calculate all tokens for a given key.\n        for (Iterator iter = tokenList.iterator(); iter.hasNext();) {\n          SegToken t1 = (SegToken) iter.next();\n          oneWordFreq = t1.weight;\n          next = t1.endOffset;\n          nextTokens = null;\n          // Find the next corresponding Token.\n          // For example: \"Sunny seashore\", the present Token is \"sunny\", next one should be \"sea\" or \"seashore\".\n          // If we cannot find the next Token, then go to the end and repeat the same cycle.\n          while (next <= maxStart) {\n            // Because the beginning position of endToken is sentenceLen, so equal to sentenceLen can find endToken.\n            if (segGraph.isStartExist(next)) {\n              nextTokens = segGraph.getStartList(next);\n              break;\n            }\n            next++;\n          }\n          if (nextTokens == null) {\n            break;\n          }\n          for (Iterator iter2 = nextTokens.iterator(); iter2.hasNext();) {\n            SegToken t2 = (SegToken) iter2.next();\n            idBuffer = new char[t1.charArray.length + t2.charArray.length + 1];\n            System.arraycopy(t1.charArray, 0, idBuffer, 0, t1.charArray.length);\n            idBuffer[t1.charArray.length] = BigramDictionary.WORD_SEGMENT_CHAR;\n            System.arraycopy(t2.charArray, 0, idBuffer,\n                t1.charArray.length + 1, t2.charArray.length);\n\n            // Two linked Words frequency\n            wordPairFreq = bigramDict.getFrequency(idBuffer);\n\n            // Smoothing\n\n            // -log{a*P(Ci-1)+(1-a)P(Ci|Ci-1)} Note 0<a<1\n            weight = -Math\n                .log(smooth\n                    * (1.0 + oneWordFreq)\n                    / (Utility.MAX_FREQUENCE + 0.0)\n                    + (1.0 - smooth)\n                    * ((1.0 - tinyDouble) * wordPairFreq / (1.0 + oneWordFreq) + tinyDouble));\n\n            SegTokenPair tokenPair = new SegTokenPair(idBuffer, t1.index,\n                t2.index, weight);\n            this.addSegTokenPair(tokenPair);\n          }\n        }\n      }\n      key++;\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph#generateBiSegGraph(SegGraph).mjava","pathOld":"contrib/analyzers/smartcn/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph#generateBiSegGraph(SegGraph).mjava","sourceNew":"  /*\n   * Generate a BiSegGraph based upon a SegGraph\n   */\n  private void generateBiSegGraph(SegGraph segGraph) {\n    double smooth = 0.1;\n    int wordPairFreq = 0;\n    int maxStart = segGraph.getMaxStart();\n    double oneWordFreq, weight, tinyDouble = 1.0 / Utility.MAX_FREQUENCE;\n\n    int next;\n    char[] idBuffer;\n    // get the list of tokens ordered and indexed\n    segTokenList = segGraph.makeIndex();\n    // Because the beginning position of startToken is -1, therefore startToken can be obtained when key = -1\n    int key = -1;\n    List<SegToken> nextTokens = null;\n    while (key < maxStart) {\n      if (segGraph.isStartExist(key)) {\n\n        List<SegToken> tokenList = segGraph.getStartList(key);\n\n        // Calculate all tokens for a given key.\n        for (SegToken t1 : tokenList) {\n          oneWordFreq = t1.weight;\n          next = t1.endOffset;\n          nextTokens = null;\n          // Find the next corresponding Token.\n          // For example: \"Sunny seashore\", the present Token is \"sunny\", next one should be \"sea\" or \"seashore\".\n          // If we cannot find the next Token, then go to the end and repeat the same cycle.\n          while (next <= maxStart) {\n            // Because the beginning position of endToken is sentenceLen, so equal to sentenceLen can find endToken.\n            if (segGraph.isStartExist(next)) {\n              nextTokens = segGraph.getStartList(next);\n              break;\n            }\n            next++;\n          }\n          if (nextTokens == null) {\n            break;\n          }\n          for (SegToken t2 : nextTokens) {\n            idBuffer = new char[t1.charArray.length + t2.charArray.length + 1];\n            System.arraycopy(t1.charArray, 0, idBuffer, 0, t1.charArray.length);\n            idBuffer[t1.charArray.length] = BigramDictionary.WORD_SEGMENT_CHAR;\n            System.arraycopy(t2.charArray, 0, idBuffer,\n                t1.charArray.length + 1, t2.charArray.length);\n\n            // Two linked Words frequency\n            wordPairFreq = bigramDict.getFrequency(idBuffer);\n\n            // Smoothing\n\n            // -log{a*P(Ci-1)+(1-a)P(Ci|Ci-1)} Note 0<a<1\n            weight = -Math\n                .log(smooth\n                    * (1.0 + oneWordFreq)\n                    / (Utility.MAX_FREQUENCE + 0.0)\n                    + (1.0 - smooth)\n                    * ((1.0 - tinyDouble) * wordPairFreq / (1.0 + oneWordFreq) + tinyDouble));\n\n            SegTokenPair tokenPair = new SegTokenPair(idBuffer, t1.index,\n                t2.index, weight);\n            this.addSegTokenPair(tokenPair);\n          }\n        }\n      }\n      key++;\n    }\n\n  }\n\n","sourceOld":"  /*\n   * Generate a BiSegGraph based upon a SegGraph\n   */\n  private void generateBiSegGraph(SegGraph segGraph) {\n    double smooth = 0.1;\n    int wordPairFreq = 0;\n    int maxStart = segGraph.getMaxStart();\n    double oneWordFreq, weight, tinyDouble = 1.0 / Utility.MAX_FREQUENCE;\n\n    int next;\n    char[] idBuffer;\n    // get the list of tokens ordered and indexed\n    segTokenList = segGraph.makeIndex();\n    // Because the beginning position of startToken is -1, therefore startToken can be obtained when key = -1\n    int key = -1;\n    List<SegToken> nextTokens = null;\n    while (key < maxStart) {\n      if (segGraph.isStartExist(key)) {\n\n        List<SegToken> tokenList = segGraph.getStartList(key);\n\n        // Calculate all tokens for a given key.\n        for (SegToken t1 : tokenList) {\n          oneWordFreq = t1.weight;\n          next = t1.endOffset;\n          nextTokens = null;\n          // Find the next corresponding Token.\n          // For example: \"Sunny seashore\", the present Token is \"sunny\", next one should be \"sea\" or \"seashore\".\n          // If we cannot find the next Token, then go to the end and repeat the same cycle.\n          while (next <= maxStart) {\n            // Because the beginning position of endToken is sentenceLen, so equal to sentenceLen can find endToken.\n            if (segGraph.isStartExist(next)) {\n              nextTokens = segGraph.getStartList(next);\n              break;\n            }\n            next++;\n          }\n          if (nextTokens == null) {\n            break;\n          }\n          for (SegToken t2 : nextTokens) {\n            idBuffer = new char[t1.charArray.length + t2.charArray.length + 1];\n            System.arraycopy(t1.charArray, 0, idBuffer, 0, t1.charArray.length);\n            idBuffer[t1.charArray.length] = BigramDictionary.WORD_SEGMENT_CHAR;\n            System.arraycopy(t2.charArray, 0, idBuffer,\n                t1.charArray.length + 1, t2.charArray.length);\n\n            // Two linked Words frequency\n            wordPairFreq = bigramDict.getFrequency(idBuffer);\n\n            // Smoothing\n\n            // -log{a*P(Ci-1)+(1-a)P(Ci|Ci-1)} Note 0<a<1\n            weight = -Math\n                .log(smooth\n                    * (1.0 + oneWordFreq)\n                    / (Utility.MAX_FREQUENCE + 0.0)\n                    + (1.0 - smooth)\n                    * ((1.0 - tinyDouble) * wordPairFreq / (1.0 + oneWordFreq) + tinyDouble));\n\n            SegTokenPair tokenPair = new SegTokenPair(idBuffer, t1.index,\n                t2.index, weight);\n            this.addSegTokenPair(tokenPair);\n          }\n        }\n      }\n      key++;\n    }\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"dd745d580729e528151b58aeda87ef82f1b95c9b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"573b8638be27860dcb2ce77889c694e5f8a76106":["55fa7b85159e79f79dfdca119db7f0f4cb6a2a74"],"55fa7b85159e79f79dfdca119db7f0f4cb6a2a74":["dd745d580729e528151b58aeda87ef82f1b95c9b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["573b8638be27860dcb2ce77889c694e5f8a76106"]},"commit2Childs":{"dd745d580729e528151b58aeda87ef82f1b95c9b":["55fa7b85159e79f79dfdca119db7f0f4cb6a2a74"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["dd745d580729e528151b58aeda87ef82f1b95c9b"],"573b8638be27860dcb2ce77889c694e5f8a76106":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"55fa7b85159e79f79dfdca119db7f0f4cb6a2a74":["573b8638be27860dcb2ce77889c694e5f8a76106"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}