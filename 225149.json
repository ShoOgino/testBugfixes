{"path":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms).mjava","commits":[{"id":"714aa8d007eef87d7203cfc6e0fe4dab8dd8a497","date":1417181893,"type":1,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector#TokenStreamFromTermPositionVector(Terms).mjava","sourceNew":"  /**\n   * Constructor.\n   * \n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and/or offsets.\n   */\n  public TokenStreamFromTermVector(Terms vector) throws IOException {\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","sourceOld":"  /**\n   * Constructor.\n   * \n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and/or offsets.\n   */\n  public TokenStreamFromTermPositionVector(Terms vector) throws IOException {\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4dc0de75e60c6f90667efd23879e863f5b1ca46e","date":1419308055,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms).mjava","sourceNew":"  /**\n   * Constructor.\n   * \n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and/or offsets.\n   */\n  public TokenStreamFromTermVector(Terms vector) throws IOException {\n    super(ATTRIBUTE_FACTORY);\n    assert !hasAttribute(PayloadAttribute.class) : \"AttributeFactory shouldn't have payloads *yet*\";\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","sourceOld":"  /**\n   * Constructor.\n   * \n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and/or offsets.\n   */\n  public TokenStreamFromTermVector(Terms vector) throws IOException {\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dd5cb61545fbf8394e449204e9780415b9f4c6fc","date":1429738516,"type":5,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms,int).mjava","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector#TokenStreamFromTermVector(Terms).mjava","sourceNew":"  /**\n   * Constructor. The uninversion doesn't happen here; it's delayed till the first call to\n   * {@link #incrementToken}.\n   *\n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and/or offsets.\n   * @param maxStartOffset if a token's start offset exceeds this then the token is not added. -1 disables the limit.\n   */\n  public TokenStreamFromTermVector(Terms vector, int maxStartOffset) throws IOException {\n    super(ATTRIBUTE_FACTORY);\n    this.maxStartOffset = maxStartOffset < 0 ? Integer.MAX_VALUE : maxStartOffset;\n    assert !hasAttribute(PayloadAttribute.class) : \"AttributeFactory shouldn't have payloads *yet*\";\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","sourceOld":"  /**\n   * Constructor.\n   * \n   * @param vector Terms that contains the data for\n   *        creating the TokenStream. Must have positions and/or offsets.\n   */\n  public TokenStreamFromTermVector(Terms vector) throws IOException {\n    super(ATTRIBUTE_FACTORY);\n    assert !hasAttribute(PayloadAttribute.class) : \"AttributeFactory shouldn't have payloads *yet*\";\n    if (!vector.hasPositions() && !vector.hasOffsets()) {\n      throw new IllegalArgumentException(\"The term vector needs positions and/or offsets.\");\n    }\n    assert vector.hasFreqs();\n    this.vector = vector;\n    termAttribute = addAttribute(CharTermAttribute.class);\n    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"714aa8d007eef87d7203cfc6e0fe4dab8dd8a497":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"dd5cb61545fbf8394e449204e9780415b9f4c6fc":["4dc0de75e60c6f90667efd23879e863f5b1ca46e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4dc0de75e60c6f90667efd23879e863f5b1ca46e":["714aa8d007eef87d7203cfc6e0fe4dab8dd8a497"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["dd5cb61545fbf8394e449204e9780415b9f4c6fc"]},"commit2Childs":{"714aa8d007eef87d7203cfc6e0fe4dab8dd8a497":["4dc0de75e60c6f90667efd23879e863f5b1ca46e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["714aa8d007eef87d7203cfc6e0fe4dab8dd8a497"],"dd5cb61545fbf8394e449204e9780415b9f4c6fc":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4dc0de75e60c6f90667efd23879e863f5b1ca46e":["dd5cb61545fbf8394e449204e9780415b9f4c6fc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}