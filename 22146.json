{"path":"lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","commits":[{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/preflex/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","sourceNew":"  /**\n   * Loads the segment information at segment load time.\n   * \n   * @param indexEnum\n   *          the term enum.\n   * @param indexDivisor\n   *          the index divisor.\n   * @param tiiFileLength\n   *          the size of the tii file, used to approximate the size of the\n   *          buffer.\n   * @param totalIndexInterval\n   *          the total index interval.\n   */\n  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {\n    this.totalIndexInterval = totalIndexInterval;\n    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;\n    skipInterval = indexEnum.skipInterval;\n    // this is only an inital size, it will be GCed once the build is complete\n    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;\n    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));\n    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();\n\n    GrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false);\n    String currentField = null;\n    List<String> fieldStrs = new ArrayList<String>();\n    int fieldCounter = -1;\n    for (int i = 0; indexEnum.next(); i++) {\n      Term term = indexEnum.term();\n      if (currentField == null || !currentField.equals(term.field())) {\n        currentField = term.field();\n        fieldStrs.add(currentField);\n        fieldCounter++;\n      }\n      TermInfo termInfo = indexEnum.termInfo();\n      indexToTerms.set(i, dataOutput.getPosition());\n      dataOutput.writeVInt(fieldCounter);\n      dataOutput.writeString(term.text());\n      dataOutput.writeVInt(termInfo.docFreq);\n      if (termInfo.docFreq >= skipInterval) {\n        dataOutput.writeVInt(termInfo.skipOffset);\n      }\n      dataOutput.writeVLong(termInfo.freqPointer);\n      dataOutput.writeVLong(termInfo.proxPointer);\n      dataOutput.writeVLong(indexEnum.indexPointer);\n      for (int j = 1; j < indexDivisor; j++) {\n        if (!indexEnum.next()) {\n          break;\n        }\n      }\n    }\n\n    fields = new Term[fieldStrs.size()];\n    for (int i = 0; i < fields.length; i++) {\n      fields[i] = new Term(fieldStrs.get(i));\n    }\n    \n    dataPagedBytes.freeze(true);\n    dataInput = dataPagedBytes.getDataInput();\n    indexToDataOffset = indexToTerms.getMutable();\n  }\n\n","sourceOld":"  /**\n   * Loads the segment information at segment load time.\n   * \n   * @param indexEnum\n   *          the term enum.\n   * @param indexDivisor\n   *          the index divisor.\n   * @param tiiFileLength\n   *          the size of the tii file, used to approximate the size of the\n   *          buffer.\n   * @param totalIndexInterval\n   *          the total index interval.\n   */\n  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {\n    this.totalIndexInterval = totalIndexInterval;\n    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;\n    skipInterval = indexEnum.skipInterval;\n    // this is only an inital size, it will be GCed once the build is complete\n    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;\n    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));\n    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();\n\n    GrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false);\n    String currentField = null;\n    List<String> fieldStrs = new ArrayList<String>();\n    int fieldCounter = -1;\n    for (int i = 0; indexEnum.next(); i++) {\n      Term term = indexEnum.term();\n      if (currentField == null || !currentField.equals(term.field())) {\n        currentField = term.field();\n        fieldStrs.add(currentField);\n        fieldCounter++;\n      }\n      TermInfo termInfo = indexEnum.termInfo();\n      indexToTerms.set(i, dataOutput.getPosition());\n      dataOutput.writeVInt(fieldCounter);\n      dataOutput.writeString(term.text());\n      dataOutput.writeVInt(termInfo.docFreq);\n      if (termInfo.docFreq >= skipInterval) {\n        dataOutput.writeVInt(termInfo.skipOffset);\n      }\n      dataOutput.writeVLong(termInfo.freqPointer);\n      dataOutput.writeVLong(termInfo.proxPointer);\n      dataOutput.writeVLong(indexEnum.indexPointer);\n      for (int j = 1; j < indexDivisor; j++) {\n        if (!indexEnum.next()) {\n          break;\n        }\n      }\n    }\n\n    fields = new Term[fieldStrs.size()];\n    for (int i = 0; i < fields.length; i++) {\n      fields[i] = new Term(fieldStrs.get(i));\n    }\n    \n    dataPagedBytes.freeze(true);\n    dataInput = dataPagedBytes.getDataInput();\n    indexToDataOffset = indexToTerms.getMutable();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0ae5e3ed1232483b7b8a014f175a5fe43595982","date":1324062192,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermInfosReaderIndex#TermInfosReaderIndex(SegmentTermEnum,int,long,int).mjava","sourceNew":"  /**\n   * Loads the segment information at segment load time.\n   * \n   * @param indexEnum\n   *          the term enum.\n   * @param indexDivisor\n   *          the index divisor.\n   * @param tiiFileLength\n   *          the size of the tii file, used to approximate the size of the\n   *          buffer.\n   * @param totalIndexInterval\n   *          the total index interval.\n   */\n  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {\n    this.totalIndexInterval = totalIndexInterval;\n    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;\n    skipInterval = indexEnum.skipInterval;\n    // this is only an inital size, it will be GCed once the build is complete\n    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;\n    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));\n    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();\n\n    GrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false);\n    String currentField = null;\n    List<String> fieldStrs = new ArrayList<String>();\n    int fieldCounter = -1;\n    for (int i = 0; indexEnum.next(); i++) {\n      Term term = indexEnum.term();\n      if (currentField == null || !currentField.equals(term.field())) {\n        currentField = term.field();\n        fieldStrs.add(currentField);\n        fieldCounter++;\n      }\n      TermInfo termInfo = indexEnum.termInfo();\n      indexToTerms.set(i, dataOutput.getPosition());\n      dataOutput.writeVInt(fieldCounter);\n      dataOutput.writeString(term.text());\n      dataOutput.writeVInt(termInfo.docFreq);\n      if (termInfo.docFreq >= skipInterval) {\n        dataOutput.writeVInt(termInfo.skipOffset);\n      }\n      dataOutput.writeVLong(termInfo.freqPointer);\n      dataOutput.writeVLong(termInfo.proxPointer);\n      dataOutput.writeVLong(indexEnum.indexPointer);\n      for (int j = 1; j < indexDivisor; j++) {\n        if (!indexEnum.next()) {\n          break;\n        }\n      }\n    }\n\n    fields = new Term[fieldStrs.size()];\n    for (int i = 0; i < fields.length; i++) {\n      fields[i] = new Term(fieldStrs.get(i));\n    }\n    \n    dataPagedBytes.freeze(true);\n    dataInput = dataPagedBytes.getDataInput();\n    indexToDataOffset = indexToTerms.getMutable();\n  }\n\n","sourceOld":"  /**\n   * Loads the segment information at segment load time.\n   * \n   * @param indexEnum\n   *          the term enum.\n   * @param indexDivisor\n   *          the index divisor.\n   * @param tiiFileLength\n   *          the size of the tii file, used to approximate the size of the\n   *          buffer.\n   * @param totalIndexInterval\n   *          the total index interval.\n   */\n  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {\n    this.totalIndexInterval = totalIndexInterval;\n    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;\n    skipInterval = indexEnum.skipInterval;\n    // this is only an inital size, it will be GCed once the build is complete\n    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;\n    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));\n    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();\n\n    GrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false);\n    String currentField = null;\n    List<String> fieldStrs = new ArrayList<String>();\n    int fieldCounter = -1;\n    for (int i = 0; indexEnum.next(); i++) {\n      Term term = indexEnum.term();\n      if (currentField == null || !currentField.equals(term.field())) {\n        currentField = term.field();\n        fieldStrs.add(currentField);\n        fieldCounter++;\n      }\n      TermInfo termInfo = indexEnum.termInfo();\n      indexToTerms.set(i, dataOutput.getPosition());\n      dataOutput.writeVInt(fieldCounter);\n      dataOutput.writeString(term.text());\n      dataOutput.writeVInt(termInfo.docFreq);\n      if (termInfo.docFreq >= skipInterval) {\n        dataOutput.writeVInt(termInfo.skipOffset);\n      }\n      dataOutput.writeVLong(termInfo.freqPointer);\n      dataOutput.writeVLong(termInfo.proxPointer);\n      dataOutput.writeVLong(indexEnum.indexPointer);\n      for (int j = 1; j < indexDivisor; j++) {\n        if (!indexEnum.next()) {\n          break;\n        }\n      }\n    }\n\n    fields = new Term[fieldStrs.size()];\n    for (int i = 0; i < fields.length; i++) {\n      fields[i] = new Term(fieldStrs.get(i));\n    }\n    \n    dataPagedBytes.freeze(true);\n    dataInput = dataPagedBytes.getDataInput();\n    indexToDataOffset = indexToTerms.getMutable();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7b91922b55d15444d554721b352861d028eb8278":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["7b91922b55d15444d554721b352861d028eb8278"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"]},"commit2Childs":{"7b91922b55d15444d554721b352861d028eb8278":["a0ae5e3ed1232483b7b8a014f175a5fe43595982"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7b91922b55d15444d554721b352861d028eb8278"],"a0ae5e3ed1232483b7b8a014f175a5fe43595982":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}