{"path":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","commits":[{"id":"893269407f5c988a4b2ee13c7ffc01ec43136c1d","date":1268598046,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"/dev/null","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","date":1268599006,"type":4,"author":"Mark Robert Miller","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":null,"sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1da8d55113b689b06716246649de6f62430f15c0","date":1453508340,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"/dev/null","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c220849f876de24a79f756f65b3eb045db59f63f","date":1294902803,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":["815287248ca7a77db68038baad5698c5767f36a7","815287248ca7a77db68038baad5698c5767f36a7","815287248ca7a77db68038baad5698c5767f36a7","815287248ca7a77db68038baad5698c5767f36a7","815287248ca7a77db68038baad5698c5767f36a7","815287248ca7a77db68038baad5698c5767f36a7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c45bbf26db88631f7a389cbff0f4eab70f55ec64","date":1303271007,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":["5df1793b9dbc0f17ba1d1dddb8a15748fdc3aaf3"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d493718201f0d0c54c773fb323d87bbd2fbffe41","date":1303546048,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab0e9f7ce724e6aea1fea746dded19e76d231cf8","date":1304774078,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    SolrIndexSearcher searcher = req.getSearcher();\n    IndexSchema schema = searcher.getSchema();\n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n                   \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        tstream = new CachingTokenFilter(tstream);\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"baea56f5b21587987fe0d10e8153a3cf2a9b423b","date":1310091971,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1291e4568eb7d9463d751627596ef14baf4c1603","date":1310112572,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        // TODO: this is not always necessary - eventually we would like to avoid this wrap\n        //       when it is not needed.\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","pathOld":"solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter#doHighlightingByHighlighter(Query,SolrQueryRequest,NamedList,int,Document,String).mjava","sourceNew":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","sourceOld":"  private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    \n    SolrParams params = req.getParams(); \n    String[] docTexts = doc.getValues(fieldName);\n    // according to Document javadoc, doc.getValues() never returns null. check empty instead of null\n    if (docTexts.length == 0) return;\n    \n    TokenStream tstream = null;\n    int numFragments = getMaxSnippets(fieldName, params);\n    boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);\n\n    String[] summaries = null;\n    List<TextFragment> frags = new ArrayList<TextFragment>();\n\n    TermOffsetsTokenStream tots = null; // to be non-null iff we're using TermOffsets optimization\n    try {\n        TokenStream tvStream = TokenSources.getTokenStream(searcher.getIndexReader(), docId, fieldName);\n        if (tvStream != null) {\n          tots = new TermOffsetsTokenStream(tvStream);\n        }\n    }\n    catch (IllegalArgumentException e) {\n      // No problem. But we can't use TermOffsets optimization.\n    }\n\n    for (int j = 0; j < docTexts.length; j++) {\n      if( tots != null ) {\n        // if we're using TermOffsets optimization, then get the next\n        // field value's TokenStream (i.e. get field j's TokenStream) from tots:\n        tstream = tots.getMultiValuedTokenStream( docTexts[j].length() );\n      } else {\n        // fall back to analyzer\n        tstream = createAnalyzerTStream(schema, fieldName, docTexts[j]);\n      }\n      \n      int maxCharsToAnalyze = params.getFieldInt(fieldName,\n          HighlightParams.MAX_CHARS,\n          Highlighter.DEFAULT_MAX_CHARS_TO_ANALYZE);\n      \n      Highlighter highlighter;\n      if (Boolean.valueOf(req.getParams().get(HighlightParams.USE_PHRASE_HIGHLIGHTER, \"true\"))) {\n        if (maxCharsToAnalyze < 0) {\n          tstream = new CachingTokenFilter(tstream);\n        } else {\n          tstream = new CachingTokenFilter(new OffsetLimitTokenFilter(tstream, maxCharsToAnalyze));\n        }\n        \n        // get highlighter\n        highlighter = getPhraseHighlighter(query, fieldName, req, (CachingTokenFilter) tstream);\n         \n        // after highlighter initialization, reset tstream since construction of highlighter already used it\n        tstream.reset();\n      }\n      else {\n        // use \"the old way\"\n        highlighter = getHighlighter(query, fieldName, req);\n      }\n      \n      if (maxCharsToAnalyze < 0) {\n        highlighter.setMaxDocCharsToAnalyze(docTexts[j].length());\n      } else {\n        highlighter.setMaxDocCharsToAnalyze(maxCharsToAnalyze);\n      }\n\n      try {\n        TextFragment[] bestTextFragments = highlighter.getBestTextFragments(tstream, docTexts[j], mergeContiguousFragments, numFragments);\n        for (int k = 0; k < bestTextFragments.length; k++) {\n          if ((bestTextFragments[k] != null) && (bestTextFragments[k].getScore() > 0)) {\n            frags.add(bestTextFragments[k]);\n          }\n        }\n      } catch (InvalidTokenOffsetsException e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    // sort such that the fragments with the highest score come first\n    Collections.sort(frags, new Comparator<TextFragment>() {\n      public int compare(TextFragment arg0, TextFragment arg1) {\n        return Math.round(arg1.getScore() - arg0.getScore());\n      }\n    });\n    \n     // convert fragments back into text\n     // TODO: we can include score and position information in output as snippet attributes\n    if (frags.size() > 0) {\n      ArrayList<String> fragTexts = new ArrayList<String>();\n      for (TextFragment fragment: frags) {\n        if ((fragment != null) && (fragment.getScore() > 0)) {\n          fragTexts.add(fragment.toString());\n        }\n        if (fragTexts.size() >= numFragments) break;\n      }\n      summaries = fragTexts.toArray(new String[0]);\n      if (summaries.length > 0) \n      docSummaries.add(fieldName, summaries);\n    }\n    // no summeries made, copy text from alternate field\n    if (summaries == null || summaries.length == 0) {\n      alternateField( docSummaries, params, doc, fieldName );\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ab0e9f7ce724e6aea1fea746dded19e76d231cf8":["c45bbf26db88631f7a389cbff0f4eab70f55ec64"],"c45bbf26db88631f7a389cbff0f4eab70f55ec64":["c220849f876de24a79f756f65b3eb045db59f63f"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"c26f00b574427b55127e869b935845554afde1fa":["baea56f5b21587987fe0d10e8153a3cf2a9b423b","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","c45bbf26db88631f7a389cbff0f4eab70f55ec64"],"d493718201f0d0c54c773fb323d87bbd2fbffe41":["868da859b43505d9d2a023bfeae6dd0c795f5295","c45bbf26db88631f7a389cbff0f4eab70f55ec64"],"1da8d55113b689b06716246649de6f62430f15c0":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","ad94625fb8d088209f46650c8097196fec67f00c"],"1291e4568eb7d9463d751627596ef14baf4c1603":["ab0e9f7ce724e6aea1fea746dded19e76d231cf8","baea56f5b21587987fe0d10e8153a3cf2a9b423b"],"c220849f876de24a79f756f65b3eb045db59f63f":["1da8d55113b689b06716246649de6f62430f15c0"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a3776dccca01c11e7046323cfad46a3b4a471233"],"a3776dccca01c11e7046323cfad46a3b4a471233":["c220849f876de24a79f756f65b3eb045db59f63f","ab0e9f7ce724e6aea1fea746dded19e76d231cf8"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["1da8d55113b689b06716246649de6f62430f15c0","c220849f876de24a79f756f65b3eb045db59f63f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","ab0e9f7ce724e6aea1fea746dded19e76d231cf8"],"baea56f5b21587987fe0d10e8153a3cf2a9b423b":["ab0e9f7ce724e6aea1fea746dded19e76d231cf8"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["baea56f5b21587987fe0d10e8153a3cf2a9b423b"],"ad94625fb8d088209f46650c8097196fec67f00c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["1da8d55113b689b06716246649de6f62430f15c0","c220849f876de24a79f756f65b3eb045db59f63f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"]},"commit2Childs":{"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8"],"ab0e9f7ce724e6aea1fea746dded19e76d231cf8":["1291e4568eb7d9463d751627596ef14baf4c1603","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","baea56f5b21587987fe0d10e8153a3cf2a9b423b"],"c45bbf26db88631f7a389cbff0f4eab70f55ec64":["ab0e9f7ce724e6aea1fea746dded19e76d231cf8","135621f3a0670a9394eb563224a3b76cc4dddc0f","d493718201f0d0c54c773fb323d87bbd2fbffe41"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["1da8d55113b689b06716246649de6f62430f15c0"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ad94625fb8d088209f46650c8097196fec67f00c"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"d493718201f0d0c54c773fb323d87bbd2fbffe41":[],"1da8d55113b689b06716246649de6f62430f15c0":["c220849f876de24a79f756f65b3eb045db59f63f","29ef99d61cda9641b6250bf9567329a6e65f901d","868da859b43505d9d2a023bfeae6dd0c795f5295"],"1291e4568eb7d9463d751627596ef14baf4c1603":[],"c220849f876de24a79f756f65b3eb045db59f63f":["c45bbf26db88631f7a389cbff0f4eab70f55ec64","a3776dccca01c11e7046323cfad46a3b4a471233","29ef99d61cda9641b6250bf9567329a6e65f901d","868da859b43505d9d2a023bfeae6dd0c795f5295"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":[],"baea56f5b21587987fe0d10e8153a3cf2a9b423b":["c26f00b574427b55127e869b935845554afde1fa","1291e4568eb7d9463d751627596ef14baf4c1603","a258fbb26824fd104ed795e5d9033d2d040049ee"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"ad94625fb8d088209f46650c8097196fec67f00c":["1da8d55113b689b06716246649de6f62430f15c0"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["d493718201f0d0c54c773fb323d87bbd2fbffe41"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d493718201f0d0c54c773fb323d87bbd2fbffe41","1291e4568eb7d9463d751627596ef14baf4c1603","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}