{"path":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","commits":[{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"/dev/null","sourceNew":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          Token token = perThread.localToken;\n          token.clear();\n          char[] termBuffer = token.termBuffer();\n          if (termBuffer.length < valueLength)\n            termBuffer = token.resizeTermBuffer(valueLength);\n          stringValue.getChars(0, valueLength, termBuffer, 0);\n          token.setTermLength(valueLength);\n          token.setStartOffset(fieldState.offset);\n          token.setEndOffset(fieldState.offset + stringValue.length());\n          boolean success = false;\n          try {\n            consumer.add(token);\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += stringValue.length();\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            final Token localToken = perThread.localToken;\n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              Token token = stream.next(localToken);\n\n              if (token == null) break;\n              fieldState.position += (token.getPositionIncrement() - 1);\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add(token);\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + token.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e2cb543b41c145f33390f460ee743d6693c9c6c","date":1219243087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          Token token = perThread.localToken.reinit(stringValue, fieldState.offset, fieldState.offset + valueLength);\n          boolean success = false;\n          try {\n            consumer.add(token);\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            final Token localToken = perThread.localToken;\n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              Token token = stream.next(localToken);\n\n              if (token == null) break;\n              fieldState.position += (token.getPositionIncrement() - 1);\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add(token);\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + token.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          Token token = perThread.localToken;\n          token.clear();\n          char[] termBuffer = token.termBuffer();\n          if (termBuffer.length < valueLength)\n            termBuffer = token.resizeTermBuffer(valueLength);\n          stringValue.getChars(0, valueLength, termBuffer, 0);\n          token.setTermLength(valueLength);\n          token.setStartOffset(fieldState.offset);\n          token.setEndOffset(fieldState.offset + stringValue.length());\n          boolean success = false;\n          try {\n            consumer.add(token);\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += stringValue.length();\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            final Token localToken = perThread.localToken;\n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              Token token = stream.next(localToken);\n\n              if (token == null) break;\n              fieldState.position += (token.getPositionIncrement() - 1);\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add(token);\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + token.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":["af2505fad8b3090c182da766589b31256e69bf3f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d41ab32fc87982eab82a896cee390971b5c08fd2","date":1225735438,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          Token token = perThread.localToken.reinit(stringValue, fieldState.offset, fieldState.offset + valueLength);\n          boolean success = false;\n          try {\n            consumer.add(token);\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            final Token localToken = perThread.localToken;\n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              Token token = stream.next(localToken);\n\n              if (token == null) break;\n              final int posIncr = token.getPositionIncrement();\n              fieldState.position += posIncr - 1;\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add(token);\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + token.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          Token token = perThread.localToken.reinit(stringValue, fieldState.offset, fieldState.offset + valueLength);\n          boolean success = false;\n          try {\n            consumer.add(token);\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            final Token localToken = perThread.localToken;\n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              Token token = stream.next(localToken);\n\n              if (token == null) break;\n              fieldState.position += (token.getPositionIncrement() - 1);\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add(token);\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + token.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":["9c576fba1e3c6d11c61fa0802214d5150fb5c633"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"af2505fad8b3090c182da766589b31256e69bf3f","date":1226086659,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          Token token = perThread.localToken.reinit(stringValue, 0, valueLength);\n          boolean success = false;\n          try {\n            consumer.add(token);\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            final Token localToken = perThread.localToken;\n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              Token token = stream.next(localToken);\n\n              if (token == null) break;\n              final int posIncr = token.getPositionIncrement();\n              fieldState.position += posIncr - 1;\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add(token);\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + token.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          Token token = perThread.localToken.reinit(stringValue, fieldState.offset, fieldState.offset + valueLength);\n          boolean success = false;\n          try {\n            consumer.add(token);\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            final Token localToken = perThread.localToken;\n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              Token token = stream.next(localToken);\n\n              if (token == null) break;\n              final int posIncr = token.getPositionIncrement();\n              fieldState.position += posIncr - 1;\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add(token);\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + token.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223","date":1227051709,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          perThread.localTokenStream.reset();\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean useNewTokenStreamAPI = stream.useNewAPI();\n            Token localToken = null;\n            \n            if (useNewTokenStreamAPI) {\n              fieldState.attributeSource = stream;\n            } else {              \n              fieldState.attributeSource = perThread.localTokenStream;\n              localToken = perThread.localToken;\n            }         \n            \n            consumer.start(field);\n\n            OffsetAttribute offsetAttribute = (OffsetAttribute) fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              Token token = null;\n              if (useNewTokenStreamAPI) {\n                if (!stream.incrementToken()) break;\n              } else {\n                token = stream.next(localToken);\n                if (token == null) break;\n                perThread.localTokenStream.set(token);\n              }\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr - 1;\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          Token token = perThread.localToken.reinit(stringValue, 0, valueLength);\n          boolean success = false;\n          try {\n            consumer.add(token);\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            final Token localToken = perThread.localToken;\n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              Token token = stream.next(localToken);\n\n              if (token == null) break;\n              final int posIncr = token.getPositionIncrement();\n              fieldState.position += posIncr - 1;\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add(token);\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + token.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9c576fba1e3c6d11c61fa0802214d5150fb5c633","date":1243676170,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          perThread.localTokenStream.reset();\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          // deprecated\n          final boolean allowMinus1Position = docState.allowMinus1Position;\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean useNewTokenStreamAPI = stream.useNewAPI();\n            Token localToken = null;\n            \n            if (useNewTokenStreamAPI) {\n              fieldState.attributeSource = stream;\n            } else {              \n              fieldState.attributeSource = perThread.localTokenStream;\n              localToken = perThread.localToken;\n            }         \n            \n            consumer.start(field);\n\n            OffsetAttribute offsetAttribute = (OffsetAttribute) fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              Token token = null;\n              if (useNewTokenStreamAPI) {\n                if (!stream.incrementToken()) break;\n              } else {\n                token = stream.next(localToken);\n                if (token == null) break;\n                perThread.localTokenStream.set(token);\n              }\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (allowMinus1Position || fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          perThread.localTokenStream.reset();\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean useNewTokenStreamAPI = stream.useNewAPI();\n            Token localToken = null;\n            \n            if (useNewTokenStreamAPI) {\n              fieldState.attributeSource = stream;\n            } else {              \n              fieldState.attributeSource = perThread.localTokenStream;\n              localToken = perThread.localToken;\n            }         \n            \n            consumer.start(field);\n\n            OffsetAttribute offsetAttribute = (OffsetAttribute) fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              Token token = null;\n              if (useNewTokenStreamAPI) {\n                if (!stream.incrementToken()) break;\n              } else {\n                token = stream.next(localToken);\n                if (token == null) break;\n                perThread.localTokenStream.set(token);\n              }\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr - 1;\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":["d41ab32fc87982eab82a896cee390971b5c08fd2"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ec8b5a20a12931b8d7e616c79c5248ae06cc5568","date":1248471948,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          // deprecated\n          final boolean allowMinus1Position = docState.allowMinus1Position;\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = (OffsetAttribute) fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (allowMinus1Position || fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          perThread.localTokenStream.reset();\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          // deprecated\n          final boolean allowMinus1Position = docState.allowMinus1Position;\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean useNewTokenStreamAPI = stream.useNewAPI();\n            Token localToken = null;\n            \n            if (useNewTokenStreamAPI) {\n              fieldState.attributeSource = stream;\n            } else {              \n              fieldState.attributeSource = perThread.localTokenStream;\n              localToken = perThread.localToken;\n            }         \n            \n            consumer.start(field);\n\n            OffsetAttribute offsetAttribute = (OffsetAttribute) fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              Token token = null;\n              if (useNewTokenStreamAPI) {\n                if (!stream.incrementToken()) break;\n              } else {\n                token = stream.next(localToken);\n                if (token == null) break;\n                perThread.localTokenStream.set(token);\n              }\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (allowMinus1Position || fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"457c790b0d3d5883da64fb842ea54813004bb796","date":1248495093,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          // deprecated\n          final boolean allowMinus1Position = docState.allowMinus1Position;\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = (OffsetAttribute) fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (allowMinus1Position || fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          // deprecated\n          final boolean allowMinus1Position = docState.allowMinus1Position;\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = (OffsetAttribute) fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (allowMinus1Position || fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            fieldState.offset = offsetEnd+1;\n          } finally {\n            stream.close();\n          }\n        }\n\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d78f014fded44fbde905f4f84cdc21907b371e8","date":1254383623,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          // deprecated\n          final boolean allowMinus1Position = docState.allowMinus1Position;\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (allowMinus1Position || fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          // deprecated\n          final boolean allowMinus1Position = docState.allowMinus1Position;\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = (OffsetAttribute) fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = (PositionIncrementAttribute) fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (allowMinus1Position || fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7","date":1255555265,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          // deprecated\n          final boolean allowMinus1Position = docState.allowMinus1Position;\n\n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (allowMinus1Position || fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a9e385641d717e641408d8fbbc62be8fc766357","date":1256746606,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"27189b7ecdd36912cf29af155cdfcca5341cf26f","date":1256748271,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleTokenTokenStream.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleTokenTokenStream;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e0c9f06005a994ad48f892ad6d8995317f921c69","date":1265553588,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            int offsetEnd = fieldState.offset-1;\n            \n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              offsetEnd = fieldState.offset + offsetAttribute.endOffset();\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","pathOld":"src/java/org/apache/lucene/index/DocInverterPerField#processFields(Fieldable[],int).mjava","sourceNew":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","sourceOld":"  @Override\n  public void processFields(final Fieldable[] fields,\n                            final int count) throws IOException {\n\n    fieldState.reset(docState.doc.getBoost());\n\n    final int maxFieldLength = docState.maxFieldLength;\n\n    final boolean doInvert = consumer.start(fields, count);\n\n    for(int i=0;i<count;i++) {\n\n      final Fieldable field = fields[i];\n\n      // TODO FI: this should be \"genericized\" to querying\n      // consumer if it wants to see this particular field\n      // tokenized.\n      if (field.isIndexed() && doInvert) {\n\n        final boolean anyToken;\n        \n        if (fieldState.length > 0)\n          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);\n\n        if (!field.isTokenized()) {\t\t  // un-tokenized field\n          String stringValue = field.stringValue();\n          final int valueLength = stringValue.length();\n          perThread.singleToken.reinit(stringValue, 0, valueLength);\n          fieldState.attributeSource = perThread.singleToken;\n          consumer.start(field);\n\n          boolean success = false;\n          try {\n            consumer.add();\n            success = true;\n          } finally {\n            if (!success)\n              docState.docWriter.setAborting();\n          }\n          fieldState.offset += valueLength;\n          fieldState.length++;\n          fieldState.position++;\n          anyToken = valueLength > 0;\n        } else {                                  // tokenized field\n          final TokenStream stream;\n          final TokenStream streamValue = field.tokenStreamValue();\n\n          if (streamValue != null) \n            stream = streamValue;\n          else {\n            // the field does not have a TokenStream,\n            // so we have to obtain one from the analyzer\n            final Reader reader;\t\t\t  // find or make Reader\n            final Reader readerValue = field.readerValue();\n\n            if (readerValue != null)\n              reader = readerValue;\n            else {\n              String stringValue = field.stringValue();\n              if (stringValue == null)\n                throw new IllegalArgumentException(\"field must have either TokenStream, String or Reader value\");\n              perThread.stringReader.init(stringValue);\n              reader = perThread.stringReader;\n            }\n          \n            // Tokenize field and add to postingTable\n            stream = docState.analyzer.reusableTokenStream(fieldInfo.name, reader);\n          }\n\n          // reset the TokenStream to the first token\n          stream.reset();\n\n          final int startLength = fieldState.length;\n          \n          try {\n            boolean hasMoreTokens = stream.incrementToken();\n\n            fieldState.attributeSource = stream;\n\n            OffsetAttribute offsetAttribute = fieldState.attributeSource.addAttribute(OffsetAttribute.class);\n            PositionIncrementAttribute posIncrAttribute = fieldState.attributeSource.addAttribute(PositionIncrementAttribute.class);\n            \n            consumer.start(field);\n            \n            for(;;) {\n\n              // If we hit an exception in stream.next below\n              // (which is fairly common, eg if analyzer\n              // chokes on a given document), then it's\n              // non-aborting and (above) this one document\n              // will be marked as deleted, but still\n              // consume a docID\n              \n              if (!hasMoreTokens) break;\n              \n              final int posIncr = posIncrAttribute.getPositionIncrement();\n              fieldState.position += posIncr;\n              if (fieldState.position > 0) {\n                fieldState.position--;\n              }\n\n              if (posIncr == 0)\n                fieldState.numOverlap++;\n\n              boolean success = false;\n              try {\n                // If we hit an exception in here, we abort\n                // all buffered documents since the last\n                // flush, on the likelihood that the\n                // internal state of the consumer is now\n                // corrupt and should not be flushed to a\n                // new segment:\n                consumer.add();\n                success = true;\n              } finally {\n                if (!success)\n                  docState.docWriter.setAborting();\n              }\n              fieldState.position++;\n              if (++fieldState.length >= maxFieldLength) {\n                if (docState.infoStream != null)\n                  docState.infoStream.println(\"maxFieldLength \" +maxFieldLength+ \" reached for field \" + fieldInfo.name + \", ignoring following tokens\");\n                break;\n              }\n\n              hasMoreTokens = stream.incrementToken();\n            }\n            // trigger streams to perform end-of-stream operations\n            stream.end();\n            \n            fieldState.offset += offsetAttribute.endOffset();\n            anyToken = fieldState.length > startLength;\n          } finally {\n            stream.close();\n          }\n        }\n\n        if (anyToken)\n          fieldState.offset += docState.analyzer.getOffsetGap(field);\n        fieldState.boost *= field.getBoost();\n      }\n    }\n\n    consumer.finish();\n    endConsumer.finish();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["5350389bf83287111f7760b9e3db3af8e3648474"],"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["af2505fad8b3090c182da766589b31256e69bf3f"],"8a9e385641d717e641408d8fbbc62be8fc766357":["560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7"],"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"27189b7ecdd36912cf29af155cdfcca5341cf26f":["8a9e385641d717e641408d8fbbc62be8fc766357"],"e0c9f06005a994ad48f892ad6d8995317f921c69":["27189b7ecdd36912cf29af155cdfcca5341cf26f"],"d41ab32fc87982eab82a896cee390971b5c08fd2":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"457c790b0d3d5883da64fb842ea54813004bb796":["ec8b5a20a12931b8d7e616c79c5248ae06cc5568"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8d78f014fded44fbde905f4f84cdc21907b371e8":["457c790b0d3d5883da64fb842ea54813004bb796"],"af2505fad8b3090c182da766589b31256e69bf3f":["d41ab32fc87982eab82a896cee390971b5c08fd2"],"5350389bf83287111f7760b9e3db3af8e3648474":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["e0c9f06005a994ad48f892ad6d8995317f921c69"],"ec8b5a20a12931b8d7e616c79c5248ae06cc5568":["9c576fba1e3c6d11c61fa0802214d5150fb5c633"],"9c576fba1e3c6d11c61fa0802214d5150fb5c633":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"]},"commit2Childs":{"7e2cb543b41c145f33390f460ee743d6693c9c6c":["d41ab32fc87982eab82a896cee390971b5c08fd2"],"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["9c576fba1e3c6d11c61fa0802214d5150fb5c633"],"8a9e385641d717e641408d8fbbc62be8fc766357":["27189b7ecdd36912cf29af155cdfcca5341cf26f"],"560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7":["8a9e385641d717e641408d8fbbc62be8fc766357"],"27189b7ecdd36912cf29af155cdfcca5341cf26f":["e0c9f06005a994ad48f892ad6d8995317f921c69"],"e0c9f06005a994ad48f892ad6d8995317f921c69":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"d41ab32fc87982eab82a896cee390971b5c08fd2":["af2505fad8b3090c182da766589b31256e69bf3f"],"457c790b0d3d5883da64fb842ea54813004bb796":["8d78f014fded44fbde905f4f84cdc21907b371e8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5350389bf83287111f7760b9e3db3af8e3648474"],"8d78f014fded44fbde905f4f84cdc21907b371e8":["560b3a3bb8efcae105d6ae5fbee0f8b03c7decc7"],"af2505fad8b3090c182da766589b31256e69bf3f":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"5350389bf83287111f7760b9e3db3af8e3648474":["7e2cb543b41c145f33390f460ee743d6693c9c6c"],"ec8b5a20a12931b8d7e616c79c5248ae06cc5568":["457c790b0d3d5883da64fb842ea54813004bb796"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9c576fba1e3c6d11c61fa0802214d5150fb5c633":["ec8b5a20a12931b8d7e616c79c5248ae06cc5568"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}