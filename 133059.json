{"path":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","commits":[{"id":"abb23fcc2461782ab204e61213240feb77d355aa","date":1422029612,"type":1,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#doTest().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrClient client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  public void doTest() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrClient client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cc3b13b430571c2e169f98fe38e1e7666f88522d","date":1422446157,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n      CloudSolrClient client = createCloudClient(\"collection1\");\n      try {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      } finally {\n        client.shutdown();\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["d2f514ff99d806c2911cbc0e81d02d3d756e0ae6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98287baa2c8d136e801f366a73e27a23285b7b98","date":1427241813,"type":3,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = new FullThrottleStoppableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = new FullThrottleStoppableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    boolean testsSuccesful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n       del(\"*:*\");\n      \n      List<StopableThread> threads = new ArrayList<>();\n      List<StopableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableIndexingThread indexThread = new StopableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StopableSearchThread searchThread = new StopableSearchThread();\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStopableIndexingThread ftIndexThread = new FullThrottleStopableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n      \n      for (StopableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StopableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StopableThread indexThread : threads) {\n        if (indexThread instanceof StopableIndexingThread && !(indexThread instanceof FullThrottleStopableIndexingThread)) {\n          assertFalse(\"There were too many update fails - we expect it can happen, but shouldn't easily\", ((StopableIndexingThread) indexThread).getFailCount() > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        zkServer.shutdown();\n        zkServer = new ZkTestServer(zkServer.getZkDir(), zkServer.getPort());\n        zkServer.run();\n      }\n      \n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testsSuccesful = true;\n    } finally {\n      if (!testsSuccesful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"102da6baafc0f534a59f31729343dbab9d3b9e9a","date":1438410244,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = new FullThrottleStoppableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = new FullThrottleStoppableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState(true);\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f","date":1457343183,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = new FullThrottleStoppableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = new FullThrottleStoppableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateClusterState();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"859081acf00749f5dd462772c571d611d4a4d2db","date":1459527719,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    cloudClient.setSoTimeout(clientSoTimeout);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = new FullThrottleStoppableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = new FullThrottleStoppableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"61c45e99cf6676da48f19d7511c73712ad39402b","date":1495508331,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    cloudClient.setSoTimeout(clientSoTimeout);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    cloudClient.setSoTimeout(clientSoTimeout);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = new FullThrottleStoppableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    cloudClient.setSoTimeout(clientSoTimeout);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    cloudClient.setSoTimeout(clientSoTimeout);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = new FullThrottleStoppableIndexingThread(\n            clients, \"ft1\", true);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"43d1e498704edd2bba13548a189eed4dfccff11b","date":1499143458,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    cloudClient.setSoTimeout(clientSoTimeout);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2ea161f828a3a7a6eb9410a431aecda6d7ab1065","date":1499213384,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    cloudClient.setSoTimeout(clientSoTimeout);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d2f514ff99d806c2911cbc0e81d02d3d756e0ae6","date":1499288484,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e73d8d559120669b47658108d818b637df5456ea","date":1499401413,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\")) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c9595c75582a7ea7efb585014102ed83f2d9c8b","date":1523581112,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6b87d1f8719d7f05be003f3477450b74af13706a","date":1523590376,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f607a0a2e930f55385c7a24afb68ef661ef7e3ee","date":1530823671,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108","date":1533256859,"type":3,"author":"Erick","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb222a3f9d9421d5c95afce73013fbd8de07ea1f","date":1543514331,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n\n    boolean testSuccessful = false;\n    try  (CloudSolrClient ourCloudClient = createCloudClient(DEFAULT_COLLECTION)) {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      if (runFullThrottle) {\n        ftIndexThread = \n            new FullThrottleStoppableIndexingThread(cloudClient.getHttpClient(),controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      \n      if (runFullThrottle) {\n        ftIndexThread.safeStop();\n      }\n      \n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      waitForThingsToLevelOut(20);\n      \n      commit();\n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n       // restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n    cloudClient = createCloudClient(DEFAULT_COLLECTION);\n    boolean testSuccessful = false;\n    try {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      // TODO: we only do this sometimes so that we can sometimes compare against control,\n      // it's currently hard to know what requests failed when using ConcurrentSolrUpdateServer\n      boolean runFullThrottle = random().nextBoolean();\n      if (runFullThrottle) {\n        FullThrottleStoppableIndexingThread ftIndexThread = \n            new FullThrottleStoppableIndexingThread(controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        threads.add(ftIndexThread);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n        restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":["2c007e7c4cf8c55bc2a5884e315123afaaeec87f","a6378064655e76cd7b908b1cab4ce425b384b508","98287baa2c8d136e801f366a73e27a23285b7b98","43d1e498704edd2bba13548a189eed4dfccff11b","02c6a0e240c698414e7728a55f07361be84852d8","6013b4c7388f1627659c8f96c44abd10a294d3a6","61c45e99cf6676da48f19d7511c73712ad39402b"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8d35c84fdef07284c122012ca4000d3b7285a66e","date":1545962630,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  // commented out on: 24-Dec-2018   @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n\n    boolean testSuccessful = false;\n    try  (CloudSolrClient ourCloudClient = createCloudClient(DEFAULT_COLLECTION)) {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      if (runFullThrottle) {\n        ftIndexThread = \n            new FullThrottleStoppableIndexingThread(cloudClient.getHttpClient(),controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      \n      if (runFullThrottle) {\n        ftIndexThread.safeStop();\n      }\n      \n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      waitForThingsToLevelOut(20);\n      \n      commit();\n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n       // restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n\n    boolean testSuccessful = false;\n    try  (CloudSolrClient ourCloudClient = createCloudClient(DEFAULT_COLLECTION)) {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      if (runFullThrottle) {\n        ftIndexThread = \n            new FullThrottleStoppableIndexingThread(cloudClient.getHttpClient(),controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      \n      if (runFullThrottle) {\n        ftIndexThread.safeStop();\n      }\n      \n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      waitForThingsToLevelOut(20);\n      \n      commit();\n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n       // restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"add1e7dd742ea533ff4318cea83ca0a1f669f662","date":1585262285,"type":3,"author":"Mike Drob","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  // commented out on: 24-Dec-2018   @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n\n    boolean testSuccessful = false;\n    try  (CloudSolrClient ourCloudClient = createCloudClient(DEFAULT_COLLECTION)) {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      if (runFullThrottle) {\n        ftIndexThread = \n            new FullThrottleStoppableIndexingThread(cloudClient.getHttpClient(),controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      \n      if (runFullThrottle) {\n        ftIndexThread.safeStop();\n      }\n      \n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      waitForThingsToLevelOut(20, TimeUnit.SECONDS);\n      \n      commit();\n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n       // restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  // commented out on: 24-Dec-2018   @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n\n    boolean testSuccessful = false;\n    try  (CloudSolrClient ourCloudClient = createCloudClient(DEFAULT_COLLECTION)) {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      if (runFullThrottle) {\n        ftIndexThread = \n            new FullThrottleStoppableIndexingThread(cloudClient.getHttpClient(),controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      \n      if (runFullThrottle) {\n        ftIndexThread.safeStop();\n      }\n      \n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut(Integer.MAX_VALUE);//Math.round((runLength / 1000.0f / 3.0f)));\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      waitForThingsToLevelOut(20);\n      \n      commit();\n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n       // restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aa2585c33d5d66a1c837c312221eb55ddb3c4300","date":1592493170,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  // commented out on: 24-Dec-2018   @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  @SuppressWarnings({\"try\"})\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n\n    boolean testSuccessful = false;\n    try  (CloudSolrClient ourCloudClient = createCloudClient(DEFAULT_COLLECTION)) {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      if (runFullThrottle) {\n        ftIndexThread = \n            new FullThrottleStoppableIndexingThread(cloudClient.getHttpClient(),controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      \n      if (runFullThrottle) {\n        ftIndexThread.safeStop();\n      }\n      \n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      waitForThingsToLevelOut(20, TimeUnit.SECONDS);\n      \n      commit();\n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n       // restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  // commented out on: 24-Dec-2018   @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n\n    boolean testSuccessful = false;\n    try  (CloudSolrClient ourCloudClient = createCloudClient(DEFAULT_COLLECTION)) {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      if (runFullThrottle) {\n        ftIndexThread = \n            new FullThrottleStoppableIndexingThread(cloudClient.getHttpClient(),controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      \n      if (runFullThrottle) {\n        ftIndexThread.safeStop();\n      }\n      \n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      waitForThingsToLevelOut(20, TimeUnit.SECONDS);\n      \n      commit();\n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n       // restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest#test().mjava","sourceNew":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  // commented out on: 24-Dec-2018   @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  @SuppressWarnings({\"try\"})\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n\n    boolean testSuccessful = false;\n    try  (CloudSolrClient ourCloudClient = createCloudClient(DEFAULT_COLLECTION)) {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      if (runFullThrottle) {\n        ftIndexThread = \n            new FullThrottleStoppableIndexingThread(cloudClient.getHttpClient(),controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      \n      if (runFullThrottle) {\n        ftIndexThread.safeStop();\n      }\n      \n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      waitForThingsToLevelOut(20, TimeUnit.SECONDS);\n      \n      commit();\n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n       // restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","sourceOld":"  @Test\n  //05-Jul-2018 @BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 09-Apr-2018\n  // commented out on: 24-Dec-2018   @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  @SuppressWarnings({\"try\"})\n  public void test() throws Exception {\n    // None of the operations used here are particularly costly, so this should work.\n    // Using this low timeout will also help us catch index stalling.\n    clientSoTimeout = 5000;\n\n    boolean testSuccessful = false;\n    try  (CloudSolrClient ourCloudClient = createCloudClient(DEFAULT_COLLECTION)) {\n      handle.clear();\n      handle.put(\"timestamp\", SKIPVAL);\n      ZkStateReader zkStateReader = cloudClient.getZkStateReader();\n      // make sure we have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 10000);\n      }      // make sure we again have leaders for each shard\n      \n      waitForRecoveriesToFinish(false);\n      \n      // we cannot do delete by query\n      // as it's not supported for recovery\n      del(\"*:*\");\n      \n      List<StoppableThread> threads = new ArrayList<>();\n      List<StoppableIndexingThread> indexTreads = new ArrayList<>();\n      int threadCount = TEST_NIGHTLY ? 3 : 1;\n      int i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableIndexingThread indexThread = new StoppableIndexingThread(controlClient, cloudClient, Integer.toString(i), true);\n        threads.add(indexThread);\n        indexTreads.add(indexThread);\n        indexThread.start();\n      }\n      \n      threadCount = 1;\n      i = 0;\n      for (i = 0; i < threadCount; i++) {\n        StoppableSearchThread searchThread = new StoppableSearchThread(cloudClient);\n        threads.add(searchThread);\n        searchThread.start();\n      }\n      \n      if (runFullThrottle) {\n        ftIndexThread = \n            new FullThrottleStoppableIndexingThread(cloudClient.getHttpClient(),controlClient, cloudClient, clients, \"ft1\", true, this.clientSoTimeout);\n        ftIndexThread.start();\n      }\n      \n      chaosMonkey.startTheMonkey(true, 10000);\n      try {\n        long runLength;\n        if (RUN_LENGTH != -1) {\n          runLength = RUN_LENGTH;\n        } else {\n          int[] runTimes;\n          if (TEST_NIGHTLY) {\n            runTimes = new int[] {5000, 6000, 10000, 15000, 25000, 30000,\n                30000, 45000, 90000, 120000};\n          } else {\n            runTimes = new int[] {5000, 7000, 15000};\n          }\n          runLength = runTimes[random().nextInt(runTimes.length - 1)];\n        }\n        \n        Thread.sleep(runLength);\n      } finally {\n        chaosMonkey.stopTheMonkey();\n      }\n\n      // ideally this should go into chaosMonkey\n      restartZk(1000 * (5 + random().nextInt(4)));\n\n      \n      if (runFullThrottle) {\n        ftIndexThread.safeStop();\n      }\n      \n      for (StoppableThread indexThread : threads) {\n        indexThread.safeStop();\n      }\n      \n      // start any downed jetties to be sure we still will end up with a leader per shard...\n      \n      // wait for stop...\n      for (StoppableThread indexThread : threads) {\n        indexThread.join();\n      }\n      \n      // try and wait for any replications and what not to finish...\n      \n      Thread.sleep(2000);\n      \n      // wait until there are no recoveries...\n      waitForThingsToLevelOut();\n      \n      // make sure we again have leaders for each shard\n      for (int j = 1; j < sliceCount; j++) {\n        zkStateReader.getLeaderRetry(DEFAULT_COLLECTION, \"shard\" + j, 30000);\n      }\n      \n      commit();\n      \n      // TODO: assert we didnt kill everyone\n      \n      zkStateReader.updateLiveNodes();\n      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);\n      \n      // we expect full throttle fails, but cloud client should not easily fail\n      for (StoppableThread indexThread : threads) {\n        if (indexThread instanceof StoppableIndexingThread && !(indexThread instanceof FullThrottleStoppableIndexingThread)) {\n          int failCount = ((StoppableIndexingThread) indexThread).getFailCount();\n          assertFalse(\"There were too many update fails (\" + failCount + \" > \" + FAIL_TOLERANCE\n              + \") - we expect it can happen, but shouldn't easily\", failCount > FAIL_TOLERANCE);\n        }\n      }\n      \n      \n      waitForThingsToLevelOut(20, TimeUnit.SECONDS);\n      \n      commit();\n      \n      Set<String> addFails = getAddFails(indexTreads);\n      Set<String> deleteFails = getDeleteFails(indexTreads);\n      // full throttle thread can\n      // have request fails \n      checkShardConsistency(!runFullThrottle, true, addFails, deleteFails);\n      \n      long ctrlDocs = controlClient.query(new SolrQuery(\"*:*\")).getResults()\n      .getNumFound(); \n      \n      // ensure we have added more than 0 docs\n      long cloudClientDocs = cloudClient.query(new SolrQuery(\"*:*\"))\n          .getResults().getNumFound();\n      \n      assertTrue(\"Found \" + ctrlDocs + \" control docs\", cloudClientDocs > 0);\n      \n      if (VERBOSE) System.out.println(\"control docs:\"\n          + controlClient.query(new SolrQuery(\"*:*\")).getResults()\n              .getNumFound() + \"\\n\\n\");\n      \n      // try and make a collection to make sure the overseer has survived the expiration and session loss\n\n      // sometimes we restart zookeeper as well\n      if (random().nextBoolean()) {\n       // restartZk(1000 * (5 + random().nextInt(4)));\n      }\n\n      try (CloudSolrClient client = createCloudClient(\"collection1\", 30000)) {\n          createCollection(null, \"testcollection\",\n              1, 1, 1, client, null, \"conf1\");\n\n      }\n      List<Integer> numShardsNumReplicas = new ArrayList<>(2);\n      numShardsNumReplicas.add(1);\n      numShardsNumReplicas.add(1);\n      checkForCollection(\"testcollection\", numShardsNumReplicas, null);\n      \n      testSuccessful = true;\n    } finally {\n      if (!testSuccessful) {\n        printLayout();\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108":["f607a0a2e930f55385c7a24afb68ef661ef7e3ee"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["05a3c9b5f1dfb39879069eb1dac3ca104d3e4108"],"abb23fcc2461782ab204e61213240feb77d355aa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3c9595c75582a7ea7efb585014102ed83f2d9c8b":["d2f514ff99d806c2911cbc0e81d02d3d756e0ae6"],"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f":["102da6baafc0f534a59f31729343dbab9d3b9e9a"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["aa2585c33d5d66a1c837c312221eb55ddb3c4300"],"8d35c84fdef07284c122012ca4000d3b7285a66e":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"43d1e498704edd2bba13548a189eed4dfccff11b":["61c45e99cf6676da48f19d7511c73712ad39402b"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["859081acf00749f5dd462772c571d611d4a4d2db","61c45e99cf6676da48f19d7511c73712ad39402b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["cc3b13b430571c2e169f98fe38e1e7666f88522d","98287baa2c8d136e801f366a73e27a23285b7b98"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["6b87d1f8719d7f05be003f3477450b74af13706a","f607a0a2e930f55385c7a24afb68ef661ef7e3ee"],"aa2585c33d5d66a1c837c312221eb55ddb3c4300":["add1e7dd742ea533ff4318cea83ca0a1f669f662"],"2ea161f828a3a7a6eb9410a431aecda6d7ab1065":["e9017cf144952056066919f1ebc7897ff9bd71b1","43d1e498704edd2bba13548a189eed4dfccff11b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"add1e7dd742ea533ff4318cea83ca0a1f669f662":["8d35c84fdef07284c122012ca4000d3b7285a66e"],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["abb23fcc2461782ab204e61213240feb77d355aa"],"f607a0a2e930f55385c7a24afb68ef661ef7e3ee":["6b87d1f8719d7f05be003f3477450b74af13706a"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["6b87d1f8719d7f05be003f3477450b74af13706a","f607a0a2e930f55385c7a24afb68ef661ef7e3ee"],"61c45e99cf6676da48f19d7511c73712ad39402b":["859081acf00749f5dd462772c571d611d4a4d2db"],"102da6baafc0f534a59f31729343dbab9d3b9e9a":["98287baa2c8d136e801f366a73e27a23285b7b98"],"98287baa2c8d136e801f366a73e27a23285b7b98":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"859081acf00749f5dd462772c571d611d4a4d2db":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f"],"d2f514ff99d806c2911cbc0e81d02d3d756e0ae6":["43d1e498704edd2bba13548a189eed4dfccff11b"],"6b87d1f8719d7f05be003f3477450b74af13706a":["d2f514ff99d806c2911cbc0e81d02d3d756e0ae6","3c9595c75582a7ea7efb585014102ed83f2d9c8b"],"e73d8d559120669b47658108d818b637df5456ea":["2ea161f828a3a7a6eb9410a431aecda6d7ab1065","d2f514ff99d806c2911cbc0e81d02d3d756e0ae6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"]},"commit2Childs":{"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["8d35c84fdef07284c122012ca4000d3b7285a66e"],"abb23fcc2461782ab204e61213240feb77d355aa":["cc3b13b430571c2e169f98fe38e1e7666f88522d"],"3c9595c75582a7ea7efb585014102ed83f2d9c8b":["6b87d1f8719d7f05be003f3477450b74af13706a"],"7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f":["859081acf00749f5dd462772c571d611d4a4d2db"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"8d35c84fdef07284c122012ca4000d3b7285a66e":["add1e7dd742ea533ff4318cea83ca0a1f669f662"],"43d1e498704edd2bba13548a189eed4dfccff11b":["2ea161f828a3a7a6eb9410a431aecda6d7ab1065","d2f514ff99d806c2911cbc0e81d02d3d756e0ae6"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["2ea161f828a3a7a6eb9410a431aecda6d7ab1065"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"aa2585c33d5d66a1c837c312221eb55ddb3c4300":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"2ea161f828a3a7a6eb9410a431aecda6d7ab1065":["e73d8d559120669b47658108d818b637df5456ea"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["abb23fcc2461782ab204e61213240feb77d355aa"],"add1e7dd742ea533ff4318cea83ca0a1f669f662":["aa2585c33d5d66a1c837c312221eb55ddb3c4300"],"cc3b13b430571c2e169f98fe38e1e7666f88522d":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","98287baa2c8d136e801f366a73e27a23285b7b98"],"f607a0a2e930f55385c7a24afb68ef661ef7e3ee":["05a3c9b5f1dfb39879069eb1dac3ca104d3e4108","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[],"61c45e99cf6676da48f19d7511c73712ad39402b":["43d1e498704edd2bba13548a189eed4dfccff11b","e9017cf144952056066919f1ebc7897ff9bd71b1"],"102da6baafc0f534a59f31729343dbab9d3b9e9a":["7c3f4fed97dabfe4bcddc3566fd190a7c909bc4f"],"98287baa2c8d136e801f366a73e27a23285b7b98":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","102da6baafc0f534a59f31729343dbab9d3b9e9a"],"859081acf00749f5dd462772c571d611d4a4d2db":["e9017cf144952056066919f1ebc7897ff9bd71b1","61c45e99cf6676da48f19d7511c73712ad39402b"],"d2f514ff99d806c2911cbc0e81d02d3d756e0ae6":["3c9595c75582a7ea7efb585014102ed83f2d9c8b","6b87d1f8719d7f05be003f3477450b74af13706a","e73d8d559120669b47658108d818b637df5456ea"],"6b87d1f8719d7f05be003f3477450b74af13706a":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","f607a0a2e930f55385c7a24afb68ef661ef7e3ee","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"e73d8d559120669b47658108d818b637df5456ea":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","e73d8d559120669b47658108d818b637df5456ea","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}