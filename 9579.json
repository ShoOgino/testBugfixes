{"path":"sandbox/contributions/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestDocFragments(TokenStream,String,StringBuffer,int).mjava","commits":[{"id":"db2e1c87dfa9ca908febe5b39f6dd3dee2fbe9e2","date":1081470871,"type":0,"author":"Erik Hatcher","isMerge":false,"pathNew":"sandbox/contributions/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestDocFragments(TokenStream,String,StringBuffer,int).mjava","pathOld":"/dev/null","sourceNew":"\t/**\n\t * Low level api to get the most relevant sections of the document\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @return \n\t * @throws IOException\n\t */\n\tprivate final TextFragment[] getBestDocFragments(\n\t\tTokenStream tokenStream,\t\n\t\tString text,\n\t\tStringBuffer newText,\n\t\tint maxNumFragments)\n\t\tthrows IOException\n\t{\n\t\tArrayList docFrags = new ArrayList();\n\n\t\tTextFragment currentFrag =\tnew TextFragment(newText.length(), docFrags.size());\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\t\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text);\n\n\t\t\twhile ((token = tokenStream.next()) != null)\n\t\t\t{\n\t\t\t\t\n\t\t\t\tstartOffset = token.startOffset();\n\t\t\t\tendOffset = token.endOffset();\t\t\n\t\t\t\t//FIXME an issue was reported with CJKTokenizer that I couldnt reproduce\n\t\t\t\t// where the analyzer was producing overlapping tokens.\n\t\t\t\t// I suspect the fix is to make startOffset=Math.max(startOffset,lastEndOffset+1)\n\t\t\t\t// but cant be sure so I'll just leave this comment in for now\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\n\n\t\t\t\t// append text between end of last token (or beginning of text) and start of current token\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(text.substring(lastEndOffset, startOffset));\n\n\t\t\t\t// does query contain current token?\n\t\t\t\tfloat score=fragmentScorer.getTokenScore(token);\t\t\t\n\t\t\t\tnewText.append(formatter.highlightTerm(tokenText, token.termText(), score, startOffset));\n\t\t\t\t\n\n\t\t\t\tif(textFragmenter.isNewFragment(token))\n\t\t\t\t{\n\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\tcurrentFrag =new TextFragment(newText.length(), docFrags.size());\n\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t}\n\n\t\t\t\tlastEndOffset = endOffset;\n\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\n\n\t\t\t// append text after end of last token\n\t\t\tif (lastEndOffset < text.length())\n\t\t\t\tnewText.append(text.substring(lastEndOffset));\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tint minScore = 0;\n\t\t\tfor (Iterator i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = (TextFragment) i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\t\t\t\t\t\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insert(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = (TextFragment) fragQueue.pop();\n\t\t\t}\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":null,"bugFix":null,"bugIntro":["9532290bd66e70c1787e80aae9fb0427ce080194"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4836b0ad75558e4c39cb1c6ca188c153a48f8e98","date":1090874387,"type":3,"author":"Mark Harwood","isMerge":false,"pathNew":"sandbox/contributions/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestDocFragments(TokenStream,String,StringBuffer,int).mjava","pathOld":"sandbox/contributions/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestDocFragments(TokenStream,String,StringBuffer,int).mjava","sourceNew":"\t/**\n\t * Low level api to get the most relevant sections of the document\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @return \n\t * @throws IOException\n\t */\n\tprivate final TextFragment[] getBestDocFragments(\n\t\tTokenStream tokenStream,\t\n\t\tString text,\n\t\tStringBuffer newText,\n\t\tint maxNumFragments)\n\t\tthrows IOException\n\t{\n\t\tArrayList docFrags = new ArrayList();\n\n\t\tTextFragment currentFrag =\tnew TextFragment(newText.length(), docFrags.size());\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\t\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text);\n\t\t\n\t\t\tTokenGroup tokenGroup=new TokenGroup();\n\n\t\t\twhile ((token = tokenStream.next()) != null)\n\t\t\t{\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct(token)))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens - \n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.startOffset;\n\t\t\t\t\tendOffset = tokenGroup.endOffset;\t\t\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(tokenText, tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(text.substring(lastEndOffset, startOffset));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=endOffset;\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\t\t\t\t\t\t\n\t\t\t\t\tif(textFragmenter.isNewFragment(token))\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t\t\t\n\t\t\t\ttokenGroup.addToken(token,fragmentScorer.getTokenScore(token));\n\t\t\t\t\n\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.startOffset;\n\t\t\t\tendOffset = tokenGroup.endOffset;\t\t\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(tokenText, tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(text.substring(lastEndOffset, startOffset));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=endOffset;\t\t\t\t\t\t\n\t\t\t}\n\n\t\t\t// append text after end of last token\n\t\t\tif (lastEndOffset < text.length())\n\t\t\t\tnewText.append(text.substring(lastEndOffset));\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tint minScore = 0;\n\t\t\tfor (Iterator i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = (TextFragment) i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\t\t\t\t\t\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insert(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = (TextFragment) fragQueue.pop();\n\t\t\t}\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant sections of the document\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @return \n\t * @throws IOException\n\t */\n\tprivate final TextFragment[] getBestDocFragments(\n\t\tTokenStream tokenStream,\t\n\t\tString text,\n\t\tStringBuffer newText,\n\t\tint maxNumFragments)\n\t\tthrows IOException\n\t{\n\t\tArrayList docFrags = new ArrayList();\n\n\t\tTextFragment currentFrag =\tnew TextFragment(newText.length(), docFrags.size());\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\t\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text);\n\n\t\t\twhile ((token = tokenStream.next()) != null)\n\t\t\t{\n\t\t\t\t\n\t\t\t\tstartOffset = token.startOffset();\n\t\t\t\tendOffset = token.endOffset();\t\t\n\t\t\t\t//FIXME an issue was reported with CJKTokenizer that I couldnt reproduce\n\t\t\t\t// where the analyzer was producing overlapping tokens.\n\t\t\t\t// I suspect the fix is to make startOffset=Math.max(startOffset,lastEndOffset+1)\n\t\t\t\t// but cant be sure so I'll just leave this comment in for now\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\n\n\t\t\t\t// append text between end of last token (or beginning of text) and start of current token\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(text.substring(lastEndOffset, startOffset));\n\n\t\t\t\t// does query contain current token?\n\t\t\t\tfloat score=fragmentScorer.getTokenScore(token);\t\t\t\n\t\t\t\tnewText.append(formatter.highlightTerm(tokenText, token.termText(), score, startOffset));\n\t\t\t\t\n\n\t\t\t\tif(textFragmenter.isNewFragment(token))\n\t\t\t\t{\n\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\tcurrentFrag =new TextFragment(newText.length(), docFrags.size());\n\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t}\n\n\t\t\t\tlastEndOffset = endOffset;\n\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\n\n\t\t\t// append text after end of last token\n\t\t\tif (lastEndOffset < text.length())\n\t\t\t\tnewText.append(text.substring(lastEndOffset));\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tint minScore = 0;\n\t\t\tfor (Iterator i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = (TextFragment) i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\t\t\t\t\t\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insert(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = (TextFragment) fragQueue.pop();\n\t\t\t}\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"071ff465fbf5c9c916ccaaf73bb8f2112a26de61","date":1092348577,"type":5,"author":"Mark Harwood","isMerge":false,"pathNew":"sandbox/contributions/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"sandbox/contributions/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestDocFragments(TokenStream,String,StringBuffer,int).mjava","sourceNew":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.  \n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @return \n\t * @throws IOException\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\t\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException\n\t{\n\t\tArrayList docFrags = new ArrayList();\n\t\tStringBuffer newText=new StringBuffer();\n\n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\t\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text);\n\t\t\n\t\t\tTokenGroup tokenGroup=new TokenGroup();\n\n\t\t\twhile ((token = tokenStream.next()) != null)\n\t\t\t{\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct(token)))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens - \n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.startOffset;\n\t\t\t\t\tendOffset = tokenGroup.endOffset;\t\t\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(tokenText, tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(text.substring(lastEndOffset, startOffset));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=endOffset;\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\t\t\t\t\t\t\n\t\t\t\t\tif(textFragmenter.isNewFragment(token))\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t\t\t\n\t\t\t\ttokenGroup.addToken(token,fragmentScorer.getTokenScore(token));\n\t\t\t\t\n\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.startOffset;\n\t\t\t\tendOffset = tokenGroup.endOffset;\t\t\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(tokenText, tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(text.substring(lastEndOffset, startOffset));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=endOffset;\t\t\t\t\t\t\n\t\t\t}\n\n\t\t\t// append text after end of last token\n\t\t\tif (lastEndOffset < text.length())\n\t\t\t\tnewText.append(text.substring(lastEndOffset));\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tint minScore = 0;\n\t\t\tfor (Iterator i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = (TextFragment) i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\t\t\t\t\t\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insert(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = (TextFragment) fragQueue.pop();\n\t\t\t}\n\t\t\t\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList fragTexts = new ArrayList();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= (TextFragment[]) fragTexts.toArray(new TextFragment[0]);\t\t\t\t\n\t\t\t}\n\t\t\t\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant sections of the document\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @return \n\t * @throws IOException\n\t */\n\tprivate final TextFragment[] getBestDocFragments(\n\t\tTokenStream tokenStream,\t\n\t\tString text,\n\t\tStringBuffer newText,\n\t\tint maxNumFragments)\n\t\tthrows IOException\n\t{\n\t\tArrayList docFrags = new ArrayList();\n\n\t\tTextFragment currentFrag =\tnew TextFragment(newText.length(), docFrags.size());\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\t\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\t\t\torg.apache.lucene.analysis.Token token;\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text);\n\t\t\n\t\t\tTokenGroup tokenGroup=new TokenGroup();\n\n\t\t\twhile ((token = tokenStream.next()) != null)\n\t\t\t{\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct(token)))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens - \n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.startOffset;\n\t\t\t\t\tendOffset = tokenGroup.endOffset;\t\t\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(tokenText, tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(text.substring(lastEndOffset, startOffset));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=endOffset;\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\t\t\t\t\t\t\n\t\t\t\t\tif(textFragmenter.isNewFragment(token))\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t\t\t\n\t\t\t\ttokenGroup.addToken(token,fragmentScorer.getTokenScore(token));\n\t\t\t\t\n\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n\t\t\t\t{\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.startOffset;\n\t\t\t\tendOffset = tokenGroup.endOffset;\t\t\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(tokenText, tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(text.substring(lastEndOffset, startOffset));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=endOffset;\t\t\t\t\t\t\n\t\t\t}\n\n\t\t\t// append text after end of last token\n\t\t\tif (lastEndOffset < text.length())\n\t\t\t\tnewText.append(text.substring(lastEndOffset));\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tint minScore = 0;\n\t\t\tfor (Iterator i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = (TextFragment) i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\t\t\t\t\t\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insert(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = (TextFragment) fragQueue.pop();\n\t\t\t}\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"071ff465fbf5c9c916ccaaf73bb8f2112a26de61":["4836b0ad75558e4c39cb1c6ca188c153a48f8e98"],"4836b0ad75558e4c39cb1c6ca188c153a48f8e98":["db2e1c87dfa9ca908febe5b39f6dd3dee2fbe9e2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["071ff465fbf5c9c916ccaaf73bb8f2112a26de61"],"db2e1c87dfa9ca908febe5b39f6dd3dee2fbe9e2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["db2e1c87dfa9ca908febe5b39f6dd3dee2fbe9e2"],"071ff465fbf5c9c916ccaaf73bb8f2112a26de61":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4836b0ad75558e4c39cb1c6ca188c153a48f8e98":["071ff465fbf5c9c916ccaaf73bb8f2112a26de61"],"db2e1c87dfa9ca908febe5b39f6dd3dee2fbe9e2":["4836b0ad75558e4c39cb1c6ca188c153a48f8e98"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}