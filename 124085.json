{"path":"sandbox/contributions/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","commits":[{"id":"d5a0b529d2a1f873f1f11db833a891b53909a7bc","date":1104492147,"type":0,"author":"Mark Harwood","isMerge":false,"pathNew":"sandbox/contributions/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"/dev/null","sourceNew":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous)\n    {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream\n        {\n            Token tokens[];\n            int currentToken=0;\n            StoredTokenStream(Token tokens[])\n            {\n                this.tokens=tokens;\n            }\n            public Token next() throws IOException\n            {\n                if(currentToken>=tokens.length)\n                {\n                    return null;\n                }\n                return tokens[currentToken++];\n            }            \n        }        \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                    unsortedTokens.add(new Token(terms[t],\n                        offsets[tp].getStartOffset(),\n                        offsets[tp].getEndOffset()));\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                    tokensInOriginalOrder[pos[tp]]=new Token(terms[t],\n                            offsets[tp].getStartOffset(),\n                            offsets[tp].getEndOffset());\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.startOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"043c298cb215f13ba7b9b81d20760704e8f93d66","date":1107566743,"type":5,"author":"Erik Hatcher","isMerge":false,"pathNew":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","pathOld":"sandbox/contributions/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources#getTokenStream(TermPositionVector,boolean).mjava","sourceNew":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous)\n    {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream\n        {\n            Token tokens[];\n            int currentToken=0;\n            StoredTokenStream(Token tokens[])\n            {\n                this.tokens=tokens;\n            }\n            public Token next() throws IOException\n            {\n                if(currentToken>=tokens.length)\n                {\n                    return null;\n                }\n                return tokens[currentToken++];\n            }            \n        }        \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                    unsortedTokens.add(new Token(terms[t],\n                        offsets[tp].getStartOffset(),\n                        offsets[tp].getEndOffset()));\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                    tokensInOriginalOrder[pos[tp]]=new Token(terms[t],\n                            offsets[tp].getStartOffset(),\n                            offsets[tp].getEndOffset());\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.startOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","sourceOld":"    /**\n     * Low level api.\n     * Returns a token stream or null if no offset info available in index.\n     * This can be used to feed the highlighter with a pre-parsed token stream \n     * \n     * In my tests the speeds to recreate 1000 token streams using this method are:\n     * - with TermVector offset only data stored - 420  milliseconds \n     * - with TermVector offset AND position data stored - 271 milliseconds\n     *  (nb timings for TermVector with position data are based on a tokenizer with contiguous\n     *  positions - no overlaps or gaps)\n     * The cost of not using TermPositionVector to store\n     * pre-parsed content and using an analyzer to re-parse the original content: \n     * - reanalyzing the original content - 980 milliseconds\n     * \n     * The re-analyze timings will typically vary depending on -\n     * \t1) The complexity of the analyzer code (timings above were using a \n     * \t   stemmer/lowercaser/stopword combo)\n     *  2) The  number of other fields (Lucene reads ALL fields off the disk \n     *     when accessing just one document field - can cost dear!)\n     *  3) Use of compression on field storage - could be faster cos of compression (less disk IO)\n     *     or slower (more CPU burn) depending on the content.\n     *\n     * @param tpv\n     * @param tokenPositionsGuaranteedContiguous true if the token position numbers have no overlaps or gaps. If looking\n     * to eek out the last drops of performance, set to true. If in doubt, set to false.\n     */\n    public static TokenStream getTokenStream(TermPositionVector tpv, boolean tokenPositionsGuaranteedContiguous)\n    {\n        //an object used to iterate across an array of tokens\n        class StoredTokenStream extends TokenStream\n        {\n            Token tokens[];\n            int currentToken=0;\n            StoredTokenStream(Token tokens[])\n            {\n                this.tokens=tokens;\n            }\n            public Token next() throws IOException\n            {\n                if(currentToken>=tokens.length)\n                {\n                    return null;\n                }\n                return tokens[currentToken++];\n            }            \n        }        \n        //code to reconstruct the original sequence of Tokens\n        String[] terms=tpv.getTerms();          \n        int[] freq=tpv.getTermFrequencies();\n        int totalTokens=0;\n        for (int t = 0; t < freq.length; t++)\n        {\n            totalTokens+=freq[t];\n        }\n        Token tokensInOriginalOrder[]=new Token[totalTokens];\n        ArrayList unsortedTokens = null;\n        for (int t = 0; t < freq.length; t++)\n        {\n            TermVectorOffsetInfo[] offsets=tpv.getOffsets(t);\n            if(offsets==null)\n            {\n                return null;\n            }\n            \n            int[] pos=null;\n            if(tokenPositionsGuaranteedContiguous)\n            {\n                //try get the token position info to speed up assembly of tokens into sorted sequence\n                pos=tpv.getTermPositions(t);\n            }\n            if(pos==null)\n            {\t\n                //tokens NOT stored with positions or not guaranteed contiguous - must add to list and sort later\n                if(unsortedTokens==null)\n                {\n                    unsortedTokens=new ArrayList();\n                }\n                for (int tp = 0; tp < offsets.length; tp++)\n                {\n                    unsortedTokens.add(new Token(terms[t],\n                        offsets[tp].getStartOffset(),\n                        offsets[tp].getEndOffset()));\n                }\n            }\n            else\n            {\n                //We have positions stored and a guarantee that the token position information is contiguous\n                \n                // This may be fast BUT wont work if Tokenizers used which create >1 token in same position or\n                // creates jumps in position numbers - this code would fail under those circumstances\n                \n                //tokens stored with positions - can use this to index straight into sorted array\n                for (int tp = 0; tp < pos.length; tp++)\n                {\n                    tokensInOriginalOrder[pos[tp]]=new Token(terms[t],\n                            offsets[tp].getStartOffset(),\n                            offsets[tp].getEndOffset());\n                }                \n            }\n        }\n        //If the field has been stored without position data we must perform a sort        \n        if(unsortedTokens!=null)\n        {\n            tokensInOriginalOrder=(Token[]) unsortedTokens.toArray(new Token[unsortedTokens.size()]);\n            Arrays.sort(tokensInOriginalOrder, new Comparator(){\n                public int compare(Object o1, Object o2)\n                {\n                    Token t1=(Token) o1;\n                    Token t2=(Token) o2;\n                    if(t1.startOffset()>t2.startOffset())\n                        return 1;\n                    if(t1.startOffset()<t2.startOffset())\n                        return -1;\n                    return 0;\n                }});\n        }\n        return new StoredTokenStream(tokensInOriginalOrder);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"043c298cb215f13ba7b9b81d20760704e8f93d66":["d5a0b529d2a1f873f1f11db833a891b53909a7bc"],"d5a0b529d2a1f873f1f11db833a891b53909a7bc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["043c298cb215f13ba7b9b81d20760704e8f93d66"]},"commit2Childs":{"043c298cb215f13ba7b9b81d20760704e8f93d66":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d5a0b529d2a1f873f1f11db833a891b53909a7bc":["043c298cb215f13ba7b9b81d20760704e8f93d66"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d5a0b529d2a1f873f1f11db833a891b53909a7bc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}