{"path":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","commits":[{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndLiveDocs,boolean).mjava","sourceNew":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      // Pool still holds a ref:\n      assert rld.refCount() >= 1;\n\n      if (!poolReaders && rld.refCount() == 1) {\n        // This is the last ref to this RLD, and we're not\n        // pooling, so remove it:\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] ReaderPool.release: \" + rld.info);\n        if (rld.writeLiveDocs(directory)) {\n          // Make sure we only write del docs for a live segment:\n          assert assertInfoLive == false || infoIsLive(rld.info);\n          // Must checkpoint because we just\n          // created new _X_N.del and field updates files;\n          // don't call IW.checkpoint because that also\n          // increments SIS.version, which we do not want to\n          // do here: it was done previously (after we\n          // invoked BDS.applyDeletes), whereas here all we\n          // did was move the state to disk:\n          checkpointNoSIS();\n        }\n        //System.out.println(\"IW: done writeLiveDocs for info=\" + rld.info);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] ReaderPool.release: drop readers \" + rld.info);\n        rld.dropReaders();\n        readerMap.remove(rld.info);\n      }\n    }\n\n","sourceOld":"    public synchronized void release(ReadersAndLiveDocs rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      // Pool still holds a ref:\n      assert rld.refCount() >= 1;\n\n      if (!poolReaders && rld.refCount() == 1) {\n        // This is the last ref to this RLD, and we're not\n        // pooling, so remove it:\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] ReaderPool.release: \" + rld.info);\n        if (rld.writeLiveDocs(directory)) {\n          // Make sure we only write del docs for a live segment:\n          assert assertInfoLive == false || infoIsLive(rld.info);\n          // Must checkpoint because we just\n          // created new _X_N.del and field updates files;\n          // don't call IW.checkpoint because that also\n          // increments SIS.version, which we do not want to\n          // do here: it was done previously (after we\n          // invoked BDS.applyDeletes), whereas here all we\n          // did was move the state to disk:\n          checkpointNoSIS();\n        }\n        //System.out.println(\"IW: done writeLiveDocs for info=\" + rld.info);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] ReaderPool.release: drop readers \" + rld.info);\n        rld.dropReaders();\n        readerMap.remove(rld.info);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","sourceNew":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      if (rld.refCount() == 0) {\n        // This happens if the segment was just merged away, while a buffered deletes packet was still applying deletes/updates to it.\n        assert readerMap.containsKey(rld.info) == false: \"seg=\" + rld.info + \" has refCount 0 but still unexpectedly exists in the reader pool\";\n      } else {\n\n        // Pool still holds a ref:\n        assert rld.refCount() > 0: \"refCount=\" + rld.refCount() + \" reader=\" + rld.info;\n\n        if (!poolReaders && rld.refCount() == 1 && readerMap.containsKey(rld.info)) {\n          // This is the last ref to this RLD, and we're not\n          // pooling, so remove it:\n          if (rld.writeLiveDocs(directory)) {\n            // Make sure we only write del docs for a live segment:\n            assert assertInfoLive == false || assertInfoIsLive(rld.info);\n            // Must checkpoint because we just\n            // created new _X_N.del and field updates files;\n            // don't call IW.checkpoint because that also\n            // increments SIS.version, which we do not want to\n            // do here: it was done previously (after we\n            // invoked BDS.applyDeletes), whereas here all we\n            // did was move the state to disk:\n            checkpointNoSIS();\n          }\n\n          rld.writeFieldUpdates(directory, bufferedUpdatesStream.getCompletedDelGen(), infoStream);\n\n          if (rld.getNumDVUpdates() == 0) {\n            rld.dropReaders();\n            readerMap.remove(rld.info);\n          } else {\n            // We are forced to pool this segment until its deletes fully apply (no delGen gaps)\n          }\n        }\n      }\n    }\n\n","sourceOld":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      // Pool still holds a ref:\n      assert rld.refCount() >= 1;\n\n      if (!poolReaders && rld.refCount() == 1) {\n        // This is the last ref to this RLD, and we're not\n        // pooling, so remove it:\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] ReaderPool.release: \" + rld.info);\n        if (rld.writeLiveDocs(directory)) {\n          // Make sure we only write del docs for a live segment:\n          assert assertInfoLive == false || infoIsLive(rld.info);\n          // Must checkpoint because we just\n          // created new _X_N.del and field updates files;\n          // don't call IW.checkpoint because that also\n          // increments SIS.version, which we do not want to\n          // do here: it was done previously (after we\n          // invoked BDS.applyDeletes), whereas here all we\n          // did was move the state to disk:\n          checkpointNoSIS();\n        }\n        //System.out.println(\"IW: done writeLiveDocs for info=\" + rld.info);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] ReaderPool.release: drop readers \" + rld.info);\n        rld.dropReaders();\n        readerMap.remove(rld.info);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","sourceNew":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      if (rld.refCount() == 0) {\n        // This happens if the segment was just merged away, while a buffered deletes packet was still applying deletes/updates to it.\n        assert readerMap.containsKey(rld.info) == false: \"seg=\" + rld.info + \" has refCount 0 but still unexpectedly exists in the reader pool\";\n      } else {\n\n        // Pool still holds a ref:\n        assert rld.refCount() > 0: \"refCount=\" + rld.refCount() + \" reader=\" + rld.info;\n\n        if (!poolReaders && rld.refCount() == 1 && readerMap.containsKey(rld.info)) {\n          // This is the last ref to this RLD, and we're not\n          // pooling, so remove it:\n          if (rld.writeLiveDocs(directory)) {\n            // Make sure we only write del docs for a live segment:\n            assert assertInfoLive == false || assertInfoIsLive(rld.info);\n            // Must checkpoint because we just\n            // created new _X_N.del and field updates files;\n            // don't call IW.checkpoint because that also\n            // increments SIS.version, which we do not want to\n            // do here: it was done previously (after we\n            // invoked BDS.applyDeletes), whereas here all we\n            // did was move the state to disk:\n            checkpointNoSIS();\n          }\n\n          rld.writeFieldUpdates(directory, bufferedUpdatesStream.getCompletedDelGen(), infoStream);\n\n          if (rld.getNumDVUpdates() == 0) {\n            rld.dropReaders();\n            readerMap.remove(rld.info);\n          } else {\n            // We are forced to pool this segment until its deletes fully apply (no delGen gaps)\n          }\n        }\n      }\n    }\n\n","sourceOld":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      // Pool still holds a ref:\n      assert rld.refCount() >= 1;\n\n      if (!poolReaders && rld.refCount() == 1) {\n        // This is the last ref to this RLD, and we're not\n        // pooling, so remove it:\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] ReaderPool.release: \" + rld.info);\n        if (rld.writeLiveDocs(directory)) {\n          // Make sure we only write del docs for a live segment:\n          assert assertInfoLive == false || infoIsLive(rld.info);\n          // Must checkpoint because we just\n          // created new _X_N.del and field updates files;\n          // don't call IW.checkpoint because that also\n          // increments SIS.version, which we do not want to\n          // do here: it was done previously (after we\n          // invoked BDS.applyDeletes), whereas here all we\n          // did was move the state to disk:\n          checkpointNoSIS();\n        }\n        //System.out.println(\"IW: done writeLiveDocs for info=\" + rld.info);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] ReaderPool.release: drop readers \" + rld.info);\n        rld.dropReaders();\n        readerMap.remove(rld.info);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","sourceNew":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      if (rld.refCount() == 0) {\n        // This happens if the segment was just merged away, while a buffered deletes packet was still applying deletes/updates to it.\n        assert readerMap.containsKey(rld.info) == false: \"seg=\" + rld.info + \" has refCount 0 but still unexpectedly exists in the reader pool\";\n      } else {\n\n        // Pool still holds a ref:\n        assert rld.refCount() > 0: \"refCount=\" + rld.refCount() + \" reader=\" + rld.info;\n\n        if (!poolReaders && rld.refCount() == 1 && readerMap.containsKey(rld.info)) {\n          // This is the last ref to this RLD, and we're not\n          // pooling, so remove it:\n          if (rld.writeLiveDocs(directory)) {\n            // Make sure we only write del docs for a live segment:\n            assert assertInfoLive == false || assertInfoIsLive(rld.info);\n            // Must checkpoint because we just\n            // created new _X_N.del and field updates files;\n            // don't call IW.checkpoint because that also\n            // increments SIS.version, which we do not want to\n            // do here: it was done previously (after we\n            // invoked BDS.applyDeletes), whereas here all we\n            // did was move the state to disk:\n            checkpointNoSIS();\n          }\n\n          rld.writeFieldUpdates(directory, bufferedUpdatesStream.getCompletedDelGen(), infoStream);\n\n          if (rld.getNumDVUpdates() == 0) {\n            rld.dropReaders();\n            readerMap.remove(rld.info);\n          } else {\n            // We are forced to pool this segment until its deletes fully apply (no delGen gaps)\n          }\n        }\n      }\n    }\n\n","sourceOld":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      // Pool still holds a ref:\n      assert rld.refCount() >= 1;\n\n      if (!poolReaders && rld.refCount() == 1) {\n        // This is the last ref to this RLD, and we're not\n        // pooling, so remove it:\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] ReaderPool.release: \" + rld.info);\n        if (rld.writeLiveDocs(directory)) {\n          // Make sure we only write del docs for a live segment:\n          assert assertInfoLive == false || infoIsLive(rld.info);\n          // Must checkpoint because we just\n          // created new _X_N.del and field updates files;\n          // don't call IW.checkpoint because that also\n          // increments SIS.version, which we do not want to\n          // do here: it was done previously (after we\n          // invoked BDS.applyDeletes), whereas here all we\n          // did was move the state to disk:\n          checkpointNoSIS();\n        }\n        //System.out.println(\"IW: done writeLiveDocs for info=\" + rld.info);\n\n//        System.out.println(\"[\" + Thread.currentThread().getName() + \"] ReaderPool.release: drop readers \" + rld.info);\n        rld.dropReaders();\n        readerMap.remove(rld.info);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d60c1bb96a28a26d197c36299f7b6c9c5da617a1","date":1522484702,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","sourceNew":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      if (rld.refCount() == 0) {\n        // This happens if the segment was just merged away, while a buffered deletes packet was still applying deletes/updates to it.\n        assert readerMap.containsKey(rld.info) == false: \"seg=\" + rld.info + \" has refCount 0 but still unexpectedly exists in the reader pool\";\n      } else {\n\n        // Pool still holds a ref:\n        assert rld.refCount() > 0: \"refCount=\" + rld.refCount() + \" reader=\" + rld.info;\n\n        if (!poolReaders && rld.refCount() == 1 && readerMap.containsKey(rld.info)) {\n          // This is the last ref to this RLD, and we're not\n          // pooling, so remove it:\n          boolean changed = rld.writeLiveDocs(directory);\n          changed |= rld.writeFieldUpdates(directory, globalFieldNumberMap, bufferedUpdatesStream.getCompletedDelGen(), infoStream);\n\n          if (changed) {\n            // Make sure we only write del docs for a live segment:\n            assert assertInfoLive == false || assertInfoIsLive(rld.info);\n            // Must checkpoint because we just\n            // created new _X_N.del and field updates files;\n            // don't call IW.checkpoint because that also\n            // increments SIS.version, which we do not want to\n            // do here: it was done previously (after we\n            // invoked BDS.applyDeletes), whereas here all we\n            // did was move the state to disk:\n            checkpointNoSIS();\n          }\n          if (rld.getNumDVUpdates() == 0) {\n            rld.dropReaders();\n            readerMap.remove(rld.info);\n          } else {\n            // We are forced to pool this segment until its deletes fully apply (no delGen gaps)\n          }\n        }\n      }\n    }\n\n","sourceOld":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      if (rld.refCount() == 0) {\n        // This happens if the segment was just merged away, while a buffered deletes packet was still applying deletes/updates to it.\n        assert readerMap.containsKey(rld.info) == false: \"seg=\" + rld.info + \" has refCount 0 but still unexpectedly exists in the reader pool\";\n      } else {\n\n        // Pool still holds a ref:\n        assert rld.refCount() > 0: \"refCount=\" + rld.refCount() + \" reader=\" + rld.info;\n\n        if (!poolReaders && rld.refCount() == 1 && readerMap.containsKey(rld.info)) {\n          // This is the last ref to this RLD, and we're not\n          // pooling, so remove it:\n          if (rld.writeLiveDocs(directory)) {\n            // Make sure we only write del docs for a live segment:\n            assert assertInfoLive == false || assertInfoIsLive(rld.info);\n            // Must checkpoint because we just\n            // created new _X_N.del and field updates files;\n            // don't call IW.checkpoint because that also\n            // increments SIS.version, which we do not want to\n            // do here: it was done previously (after we\n            // invoked BDS.applyDeletes), whereas here all we\n            // did was move the state to disk:\n            checkpointNoSIS();\n          }\n\n          rld.writeFieldUpdates(directory, bufferedUpdatesStream.getCompletedDelGen(), infoStream);\n\n          if (rld.getNumDVUpdates() == 0) {\n            rld.dropReaders();\n            readerMap.remove(rld.info);\n          } else {\n            // We are forced to pool this segment until its deletes fully apply (no delGen gaps)\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ace62484d1fb66ec273f42ff92e381349eb78e68","date":1522499076,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","sourceNew":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      if (rld.refCount() == 0) {\n        // This happens if the segment was just merged away, while a buffered deletes packet was still applying deletes/updates to it.\n        assert readerMap.containsKey(rld.info) == false: \"seg=\" + rld.info + \" has refCount 0 but still unexpectedly exists in the reader pool\";\n      } else {\n\n        // Pool still holds a ref:\n        assert rld.refCount() > 0: \"refCount=\" + rld.refCount() + \" reader=\" + rld.info;\n\n        if (!poolReaders && rld.refCount() == 1 && readerMap.containsKey(rld.info)) {\n          // This is the last ref to this RLD, and we're not\n          // pooling, so remove it:\n          if (rld.writeLiveDocs(directory)) {\n            // Make sure we only write del docs for a live segment:\n            assert assertInfoLive == false || assertInfoIsLive(rld.info);\n            // Must checkpoint because we just\n            // created new _X_N.del and field updates files;\n            // don't call IW.checkpoint because that also\n            // increments SIS.version, which we do not want to\n            // do here: it was done previously (after we\n            // invoked BDS.applyDeletes), whereas here all we\n            // did was move the state to disk:\n            checkpointNoSIS();\n          }\n          if (rld.writeFieldUpdates(directory, globalFieldNumberMap, bufferedUpdatesStream.getCompletedDelGen(), infoStream)) {\n            checkpointNoSIS();\n          }\n          if (rld.getNumDVUpdates() == 0) {\n            rld.dropReaders();\n            readerMap.remove(rld.info);\n          } else {\n            // We are forced to pool this segment until its deletes fully apply (no delGen gaps)\n          }\n        }\n      }\n    }\n\n","sourceOld":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      if (rld.refCount() == 0) {\n        // This happens if the segment was just merged away, while a buffered deletes packet was still applying deletes/updates to it.\n        assert readerMap.containsKey(rld.info) == false: \"seg=\" + rld.info + \" has refCount 0 but still unexpectedly exists in the reader pool\";\n      } else {\n\n        // Pool still holds a ref:\n        assert rld.refCount() > 0: \"refCount=\" + rld.refCount() + \" reader=\" + rld.info;\n\n        if (!poolReaders && rld.refCount() == 1 && readerMap.containsKey(rld.info)) {\n          // This is the last ref to this RLD, and we're not\n          // pooling, so remove it:\n          boolean changed = rld.writeLiveDocs(directory);\n          changed |= rld.writeFieldUpdates(directory, globalFieldNumberMap, bufferedUpdatesStream.getCompletedDelGen(), infoStream);\n\n          if (changed) {\n            // Make sure we only write del docs for a live segment:\n            assert assertInfoLive == false || assertInfoIsLive(rld.info);\n            // Must checkpoint because we just\n            // created new _X_N.del and field updates files;\n            // don't call IW.checkpoint because that also\n            // increments SIS.version, which we do not want to\n            // do here: it was done previously (after we\n            // invoked BDS.applyDeletes), whereas here all we\n            // did was move the state to disk:\n            checkpointNoSIS();\n          }\n          if (rld.getNumDVUpdates() == 0) {\n            rld.dropReaders();\n            readerMap.remove(rld.info);\n          } else {\n            // We are forced to pool this segment until its deletes fully apply (no delGen gaps)\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aa5e39259dfd4a68287c824d3b7e1bc9097dc895","date":1522505041,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","sourceNew":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      if (rld.refCount() == 0) {\n        // This happens if the segment was just merged away, while a buffered deletes packet was still applying deletes/updates to it.\n        assert readerMap.containsKey(rld.info) == false: \"seg=\" + rld.info + \" has refCount 0 but still unexpectedly exists in the reader pool\";\n      } else {\n\n        // Pool still holds a ref:\n        assert rld.refCount() > 0: \"refCount=\" + rld.refCount() + \" reader=\" + rld.info;\n\n        if (!poolReaders && rld.refCount() == 1 && readerMap.containsKey(rld.info)) {\n          // This is the last ref to this RLD, and we're not\n          // pooling, so remove it:\n          if (rld.writeLiveDocs(directory)) {\n            // Make sure we only write del docs for a live segment:\n            assert assertInfoLive == false || assertInfoIsLive(rld.info);\n            // Must checkpoint because we just\n            // created new _X_N.del and field updates files;\n            // don't call IW.checkpoint because that also\n            // increments SIS.version, which we do not want to\n            // do here: it was done previously (after we\n            // invoked BDS.applyDeletes), whereas here all we\n            // did was move the state to disk:\n            checkpointNoSIS();\n          }\n          if (rld.writeFieldUpdates(directory, globalFieldNumberMap, bufferedUpdatesStream.getCompletedDelGen(), infoStream)) {\n            checkpointNoSIS();\n          }\n          if (rld.getNumDVUpdates() == 0) {\n            rld.dropReaders();\n            readerMap.remove(rld.info);\n          } else {\n            // We are forced to pool this segment until its deletes fully apply (no delGen gaps)\n          }\n        }\n      }\n    }\n\n","sourceOld":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      if (rld.refCount() == 0) {\n        // This happens if the segment was just merged away, while a buffered deletes packet was still applying deletes/updates to it.\n        assert readerMap.containsKey(rld.info) == false: \"seg=\" + rld.info + \" has refCount 0 but still unexpectedly exists in the reader pool\";\n      } else {\n\n        // Pool still holds a ref:\n        assert rld.refCount() > 0: \"refCount=\" + rld.refCount() + \" reader=\" + rld.info;\n\n        if (!poolReaders && rld.refCount() == 1 && readerMap.containsKey(rld.info)) {\n          // This is the last ref to this RLD, and we're not\n          // pooling, so remove it:\n          if (rld.writeLiveDocs(directory)) {\n            // Make sure we only write del docs for a live segment:\n            assert assertInfoLive == false || assertInfoIsLive(rld.info);\n            // Must checkpoint because we just\n            // created new _X_N.del and field updates files;\n            // don't call IW.checkpoint because that also\n            // increments SIS.version, which we do not want to\n            // do here: it was done previously (after we\n            // invoked BDS.applyDeletes), whereas here all we\n            // did was move the state to disk:\n            checkpointNoSIS();\n          }\n\n          rld.writeFieldUpdates(directory, bufferedUpdatesStream.getCompletedDelGen(), infoStream);\n\n          if (rld.getNumDVUpdates() == 0) {\n            rld.dropReaders();\n            readerMap.remove(rld.info);\n          } else {\n            // We are forced to pool this segment until its deletes fully apply (no delGen gaps)\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1926100d9b67becc9701c54266fee3ba7878a5f0","date":1524472150,"type":4,"author":"Simon Willnauer","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter.ReaderPool#release(ReadersAndUpdates,boolean).mjava","sourceNew":null,"sourceOld":"    public synchronized void release(ReadersAndUpdates rld, boolean assertInfoLive) throws IOException {\n\n      // Matches incRef in get:\n      rld.decRef();\n\n      if (rld.refCount() == 0) {\n        // This happens if the segment was just merged away, while a buffered deletes packet was still applying deletes/updates to it.\n        assert readerMap.containsKey(rld.info) == false: \"seg=\" + rld.info + \" has refCount 0 but still unexpectedly exists in the reader pool\";\n      } else {\n\n        // Pool still holds a ref:\n        assert rld.refCount() > 0: \"refCount=\" + rld.refCount() + \" reader=\" + rld.info;\n\n        if (!poolReaders && rld.refCount() == 1 && readerMap.containsKey(rld.info)) {\n          // This is the last ref to this RLD, and we're not\n          // pooling, so remove it:\n          if (rld.writeLiveDocs(directory)) {\n            // Make sure we only write del docs for a live segment:\n            assert assertInfoLive == false || assertInfoIsLive(rld.info);\n            // Must checkpoint because we just\n            // created new _X_N.del and field updates files;\n            // don't call IW.checkpoint because that also\n            // increments SIS.version, which we do not want to\n            // do here: it was done previously (after we\n            // invoked BDS.applyDeletes), whereas here all we\n            // did was move the state to disk:\n            checkpointNoSIS();\n          }\n          if (rld.writeFieldUpdates(directory, globalFieldNumberMap, bufferedUpdatesStream.getCompletedDelGen(), infoStream)) {\n            checkpointNoSIS();\n          }\n          if (rld.getNumDVUpdates() == 0) {\n            rld.dropReaders();\n            readerMap.remove(rld.info);\n          } else {\n            // We are forced to pool this segment until its deletes fully apply (no delGen gaps)\n          }\n        }\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"aa5e39259dfd4a68287c824d3b7e1bc9097dc895":["28288370235ed02234a64753cdbf0c6ec096304a","ace62484d1fb66ec273f42ff92e381349eb78e68"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["aa5e39259dfd4a68287c824d3b7e1bc9097dc895"],"d60c1bb96a28a26d197c36299f7b6c9c5da617a1":["28288370235ed02234a64753cdbf0c6ec096304a"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"ace62484d1fb66ec273f42ff92e381349eb78e68":["d60c1bb96a28a26d197c36299f7b6c9c5da617a1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1926100d9b67becc9701c54266fee3ba7878a5f0"]},"commit2Childs":{"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"aa5e39259dfd4a68287c824d3b7e1bc9097dc895":["1926100d9b67becc9701c54266fee3ba7878a5f0"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d60c1bb96a28a26d197c36299f7b6c9c5da617a1":["ace62484d1fb66ec273f42ff92e381349eb78e68"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["aa5e39259dfd4a68287c824d3b7e1bc9097dc895","d60c1bb96a28a26d197c36299f7b6c9c5da617a1"],"ace62484d1fb66ec273f42ff92e381349eb78e68":["aa5e39259dfd4a68287c824d3b7e1bc9097dc895"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}