{"path":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","commits":[{"id":"7a2926dd4be586592be94a073d71c94db8bc7645","date":1383596258,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","pathOld":"/dev/null","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<Document>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final Sorter.DocComparator parentComparator = new Sorter.DocComparator() {\n      @Override\n      public int compare(int docID1, int docID2) {\n        assertTrue(parentBits.get(docID1));\n        assertTrue(parentBits.get(docID2));\n        return Long.compare(parentValues.get(docID1), parentValues.get(docID2));\n      }\n    };\n\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n    final Sorter.DocComparator childComparator = new Sorter.DocComparator() {\n      @Override\n      public int compare(int docID1, int docID2) {\n        assertFalse(parentBits.get(docID1));\n        assertFalse(parentBits.get(docID2));\n        return Long.compare(childValues.get(docID1), childValues.get(docID2));\n      }\n    };\n\n    final Sorter sorter = new BlockJoinSorter(parentsFilter) {\n      \n      @Override\n      public String getID() {\n        return \"Dummy\";\n      }\n      \n      @Override\n      protected DocComparator getParentComparator(AtomicReader r) {\n        assertEquals(reader, r);\n        return parentComparator;\n      }\n\n      @Override\n      protected DocComparator getChildComparator(AtomicReader r) {\n        assertEquals(reader, r);\n        return childComparator;\n      }\n\n    };\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["d2dce5e63b0228e94e989139c2503dd4018d8b45","4637747f71df783fc2014ef1f1e0418466e3bed6"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cbcbc6c6114d6e02f478dc15249bd5708eea06eb","date":1394118481,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<Document>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new SortSorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<Document>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final Sorter.DocComparator parentComparator = new Sorter.DocComparator() {\n      @Override\n      public int compare(int docID1, int docID2) {\n        assertTrue(parentBits.get(docID1));\n        assertTrue(parentBits.get(docID2));\n        return Long.compare(parentValues.get(docID1), parentValues.get(docID2));\n      }\n    };\n\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n    final Sorter.DocComparator childComparator = new Sorter.DocComparator() {\n      @Override\n      public int compare(int docID1, int docID2) {\n        assertFalse(parentBits.get(docID1));\n        assertFalse(parentBits.get(docID2));\n        return Long.compare(childValues.get(docID1), childValues.get(docID2));\n      }\n    };\n\n    final Sorter sorter = new BlockJoinSorter(parentsFilter) {\n      \n      @Override\n      public String getID() {\n        return \"Dummy\";\n      }\n      \n      @Override\n      protected DocComparator getParentComparator(AtomicReader r) {\n        assertEquals(reader, r);\n        return parentComparator;\n      }\n\n      @Override\n      protected DocComparator getChildComparator(AtomicReader r) {\n        assertEquals(reader, r);\n        return childComparator;\n      }\n\n    };\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3e4cbd3e4a2b5f0b9cfcac79bfbccd35a0de5036","date":1394123292,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<Document>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<Document>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new SortSorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4637747f71df783fc2014ef1f1e0418466e3bed6","date":1394196311,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<Document>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<Document>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final Sorter.DocComparator parentComparator = new Sorter.DocComparator() {\n      @Override\n      public int compare(int docID1, int docID2) {\n        assertTrue(parentBits.get(docID1));\n        assertTrue(parentBits.get(docID2));\n        return Long.compare(parentValues.get(docID1), parentValues.get(docID2));\n      }\n    };\n\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n    final Sorter.DocComparator childComparator = new Sorter.DocComparator() {\n      @Override\n      public int compare(int docID1, int docID2) {\n        assertFalse(parentBits.get(docID1));\n        assertFalse(parentBits.get(docID2));\n        return Long.compare(childValues.get(docID1), childValues.get(docID2));\n      }\n    };\n\n    final Sorter sorter = new BlockJoinSorter(parentsFilter) {\n      \n      @Override\n      public String getID() {\n        return \"Dummy\";\n      }\n      \n      @Override\n      protected DocComparator getParentComparator(AtomicReader r) {\n        assertEquals(reader, r);\n        return parentComparator;\n      }\n\n      @Override\n      protected DocComparator getChildComparator(AtomicReader r) {\n        assertEquals(reader, r);\n        return childComparator;\n      }\n\n    };\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":["7a2926dd4be586592be94a073d71c94db8bc7645"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<Document>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<Document>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final Sorter.DocComparator parentComparator = new Sorter.DocComparator() {\n      @Override\n      public int compare(int docID1, int docID2) {\n        assertTrue(parentBits.get(docID1));\n        assertTrue(parentBits.get(docID2));\n        return Long.compare(parentValues.get(docID1), parentValues.get(docID2));\n      }\n    };\n\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n    final Sorter.DocComparator childComparator = new Sorter.DocComparator() {\n      @Override\n      public int compare(int docID1, int docID2) {\n        assertFalse(parentBits.get(docID1));\n        assertFalse(parentBits.get(docID2));\n        return Long.compare(childValues.get(docID1), childValues.get(docID2));\n      }\n    };\n\n    final Sorter sorter = new BlockJoinSorter(parentsFilter) {\n      \n      @Override\n      public String getID() {\n        return \"Dummy\";\n      }\n      \n      @Override\n      protected DocComparator getParentComparator(AtomicReader r) {\n        assertEquals(reader, r);\n        return parentComparator;\n      }\n\n      @Override\n      protected DocComparator getChildComparator(AtomicReader r) {\n        assertEquals(reader, r);\n        return childComparator;\n      }\n\n    };\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<Document>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.shutdown();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.shutdown();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.shutdown();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.shutdown();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final AtomicReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":["d2dce5e63b0228e94e989139c2503dd4018d8b45"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"80c55596a764e2d397e982828e75fcac5ce430a0","date":1413987559,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null).bits();\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null);\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e","date":1419346542,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/index/TestBlockJoinSorter#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/index/sorter/TestBlockJoinSorter#test().mjava","sourceNew":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null).bits();\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    final int numParents = atLeast(200);\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    cfg.setMergePolicy(newLogMergePolicy());\n    final RandomIndexWriter writer = new RandomIndexWriter(random(), newDirectory(), cfg);\n    final Document parentDoc = new Document();\n    final NumericDocValuesField parentVal = new NumericDocValuesField(\"parent_val\", 0L);\n    parentDoc.add(parentVal);\n    final StringField parent = new StringField(\"parent\", \"true\", Store.YES);\n    parentDoc.add(parent);\n    for (int i = 0; i < numParents; ++i) {\n      List<Document> documents = new ArrayList<>();\n      final int numChildren = random().nextInt(10);\n      for (int j = 0; j < numChildren; ++j) {\n        final Document childDoc = new Document();\n        childDoc.add(new NumericDocValuesField(\"child_val\", random().nextInt(5)));\n        documents.add(childDoc);\n      }\n      parentVal.setLongValue(random().nextInt(50));\n      documents.add(parentDoc);\n      writer.addDocuments(documents);\n    }\n    writer.forceMerge(1);\n    final DirectoryReader indexReader = writer.getReader();\n    writer.close();\n\n    final LeafReader reader = getOnlySegmentReader(indexReader);\n    final Filter parentsFilter = new FixedBitSetCachingWrapperFilter(new QueryWrapperFilter(new TermQuery(new Term(\"parent\", \"true\"))));\n    final FixedBitSet parentBits = (FixedBitSet) parentsFilter.getDocIdSet(reader.getContext(), null).bits();\n    final NumericDocValues parentValues = reader.getNumericDocValues(\"parent_val\");\n    final NumericDocValues childValues = reader.getNumericDocValues(\"child_val\");\n\n    final Sort parentSort = new Sort(new SortField(\"parent_val\", SortField.Type.LONG));\n    final Sort childSort = new Sort(new SortField(\"child_val\", SortField.Type.LONG));\n\n    final Sort sort = new Sort(new SortField(\"custom\", new BlockJoinComparatorSource(parentsFilter, parentSort, childSort)));\n    final Sorter sorter = new Sorter(sort);\n    final Sorter.DocMap docMap = sorter.sort(reader);\n    assertEquals(reader.maxDoc(), docMap.size());\n\n    int[] children = new int[1];\n    int numChildren = 0;\n    int previousParent = -1;\n    for (int i = 0; i < docMap.size(); ++i) {\n      final int oldID = docMap.newToOld(i);\n      if (parentBits.get(oldID)) {\n        // check that we have the right children\n        for (int j = 0; j < numChildren; ++j) {\n          assertEquals(oldID, parentBits.nextSetBit(children[j]));\n        }\n        // check that children are sorted\n        for (int j = 1; j < numChildren; ++j) {\n          final int doc1 = children[j-1];\n          final int doc2 = children[j];\n          if (childValues.get(doc1) == childValues.get(doc2)) {\n            assertTrue(doc1 < doc2); // sort is stable\n          } else {\n            assertTrue(childValues.get(doc1) < childValues.get(doc2));\n          }\n        }\n        // check that parents are sorted\n        if (previousParent != -1) {\n          if (parentValues.get(previousParent) == parentValues.get(oldID)) {\n            assertTrue(previousParent < oldID);\n          } else {\n            assertTrue(parentValues.get(previousParent) < parentValues.get(oldID));\n          }\n        }\n        // reset\n        previousParent = oldID;\n        numChildren = 0;\n      } else {\n        children = ArrayUtil.grow(children, numChildren+1);\n        children[numChildren++] = oldID;\n      }\n    }\n    indexReader.close();\n    writer.w.getDirectory().close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["4637747f71df783fc2014ef1f1e0418466e3bed6"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["7a2926dd4be586592be94a073d71c94db8bc7645","4637747f71df783fc2014ef1f1e0418466e3bed6"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"7a2926dd4be586592be94a073d71c94db8bc7645":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e":["80c55596a764e2d397e982828e75fcac5ce430a0"],"cbcbc6c6114d6e02f478dc15249bd5708eea06eb":["7a2926dd4be586592be94a073d71c94db8bc7645"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"3e4cbd3e4a2b5f0b9cfcac79bfbccd35a0de5036":["cbcbc6c6114d6e02f478dc15249bd5708eea06eb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"80c55596a764e2d397e982828e75fcac5ce430a0":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"4637747f71df783fc2014ef1f1e0418466e3bed6":["7a2926dd4be586592be94a073d71c94db8bc7645","3e4cbd3e4a2b5f0b9cfcac79bfbccd35a0de5036"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"7a2926dd4be586592be94a073d71c94db8bc7645":["96ea64d994d340044e0d57aeb6a5871539d10ca5","cbcbc6c6114d6e02f478dc15249bd5708eea06eb","4637747f71df783fc2014ef1f1e0418466e3bed6"],"82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cbcbc6c6114d6e02f478dc15249bd5708eea06eb":["3e4cbd3e4a2b5f0b9cfcac79bfbccd35a0de5036"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["80c55596a764e2d397e982828e75fcac5ce430a0"],"3e4cbd3e4a2b5f0b9cfcac79bfbccd35a0de5036":["4637747f71df783fc2014ef1f1e0418466e3bed6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7a2926dd4be586592be94a073d71c94db8bc7645"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"80c55596a764e2d397e982828e75fcac5ce430a0":["82dc51e5dcd7a80b71e40ebe8959b1c43b63f95e"],"4637747f71df783fc2014ef1f1e0418466e3bed6":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","96ea64d994d340044e0d57aeb6a5871539d10ca5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}