{"path":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","commits":[{"id":"9cd756794a4a9e33f786119818f9d56162965cf8","date":1412591300,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","pathOld":"/dev/null","sourceNew":"    private void flush() {\n      assert currentBlockCardinality <= BLOCK_SIZE;\n      if (currentBlockCardinality <= MAX_ARRAY_LENGTH) {\n        // Use sparse encoding\n        assert denseBuffer == null;\n        if (currentBlockCardinality > 0) {\n          sets[currentBlock] = new ShortArrayDocIdSet(Arrays.copyOf(buffer, currentBlockCardinality));\n        }\n      } else {\n        assert denseBuffer != null;\n        assert denseBuffer.cardinality() == currentBlockCardinality;\n        if (denseBuffer.length() == BLOCK_SIZE && BLOCK_SIZE - currentBlockCardinality < MAX_ARRAY_LENGTH) {\n          // Doc ids are very dense, inverse the encoding\n          final short[] excludedDocs = new short[BLOCK_SIZE - currentBlockCardinality];\n          denseBuffer.flip(0, denseBuffer.length());\n          int excludedDoc = -1;\n          for (int i = 0; i < excludedDocs.length; ++i) {\n            excludedDoc = denseBuffer.nextSetBit(excludedDoc + 1);\n            assert excludedDoc != -1;\n            excludedDocs[i] = (short) excludedDoc;\n          }\n          assert excludedDoc + 1 == denseBuffer.length() || denseBuffer.nextSetBit(excludedDoc + 1) == -1;\n          sets[currentBlock] = new NotDocIdSet(BLOCK_SIZE, new ShortArrayDocIdSet(excludedDocs));\n        } else {\n          // Neither sparse nor super dense, use a fixed bit set\n          sets[currentBlock] = denseBuffer;\n        }\n        denseBuffer = null;\n      }\n\n      cardinality += currentBlockCardinality;\n      denseBuffer = null;\n      currentBlockCardinality = 0;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","pathOld":"/dev/null","sourceNew":"    private void flush() {\n      assert currentBlockCardinality <= BLOCK_SIZE;\n      if (currentBlockCardinality <= MAX_ARRAY_LENGTH) {\n        // Use sparse encoding\n        assert denseBuffer == null;\n        if (currentBlockCardinality > 0) {\n          sets[currentBlock] = new ShortArrayDocIdSet(Arrays.copyOf(buffer, currentBlockCardinality));\n        }\n      } else {\n        assert denseBuffer != null;\n        assert denseBuffer.cardinality() == currentBlockCardinality;\n        if (denseBuffer.length() == BLOCK_SIZE && BLOCK_SIZE - currentBlockCardinality < MAX_ARRAY_LENGTH) {\n          // Doc ids are very dense, inverse the encoding\n          final short[] excludedDocs = new short[BLOCK_SIZE - currentBlockCardinality];\n          denseBuffer.flip(0, denseBuffer.length());\n          int excludedDoc = -1;\n          for (int i = 0; i < excludedDocs.length; ++i) {\n            excludedDoc = denseBuffer.nextSetBit(excludedDoc + 1);\n            assert excludedDoc != -1;\n            excludedDocs[i] = (short) excludedDoc;\n          }\n          assert excludedDoc + 1 == denseBuffer.length() || denseBuffer.nextSetBit(excludedDoc + 1) == -1;\n          sets[currentBlock] = new NotDocIdSet(BLOCK_SIZE, new ShortArrayDocIdSet(excludedDocs));\n        } else {\n          // Neither sparse nor super dense, use a fixed bit set\n          sets[currentBlock] = denseBuffer;\n        }\n        denseBuffer = null;\n      }\n\n      cardinality += currentBlockCardinality;\n      denseBuffer = null;\n      currentBlockCardinality = 0;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"80c55596a764e2d397e982828e75fcac5ce430a0","date":1413987559,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","sourceNew":"    private void flush() {\n      assert currentBlockCardinality <= BLOCK_SIZE;\n      if (currentBlockCardinality <= MAX_ARRAY_LENGTH) {\n        // Use sparse encoding\n        assert denseBuffer == null;\n        if (currentBlockCardinality > 0) {\n          sets[currentBlock] = new ShortArrayDocIdSet(Arrays.copyOf(buffer, currentBlockCardinality));\n        }\n      } else {\n        assert denseBuffer != null;\n        assert denseBuffer.cardinality() == currentBlockCardinality;\n        if (denseBuffer.length() == BLOCK_SIZE && BLOCK_SIZE - currentBlockCardinality < MAX_ARRAY_LENGTH) {\n          // Doc ids are very dense, inverse the encoding\n          final short[] excludedDocs = new short[BLOCK_SIZE - currentBlockCardinality];\n          denseBuffer.flip(0, denseBuffer.length());\n          int excludedDoc = -1;\n          for (int i = 0; i < excludedDocs.length; ++i) {\n            excludedDoc = denseBuffer.nextSetBit(excludedDoc + 1);\n            assert excludedDoc != -1;\n            excludedDocs[i] = (short) excludedDoc;\n          }\n          assert excludedDoc + 1 == denseBuffer.length() || denseBuffer.nextSetBit(excludedDoc + 1) == -1;\n          sets[currentBlock] = new NotDocIdSet(BLOCK_SIZE, new ShortArrayDocIdSet(excludedDocs));\n        } else {\n          // Neither sparse nor super dense, use a fixed bit set\n          sets[currentBlock] = new FixedBitDocIdSet(denseBuffer, currentBlockCardinality);\n        }\n        denseBuffer = null;\n      }\n\n      cardinality += currentBlockCardinality;\n      denseBuffer = null;\n      currentBlockCardinality = 0;\n    }\n\n","sourceOld":"    private void flush() {\n      assert currentBlockCardinality <= BLOCK_SIZE;\n      if (currentBlockCardinality <= MAX_ARRAY_LENGTH) {\n        // Use sparse encoding\n        assert denseBuffer == null;\n        if (currentBlockCardinality > 0) {\n          sets[currentBlock] = new ShortArrayDocIdSet(Arrays.copyOf(buffer, currentBlockCardinality));\n        }\n      } else {\n        assert denseBuffer != null;\n        assert denseBuffer.cardinality() == currentBlockCardinality;\n        if (denseBuffer.length() == BLOCK_SIZE && BLOCK_SIZE - currentBlockCardinality < MAX_ARRAY_LENGTH) {\n          // Doc ids are very dense, inverse the encoding\n          final short[] excludedDocs = new short[BLOCK_SIZE - currentBlockCardinality];\n          denseBuffer.flip(0, denseBuffer.length());\n          int excludedDoc = -1;\n          for (int i = 0; i < excludedDocs.length; ++i) {\n            excludedDoc = denseBuffer.nextSetBit(excludedDoc + 1);\n            assert excludedDoc != -1;\n            excludedDocs[i] = (short) excludedDoc;\n          }\n          assert excludedDoc + 1 == denseBuffer.length() || denseBuffer.nextSetBit(excludedDoc + 1) == -1;\n          sets[currentBlock] = new NotDocIdSet(BLOCK_SIZE, new ShortArrayDocIdSet(excludedDocs));\n        } else {\n          // Neither sparse nor super dense, use a fixed bit set\n          sets[currentBlock] = denseBuffer;\n        }\n        denseBuffer = null;\n      }\n\n      cardinality += currentBlockCardinality;\n      denseBuffer = null;\n      currentBlockCardinality = 0;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0abcec02c9851c46c70a75bd42fb6e4d5348ac9e","date":1414135939,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","sourceNew":"    private void flush() {\n      assert currentBlockCardinality <= BLOCK_SIZE;\n      if (currentBlockCardinality <= MAX_ARRAY_LENGTH) {\n        // Use sparse encoding\n        assert denseBuffer == null;\n        if (currentBlockCardinality > 0) {\n          sets[currentBlock] = new ShortArrayDocIdSet(Arrays.copyOf(buffer, currentBlockCardinality));\n        }\n      } else {\n        assert denseBuffer != null;\n        assert denseBuffer.cardinality() == currentBlockCardinality;\n        if (denseBuffer.length() == BLOCK_SIZE && BLOCK_SIZE - currentBlockCardinality < MAX_ARRAY_LENGTH) {\n          // Doc ids are very dense, inverse the encoding\n          final short[] excludedDocs = new short[BLOCK_SIZE - currentBlockCardinality];\n          denseBuffer.flip(0, denseBuffer.length());\n          int excludedDoc = -1;\n          for (int i = 0; i < excludedDocs.length; ++i) {\n            excludedDoc = denseBuffer.nextSetBit(excludedDoc + 1);\n            assert excludedDoc != DocIdSetIterator.NO_MORE_DOCS;\n            excludedDocs[i] = (short) excludedDoc;\n          }\n          assert excludedDoc + 1 == denseBuffer.length() || denseBuffer.nextSetBit(excludedDoc + 1) == DocIdSetIterator.NO_MORE_DOCS;\n          sets[currentBlock] = new NotDocIdSet(BLOCK_SIZE, new ShortArrayDocIdSet(excludedDocs));\n        } else {\n          // Neither sparse nor super dense, use a fixed bit set\n          sets[currentBlock] = new BitDocIdSet(denseBuffer, currentBlockCardinality);\n        }\n        denseBuffer = null;\n      }\n\n      cardinality += currentBlockCardinality;\n      denseBuffer = null;\n      currentBlockCardinality = 0;\n    }\n\n","sourceOld":"    private void flush() {\n      assert currentBlockCardinality <= BLOCK_SIZE;\n      if (currentBlockCardinality <= MAX_ARRAY_LENGTH) {\n        // Use sparse encoding\n        assert denseBuffer == null;\n        if (currentBlockCardinality > 0) {\n          sets[currentBlock] = new ShortArrayDocIdSet(Arrays.copyOf(buffer, currentBlockCardinality));\n        }\n      } else {\n        assert denseBuffer != null;\n        assert denseBuffer.cardinality() == currentBlockCardinality;\n        if (denseBuffer.length() == BLOCK_SIZE && BLOCK_SIZE - currentBlockCardinality < MAX_ARRAY_LENGTH) {\n          // Doc ids are very dense, inverse the encoding\n          final short[] excludedDocs = new short[BLOCK_SIZE - currentBlockCardinality];\n          denseBuffer.flip(0, denseBuffer.length());\n          int excludedDoc = -1;\n          for (int i = 0; i < excludedDocs.length; ++i) {\n            excludedDoc = denseBuffer.nextSetBit(excludedDoc + 1);\n            assert excludedDoc != -1;\n            excludedDocs[i] = (short) excludedDoc;\n          }\n          assert excludedDoc + 1 == denseBuffer.length() || denseBuffer.nextSetBit(excludedDoc + 1) == -1;\n          sets[currentBlock] = new NotDocIdSet(BLOCK_SIZE, new ShortArrayDocIdSet(excludedDocs));\n        } else {\n          // Neither sparse nor super dense, use a fixed bit set\n          sets[currentBlock] = new FixedBitDocIdSet(denseBuffer, currentBlockCardinality);\n        }\n        denseBuffer = null;\n      }\n\n      cardinality += currentBlockCardinality;\n      denseBuffer = null;\n      currentBlockCardinality = 0;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9798d0818e7a880546802b509792d3f3d57babd2","date":1528358901,"type":3,"author":"Nhat Nguyen","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","sourceNew":"    private void flush() {\n      assert currentBlockCardinality <= BLOCK_SIZE;\n      if (currentBlockCardinality <= MAX_ARRAY_LENGTH) {\n        // Use sparse encoding\n        assert denseBuffer == null;\n        if (currentBlockCardinality > 0) {\n          sets[currentBlock] = new ShortArrayDocIdSet(ArrayUtil.copyOfSubArray(buffer, 0, currentBlockCardinality));\n        }\n      } else {\n        assert denseBuffer != null;\n        assert denseBuffer.cardinality() == currentBlockCardinality;\n        if (denseBuffer.length() == BLOCK_SIZE && BLOCK_SIZE - currentBlockCardinality < MAX_ARRAY_LENGTH) {\n          // Doc ids are very dense, inverse the encoding\n          final short[] excludedDocs = new short[BLOCK_SIZE - currentBlockCardinality];\n          denseBuffer.flip(0, denseBuffer.length());\n          int excludedDoc = -1;\n          for (int i = 0; i < excludedDocs.length; ++i) {\n            excludedDoc = denseBuffer.nextSetBit(excludedDoc + 1);\n            assert excludedDoc != DocIdSetIterator.NO_MORE_DOCS;\n            excludedDocs[i] = (short) excludedDoc;\n          }\n          assert excludedDoc + 1 == denseBuffer.length() || denseBuffer.nextSetBit(excludedDoc + 1) == DocIdSetIterator.NO_MORE_DOCS;\n          sets[currentBlock] = new NotDocIdSet(BLOCK_SIZE, new ShortArrayDocIdSet(excludedDocs));\n        } else {\n          // Neither sparse nor super dense, use a fixed bit set\n          sets[currentBlock] = new BitDocIdSet(denseBuffer, currentBlockCardinality);\n        }\n        denseBuffer = null;\n      }\n\n      cardinality += currentBlockCardinality;\n      denseBuffer = null;\n      currentBlockCardinality = 0;\n    }\n\n","sourceOld":"    private void flush() {\n      assert currentBlockCardinality <= BLOCK_SIZE;\n      if (currentBlockCardinality <= MAX_ARRAY_LENGTH) {\n        // Use sparse encoding\n        assert denseBuffer == null;\n        if (currentBlockCardinality > 0) {\n          sets[currentBlock] = new ShortArrayDocIdSet(Arrays.copyOf(buffer, currentBlockCardinality));\n        }\n      } else {\n        assert denseBuffer != null;\n        assert denseBuffer.cardinality() == currentBlockCardinality;\n        if (denseBuffer.length() == BLOCK_SIZE && BLOCK_SIZE - currentBlockCardinality < MAX_ARRAY_LENGTH) {\n          // Doc ids are very dense, inverse the encoding\n          final short[] excludedDocs = new short[BLOCK_SIZE - currentBlockCardinality];\n          denseBuffer.flip(0, denseBuffer.length());\n          int excludedDoc = -1;\n          for (int i = 0; i < excludedDocs.length; ++i) {\n            excludedDoc = denseBuffer.nextSetBit(excludedDoc + 1);\n            assert excludedDoc != DocIdSetIterator.NO_MORE_DOCS;\n            excludedDocs[i] = (short) excludedDoc;\n          }\n          assert excludedDoc + 1 == denseBuffer.length() || denseBuffer.nextSetBit(excludedDoc + 1) == DocIdSetIterator.NO_MORE_DOCS;\n          sets[currentBlock] = new NotDocIdSet(BLOCK_SIZE, new ShortArrayDocIdSet(excludedDocs));\n        } else {\n          // Neither sparse nor super dense, use a fixed bit set\n          sets[currentBlock] = new BitDocIdSet(denseBuffer, currentBlockCardinality);\n        }\n        denseBuffer = null;\n      }\n\n      cardinality += currentBlockCardinality;\n      denseBuffer = null;\n      currentBlockCardinality = 0;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","sourceNew":"    private void flush() {\n      assert currentBlockCardinality <= BLOCK_SIZE;\n      if (currentBlockCardinality <= MAX_ARRAY_LENGTH) {\n        // Use sparse encoding\n        assert denseBuffer == null;\n        if (currentBlockCardinality > 0) {\n          sets[currentBlock] = new ShortArrayDocIdSet(ArrayUtil.copyOfSubArray(buffer, 0, currentBlockCardinality));\n        }\n      } else {\n        assert denseBuffer != null;\n        assert denseBuffer.cardinality() == currentBlockCardinality;\n        if (denseBuffer.length() == BLOCK_SIZE && BLOCK_SIZE - currentBlockCardinality < MAX_ARRAY_LENGTH) {\n          // Doc ids are very dense, inverse the encoding\n          final short[] excludedDocs = new short[BLOCK_SIZE - currentBlockCardinality];\n          denseBuffer.flip(0, denseBuffer.length());\n          int excludedDoc = -1;\n          for (int i = 0; i < excludedDocs.length; ++i) {\n            excludedDoc = denseBuffer.nextSetBit(excludedDoc + 1);\n            assert excludedDoc != DocIdSetIterator.NO_MORE_DOCS;\n            excludedDocs[i] = (short) excludedDoc;\n          }\n          assert excludedDoc + 1 == denseBuffer.length() || denseBuffer.nextSetBit(excludedDoc + 1) == DocIdSetIterator.NO_MORE_DOCS;\n          sets[currentBlock] = new NotDocIdSet(BLOCK_SIZE, new ShortArrayDocIdSet(excludedDocs));\n        } else {\n          // Neither sparse nor super dense, use a fixed bit set\n          sets[currentBlock] = new BitDocIdSet(denseBuffer, currentBlockCardinality);\n        }\n        denseBuffer = null;\n      }\n\n      cardinality += currentBlockCardinality;\n      denseBuffer = null;\n      currentBlockCardinality = 0;\n    }\n\n","sourceOld":"    private void flush() {\n      assert currentBlockCardinality <= BLOCK_SIZE;\n      if (currentBlockCardinality <= MAX_ARRAY_LENGTH) {\n        // Use sparse encoding\n        assert denseBuffer == null;\n        if (currentBlockCardinality > 0) {\n          sets[currentBlock] = new ShortArrayDocIdSet(Arrays.copyOf(buffer, currentBlockCardinality));\n        }\n      } else {\n        assert denseBuffer != null;\n        assert denseBuffer.cardinality() == currentBlockCardinality;\n        if (denseBuffer.length() == BLOCK_SIZE && BLOCK_SIZE - currentBlockCardinality < MAX_ARRAY_LENGTH) {\n          // Doc ids are very dense, inverse the encoding\n          final short[] excludedDocs = new short[BLOCK_SIZE - currentBlockCardinality];\n          denseBuffer.flip(0, denseBuffer.length());\n          int excludedDoc = -1;\n          for (int i = 0; i < excludedDocs.length; ++i) {\n            excludedDoc = denseBuffer.nextSetBit(excludedDoc + 1);\n            assert excludedDoc != DocIdSetIterator.NO_MORE_DOCS;\n            excludedDocs[i] = (short) excludedDoc;\n          }\n          assert excludedDoc + 1 == denseBuffer.length() || denseBuffer.nextSetBit(excludedDoc + 1) == DocIdSetIterator.NO_MORE_DOCS;\n          sets[currentBlock] = new NotDocIdSet(BLOCK_SIZE, new ShortArrayDocIdSet(excludedDocs));\n        } else {\n          // Neither sparse nor super dense, use a fixed bit set\n          sets[currentBlock] = new BitDocIdSet(denseBuffer, currentBlockCardinality);\n        }\n        denseBuffer = null;\n      }\n\n      cardinality += currentBlockCardinality;\n      denseBuffer = null;\n      currentBlockCardinality = 0;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/RoaringDocIdSet.Builder#flush().mjava","sourceNew":"    private void flush() {\n      assert currentBlockCardinality <= BLOCK_SIZE;\n      if (currentBlockCardinality <= MAX_ARRAY_LENGTH) {\n        // Use sparse encoding\n        assert denseBuffer == null;\n        if (currentBlockCardinality > 0) {\n          sets[currentBlock] = new ShortArrayDocIdSet(ArrayUtil.copyOfSubArray(buffer, 0, currentBlockCardinality));\n        }\n      } else {\n        assert denseBuffer != null;\n        assert denseBuffer.cardinality() == currentBlockCardinality;\n        if (denseBuffer.length() == BLOCK_SIZE && BLOCK_SIZE - currentBlockCardinality < MAX_ARRAY_LENGTH) {\n          // Doc ids are very dense, inverse the encoding\n          final short[] excludedDocs = new short[BLOCK_SIZE - currentBlockCardinality];\n          denseBuffer.flip(0, denseBuffer.length());\n          int excludedDoc = -1;\n          for (int i = 0; i < excludedDocs.length; ++i) {\n            excludedDoc = denseBuffer.nextSetBit(excludedDoc + 1);\n            assert excludedDoc != DocIdSetIterator.NO_MORE_DOCS;\n            excludedDocs[i] = (short) excludedDoc;\n          }\n          assert excludedDoc + 1 == denseBuffer.length() || denseBuffer.nextSetBit(excludedDoc + 1) == DocIdSetIterator.NO_MORE_DOCS;\n          sets[currentBlock] = new NotDocIdSet(BLOCK_SIZE, new ShortArrayDocIdSet(excludedDocs));\n        } else {\n          // Neither sparse nor super dense, use a fixed bit set\n          sets[currentBlock] = new BitDocIdSet(denseBuffer, currentBlockCardinality);\n        }\n        denseBuffer = null;\n      }\n\n      cardinality += currentBlockCardinality;\n      denseBuffer = null;\n      currentBlockCardinality = 0;\n    }\n\n","sourceOld":"    private void flush() {\n      assert currentBlockCardinality <= BLOCK_SIZE;\n      if (currentBlockCardinality <= MAX_ARRAY_LENGTH) {\n        // Use sparse encoding\n        assert denseBuffer == null;\n        if (currentBlockCardinality > 0) {\n          sets[currentBlock] = new ShortArrayDocIdSet(Arrays.copyOf(buffer, currentBlockCardinality));\n        }\n      } else {\n        assert denseBuffer != null;\n        assert denseBuffer.cardinality() == currentBlockCardinality;\n        if (denseBuffer.length() == BLOCK_SIZE && BLOCK_SIZE - currentBlockCardinality < MAX_ARRAY_LENGTH) {\n          // Doc ids are very dense, inverse the encoding\n          final short[] excludedDocs = new short[BLOCK_SIZE - currentBlockCardinality];\n          denseBuffer.flip(0, denseBuffer.length());\n          int excludedDoc = -1;\n          for (int i = 0; i < excludedDocs.length; ++i) {\n            excludedDoc = denseBuffer.nextSetBit(excludedDoc + 1);\n            assert excludedDoc != DocIdSetIterator.NO_MORE_DOCS;\n            excludedDocs[i] = (short) excludedDoc;\n          }\n          assert excludedDoc + 1 == denseBuffer.length() || denseBuffer.nextSetBit(excludedDoc + 1) == DocIdSetIterator.NO_MORE_DOCS;\n          sets[currentBlock] = new NotDocIdSet(BLOCK_SIZE, new ShortArrayDocIdSet(excludedDocs));\n        } else {\n          // Neither sparse nor super dense, use a fixed bit set\n          sets[currentBlock] = new BitDocIdSet(denseBuffer, currentBlockCardinality);\n        }\n        denseBuffer = null;\n      }\n\n      cardinality += currentBlockCardinality;\n      denseBuffer = null;\n      currentBlockCardinality = 0;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9cd756794a4a9e33f786119818f9d56162965cf8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"55980207f1977bd1463465de1659b821347e2fa8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","9cd756794a4a9e33f786119818f9d56162965cf8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["0abcec02c9851c46c70a75bd42fb6e4d5348ac9e","9798d0818e7a880546802b509792d3f3d57babd2"],"80c55596a764e2d397e982828e75fcac5ce430a0":["9cd756794a4a9e33f786119818f9d56162965cf8"],"9798d0818e7a880546802b509792d3f3d57babd2":["0abcec02c9851c46c70a75bd42fb6e4d5348ac9e"],"0abcec02c9851c46c70a75bd42fb6e4d5348ac9e":["80c55596a764e2d397e982828e75fcac5ce430a0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9798d0818e7a880546802b509792d3f3d57babd2"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["0abcec02c9851c46c70a75bd42fb6e4d5348ac9e","9798d0818e7a880546802b509792d3f3d57babd2"]},"commit2Childs":{"9cd756794a4a9e33f786119818f9d56162965cf8":["55980207f1977bd1463465de1659b821347e2fa8","80c55596a764e2d397e982828e75fcac5ce430a0"],"55980207f1977bd1463465de1659b821347e2fa8":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9cd756794a4a9e33f786119818f9d56162965cf8","55980207f1977bd1463465de1659b821347e2fa8"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"80c55596a764e2d397e982828e75fcac5ce430a0":["0abcec02c9851c46c70a75bd42fb6e4d5348ac9e"],"9798d0818e7a880546802b509792d3f3d57babd2":["b70042a8a492f7054d480ccdd2be9796510d4327","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"0abcec02c9851c46c70a75bd42fb6e4d5348ac9e":["b70042a8a492f7054d480ccdd2be9796510d4327","9798d0818e7a880546802b509792d3f3d57babd2","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["55980207f1977bd1463465de1659b821347e2fa8","b70042a8a492f7054d480ccdd2be9796510d4327","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}