{"path":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","commits":[{"id":"5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69","date":1352818449,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (triggerFlush()) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (triggerFlush()) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (triggerFlush()) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (triggerFlush()) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc7a7bb1aa79cf53564793bb5ffa270250c679da","date":1357817084,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (triggerFlush()) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["c33c9c4ca6dc47739595c708779c537e8fb8813d","b547be573e7e33d9a40568f89ceb5642382c96e2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4e6354dd7c71fe122926fc53d7d29f715b1283db","date":1357915185,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              endWithPreviousDocument();\n              if (triggerFlush()) {\n                flush();\n              }\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"47081d784f5fff71bb715c806c824b50901392fb","date":1378303234,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT) { // means reader version is not the same as the writer version\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null) {\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["b547be573e7e33d9a40568f89ceb5642382c96e2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b547be573e7e33d9a40568f89ceb5642382c96e2","date":1384532484,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT) { // means reader version is not the same as the writer version\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":["7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa","fc7a7bb1aa79cf53564793bb5ffa270250c679da","47081d784f5fff71bb715c806c824b50901392fb"],"bugIntro":["c33c9c4ca6dc47739595c708779c537e8fb8813d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT) { // means reader version is not the same as the writer version\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode\n                && numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ba718737348b631d49101cf505322f868a130196","date":1397033548,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"52c7e49be259508735752fba88085255014a6ecf","date":1398706273,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument();\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                numStoredFieldsInDoc = it.numStoredFields[diff];\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["c33c9c4ca6dc47739595c708779c537e8fb8813d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3394716f52b34ab259ad5247e7595d9f9db6e935","date":1398791921,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument();\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                numStoredFieldsInDoc = it.numStoredFields[diff];\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","date":1398844771,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument();\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                numStoredFieldsInDoc = it.numStoredFields[diff];\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument(it.numStoredFields[diff]);\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c33c9c4ca6dc47739595c708779c537e8fb8813d","date":1399378909,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            if (numBufferedDocs == 0 // starting a new chunk\n                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough\n                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk\n              assert docID == it.docBase;\n\n              // no need to decompress, just copy data\n              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());\n              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);\n              it.copyCompressedData(fieldsStream);\n              this.docBase += it.chunkDocs;\n              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);\n              docCount += it.chunkDocs;\n              mergeState.checkAbort.work(300 * it.chunkDocs);\n            } else {\n              // decompress\n              it.decompress();\n              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n                throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n              }\n              // copy non-deleted docs\n              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n                final int diff = docID - it.docBase;\n                startDocument();\n                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n                numStoredFieldsInDoc = it.numStoredFields[diff];\n                finishDocument();\n                ++docCount;\n                mergeState.checkAbort.work(300);\n              }\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":["7d1467e0527cb2aeb9d7a05c26948ac9d82d81fa","b547be573e7e33d9a40568f89ceb5642382c96e2","5af6a67fb827380f7fe2fdf3baa34b10b783f2f1","fc7a7bb1aa79cf53564793bb5ffa270250c679da","eba3cb2a268b9fb6f5be011fbaaf698699dcf24c","52c7e49be259508735752fba88085255014a6ecf"],"bugIntro":["9a70ce9bddc6f985feb8e5e182aebe20872328d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"22a2e66dfda83847e80095b8693c660742ab3e9c","date":1408628796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n\n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9a70ce9bddc6f985feb8e5e182aebe20872328d4","date":1411172748,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length, it.fieldsStream);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":["c33c9c4ca6dc47739595c708779c537e8fb8813d"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (LeafReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length, it.fieldsStream);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (AtomicReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length, it.fieldsStream);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2131047ecceac64b54ba70feec3d26bbd7e483d7","date":1411862069,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          DocumentStoredFieldVisitor visitor = new DocumentStoredFieldVisitor();\n          storedFieldsReader.visitDocument(i, visitor);\n          addDocument(visitor.getDocument(), mergeState.mergeFieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length, it.fieldsStream);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (LeafReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length, it.fieldsStream);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a661533765521d46fcaf46aee272301b6afb6376","date":1411919137,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          DocumentStoredFieldVisitor visitor = new DocumentStoredFieldVisitor();\n          storedFieldsReader.visitDocument(i, visitor);\n          addDocument(visitor.getDocument(), mergeState.mergeFieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length, it.fieldsStream);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          DocumentStoredFieldVisitor visitor = new DocumentStoredFieldVisitor();\n          storedFieldsReader.visitDocument(i, visitor);\n          addDocument(visitor.getDocument(), mergeState.mergeFieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length, it.fieldsStream);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          DocumentStoredFieldVisitor visitor = new DocumentStoredFieldVisitor();\n          storedFieldsReader.visitDocument(i, visitor);\n          addDocument(visitor.getDocument(), mergeState.mergeFieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length, it.fieldsStream);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int idx = 0;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (LeafReader reader : mergeState.readers) {\n      final SegmentReader matchingSegmentReader = matching.matchingSegmentReaders[idx++];\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matchingSegmentReader != null) {\n        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = reader.maxDoc();\n      final Bits liveDocs = reader.getLiveDocs();\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          StoredDocument doc = reader.document(i);\n          addDocument(doc, mergeState.fieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length, it.fieldsStream);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.fieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1f09f483a0844bb9dc34fb10380cb053aa96219b","date":1418894001,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          DocumentStoredFieldVisitor visitor = new DocumentStoredFieldVisitor();\n          storedFieldsReader.visitDocument(docID, visitor);\n          addDocument(visitor.getDocument(), mergeState.mergeFieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        matchingFieldsReader.checkIntegrity();\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {\n          DocumentStoredFieldVisitor visitor = new DocumentStoredFieldVisitor();\n          storedFieldsReader.visitDocument(i, visitor);\n          addDocument(visitor.getDocument(), mergeState.mergeFieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        int docID = nextLiveDoc(0, liveDocs, maxDoc);\n        if (docID < maxDoc) {\n          // not all docs were deleted\n          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);\n          int[] startOffsets = new int[0];\n          do {\n            // go to the next chunk that contains docID\n            it.next(docID);\n            // transform lengths into offsets\n            if (startOffsets.length < it.chunkDocs) {\n              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];\n            }\n            for (int i = 1; i < it.chunkDocs; ++i) {\n              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];\n            }\n\n            // decompress\n            it.decompress();\n            if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {\n              throw new CorruptIndexException(\"Corrupted: expected chunk size=\" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + \", got \" + it.bytes.length, it.fieldsStream);\n            }\n            // copy non-deleted docs\n            for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {\n              final int diff = docID - it.docBase;\n              startDocument();\n              bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);\n              numStoredFieldsInDoc = it.numStoredFields[diff];\n              finishDocument();\n              ++docCount;\n              mergeState.checkAbort.work(300);\n            }\n          } while (docID < maxDoc);\n\n          it.checkIntegrity();\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"292509a05117b3f385b4b7258087c7c71fa1cc41","date":1419782107,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          DocumentStoredFieldVisitor visitor = new DocumentStoredFieldVisitor();\n          storedFieldsReader.visitDocument(docID, visitor);\n          addDocument(visitor.getDocument(), mergeState.mergeFieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        matchingFieldsReader.checkIntegrity();\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      if (matchingFieldsReader == null\n          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version\n          || matchingFieldsReader.getCompressionMode() != compressionMode\n          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          DocumentStoredFieldVisitor visitor = new DocumentStoredFieldVisitor();\n          storedFieldsReader.visitDocument(docID, visitor);\n          addDocument(visitor.getDocument(), mergeState.mergeFieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        matchingFieldsReader.checkIntegrity();\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f756f355450b30d33fe4479d81dad3e4d100ded4","date":1419858140,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        matchingFieldsReader.checkIntegrity();\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          DocumentStoredFieldVisitor visitor = new DocumentStoredFieldVisitor();\n          storedFieldsReader.visitDocument(docID, visitor);\n          addDocument(visitor.getDocument(), mergeState.mergeFieldInfos);\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        matchingFieldsReader.checkIntegrity();\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5faf65b6692f15cca0f87bf8666c87899afc619f","date":1420468108,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        matchingFieldsReader.checkIntegrity();\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        matchingFieldsReader.checkIntegrity();\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n          mergeState.checkAbort.work(300);\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bd7962f4da329a4e559727022b752c5cefaee5da","date":1421356185,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        matchingFieldsReader.checkIntegrity();\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"95d12d386a346adadc5b4ee9224494c4107c4e97","date":1421386314,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8","date":1462567286,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"653128722fb3b4713ac331c621491a93f34a4a22","date":1479841816,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"727bb765ff2542275f6d31f67be18d7104bae148","date":1480353976,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.segmentInfo.getIndexSort() != null) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"86a0a50d2d14aaee1e635bbec914468551f7f9a2","date":1482234306,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    if (mergeState.needsIndexSort) {\n      /**\n       * If all readers are compressed and they have the same fieldinfos then we can merge the serialized document\n       * directly.\n       */\n      List<CompressingStoredFieldsMergeSub> subs = new ArrayList<>();\n      for(int i=0;i<mergeState.storedFieldsReaders.length;i++) {\n        if (matching.matchingReaders[i] &&\n            mergeState.storedFieldsReaders[i] instanceof CompressingStoredFieldsReader) {\n          CompressingStoredFieldsReader storedFieldsReader = (CompressingStoredFieldsReader) mergeState.storedFieldsReaders[i];\n          storedFieldsReader.checkIntegrity();\n          subs.add(new CompressingStoredFieldsMergeSub(storedFieldsReader, mergeState.docMaps[i], mergeState.maxDocs[i]));\n        } else {\n          return super.merge(mergeState);\n        }\n      }\n\n      final DocIDMerger<CompressingStoredFieldsMergeSub> docIDMerger =\n          new DocIDMerger<>(subs, true);\n      while (true) {\n        CompressingStoredFieldsMergeSub sub = docIDMerger.next();\n        if (sub == null) {\n          break;\n        }\n        assert sub.mappedDocID == docCount;\n        SerializedDocument doc = sub.reader.document(sub.docID);\n        startDocument();\n        bufferedDocs.copyBytes(doc.in, doc.length);\n        numStoredFieldsInDoc = doc.numStoredFields;\n        finishDocument();\n        ++docCount;\n      }\n      finish(mergeState.mergeFieldInfos, docCount);\n      return docCount;\n    }\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":["653128722fb3b4713ac331c621491a93f34a4a22","fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","date":1482251961,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    if (mergeState.needsIndexSort) {\n      /**\n       * If all readers are compressed and they have the same fieldinfos then we can merge the serialized document\n       * directly.\n       */\n      List<CompressingStoredFieldsMergeSub> subs = new ArrayList<>();\n      for(int i=0;i<mergeState.storedFieldsReaders.length;i++) {\n        if (matching.matchingReaders[i] &&\n            mergeState.storedFieldsReaders[i] instanceof CompressingStoredFieldsReader) {\n          CompressingStoredFieldsReader storedFieldsReader = (CompressingStoredFieldsReader) mergeState.storedFieldsReaders[i];\n          storedFieldsReader.checkIntegrity();\n          subs.add(new CompressingStoredFieldsMergeSub(storedFieldsReader, mergeState.docMaps[i], mergeState.maxDocs[i]));\n        } else {\n          return super.merge(mergeState);\n        }\n      }\n\n      final DocIDMerger<CompressingStoredFieldsMergeSub> docIDMerger =\n          new DocIDMerger<>(subs, true);\n      while (true) {\n        CompressingStoredFieldsMergeSub sub = docIDMerger.next();\n        if (sub == null) {\n          break;\n        }\n        assert sub.mappedDocID == docCount;\n        SerializedDocument doc = sub.reader.document(sub.docID);\n        startDocument();\n        bufferedDocs.copyBytes(doc.in, doc.length);\n        numStoredFieldsInDoc = doc.numStoredFields;\n        finishDocument();\n        ++docCount;\n      }\n      finish(mergeState.mergeFieldInfos, docCount);\n      return docCount;\n    }\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    if (mergeState.needsIndexSort) {\n      // TODO: can we gain back some optos even if index is sorted?  E.g. if sort results in large chunks of contiguous docs from one sub\n      // being copied over...?\n      return super.merge(mergeState);\n    }\n\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d85b6e22926e7564c040d2a864f4887f6c59fa92","date":1482349496,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    if (mergeState.needsIndexSort) {\n      /**\n       * If all readers are compressed and they have the same fieldinfos then we can merge the serialized document\n       * directly.\n       */\n      List<CompressingStoredFieldsMergeSub> subs = new ArrayList<>();\n      for(int i=0;i<mergeState.storedFieldsReaders.length;i++) {\n        if (matching.matchingReaders[i] &&\n            mergeState.storedFieldsReaders[i] instanceof CompressingStoredFieldsReader) {\n          CompressingStoredFieldsReader storedFieldsReader = (CompressingStoredFieldsReader) mergeState.storedFieldsReaders[i];\n          storedFieldsReader.checkIntegrity();\n          subs.add(new CompressingStoredFieldsMergeSub(storedFieldsReader, mergeState.docMaps[i], mergeState.maxDocs[i]));\n        } else {\n          return super.merge(mergeState);\n        }\n      }\n\n      final DocIDMerger<CompressingStoredFieldsMergeSub> docIDMerger =\n          DocIDMerger.of(subs, true);\n      while (true) {\n        CompressingStoredFieldsMergeSub sub = docIDMerger.next();\n        if (sub == null) {\n          break;\n        }\n        assert sub.mappedDocID == docCount;\n        SerializedDocument doc = sub.reader.document(sub.docID);\n        startDocument();\n        bufferedDocs.copyBytes(doc.in, doc.length);\n        numStoredFieldsInDoc = doc.numStoredFields;\n        finishDocument();\n        ++docCount;\n      }\n      finish(mergeState.mergeFieldInfos, docCount);\n      return docCount;\n    }\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    if (mergeState.needsIndexSort) {\n      /**\n       * If all readers are compressed and they have the same fieldinfos then we can merge the serialized document\n       * directly.\n       */\n      List<CompressingStoredFieldsMergeSub> subs = new ArrayList<>();\n      for(int i=0;i<mergeState.storedFieldsReaders.length;i++) {\n        if (matching.matchingReaders[i] &&\n            mergeState.storedFieldsReaders[i] instanceof CompressingStoredFieldsReader) {\n          CompressingStoredFieldsReader storedFieldsReader = (CompressingStoredFieldsReader) mergeState.storedFieldsReaders[i];\n          storedFieldsReader.checkIntegrity();\n          subs.add(new CompressingStoredFieldsMergeSub(storedFieldsReader, mergeState.docMaps[i], mergeState.maxDocs[i]));\n        } else {\n          return super.merge(mergeState);\n        }\n      }\n\n      final DocIDMerger<CompressingStoredFieldsMergeSub> docIDMerger =\n          new DocIDMerger<>(subs, true);\n      while (true) {\n        CompressingStoredFieldsMergeSub sub = docIDMerger.next();\n        if (sub == null) {\n          break;\n        }\n        assert sub.mappedDocID == docCount;\n        SerializedDocument doc = sub.reader.document(sub.docID);\n        startDocument();\n        bufferedDocs.copyBytes(doc.in, doc.length);\n        numStoredFieldsInDoc = doc.numStoredFields;\n        finishDocument();\n        ++docCount;\n      }\n      finish(mergeState.mergeFieldInfos, docCount);\n      return docCount;\n    }\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    if (mergeState.needsIndexSort) {\n      /**\n       * If all readers are compressed and they have the same fieldinfos then we can merge the serialized document\n       * directly.\n       */\n      List<CompressingStoredFieldsMergeSub> subs = new ArrayList<>();\n      for(int i=0;i<mergeState.storedFieldsReaders.length;i++) {\n        if (matching.matchingReaders[i] &&\n            mergeState.storedFieldsReaders[i] instanceof CompressingStoredFieldsReader) {\n          CompressingStoredFieldsReader storedFieldsReader = (CompressingStoredFieldsReader) mergeState.storedFieldsReaders[i];\n          storedFieldsReader.checkIntegrity();\n          subs.add(new CompressingStoredFieldsMergeSub(storedFieldsReader, mergeState.docMaps[i], mergeState.maxDocs[i]));\n        } else {\n          return super.merge(mergeState);\n        }\n      }\n\n      final DocIDMerger<CompressingStoredFieldsMergeSub> docIDMerger =\n          DocIDMerger.of(subs, true);\n      while (true) {\n        CompressingStoredFieldsMergeSub sub = docIDMerger.next();\n        if (sub == null) {\n          break;\n        }\n        assert sub.mappedDocID == docCount;\n        SerializedDocument doc = sub.reader.document(sub.docID);\n        startDocument();\n        bufferedDocs.copyBytes(doc.in, doc.length);\n        numStoredFieldsInDoc = doc.numStoredFields;\n        finishDocument();\n        ++docCount;\n      }\n      finish(mergeState.mergeFieldInfos, docCount);\n      return docCount;\n    }\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    if (mergeState.needsIndexSort) {\n      /**\n       * If all readers are compressed and they have the same fieldinfos then we can merge the serialized document\n       * directly.\n       */\n      List<CompressingStoredFieldsMergeSub> subs = new ArrayList<>();\n      for(int i=0;i<mergeState.storedFieldsReaders.length;i++) {\n        if (matching.matchingReaders[i] &&\n            mergeState.storedFieldsReaders[i] instanceof CompressingStoredFieldsReader) {\n          CompressingStoredFieldsReader storedFieldsReader = (CompressingStoredFieldsReader) mergeState.storedFieldsReaders[i];\n          storedFieldsReader.checkIntegrity();\n          subs.add(new CompressingStoredFieldsMergeSub(storedFieldsReader, mergeState.docMaps[i], mergeState.maxDocs[i]));\n        } else {\n          return super.merge(mergeState);\n        }\n      }\n\n      final DocIDMerger<CompressingStoredFieldsMergeSub> docIDMerger =\n          new DocIDMerger<>(subs, true);\n      while (true) {\n        CompressingStoredFieldsMergeSub sub = docIDMerger.next();\n        if (sub == null) {\n          break;\n        }\n        assert sub.mappedDocID == docCount;\n        SerializedDocument doc = sub.reader.document(sub.docID);\n        startDocument();\n        bufferedDocs.copyBytes(doc.in, doc.length);\n        numStoredFieldsInDoc = doc.numStoredFields;\n        finishDocument();\n        ++docCount;\n      }\n      finish(mergeState.mergeFieldInfos, docCount);\n      return docCount;\n    }\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70a4487b07c49a1861c05720e04624826ecbe9fa","date":1580924108,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    if (mergeState.needsIndexSort) {\n      /**\n       * If all readers are compressed and they have the same fieldinfos then we can merge the serialized document\n       * directly.\n       */\n      List<CompressingStoredFieldsMergeSub> subs = new ArrayList<>();\n      for(int i=0;i<mergeState.storedFieldsReaders.length;i++) {\n        if (matching.matchingReaders[i] &&\n            mergeState.storedFieldsReaders[i] instanceof CompressingStoredFieldsReader) {\n          CompressingStoredFieldsReader storedFieldsReader = (CompressingStoredFieldsReader) mergeState.storedFieldsReaders[i];\n          storedFieldsReader.checkIntegrity();\n          subs.add(new CompressingStoredFieldsMergeSub(storedFieldsReader, mergeState.docMaps[i], mergeState.maxDocs[i]));\n        } else {\n          return super.merge(mergeState);\n        }\n      }\n\n      final DocIDMerger<CompressingStoredFieldsMergeSub> docIDMerger =\n          DocIDMerger.of(subs, true);\n      while (true) {\n        CompressingStoredFieldsMergeSub sub = docIDMerger.next();\n        if (sub == null) {\n          break;\n        }\n        assert sub.mappedDocID == docCount;\n        SerializedDocument doc = sub.reader.document(sub.docID);\n        startDocument();\n        bufferedDocs.copyBytes(doc.in, doc.length);\n        numStoredFieldsInDoc = doc.numStoredFields;\n        finishDocument();\n        ++docCount;\n      }\n      finish(mergeState.mergeFieldInfos, docCount);\n      return docCount;\n    }\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        FieldsIndex index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    if (mergeState.needsIndexSort) {\n      /**\n       * If all readers are compressed and they have the same fieldinfos then we can merge the serialized document\n       * directly.\n       */\n      List<CompressingStoredFieldsMergeSub> subs = new ArrayList<>();\n      for(int i=0;i<mergeState.storedFieldsReaders.length;i++) {\n        if (matching.matchingReaders[i] &&\n            mergeState.storedFieldsReaders[i] instanceof CompressingStoredFieldsReader) {\n          CompressingStoredFieldsReader storedFieldsReader = (CompressingStoredFieldsReader) mergeState.storedFieldsReaders[i];\n          storedFieldsReader.checkIntegrity();\n          subs.add(new CompressingStoredFieldsMergeSub(storedFieldsReader, mergeState.docMaps[i], mergeState.maxDocs[i]));\n        } else {\n          return super.merge(mergeState);\n        }\n      }\n\n      final DocIDMerger<CompressingStoredFieldsMergeSub> docIDMerger =\n          DocIDMerger.of(subs, true);\n      while (true) {\n        CompressingStoredFieldsMergeSub sub = docIDMerger.next();\n        if (sub == null) {\n          break;\n        }\n        assert sub.mappedDocID == docCount;\n        SerializedDocument doc = sub.reader.document(sub.docID);\n        startDocument();\n        bufferedDocs.copyBytes(doc.in, doc.length);\n        numStoredFieldsInDoc = doc.numStoredFields;\n        finishDocument();\n        ++docCount;\n      }\n      finish(mergeState.mergeFieldInfos, docCount);\n      return docCount;\n    }\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        CompressingStoredFieldsIndexReader index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45264aed0cfa8a8a55ae1292b0e336d29cd88401","date":1600361948,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter#merge(MergeState).mjava","sourceNew":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    if (mergeState.needsIndexSort) {\n      /**\n       * If all readers are compressed and they have the same fieldinfos then we can merge the serialized document\n       * directly.\n       */\n      List<CompressingStoredFieldsMergeSub> subs = new ArrayList<>();\n      for(int i=0;i<mergeState.storedFieldsReaders.length;i++) {\n        if (matching.matchingReaders[i] &&\n            mergeState.storedFieldsReaders[i] instanceof CompressingStoredFieldsReader) {\n          CompressingStoredFieldsReader storedFieldsReader = (CompressingStoredFieldsReader) mergeState.storedFieldsReaders[i];\n          storedFieldsReader.checkIntegrity();\n          subs.add(new CompressingStoredFieldsMergeSub(storedFieldsReader, mergeState.docMaps[i], mergeState.maxDocs[i]));\n        } else {\n          return super.merge(mergeState);\n        }\n      }\n\n      final DocIDMerger<CompressingStoredFieldsMergeSub> docIDMerger =\n          DocIDMerger.of(subs, true);\n      while (true) {\n        CompressingStoredFieldsMergeSub sub = docIDMerger.next();\n        if (sub == null) {\n          break;\n        }\n        assert sub.mappedDocID == docCount;\n        SerializedDocument doc = sub.reader.document(sub.docID);\n        startDocument();\n        bufferedDocs.copyBytes(doc.in, doc.length);\n        numStoredFieldsInDoc = doc.numStoredFields;\n        finishDocument();\n        ++docCount;\n      }\n      finish(mergeState.mergeFieldInfos, docCount);\n      return docCount;\n    }\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        FieldsIndex index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n        numDirtyDocs += matchingFieldsReader.getNumDirtyDocs();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","sourceOld":"  @Override\n  public int merge(MergeState mergeState) throws IOException {\n    int docCount = 0;\n    int numReaders = mergeState.maxDocs.length;\n    \n    MatchingReaders matching = new MatchingReaders(mergeState);\n    if (mergeState.needsIndexSort) {\n      /**\n       * If all readers are compressed and they have the same fieldinfos then we can merge the serialized document\n       * directly.\n       */\n      List<CompressingStoredFieldsMergeSub> subs = new ArrayList<>();\n      for(int i=0;i<mergeState.storedFieldsReaders.length;i++) {\n        if (matching.matchingReaders[i] &&\n            mergeState.storedFieldsReaders[i] instanceof CompressingStoredFieldsReader) {\n          CompressingStoredFieldsReader storedFieldsReader = (CompressingStoredFieldsReader) mergeState.storedFieldsReaders[i];\n          storedFieldsReader.checkIntegrity();\n          subs.add(new CompressingStoredFieldsMergeSub(storedFieldsReader, mergeState.docMaps[i], mergeState.maxDocs[i]));\n        } else {\n          return super.merge(mergeState);\n        }\n      }\n\n      final DocIDMerger<CompressingStoredFieldsMergeSub> docIDMerger =\n          DocIDMerger.of(subs, true);\n      while (true) {\n        CompressingStoredFieldsMergeSub sub = docIDMerger.next();\n        if (sub == null) {\n          break;\n        }\n        assert sub.mappedDocID == docCount;\n        SerializedDocument doc = sub.reader.document(sub.docID);\n        startDocument();\n        bufferedDocs.copyBytes(doc.in, doc.length);\n        numStoredFieldsInDoc = doc.numStoredFields;\n        finishDocument();\n        ++docCount;\n      }\n      finish(mergeState.mergeFieldInfos, docCount);\n      return docCount;\n    }\n    \n    for (int readerIndex=0;readerIndex<numReaders;readerIndex++) {\n      MergeVisitor visitor = new MergeVisitor(mergeState, readerIndex);\n      CompressingStoredFieldsReader matchingFieldsReader = null;\n      if (matching.matchingReaders[readerIndex]) {\n        final StoredFieldsReader fieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader\n        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {\n          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;\n        }\n      }\n\n      final int maxDoc = mergeState.maxDocs[readerIndex];\n      final Bits liveDocs = mergeState.liveDocs[readerIndex];\n\n      // if its some other format, or an older version of this format, or safety switch:\n      if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT || BULK_MERGE_ENABLED == false) {\n        // naive merge...\n        StoredFieldsReader storedFieldsReader = mergeState.storedFieldsReaders[readerIndex];\n        if (storedFieldsReader != null) {\n          storedFieldsReader.checkIntegrity();\n        }\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          startDocument();\n          storedFieldsReader.visitDocument(docID, visitor);\n          finishDocument();\n          ++docCount;\n        }\n      } else if (matchingFieldsReader.getCompressionMode() == compressionMode && \n                 matchingFieldsReader.getChunkSize() == chunkSize && \n                 matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &&\n                 liveDocs == null &&\n                 !tooDirty(matchingFieldsReader)) { \n        // optimized merge, raw byte copy\n        // its not worth fine-graining this if there are deletions.\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;        \n        matchingFieldsReader.checkIntegrity();\n        \n        // flush any pending chunks\n        if (numBufferedDocs > 0) {\n          flush();\n          numDirtyChunks++; // incomplete: we had to force this flush\n        }\n        \n        // iterate over each chunk. we use the stored fields index to find chunk boundaries,\n        // read the docstart + doccount from the chunk header (we write a new header, since doc numbers will change),\n        // and just copy the bytes directly.\n        IndexInput rawDocs = matchingFieldsReader.getFieldsStream();\n        FieldsIndex index = matchingFieldsReader.getIndexReader();\n        rawDocs.seek(index.getStartPointer(0));\n        int docID = 0;\n        while (docID < maxDoc) {\n          // read header\n          int base = rawDocs.readVInt();\n          if (base != docID) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", docID=\" + docID, rawDocs);\n          }\n          int code = rawDocs.readVInt();\n          \n          // write a new index entry and new header for this chunk.\n          int bufferedDocs = code >>> 1;\n          indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());\n          fieldsStream.writeVInt(docBase); // rebase\n          fieldsStream.writeVInt(code);\n          docID += bufferedDocs;\n          docBase += bufferedDocs;\n          docCount += bufferedDocs;\n          \n          if (docID > maxDoc) {\n            throw new CorruptIndexException(\"invalid state: base=\" + base + \", count=\" + bufferedDocs + \", maxDoc=\" + maxDoc, rawDocs);\n          }\n          \n          // copy bytes until the next chunk boundary (or end of chunk data).\n          // using the stored fields index for this isn't the most efficient, but fast enough\n          // and is a source of redundancy for detecting bad things.\n          final long end;\n          if (docID == maxDoc) {\n            end = matchingFieldsReader.getMaxPointer();\n          } else {\n            end = index.getStartPointer(docID);\n          }\n          fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());\n        }\n               \n        if (rawDocs.getFilePointer() != matchingFieldsReader.getMaxPointer()) {\n          throw new CorruptIndexException(\"invalid state: pos=\" + rawDocs.getFilePointer() + \", max=\" + matchingFieldsReader.getMaxPointer(), rawDocs);\n        }\n        \n        // since we bulk merged all chunks, we inherit any dirty ones from this segment.\n        numChunks += matchingFieldsReader.getNumChunks();\n        numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();\n      } else {\n        // optimized merge, we copy serialized (but decompressed) bytes directly\n        // even on simple docs (1 stored field), it seems to help by about 20%\n        \n        // if the format is older, its always handled by the naive merge case above\n        assert matchingFieldsReader.getVersion() == VERSION_CURRENT;\n        matchingFieldsReader.checkIntegrity();\n\n        for (int docID = 0; docID < maxDoc; docID++) {\n          if (liveDocs != null && liveDocs.get(docID) == false) {\n            continue;\n          }\n          SerializedDocument doc = matchingFieldsReader.document(docID);\n          startDocument();\n          bufferedDocs.copyBytes(doc.in, doc.length);\n          numStoredFieldsInDoc = doc.numStoredFields;\n          finishDocument();\n          ++docCount;\n        }\n      }\n    }\n    finish(mergeState.mergeFieldInfos, docCount);\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c33c9c4ca6dc47739595c708779c537e8fb8813d":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"d85b6e22926e7564c040d2a864f4887f6c59fa92":["86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"0ad30c6a479e764150a3316e57263319775f1df2":["95d12d386a346adadc5b4ee9224494c4107c4e97","3d33e731a93d4b57e662ff094f64f94a745422d4"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["9a70ce9bddc6f985feb8e5e182aebe20872328d4"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["95d12d386a346adadc5b4ee9224494c4107c4e97","0ad30c6a479e764150a3316e57263319775f1df2"],"70a4487b07c49a1861c05720e04624826ecbe9fa":["d85b6e22926e7564c040d2a864f4887f6c59fa92"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":["727bb765ff2542275f6d31f67be18d7104bae148","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"9bb9a29a5e71a90295f175df8919802993142c9a":["c9fb5f46e264daf5ba3860defe623a89d202dd87","a661533765521d46fcaf46aee272301b6afb6376"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"45264aed0cfa8a8a55ae1292b0e336d29cd88401":["70a4487b07c49a1861c05720e04624826ecbe9fa"],"47081d784f5fff71bb715c806c824b50901392fb":["fc7a7bb1aa79cf53564793bb5ffa270250c679da"],"fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8":["95d12d386a346adadc5b4ee9224494c4107c4e97"],"4e6354dd7c71fe122926fc53d7d29f715b1283db":["407687e67faf6e1f02a211ca078d8e3eed631027","fc7a7bb1aa79cf53564793bb5ffa270250c679da"],"95d12d386a346adadc5b4ee9224494c4107c4e97":["bd7962f4da329a4e559727022b752c5cefaee5da"],"292509a05117b3f385b4b7258087c7c71fa1cc41":["1f09f483a0844bb9dc34fb10380cb053aa96219b"],"b547be573e7e33d9a40568f89ceb5642382c96e2":["47081d784f5fff71bb715c806c824b50901392fb"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":["ba718737348b631d49101cf505322f868a130196","3394716f52b34ab259ad5247e7595d9f9db6e935"],"a661533765521d46fcaf46aee272301b6afb6376":["2131047ecceac64b54ba70feec3d26bbd7e483d7"],"407687e67faf6e1f02a211ca078d8e3eed631027":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69"],"f756f355450b30d33fe4479d81dad3e4d100ded4":["292509a05117b3f385b4b7258087c7c71fa1cc41"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["95d12d386a346adadc5b4ee9224494c4107c4e97","d470c8182e92b264680e34081b75e70a9f2b3c89"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf","d85b6e22926e7564c040d2a864f4887f6c59fa92"],"3394716f52b34ab259ad5247e7595d9f9db6e935":["ba718737348b631d49101cf505322f868a130196","52c7e49be259508735752fba88085255014a6ecf"],"9a70ce9bddc6f985feb8e5e182aebe20872328d4":["22a2e66dfda83847e80095b8693c660742ab3e9c"],"727bb765ff2542275f6d31f67be18d7104bae148":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","653128722fb3b4713ac331c621491a93f34a4a22"],"5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"fc7a7bb1aa79cf53564793bb5ffa270250c679da":["5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69"],"1f09f483a0844bb9dc34fb10380cb053aa96219b":["9bb9a29a5e71a90295f175df8919802993142c9a"],"2131047ecceac64b54ba70feec3d26bbd7e483d7":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"bd7962f4da329a4e559727022b752c5cefaee5da":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"22a2e66dfda83847e80095b8693c660742ab3e9c":["c33c9c4ca6dc47739595c708779c537e8fb8813d"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["47081d784f5fff71bb715c806c824b50901392fb","b547be573e7e33d9a40568f89ceb5642382c96e2"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["f756f355450b30d33fe4479d81dad3e4d100ded4"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["95d12d386a346adadc5b4ee9224494c4107c4e97","fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["653128722fb3b4713ac331c621491a93f34a4a22"],"653128722fb3b4713ac331c621491a93f34a4a22":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"ba718737348b631d49101cf505322f868a130196":["b547be573e7e33d9a40568f89ceb5642382c96e2"],"52c7e49be259508735752fba88085255014a6ecf":["ba718737348b631d49101cf505322f868a130196"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["45264aed0cfa8a8a55ae1292b0e336d29cd88401"]},"commit2Childs":{"c33c9c4ca6dc47739595c708779c537e8fb8813d":["22a2e66dfda83847e80095b8693c660742ab3e9c"],"d85b6e22926e7564c040d2a864f4887f6c59fa92":["70a4487b07c49a1861c05720e04624826ecbe9fa","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["9bb9a29a5e71a90295f175df8919802993142c9a","2131047ecceac64b54ba70feec3d26bbd7e483d7"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","653128722fb3b4713ac331c621491a93f34a4a22"],"70a4487b07c49a1861c05720e04624826ecbe9fa":["45264aed0cfa8a8a55ae1292b0e336d29cd88401"],"5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf":["f03e4bed5023ec3ef93a771b8888cae991cf448d"],"9bb9a29a5e71a90295f175df8919802993142c9a":["1f09f483a0844bb9dc34fb10380cb053aa96219b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["407687e67faf6e1f02a211ca078d8e3eed631027","5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69"],"45264aed0cfa8a8a55ae1292b0e336d29cd88401":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"47081d784f5fff71bb715c806c824b50901392fb":["b547be573e7e33d9a40568f89ceb5642382c96e2","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"4e6354dd7c71fe122926fc53d7d29f715b1283db":[],"95d12d386a346adadc5b4ee9224494c4107c4e97":["0ad30c6a479e764150a3316e57263319775f1df2","d470c8182e92b264680e34081b75e70a9f2b3c89","fbe8fc0e68a5e2e7acce82ba880a982bd15cfab8","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","3d33e731a93d4b57e662ff094f64f94a745422d4"],"292509a05117b3f385b4b7258087c7c71fa1cc41":["f756f355450b30d33fe4479d81dad3e4d100ded4"],"b547be573e7e33d9a40568f89ceb5642382c96e2":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","ba718737348b631d49101cf505322f868a130196"],"c0cd85fde84cb318b4dc97710dcf15e2959a1bbe":[],"a661533765521d46fcaf46aee272301b6afb6376":["9bb9a29a5e71a90295f175df8919802993142c9a"],"407687e67faf6e1f02a211ca078d8e3eed631027":["4e6354dd7c71fe122926fc53d7d29f715b1283db"],"f756f355450b30d33fe4479d81dad3e4d100ded4":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["727bb765ff2542275f6d31f67be18d7104bae148"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":[],"3394716f52b34ab259ad5247e7595d9f9db6e935":["c33c9c4ca6dc47739595c708779c537e8fb8813d","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe"],"9a70ce9bddc6f985feb8e5e182aebe20872328d4":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"727bb765ff2542275f6d31f67be18d7104bae148":["5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf"],"5e04b732c631a77cbbd25b6ce43c2a8abb1e9e69":["407687e67faf6e1f02a211ca078d8e3eed631027","fc7a7bb1aa79cf53564793bb5ffa270250c679da"],"fc7a7bb1aa79cf53564793bb5ffa270250c679da":["47081d784f5fff71bb715c806c824b50901392fb","4e6354dd7c71fe122926fc53d7d29f715b1283db"],"1f09f483a0844bb9dc34fb10380cb053aa96219b":["292509a05117b3f385b4b7258087c7c71fa1cc41"],"2131047ecceac64b54ba70feec3d26bbd7e483d7":["a661533765521d46fcaf46aee272301b6afb6376"],"bd7962f4da329a4e559727022b752c5cefaee5da":["95d12d386a346adadc5b4ee9224494c4107c4e97"],"22a2e66dfda83847e80095b8693c660742ab3e9c":["9a70ce9bddc6f985feb8e5e182aebe20872328d4"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"5faf65b6692f15cca0f87bf8666c87899afc619f":["bd7962f4da329a4e559727022b752c5cefaee5da"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"86a0a50d2d14aaee1e635bbec914468551f7f9a2":["d85b6e22926e7564c040d2a864f4887f6c59fa92","5acd68c5f07f7ee604c2eeffe801f4a2d7a1a5bf"],"ba718737348b631d49101cf505322f868a130196":["c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","3394716f52b34ab259ad5247e7595d9f9db6e935","52c7e49be259508735752fba88085255014a6ecf"],"52c7e49be259508735752fba88085255014a6ecf":["3394716f52b34ab259ad5247e7595d9f9db6e935"],"653128722fb3b4713ac331c621491a93f34a4a22":["727bb765ff2542275f6d31f67be18d7104bae148","86a0a50d2d14aaee1e635bbec914468551f7f9a2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4e6354dd7c71fe122926fc53d7d29f715b1283db","c0cd85fde84cb318b4dc97710dcf15e2959a1bbe","f03e4bed5023ec3ef93a771b8888cae991cf448d","74f45af4339b0daf7a95c820ab88c1aea74fbce0","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}