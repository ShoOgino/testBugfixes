{"path":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","commits":[{"id":"70f91c8322fbffe3a3a897ef20ea19119cac10cd","date":1386170124,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","pathOld":"solr/contrib/solr-mr/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","sourceNew":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(\"ignoreTikaException\", false);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","sourceOld":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(\"ignoreTikaException\", false);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42d384b06aa87eae925b668b65f3246154f0b0fa","date":1386181725,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","sourceNew":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(ExtractingParams.IGNORE_TIKA_EXCEPTION, false);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","sourceOld":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(\"ignoreTikaException\", false);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d1806dbd0764a9ee0958f91e02dc8629b7a0ac02","date":1386199730,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","sourceNew":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","sourceOld":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    jobConf.setBoolean(ExtractingParams.IGNORE_TIKA_EXCEPTION, false);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74f45af4339b0daf7a95c820ab88c1aea74fbce0","date":1387475327,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2b7f6e3ddc2af1fbb25d2161324367db5e977c9f","date":1388973780,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","sourceNew":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","sourceOld":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9d3d365762604952e436b51980101dfc84cc1b3e","date":1396298116,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","sourceNew":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, StandardCharsets.UTF_8);\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","sourceOld":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","sourceNew":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, StandardCharsets.UTF_8);\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","sourceOld":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, \"UTF-8\");\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"12109b652e9210b8d58fca47f6c4a725d058a58e","date":1490373076,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, StandardCharsets.UTF_8);\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe1c4aa9af769a38e878f608070f672efbeac27f","date":1490594650,"type":4,"author":"Steve Rowe","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/contrib/map-reduce/src/test/org/apache/solr/hadoop/MorphlineBasicMiniMRTest#mrRun().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void mrRun() throws Exception {\n    FileSystem fs = dfsCluster.getFileSystem();\n    Path inDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/input\"));\n    fs.delete(inDir, true);\n    String DATADIR = \"/user/testing/testMapperReducer/data\";\n    Path dataDir = fs.makeQualified(new Path(DATADIR));\n    fs.delete(dataDir, true);\n    Path outDir = fs.makeQualified(new Path(\"/user/testing/testMapperReducer/output\"));\n    fs.delete(outDir, true);\n\n    assertTrue(fs.mkdirs(inDir));\n    Path INPATH = new Path(inDir, \"input.txt\");\n    OutputStream os = fs.create(INPATH);\n    Writer wr = new OutputStreamWriter(os, StandardCharsets.UTF_8);\n    wr.write(DATADIR + \"/\" + inputAvroFile);\n    wr.close();\n\n    assertTrue(fs.mkdirs(dataDir));\n    fs.copyFromLocalFile(new Path(DOCUMENTS_DIR, inputAvroFile), dataDir);\n    \n    JobConf jobConf = getJobConf();\n    jobConf.set(\"jobclient.output.filter\", \"ALL\");\n    if (ENABLE_LOCAL_JOB_RUNNER) { // enable Hadoop LocalJobRunner; this enables to run in debugger and set breakpoints\n      jobConf.set(\"mapred.job.tracker\", \"local\");\n    }\n    jobConf.setMaxMapAttempts(1);\n    jobConf.setMaxReduceAttempts(1);\n    jobConf.setJar(SEARCH_ARCHIVES_JAR);\n    \n    int shards = 2;\n    int maxReducers = Integer.MAX_VALUE;\n    if (ENABLE_LOCAL_JOB_RUNNER) {\n      // local job runner has a couple of limitations: only one reducer is supported and the DistributedCache doesn't work.\n      // see http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/\n      maxReducers = 1;\n      shards = 1;\n    }\n    \n    String[] args = new String[] {\n        \"--morphline-file=\" + tempDir + \"/test-morphlines/solrCellDocumentTypes.conf\",\n        \"--morphline-id=morphline1\",\n        \"--solr-home-dir=\" + MINIMR_CONF_DIR.getAbsolutePath(),\n        \"--output-dir=\" + outDir.toString(),\n        \"--shards=\" + shards,\n        \"--verbose\",\n        numRuns % 2 == 0 ? \"--input-list=\" + INPATH.toString() : dataDir.toString(),\n        numRuns % 3 == 0 ? \"--reducers=\" + shards : (numRuns % 3 == 1  ? \"--reducers=-1\" : \"--reducers=\" + Math.min(8, maxReducers))\n    };\n    if (numRuns % 3 == 2) {\n      args = concat(args, new String[] {\"--fanout=2\"});\n    }\n    if (numRuns == 0) {\n      // force (slow) MapReduce based randomization to get coverage for that as well\n      args = concat(new String[] {\"-D\", MapReduceIndexerTool.MAIN_MEMORY_RANDOMIZATION_THRESHOLD + \"=-1\"}, args); \n    }\n    MapReduceIndexerTool tool = createTool();\n    int res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n    Job job = tool.job;\n    assertTrue(job.isComplete());\n    assertTrue(job.isSuccessful());\n\n    if (numRuns % 3 != 2) {\n      // Only run this check if mtree merge is disabled.\n      // With mtree merge enabled the BatchWriter counters aren't available anymore because \n      // variable \"job\" now refers to the merge job rather than the indexing job\n      assertEquals(\"Invalid counter \" + SolrRecordWriter.class.getName() + \".\" + SolrCounters.DOCUMENTS_WRITTEN,\n          count, job.getCounters().findCounter(SolrCounters.class.getName(), SolrCounters.DOCUMENTS_WRITTEN.toString()).getValue());\n    }\n    \n    // Check the output is as expected\n    outDir = new Path(outDir, MapReduceIndexerTool.RESULTS_DIR);\n    Path[] outputFiles = FileUtil.stat2Paths(fs.listStatus(outDir));\n\n    System.out.println(\"outputfiles:\" + Arrays.toString(outputFiles));\n\n    UtilsForTests.validateSolrServerDocumentCount(MINIMR_CONF_DIR, fs, outDir, count, shards);\n    \n    // run again with --dryrun mode:  \n    tool = createTool();\n    args = concat(args, new String[] {\"--dry-run\"});\n    res = ToolRunner.run(jobConf, tool, args);\n    assertEquals(0, res);\n\n    numRuns++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5eb2511ababf862ea11e10761c70ee560cd84510":["2b7f6e3ddc2af1fbb25d2161324367db5e977c9f","9d3d365762604952e436b51980101dfc84cc1b3e"],"12109b652e9210b8d58fca47f6c4a725d058a58e":["9d3d365762604952e436b51980101dfc84cc1b3e"],"d1806dbd0764a9ee0958f91e02dc8629b7a0ac02":["42d384b06aa87eae925b668b65f3246154f0b0fa"],"9d3d365762604952e436b51980101dfc84cc1b3e":["2b7f6e3ddc2af1fbb25d2161324367db5e977c9f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d1806dbd0764a9ee0958f91e02dc8629b7a0ac02"],"2b7f6e3ddc2af1fbb25d2161324367db5e977c9f":["d1806dbd0764a9ee0958f91e02dc8629b7a0ac02"],"fe1c4aa9af769a38e878f608070f672efbeac27f":["9d3d365762604952e436b51980101dfc84cc1b3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["12109b652e9210b8d58fca47f6c4a725d058a58e"],"42d384b06aa87eae925b668b65f3246154f0b0fa":["70f91c8322fbffe3a3a897ef20ea19119cac10cd"]},"commit2Childs":{"70f91c8322fbffe3a3a897ef20ea19119cac10cd":["42d384b06aa87eae925b668b65f3246154f0b0fa"],"5eb2511ababf862ea11e10761c70ee560cd84510":[],"12109b652e9210b8d58fca47f6c4a725d058a58e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d1806dbd0764a9ee0958f91e02dc8629b7a0ac02":["74f45af4339b0daf7a95c820ab88c1aea74fbce0","2b7f6e3ddc2af1fbb25d2161324367db5e977c9f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["70f91c8322fbffe3a3a897ef20ea19119cac10cd","74f45af4339b0daf7a95c820ab88c1aea74fbce0"],"9d3d365762604952e436b51980101dfc84cc1b3e":["5eb2511ababf862ea11e10761c70ee560cd84510","12109b652e9210b8d58fca47f6c4a725d058a58e","fe1c4aa9af769a38e878f608070f672efbeac27f"],"74f45af4339b0daf7a95c820ab88c1aea74fbce0":[],"2b7f6e3ddc2af1fbb25d2161324367db5e977c9f":["5eb2511ababf862ea11e10761c70ee560cd84510","9d3d365762604952e436b51980101dfc84cc1b3e"],"fe1c4aa9af769a38e878f608070f672efbeac27f":[],"42d384b06aa87eae925b668b65f3246154f0b0fa":["d1806dbd0764a9ee0958f91e02dc8629b7a0ac02"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5eb2511ababf862ea11e10761c70ee560cd84510","74f45af4339b0daf7a95c820ab88c1aea74fbce0","fe1c4aa9af769a38e878f608070f672efbeac27f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}